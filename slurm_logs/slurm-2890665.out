Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
A conda environment has been detected CONDA_PREFIX=
/u/bzd2/miniconda/envs/hetarth_py10 
anaconda3_gpu is loaded. Consider running conda deactivate and reloading it.
Lmod has detected the following error: The following module(s) are unknown:
"python"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore_cache load "python"

Also make sure that all modulefiles written in TCL start with the string
#%Module




Currently Loaded Modules:
  1) gcc/11.4.0      3) cuda/11.8.0         5) slurm-env/0.1
  2) openmpi/4.1.6   4) cue-login-env/1.0   6) default-s11

 

job is starting on gpub032.delta.ncsa.illinois.edu
Retrieving notices: ...working... done
WARNING: A conda environment already exists at '/u/bzd2/miniconda/envs/hetarth_py10'
Remove existing environment (y/[n])? 

CondaSystemExit: Exiting.

usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...
conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'content-trust', 'doctor', 'repoquery', 'env')
Starting script...
Output: Requirement already satisfied: torch in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (2.1.1)
Requirement already satisfied: torchvision in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (0.16.1)
Requirement already satisfied: torchaudio in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (2.1.1)
Requirement already satisfied: filelock in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (3.13.1)
Requirement already satisfied: typing-extensions in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (4.9.0)
Requirement already satisfied: sympy in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (1.12)
Requirement already satisfied: networkx in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (3.2.1)
Requirement already satisfied: jinja2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (2023.10.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (2.18.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (2.1.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)
Requirement already satisfied: numpy in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torchvision) (1.26.2)
Requirement already satisfied: requests in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torchvision) (2.31.0)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torchvision) (10.1.0)
Requirement already satisfied: MarkupSafe>=2.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->torchvision) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->torchvision) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)
Requirement already satisfied: mpmath>=0.19 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)

Output: Requirement already satisfied: transformers in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (4.36.0.dev0)
Requirement already satisfied: filelock in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (3.13.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (1.26.2)
Requirement already satisfied: packaging>=20.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (23.2)
Requirement already satisfied: pyyaml>=5.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (2023.10.3)
Requirement already satisfied: requests in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (0.4.1)
Requirement already satisfied: tqdm>=4.27 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (4.66.1)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)

Output: Collecting git+https://github.com/huggingface/peft.git
  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-08waco18
  Resolved https://github.com/huggingface/peft.git to commit 1c1c7fdaa6e6abaa53939b865dee1eded82ad032
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (1.26.2)
Requirement already satisfied: packaging>=20.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (23.2)
Requirement already satisfied: psutil in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (5.9.6)
Requirement already satisfied: pyyaml in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (6.0.1)
Requirement already satisfied: torch>=1.13.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (2.1.1)
Requirement already satisfied: transformers in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (4.36.0.dev0)
Requirement already satisfied: tqdm in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (4.66.1)
Requirement already satisfied: accelerate>=0.21.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.25.0)
Requirement already satisfied: safetensors in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.4.1)
Requirement already satisfied: huggingface-hub>=0.17.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.19.4)
Requirement already satisfied: filelock in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.13.1)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.10.0)
Requirement already satisfied: requests in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.9.0)
Requirement already satisfied: sympy in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.12)
Requirement already satisfied: networkx in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.2.1)
Requirement already satisfied: jinja2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.2)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.18.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.7.2.dev0) (12.3.101)
Requirement already satisfied: regex!=2019.12.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers->peft==0.7.2.dev0) (2023.10.3)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers->peft==0.7.2.dev0) (0.15.0)
Requirement already satisfied: MarkupSafe>=2.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.11.17)
Requirement already satisfied: mpmath>=0.19 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)
Building wheels for collected packages: peft
  Building wheel for peft (pyproject.toml): started
  Building wheel for peft (pyproject.toml): finished with status 'done'
  Created wheel for peft: filename=peft-0.7.2.dev0-py3-none-any.whl size=183141 sha256=bff13cc82e8274096fe4146dbe9b0a4488c32059275995a3b2624182e1ffee8b
  Stored in directory: /tmp/pip-ephem-wheel-cache-6ggsr6k8/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087
Successfully built peft
Installing collected packages: peft
  Attempting uninstall: peft
    Found existing installation: peft 0.7.1.dev0
    Uninstalling peft-0.7.1.dev0:
      Successfully uninstalled peft-0.7.1.dev0
Successfully installed peft-0.7.2.dev0

Output: Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-nidfc0vx
  Resolved https://github.com/huggingface/transformers to commit f40b87de0ca234df61f76928956c4a2118c0b548
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (3.13.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (1.26.2)
Requirement already satisfied: packaging>=20.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (23.2)
Requirement already satisfied: pyyaml>=5.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (2023.10.3)
Requirement already satisfied: requests in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (0.15.0)
Requirement already satisfied: safetensors>=0.4.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (0.4.1)
Requirement already satisfied: tqdm>=4.27 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.38.0.dev0) (4.66.1)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.10.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.9.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers==4.38.0.dev0) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers==4.38.0.dev0) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers==4.38.0.dev0) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers==4.38.0.dev0) (2023.11.17)
Building wheels for collected packages: transformers
  Building wheel for transformers (pyproject.toml): started
  Building wheel for transformers (pyproject.toml): finished with status 'done'
  Created wheel for transformers: filename=transformers-4.38.0.dev0-py3-none-any.whl size=8408548 sha256=70b6d809a73cf1335244d2c89c26871c138044622a0ae9d8580f789c29e477a0
  Stored in directory: /tmp/pip-ephem-wheel-cache-7475k1i9/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b
Successfully built transformers
Installing collected packages: transformers
  Attempting uninstall: transformers
    Found existing installation: transformers 4.36.0.dev0
    Uninstalling transformers-4.36.0.dev0:
      Successfully uninstalled transformers-4.36.0.dev0
Successfully installed transformers-4.38.0.dev0

Output: Requirement already satisfied: datasets in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (2.15.0)
Requirement already satisfied: numpy>=1.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (1.26.2)
Requirement already satisfied: pyarrow>=8.0.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (14.0.1)
Requirement already satisfied: pyarrow-hotfix in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (0.6)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (0.3.7)
Requirement already satisfied: pandas in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (2.1.4)
Requirement already satisfied: requests>=2.19.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (4.66.1)
Requirement already satisfied: xxhash in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (3.4.1)
Requirement already satisfied: multiprocess in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (0.70.15)
Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)
Requirement already satisfied: aiohttp in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (3.9.1)
Requirement already satisfied: huggingface-hub>=0.18.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (0.19.4)
Requirement already satisfied: packaging in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (23.2)
Requirement already satisfied: pyyaml>=5.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (6.0.1)
Requirement already satisfied: attrs>=17.3.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)
Requirement already satisfied: yarl<2.0,>=1.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)
Requirement already satisfied: frozenlist>=1.1.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)
Requirement already satisfied: aiosignal>=1.1.2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)
Requirement already satisfied: async-timeout<5.0,>=4.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)
Requirement already satisfied: filelock in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.9.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)
Requirement already satisfied: python-dateutil>=2.8.2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas->datasets) (2023.3)
Requirement already satisfied: six>=1.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)

Output: Requirement already satisfied: accelerate in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (0.25.0)
Requirement already satisfied: numpy>=1.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (1.26.2)
Requirement already satisfied: packaging>=20.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (23.2)
Requirement already satisfied: psutil in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (5.9.6)
Requirement already satisfied: pyyaml in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (6.0.1)
Requirement already satisfied: torch>=1.10.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (2.1.1)
Requirement already satisfied: huggingface-hub in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (0.19.4)
Requirement already satisfied: safetensors>=0.3.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (0.4.1)
Requirement already satisfied: filelock in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)
Requirement already satisfied: typing-extensions in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)
Requirement already satisfied: sympy in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)
Requirement already satisfied: networkx in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)
Requirement already satisfied: jinja2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.18.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.1.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)
Requirement already satisfied: requests in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)
Requirement already satisfied: MarkupSafe>=2.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)
Requirement already satisfied: mpmath>=0.19 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)

Output: Requirement already satisfied: huggingface_hub in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (0.19.4)
Requirement already satisfied: filelock in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)
Requirement already satisfied: requests in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)
Requirement already satisfied: pyyaml>=5.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)
Requirement already satisfied: packaging>=20.9 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (23.2)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface_hub) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.11.17)

Output: Requirement already satisfied: bitsandbytes in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (0.41.3.post2)

Output: Requirement already satisfied: wandb in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (0.16.1)
Requirement already satisfied: Click!=8.0.0,>=7.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (8.1.7)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (3.1.40)
Requirement already satisfied: requests<3,>=2.0.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (2.31.0)
Requirement already satisfied: psutil>=5.0.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (5.9.6)
Requirement already satisfied: sentry-sdk>=1.0.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (1.38.0)
Requirement already satisfied: docker-pycreds>=0.4.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (0.4.0)
Requirement already satisfied: PyYAML in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (6.0.1)
Requirement already satisfied: setproctitle in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (1.3.3)
Requirement already satisfied: setuptools in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (68.0.0)
Requirement already satisfied: appdirs>=1.4.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (1.4.4)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (4.25.1)
Requirement already satisfied: six>=1.4.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)
Requirement already satisfied: smmap<6,>=3.0.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)

Output: Requirement already satisfied: scikit-learn in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (1.3.2)
Requirement already satisfied: numpy<2.0,>=1.17.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from scikit-learn) (1.26.2)
Requirement already satisfied: scipy>=1.5.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from scikit-learn) (1.11.4)
Requirement already satisfied: joblib>=1.1.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from scikit-learn) (1.3.2)
Requirement already satisfied: threadpoolctl>=2.0.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from scikit-learn) (3.2.0)

Output: Requirement already satisfied: code_bert_score in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (0.4.1)
Requirement already satisfied: torch>=1.0.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (2.1.1)
Requirement already satisfied: numpy in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (1.26.2)
Requirement already satisfied: pandas>=1.0.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (2.1.4)
Requirement already satisfied: requests in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (2.31.0)
Requirement already satisfied: tqdm>=4.31.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (4.66.1)
Requirement already satisfied: matplotlib in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (3.8.2)
Requirement already satisfied: transformers>=3.0.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (4.38.0.dev0)
Requirement already satisfied: python-dateutil>=2.8.2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas>=1.0.1->code_bert_score) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3)
Requirement already satisfied: filelock in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (3.13.1)
Requirement already satisfied: typing-extensions in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (4.9.0)
Requirement already satisfied: sympy in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (1.12)
Requirement already satisfied: networkx in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (3.2.1)
Requirement already satisfied: jinja2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (2023.10.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (2.18.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (2.1.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->code_bert_score) (12.3.101)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (0.19.4)
Requirement already satisfied: packaging>=20.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (23.2)
Requirement already satisfied: pyyaml>=5.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (2023.10.3)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (0.15.0)
Requirement already satisfied: safetensors>=0.4.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (0.4.1)
Requirement already satisfied: contourpy>=1.0.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (1.2.0)
Requirement already satisfied: cycler>=0.10 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (0.12.1)
Requirement already satisfied: fonttools>=4.22.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (4.46.0)
Requirement already satisfied: kiwisolver>=1.3.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (1.4.5)
Requirement already satisfied: pillow>=8 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (10.1.0)
Requirement already satisfied: pyparsing>=2.3.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (3.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->code_bert_score) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->code_bert_score) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->code_bert_score) (2.1.0)
Requirement already satisfied: certifi>=2017.4.17 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->code_bert_score) (2023.11.17)
Requirement already satisfied: six>=1.5 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->code_bert_score) (1.16.0)
Requirement already satisfied: MarkupSafe>=2.0 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->code_bert_score) (2.1.3)
Requirement already satisfied: mpmath>=0.19 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from sympy->torch>=1.0.0->code_bert_score) (1.3.0)

Output: Requirement already satisfied: nltk in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (3.8.1)
Requirement already satisfied: click in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from nltk) (8.1.7)
Requirement already satisfied: joblib in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from nltk) (1.3.2)
Requirement already satisfied: regex>=2021.8.3 in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from nltk) (2023.10.3)
Requirement already satisfied: tqdm in /u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages (from nltk) (4.66.1)

WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(
tokenizer_config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]tokenizer_config.json: 100%|██████████| 677/677 [00:00<00:00, 5.41MB/s]
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.78 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.78 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(
vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 9.62MB/s]
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.44 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.44 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(
merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 9.51MB/s]
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 2.06 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 2.06 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(
tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 23.6MB/s]
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/datasets/load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/datasets/load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/datasets/load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/datasets/load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'austincloudguru', 'repo_name': 'splunk_forwarder', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'download_count': '8962', 'input': 'name: Set logfile permissions', 'license': 'MIT', 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'buluma', 'repo_name': 'keepalived', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'download_count': '1012', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'azavea', 'repo_name': 'zookeeper', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'download_count': '3388', 'input': 'name: Pin Cloudera APT repositories', 'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'org_name': 'panchal_yash', 'repo_name': 'percona_xtradb_cluster_role', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'download_count': '647', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'license': '', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml', 'input': 'name: Set logfile permissions', 'repo_name': 'splunk_forwarder', 'org_name': 'austincloudguru', 'license': 'MIT', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'download_count': '8962'}{'input': 'name: Set logfile permissions', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'org_name': 'austincloudguru', 'repo_name': 'splunk_forwarder', 'license': 'MIT', 'download_count': '8962', 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml'}

{'org_name': 'robertdebock', 'repo_name': 'logwatch', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'download_count': '664', 'input': 'name: assert | Test logwatch_service', 'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'org_name': 'community', 'repo_name': 'network', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'download_link': 'https://old-galaxy.ansible.com/community/network', 'download_count': '723055', 'input': 'name: Test BGP - graceful-restart-helper', 'license': '', 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml'}{'path': 'data/repos/buluma/keepalived/tasks/assert.yml', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'repo_name': 'keepalived', 'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'download_count': '1012'}{'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'org_name': 'buluma', 'repo_name': 'keepalived', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '1012', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'org_name': 'bertvv', 'repo_name': 'tftp', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'download_count': '9882', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'license': '', 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml'}{'input': 'name: Pin Cloudera APT repositories', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'org_name': 'azavea', 'repo_name': 'zookeeper', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '3388', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml'}{'path': 'data/repos/azavea/zookeeper/tasks/main.yml', 'input': 'name: Pin Cloudera APT repositories', 'repo_name': 'zookeeper', 'org_name': 'azavea', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'download_count': '3388'}



<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'org_name': 'austincloudguru', 'license': 'MIT', 'input': 'name: Set logfile permissions', 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'repo_name': 'splunk_forwarder', 'download_count': '8962'}{'input': 'name: disable the mysql module on RHEL/CentOS 8', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'org_name': 'panchal_yash', 'repo_name': 'percona_xtradb_cluster_role', 'license': '', 'download_count': '647', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml'}
{'org_name': 'amritsingh', 'repo_name': 'ec2_monitor', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'download_count': '8094', 'input': 'name: Install apt packages', 'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml'}{'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'repo_name': 'percona_xtradb_cluster_role', 'org_name': 'panchal_yash', 'license': '', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'download_count': '647'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>



{'org_name': 'rhythmictech', 'repo_name': 'ansible_role_efs_utils', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'download_count': '1799', 'input': 'name: run Debian build script', 'license': 'MIT', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml'}{'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'repo_name': 'keepalived', 'download_count': '1012'}{'input': 'name: assert | Test logwatch_service', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'org_name': 'robertdebock', 'repo_name': 'logwatch', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '664', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml'}{'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml', 'input': 'name: assert | Test logwatch_service', 'repo_name': 'logwatch', 'org_name': 'robertdebock', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'download_count': '664'}



<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'org_name': 'buluma', 'repo_name': 'centos_base', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'download_count': '12437', 'input': 'name: Vim alias in .bashrc', 'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/buluma/centos_base/tasks/main.yml'}{'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml', 'input': 'name: Test BGP - graceful-restart-helper', 'repo_name': 'network', 'org_name': 'community', 'license': '', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'download_link': 'https://old-galaxy.ansible.com/community/network', 'download_count': '723055'}{'org_name': 'azavea', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: Pin Cloudera APT repositories', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'repo_name': 'zookeeper', 'download_count': '3388'}
{'input': 'name: Test BGP - graceful-restart-helper', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'org_name': 'community', 'repo_name': 'network', 'license': '', 'download_count': '723055', 'download_link': 'https://old-galaxy.ansible.com/community/network', 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'org_name': 'panchal_yash', 'license': '', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'repo_name': 'percona_xtradb_cluster_role', 'download_count': '647'}
{'path': 'data/repos/bertvv/tftp/tasks/selinux.yml', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'repo_name': 'tftp', 'org_name': 'bertvv', 'license': '', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'download_count': '9882'}{'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'org_name': 'bertvv', 'repo_name': 'tftp', 'license': '', 'download_count': '9882', 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'org_name': 'robertdebock', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: assert | Test logwatch_service', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'repo_name': 'logwatch', 'download_count': '664'}
{'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml', 'input': 'name: Install apt packages', 'repo_name': 'ec2_monitor', 'org_name': 'amritsingh', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'download_count': '8094'}{'input': 'name: Install apt packages', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'org_name': 'amritsingh', 'repo_name': 'ec2_monitor', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '8094', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'community', 'license': '', 'input': 'name: Test BGP - graceful-restart-helper', 'download_link': 'https://old-galaxy.ansible.com/community/network', 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'repo_name': 'network', 'download_count': '723055'}{'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml', 'input': 'name: run Debian build script', 'repo_name': 'ansible_role_efs_utils', 'org_name': 'rhythmictech', 'license': 'MIT', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'download_count': '1799'}
{'input': 'name: run Debian build script', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'org_name': 'rhythmictech', 'repo_name': 'ansible_role_efs_utils', 'license': 'MIT', 'download_count': '1799', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'path': 'data/repos/buluma/centos_base/tasks/main.yml', 'input': 'name: Vim alias in .bashrc', 'repo_name': 'centos_base', 'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'download_count': '12437'}{'input': 'name: Vim alias in .bashrc', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'org_name': 'buluma', 'repo_name': 'centos_base', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '12437', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'path': 'data/repos/buluma/centos_base/tasks/main.yml'}{'org_name': 'bertvv', 'license': '', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'repo_name': 'tftp', 'download_count': '9882'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'amritsingh', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: Install apt packages', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'repo_name': 'ec2_monitor', 'download_count': '8094'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'rhythmictech', 'license': 'MIT', 'input': 'name: run Debian build script', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'repo_name': 'ansible_role_efs_utils', 'download_count': '1799'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: Vim alias in .bashrc', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'path': 'data/repos/buluma/centos_base/tasks/main.yml', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'repo_name': 'centos_base', 'download_count': '12437'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'igor_nikiforov', 'repo_name': 'etcd', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'download_count': '1748631', 'input': 'name: Create etcd systemd service', 'license': 'MIT', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml', 'input': 'name: Create etcd systemd service', 'repo_name': 'etcd', 'org_name': 'igor_nikiforov', 'license': 'MIT', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'download_count': '1748631'}
{'input': 'name: Create etcd systemd service', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'org_name': 'igor_nikiforov', 'repo_name': 'etcd', 'license': 'MIT', 'download_count': '1748631', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'igor_nikiforov', 'license': 'MIT', 'input': 'name: Create etcd systemd service', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'repo_name': 'etcd', 'download_count': '1748631'}
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<03:34,  1.86it/s]  0%|          | 1/400 [00:00<03:34,  1.86it/s]  0%|          | 1/400 [00:00<03:34,  1.86it/s]  0%|          | 1/400 [00:00<03:34,  1.86it/s]100%|██████████| 400/400 [00:00<00:00, 663.88it/s]
100%|██████████| 400/400 [00:00<00:00, 662.04it/s]
100%|██████████| 400/400 [00:00<00:00, 661.60it/s]
The character to token ratio of the dataset is: 3.14
Loading the model
100%|██████████| 400/400 [00:00<00:00, 657.20it/s]
The character to token ratio of the dataset is: 3.14
Loading the model
The character to token ratio of the dataset is: 3.14
Loading the model
The character to token ratio of the dataset is: 3.14
Loading the model
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(
config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]config.json: 100%|██████████| 1.04k/1.04k [00:00<00:00, 9.36MB/s]
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.04 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.04 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(
pytorch_model.bin.index.json:   0%|          | 0.00/38.1k [00:00<?, ?B/s]pytorch_model.bin.index.json: 100%|██████████| 38.1k/38.1k [00:00<00:00, 251MB/s]
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 9859.13 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 9859.13 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(

pytorch_model-00001-of-00004.bin:   0%|          | 0.00/9.86G [00:00<?, ?B/s][A
pytorch_model-00001-of-00004.bin:   1%|          | 52.4M/9.86G [00:00<00:25, 388MB/s][A
pytorch_model-00001-of-00004.bin:   1%|▏         | 136M/9.86G [00:00<00:16, 580MB/s] [A
pytorch_model-00001-of-00004.bin:   2%|▏         | 210M/9.86G [00:00<00:15, 625MB/s][A
pytorch_model-00001-of-00004.bin:   3%|▎         | 283M/9.86G [00:00<00:19, 501MB/s][A
pytorch_model-00001-of-00004.bin:   4%|▎         | 346M/9.86G [00:00<00:20, 474MB/s][A
pytorch_model-00001-of-00004.bin:   4%|▍         | 419M/9.86G [00:00<00:18, 522MB/s][A
pytorch_model-00001-of-00004.bin:   5%|▍         | 493M/9.86G [00:00<00:16, 563MB/s][A
pytorch_model-00001-of-00004.bin:   6%|▌         | 556M/9.86G [00:01<00:16, 580MB/s][A
pytorch_model-00001-of-00004.bin:   6%|▋         | 619M/9.86G [00:01<00:15, 587MB/s][A
pytorch_model-00001-of-00004.bin:   7%|▋         | 692M/9.86G [00:01<00:15, 604MB/s][A
pytorch_model-00001-of-00004.bin:   8%|▊         | 755M/9.86G [00:01<00:14, 607MB/s][A
pytorch_model-00001-of-00004.bin:   8%|▊         | 818M/9.86G [00:01<00:15, 593MB/s][A
pytorch_model-00001-of-00004.bin:   9%|▉         | 881M/9.86G [00:01<00:15, 595MB/s][A
pytorch_model-00001-of-00004.bin:  10%|▉         | 954M/9.86G [00:01<00:14, 621MB/s][A
pytorch_model-00001-of-00004.bin:  10%|█         | 1.03G/9.86G [00:01<00:13, 637MB/s][A
pytorch_model-00001-of-00004.bin:  11%|█         | 1.10G/9.86G [00:01<00:13, 656MB/s][A
pytorch_model-00001-of-00004.bin:  12%|█▏        | 1.17G/9.86G [00:01<00:13, 664MB/s][A
pytorch_model-00001-of-00004.bin:  13%|█▎        | 1.25G/9.86G [00:02<00:12, 664MB/s][A
pytorch_model-00001-of-00004.bin:  13%|█▎        | 1.32G/9.86G [00:02<00:12, 658MB/s][A
pytorch_model-00001-of-00004.bin:  14%|█▍        | 1.39G/9.86G [00:02<00:12, 663MB/s][A
pytorch_model-00001-of-00004.bin:  15%|█▍        | 1.47G/9.86G [00:02<00:12, 660MB/s][A
pytorch_model-00001-of-00004.bin:  16%|█▌        | 1.54G/9.86G [00:02<00:12, 646MB/s][A
pytorch_model-00001-of-00004.bin:  16%|█▋        | 1.61G/9.86G [00:02<00:12, 641MB/s][A
pytorch_model-00001-of-00004.bin:  17%|█▋        | 1.69G/9.86G [00:02<00:12, 647MB/s][A
pytorch_model-00001-of-00004.bin:  18%|█▊        | 1.76G/9.86G [00:02<00:12, 656MB/s][A
pytorch_model-00001-of-00004.bin:  19%|█▊        | 1.84G/9.86G [00:02<00:12, 660MB/s][A
pytorch_model-00001-of-00004.bin:  19%|█▉        | 1.91G/9.86G [00:03<00:11, 665MB/s][A
pytorch_model-00001-of-00004.bin:  20%|██        | 1.98G/9.86G [00:03<00:12, 653MB/s][A
pytorch_model-00001-of-00004.bin:  21%|██        | 2.06G/9.86G [00:03<00:12, 648MB/s][A
pytorch_model-00001-of-00004.bin:  22%|██▏       | 2.13G/9.86G [00:03<00:12, 633MB/s][A
pytorch_model-00001-of-00004.bin:  22%|██▏       | 2.20G/9.86G [00:03<00:12, 627MB/s][A
pytorch_model-00001-of-00004.bin:  23%|██▎       | 2.28G/9.86G [00:03<00:11, 633MB/s][A
pytorch_model-00001-of-00004.bin:  24%|██▍       | 2.35G/9.86G [00:03<00:11, 641MB/s][A
pytorch_model-00001-of-00004.bin:  25%|██▍       | 2.42G/9.86G [00:03<00:11, 637MB/s][A
pytorch_model-00001-of-00004.bin:  25%|██▌       | 2.50G/9.86G [00:04<00:11, 634MB/s][A
pytorch_model-00001-of-00004.bin:  26%|██▌       | 2.57G/9.86G [00:04<00:11, 629MB/s][A
pytorch_model-00001-of-00004.bin:  27%|██▋       | 2.64G/9.86G [00:04<00:11, 623MB/s][A
pytorch_model-00001-of-00004.bin:  27%|██▋       | 2.71G/9.86G [00:04<00:11, 617MB/s][A
pytorch_model-00001-of-00004.bin:  28%|██▊       | 2.78G/9.86G [00:04<00:11, 624MB/s][A
pytorch_model-00001-of-00004.bin:  29%|██▉       | 2.84G/9.86G [00:04<00:11, 622MB/s][A
pytorch_model-00001-of-00004.bin:  29%|██▉       | 2.90G/9.86G [00:04<00:11, 619MB/s][A
pytorch_model-00001-of-00004.bin:  30%|███       | 2.97G/9.86G [00:04<00:11, 613MB/s][A
pytorch_model-00001-of-00004.bin:  31%|███       | 3.03G/9.86G [00:04<00:11, 611MB/s][A
pytorch_model-00001-of-00004.bin:  31%|███▏      | 3.10G/9.86G [00:05<00:10, 624MB/s][A
pytorch_model-00001-of-00004.bin:  32%|███▏      | 3.17G/9.86G [00:05<00:10, 625MB/s][A
pytorch_model-00001-of-00004.bin:  33%|███▎      | 3.24G/9.86G [00:05<00:10, 637MB/s][A
pytorch_model-00001-of-00004.bin:  34%|███▎      | 3.31G/9.86G [00:05<00:10, 649MB/s][A
pytorch_model-00001-of-00004.bin:  34%|███▍      | 3.39G/9.86G [00:05<00:10, 644MB/s][A
pytorch_model-00001-of-00004.bin:  35%|███▌      | 3.46G/9.86G [00:05<00:10, 640MB/s][A
pytorch_model-00001-of-00004.bin:  36%|███▌      | 3.53G/9.86G [00:05<00:10, 623MB/s][A
pytorch_model-00001-of-00004.bin:  36%|███▋      | 3.60G/9.86G [00:05<00:10, 615MB/s][A
pytorch_model-00001-of-00004.bin:  37%|███▋      | 3.67G/9.86G [00:05<00:09, 627MB/s][A
pytorch_model-00001-of-00004.bin:  38%|███▊      | 3.74G/9.86G [00:06<00:09, 648MB/s][A
pytorch_model-00001-of-00004.bin:  39%|███▊      | 3.82G/9.86G [00:06<00:09, 647MB/s][A
pytorch_model-00001-of-00004.bin:  39%|███▉      | 3.89G/9.86G [00:06<00:09, 647MB/s][A
pytorch_model-00001-of-00004.bin:  40%|████      | 3.96G/9.86G [00:06<00:09, 641MB/s][A
pytorch_model-00001-of-00004.bin:  41%|████      | 4.04G/9.86G [00:06<00:09, 639MB/s][A
pytorch_model-00001-of-00004.bin:  42%|████▏     | 4.11G/9.86G [00:06<00:09, 632MB/s][A
pytorch_model-00001-of-00004.bin:  42%|████▏     | 4.18G/9.86G [00:06<00:08, 631MB/s][A
pytorch_model-00001-of-00004.bin:  43%|████▎     | 4.26G/9.86G [00:06<00:08, 628MB/s][A
pytorch_model-00001-of-00004.bin:  44%|████▍     | 4.32G/9.86G [00:07<00:23, 237MB/s][A
pytorch_model-00001-of-00004.bin:  44%|████▍     | 4.37G/9.86G [00:07<00:26, 211MB/s][A
pytorch_model-00001-of-00004.bin:  45%|████▍     | 4.44G/9.86G [00:07<00:20, 260MB/s][A
pytorch_model-00001-of-00004.bin:  46%|████▌     | 4.51G/9.86G [00:08<00:16, 327MB/s][A
pytorch_model-00001-of-00004.bin:  46%|████▋     | 4.58G/9.86G [00:08<00:13, 394MB/s][A
pytorch_model-00001-of-00004.bin:  47%|████▋     | 4.66G/9.86G [00:08<00:11, 452MB/s][A
pytorch_model-00001-of-00004.bin:  48%|████▊     | 4.73G/9.86G [00:08<00:10, 497MB/s][A
pytorch_model-00001-of-00004.bin:  49%|████▊     | 4.80G/9.86G [00:08<00:09, 530MB/s][A
pytorch_model-00001-of-00004.bin:  49%|████▉     | 4.87G/9.86G [00:08<00:09, 548MB/s][A
pytorch_model-00001-of-00004.bin:  50%|█████     | 4.94G/9.86G [00:08<00:08, 570MB/s][A
pytorch_model-00001-of-00004.bin:  51%|█████     | 5.00G/9.86G [00:08<00:08, 578MB/s][A
pytorch_model-00001-of-00004.bin:  51%|█████▏    | 5.08G/9.86G [00:08<00:07, 601MB/s][A
pytorch_model-00001-of-00004.bin:  52%|█████▏    | 5.15G/9.86G [00:09<00:07, 614MB/s][A
pytorch_model-00001-of-00004.bin:  53%|█████▎    | 5.22G/9.86G [00:09<00:07, 615MB/s][A
pytorch_model-00001-of-00004.bin:  54%|█████▎    | 5.30G/9.86G [00:09<00:07, 629MB/s][A
pytorch_model-00001-of-00004.bin:  54%|█████▍    | 5.37G/9.86G [00:09<00:07, 639MB/s][A
pytorch_model-00001-of-00004.bin:  55%|█████▌    | 5.44G/9.86G [00:09<00:06, 654MB/s][A
pytorch_model-00001-of-00004.bin:  56%|█████▌    | 5.52G/9.86G [00:09<00:06, 647MB/s][A
pytorch_model-00001-of-00004.bin:  57%|█████▋    | 5.59G/9.86G [00:09<00:06, 647MB/s][A
pytorch_model-00001-of-00004.bin:  57%|█████▋    | 5.66G/9.86G [00:09<00:06, 631MB/s][A
pytorch_model-00001-of-00004.bin:  58%|█████▊    | 5.74G/9.86G [00:10<00:06, 640MB/s][A
pytorch_model-00001-of-00004.bin:  59%|█████▉    | 5.81G/9.86G [00:10<00:08, 486MB/s][A
pytorch_model-00001-of-00004.bin:  60%|█████▉    | 5.88G/9.86G [00:10<00:07, 526MB/s][A
pytorch_model-00001-of-00004.bin:  60%|██████    | 5.95G/9.86G [00:10<00:07, 534MB/s][A
pytorch_model-00001-of-00004.bin:  61%|██████    | 6.01G/9.86G [00:10<00:06, 554MB/s][A
pytorch_model-00001-of-00004.bin:  62%|██████▏   | 6.07G/9.86G [00:10<00:06, 568MB/s][A
pytorch_model-00001-of-00004.bin:  62%|██████▏   | 6.13G/9.86G [00:10<00:06, 583MB/s][A
pytorch_model-00001-of-00004.bin:  63%|██████▎   | 6.20G/9.86G [00:10<00:06, 583MB/s][A
pytorch_model-00001-of-00004.bin:  63%|██████▎   | 6.26G/9.86G [00:10<00:06, 585MB/s][A
pytorch_model-00001-of-00004.bin:  64%|██████▍   | 6.32G/9.86G [00:11<00:06, 589MB/s][A
pytorch_model-00001-of-00004.bin:  65%|██████▍   | 6.39G/9.86G [00:11<00:06, 559MB/s][A
pytorch_model-00001-of-00004.bin:  65%|██████▌   | 6.45G/9.86G [00:11<00:05, 575MB/s][A
pytorch_model-00001-of-00004.bin:  66%|██████▌   | 6.51G/9.86G [00:11<00:05, 580MB/s][A
pytorch_model-00001-of-00004.bin:  67%|██████▋   | 6.57G/9.86G [00:11<00:05, 580MB/s][A
pytorch_model-00001-of-00004.bin:  67%|██████▋   | 6.65G/9.86G [00:11<00:05, 597MB/s][A
pytorch_model-00001-of-00004.bin:  68%|██████▊   | 6.72G/9.86G [00:11<00:05, 609MB/s][A
pytorch_model-00001-of-00004.bin:  69%|██████▉   | 6.78G/9.86G [00:11<00:05, 614MB/s][A
pytorch_model-00001-of-00004.bin:  69%|██████▉   | 6.85G/9.86G [00:11<00:04, 615MB/s][A
pytorch_model-00001-of-00004.bin:  70%|███████   | 6.92G/9.86G [00:12<00:04, 627MB/s][A
pytorch_model-00001-of-00004.bin:  71%|███████   | 6.99G/9.86G [00:12<00:04, 631MB/s][A
pytorch_model-00001-of-00004.bin:  72%|███████▏  | 7.07G/9.86G [00:12<00:04, 616MB/s][A
pytorch_model-00001-of-00004.bin:  72%|███████▏  | 7.14G/9.86G [00:12<00:04, 623MB/s][A
pytorch_model-00001-of-00004.bin:  73%|███████▎  | 7.20G/9.86G [00:12<00:04, 624MB/s][A
pytorch_model-00001-of-00004.bin:  74%|███████▎  | 7.27G/9.86G [00:12<00:04, 625MB/s][A
pytorch_model-00001-of-00004.bin:  74%|███████▍  | 7.34G/9.86G [00:12<00:04, 627MB/s][A
pytorch_model-00001-of-00004.bin:  75%|███████▌  | 7.41G/9.86G [00:12<00:03, 632MB/s][A
pytorch_model-00001-of-00004.bin:  76%|███████▌  | 7.49G/9.86G [00:12<00:03, 631MB/s][A
pytorch_model-00001-of-00004.bin:  77%|███████▋  | 7.56G/9.86G [00:13<00:03, 630MB/s][A
pytorch_model-00001-of-00004.bin:  77%|███████▋  | 7.63G/9.86G [00:13<00:03, 649MB/s][A
pytorch_model-00001-of-00004.bin:  78%|███████▊  | 7.71G/9.86G [00:13<00:03, 656MB/s][A
pytorch_model-00001-of-00004.bin:  79%|███████▉  | 7.78G/9.86G [00:13<00:03, 638MB/s][A
pytorch_model-00001-of-00004.bin:  80%|███████▉  | 7.85G/9.86G [00:13<00:03, 645MB/s][A
pytorch_model-00001-of-00004.bin:  80%|████████  | 7.93G/9.86G [00:13<00:03, 636MB/s][A
pytorch_model-00001-of-00004.bin:  81%|████████  | 8.00G/9.86G [00:13<00:02, 623MB/s][A
pytorch_model-00001-of-00004.bin:  82%|████████▏ | 8.07G/9.86G [00:13<00:02, 626MB/s][A
pytorch_model-00001-of-00004.bin:  83%|████████▎ | 8.14G/9.86G [00:14<00:02, 623MB/s][A
pytorch_model-00001-of-00004.bin:  83%|████████▎ | 8.21G/9.86G [00:14<00:02, 631MB/s][A
pytorch_model-00001-of-00004.bin:  84%|████████▍ | 8.28G/9.86G [00:14<00:02, 618MB/s][A
pytorch_model-00001-of-00004.bin:  85%|████████▍ | 8.35G/9.86G [00:14<00:02, 610MB/s][A
pytorch_model-00001-of-00004.bin:  85%|████████▌ | 8.41G/9.86G [00:14<00:02, 610MB/s][A
pytorch_model-00001-of-00004.bin:  86%|████████▌ | 8.47G/9.86G [00:14<00:02, 611MB/s][A
pytorch_model-00001-of-00004.bin:  87%|████████▋ | 8.55G/9.86G [00:14<00:02, 619MB/s][A
pytorch_model-00001-of-00004.bin:  87%|████████▋ | 8.62G/9.86G [00:14<00:01, 626MB/s][A
pytorch_model-00001-of-00004.bin:  88%|████████▊ | 8.69G/9.86G [00:14<00:01, 632MB/s][A
pytorch_model-00001-of-00004.bin:  89%|████████▉ | 8.77G/9.86G [00:15<00:01, 632MB/s][A
pytorch_model-00001-of-00004.bin:  90%|████████▉ | 8.84G/9.86G [00:15<00:01, 637MB/s][A
pytorch_model-00001-of-00004.bin:  90%|█████████ | 8.91G/9.86G [00:15<00:01, 632MB/s][A
pytorch_model-00001-of-00004.bin:  91%|█████████ | 8.99G/9.86G [00:15<00:01, 635MB/s][A
pytorch_model-00001-of-00004.bin:  92%|█████████▏| 9.06G/9.86G [00:15<00:01, 639MB/s][A
pytorch_model-00001-of-00004.bin:  93%|█████████▎| 9.13G/9.86G [00:15<00:01, 646MB/s][A
pytorch_model-00001-of-00004.bin:  93%|█████████▎| 9.21G/9.86G [00:15<00:01, 646MB/s][A
pytorch_model-00001-of-00004.bin:  94%|█████████▍| 9.28G/9.86G [00:15<00:00, 649MB/s][A
pytorch_model-00001-of-00004.bin:  95%|█████████▍| 9.35G/9.86G [00:15<00:00, 641MB/s][A
pytorch_model-00001-of-00004.bin:  96%|█████████▌| 9.43G/9.86G [00:16<00:00, 629MB/s][A
pytorch_model-00001-of-00004.bin:  96%|█████████▋| 9.50G/9.86G [00:16<00:00, 628MB/s][A
pytorch_model-00001-of-00004.bin:  97%|█████████▋| 9.56G/9.86G [00:16<00:00, 617MB/s][A
pytorch_model-00001-of-00004.bin:  98%|█████████▊| 9.63G/9.86G [00:16<00:00, 595MB/s][A
pytorch_model-00001-of-00004.bin:  98%|█████████▊| 9.69G/9.86G [00:16<00:00, 594MB/s][A
pytorch_model-00001-of-00004.bin:  99%|█████████▉| 9.76G/9.86G [00:16<00:00, 607MB/s][A
pytorch_model-00001-of-00004.bin: 100%|█████████▉| 9.84G/9.86G [00:16<00:00, 614MB/s][Apytorch_model-00001-of-00004.bin: 100%|██████████| 9.86G/9.86G [00:16<00:00, 588MB/s]
Downloading shards:  25%|██▌       | 1/4 [00:17<00:51, 17.07s/it]Downloading shards:  25%|██▌       | 1/4 [00:17<00:51, 17.11s/it]Downloading shards:  25%|██▌       | 1/4 [00:17<00:51, 17.12s/it]/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 9993.54 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 9993.54 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(
Downloading shards:  25%|██▌       | 1/4 [00:17<00:51, 17.18s/it]
pytorch_model-00002-of-00004.bin:   0%|          | 0.00/9.99G [00:00<?, ?B/s][A
pytorch_model-00002-of-00004.bin:   0%|          | 21.0M/9.99G [00:18<2:25:37, 1.14MB/s][A
pytorch_model-00002-of-00004.bin:   0%|          | 31.5M/9.99G [00:18<1:26:16, 1.92MB/s][A
pytorch_model-00002-of-00004.bin:   1%|          | 73.4M/9.99G [00:18<25:47, 6.41MB/s]  [A
pytorch_model-00002-of-00004.bin:   1%|          | 115M/9.99G [00:18<13:00, 12.7MB/s] [A
pytorch_model-00002-of-00004.bin:   2%|▏         | 189M/9.99G [00:18<05:51, 27.9MB/s][A
pytorch_model-00002-of-00004.bin:   3%|▎         | 262M/9.99G [00:19<03:21, 48.4MB/s][A
pytorch_model-00002-of-00004.bin:   3%|▎         | 315M/9.99G [00:19<02:26, 66.1MB/s][A
pytorch_model-00002-of-00004.bin:   4%|▍         | 388M/9.99G [00:19<01:34, 101MB/s] [A
pytorch_model-00002-of-00004.bin:   5%|▍         | 461M/9.99G [00:19<01:05, 145MB/s][A
pytorch_model-00002-of-00004.bin:   5%|▌         | 535M/9.99G [00:19<00:48, 196MB/s][A
pytorch_model-00002-of-00004.bin:   6%|▌         | 608M/9.99G [00:19<00:36, 254MB/s][A
pytorch_model-00002-of-00004.bin:   7%|▋         | 682M/9.99G [00:19<00:29, 312MB/s][A
pytorch_model-00002-of-00004.bin:   7%|▋         | 744M/9.99G [00:19<00:25, 363MB/s][A
pytorch_model-00002-of-00004.bin:   8%|▊         | 807M/9.99G [00:19<00:22, 413MB/s][A
pytorch_model-00002-of-00004.bin:   9%|▊         | 870M/9.99G [00:20<00:20, 447MB/s][A
pytorch_model-00002-of-00004.bin:   9%|▉         | 944M/9.99G [00:20<00:18, 497MB/s][A
pytorch_model-00002-of-00004.bin:  10%|█         | 1.02G/9.99G [00:20<00:16, 538MB/s][A
pytorch_model-00002-of-00004.bin:  11%|█         | 1.09G/9.99G [00:20<00:15, 571MB/s][A
pytorch_model-00002-of-00004.bin:  12%|█▏        | 1.16G/9.99G [00:20<00:14, 596MB/s][A
pytorch_model-00002-of-00004.bin:  12%|█▏        | 1.24G/9.99G [00:20<00:14, 613MB/s][A
pytorch_model-00002-of-00004.bin:  13%|█▎        | 1.31G/9.99G [00:20<00:13, 623MB/s][A
pytorch_model-00002-of-00004.bin:  14%|█▍        | 1.38G/9.99G [00:20<00:13, 638MB/s][A
pytorch_model-00002-of-00004.bin:  15%|█▍        | 1.46G/9.99G [00:20<00:13, 638MB/s][A
pytorch_model-00002-of-00004.bin:  15%|█▌        | 1.53G/9.99G [00:21<00:12, 651MB/s][A
pytorch_model-00002-of-00004.bin:  16%|█▌        | 1.60G/9.99G [00:21<00:12, 662MB/s][A
pytorch_model-00002-of-00004.bin:  17%|█▋        | 1.68G/9.99G [00:21<00:12, 672MB/s][A
pytorch_model-00002-of-00004.bin:  18%|█▊        | 1.75G/9.99G [00:21<00:12, 668MB/s][A
pytorch_model-00002-of-00004.bin:  18%|█▊        | 1.82G/9.99G [00:21<00:12, 630MB/s][A
pytorch_model-00002-of-00004.bin:  19%|█▉        | 1.90G/9.99G [00:21<00:12, 628MB/s][A
pytorch_model-00002-of-00004.bin:  20%|█▉        | 1.97G/9.99G [00:21<00:12, 621MB/s][A
pytorch_model-00002-of-00004.bin:  20%|██        | 2.03G/9.99G [00:21<00:12, 614MB/s][A
pytorch_model-00002-of-00004.bin:  21%|██        | 2.10G/9.99G [00:21<00:12, 609MB/s][A
pytorch_model-00002-of-00004.bin:  22%|██▏       | 2.17G/9.99G [00:22<00:12, 619MB/s][A
pytorch_model-00002-of-00004.bin:  22%|██▏       | 2.24G/9.99G [00:22<00:12, 626MB/s][A
pytorch_model-00002-of-00004.bin:  23%|██▎       | 2.32G/9.99G [00:22<00:12, 625MB/s][A
pytorch_model-00002-of-00004.bin:  24%|██▍       | 2.39G/9.99G [00:22<00:11, 641MB/s][A
pytorch_model-00002-of-00004.bin:  25%|██▍       | 2.46G/9.99G [00:22<00:11, 631MB/s][A
pytorch_model-00002-of-00004.bin:  25%|██▌       | 2.54G/9.99G [00:22<00:11, 632MB/s][A
pytorch_model-00002-of-00004.bin:  26%|██▌       | 2.61G/9.99G [00:22<00:11, 638MB/s][A
pytorch_model-00002-of-00004.bin:  27%|██▋       | 2.68G/9.99G [00:22<00:11, 615MB/s][A
pytorch_model-00002-of-00004.bin:  27%|██▋       | 2.75G/9.99G [00:23<00:11, 615MB/s][A
pytorch_model-00002-of-00004.bin:  28%|██▊       | 2.81G/9.99G [00:23<00:11, 615MB/s][A
pytorch_model-00002-of-00004.bin:  29%|██▉       | 2.88G/9.99G [00:23<00:11, 623MB/s][A
pytorch_model-00002-of-00004.bin:  30%|██▉       | 2.96G/9.99G [00:23<00:11, 633MB/s][A
pytorch_model-00002-of-00004.bin:  30%|███       | 3.03G/9.99G [00:23<00:11, 630MB/s][A
pytorch_model-00002-of-00004.bin:  31%|███       | 3.10G/9.99G [00:23<00:10, 637MB/s][A
pytorch_model-00002-of-00004.bin:  32%|███▏      | 3.18G/9.99G [00:23<00:10, 642MB/s][A
pytorch_model-00002-of-00004.bin:  33%|███▎      | 3.25G/9.99G [00:23<00:10, 639MB/s][A
pytorch_model-00002-of-00004.bin:  33%|███▎      | 3.32G/9.99G [00:23<00:10, 626MB/s][A
pytorch_model-00002-of-00004.bin:  34%|███▍      | 3.40G/9.99G [00:24<00:10, 631MB/s][A
pytorch_model-00002-of-00004.bin:  35%|███▍      | 3.47G/9.99G [00:24<00:10, 628MB/s][A
pytorch_model-00002-of-00004.bin:  35%|███▌      | 3.53G/9.99G [00:24<00:10, 624MB/s][A
pytorch_model-00002-of-00004.bin:  36%|███▌      | 3.60G/9.99G [00:24<00:10, 620MB/s][A
pytorch_model-00002-of-00004.bin:  37%|███▋      | 3.67G/9.99G [00:24<00:10, 628MB/s][A
pytorch_model-00002-of-00004.bin:  37%|███▋      | 3.73G/9.99G [00:24<00:10, 612MB/s][A
pytorch_model-00002-of-00004.bin:  38%|███▊      | 3.80G/9.99G [00:24<00:10, 608MB/s][A
pytorch_model-00002-of-00004.bin:  39%|███▊      | 3.87G/9.99G [00:24<00:09, 620MB/s][A
pytorch_model-00002-of-00004.bin:  39%|███▉      | 3.94G/9.99G [00:24<00:09, 629MB/s][A
pytorch_model-00002-of-00004.bin:  40%|████      | 4.02G/9.99G [00:25<00:09, 632MB/s][A
pytorch_model-00002-of-00004.bin:  41%|████      | 4.09G/9.99G [00:25<00:09, 630MB/s][A
pytorch_model-00002-of-00004.bin:  42%|████▏     | 4.16G/9.99G [00:25<00:09, 631MB/s][A
pytorch_model-00002-of-00004.bin:  42%|████▏     | 4.24G/9.99G [00:25<00:09, 638MB/s][A
pytorch_model-00002-of-00004.bin:  43%|████▎     | 4.31G/9.99G [00:26<00:21, 262MB/s][A
pytorch_model-00002-of-00004.bin:  44%|████▎     | 4.36G/9.99G [00:26<00:23, 241MB/s][A
pytorch_model-00002-of-00004.bin:  44%|████▍     | 4.42G/9.99G [00:26<00:19, 293MB/s][A
pytorch_model-00002-of-00004.bin:  45%|████▍     | 4.49G/9.99G [00:26<00:15, 345MB/s][A
pytorch_model-00002-of-00004.bin:  46%|████▌     | 4.56G/9.99G [00:26<00:13, 405MB/s][A
pytorch_model-00002-of-00004.bin:  46%|████▋     | 4.63G/9.99G [00:26<00:11, 458MB/s][A
pytorch_model-00002-of-00004.bin:  47%|████▋     | 4.71G/9.99G [00:26<00:10, 508MB/s][A
pytorch_model-00002-of-00004.bin:  48%|████▊     | 4.78G/9.99G [00:26<00:09, 548MB/s][A
pytorch_model-00002-of-00004.bin:  49%|████▊     | 4.85G/9.99G [00:27<00:08, 576MB/s][A
pytorch_model-00002-of-00004.bin:  49%|████▉     | 4.93G/9.99G [00:27<00:08, 602MB/s][A
pytorch_model-00002-of-00004.bin:  50%|█████     | 5.00G/9.99G [00:27<00:08, 606MB/s][A
pytorch_model-00002-of-00004.bin:  51%|█████     | 5.08G/9.99G [00:27<00:07, 625MB/s][A
pytorch_model-00002-of-00004.bin:  52%|█████▏    | 5.15G/9.99G [00:27<00:07, 630MB/s][A
pytorch_model-00002-of-00004.bin:  52%|█████▏    | 5.22G/9.99G [00:27<00:07, 626MB/s][A
pytorch_model-00002-of-00004.bin:  53%|█████▎    | 5.30G/9.99G [00:27<00:07, 646MB/s][A
pytorch_model-00002-of-00004.bin:  54%|█████▎    | 5.37G/9.99G [00:27<00:07, 628MB/s][A
pytorch_model-00002-of-00004.bin:  54%|█████▍    | 5.44G/9.99G [00:28<00:07, 626MB/s][A
pytorch_model-00002-of-00004.bin:  55%|█████▌    | 5.52G/9.99G [00:28<00:07, 623MB/s][A
pytorch_model-00002-of-00004.bin:  56%|█████▌    | 5.58G/9.99G [00:28<00:07, 613MB/s][A
pytorch_model-00002-of-00004.bin:  56%|█████▋    | 5.64G/9.99G [00:28<00:07, 612MB/s][A
pytorch_model-00002-of-00004.bin:  57%|█████▋    | 5.71G/9.99G [00:28<00:06, 626MB/s][A
pytorch_model-00002-of-00004.bin:  58%|█████▊    | 5.79G/9.99G [00:28<00:06, 643MB/s][A
pytorch_model-00002-of-00004.bin:  59%|█████▊    | 5.86G/9.99G [00:28<00:06, 650MB/s][A
pytorch_model-00002-of-00004.bin:  59%|█████▉    | 5.93G/9.99G [00:28<00:06, 663MB/s][A
pytorch_model-00002-of-00004.bin:  60%|██████    | 6.01G/9.99G [00:28<00:05, 670MB/s][A
pytorch_model-00002-of-00004.bin:  61%|██████    | 6.08G/9.99G [00:28<00:05, 653MB/s][A
pytorch_model-00002-of-00004.bin:  62%|██████▏   | 6.16G/9.99G [00:29<00:05, 647MB/s][A
pytorch_model-00002-of-00004.bin:  62%|██████▏   | 6.23G/9.99G [00:29<00:05, 653MB/s][A
pytorch_model-00002-of-00004.bin:  63%|██████▎   | 6.30G/9.99G [00:29<00:05, 649MB/s][A
pytorch_model-00002-of-00004.bin:  64%|██████▍   | 6.38G/9.99G [00:29<00:05, 632MB/s][A
pytorch_model-00002-of-00004.bin:  65%|██████▍   | 6.45G/9.99G [00:29<00:05, 632MB/s][A
pytorch_model-00002-of-00004.bin:  65%|██████▌   | 6.52G/9.99G [00:29<00:05, 626MB/s][A
pytorch_model-00002-of-00004.bin:  66%|██████▌   | 6.59G/9.99G [00:29<00:05, 616MB/s][A
pytorch_model-00002-of-00004.bin:  67%|██████▋   | 6.65G/9.99G [00:30<00:08, 392MB/s][A
pytorch_model-00002-of-00004.bin:  67%|██████▋   | 6.72G/9.99G [00:30<00:07, 448MB/s][A
pytorch_model-00002-of-00004.bin:  68%|██████▊   | 6.79G/9.99G [00:30<00:06, 500MB/s][A
pytorch_model-00002-of-00004.bin:  69%|██████▊   | 6.87G/9.99G [00:30<00:05, 540MB/s][A
pytorch_model-00002-of-00004.bin:  69%|██████▉   | 6.94G/9.99G [00:30<00:05, 566MB/s][A
pytorch_model-00002-of-00004.bin:  70%|███████   | 7.01G/9.99G [00:30<00:05, 587MB/s][A
pytorch_model-00002-of-00004.bin:  71%|███████   | 7.08G/9.99G [00:30<00:04, 596MB/s][A
pytorch_model-00002-of-00004.bin:  72%|███████▏  | 7.15G/9.99G [00:30<00:04, 621MB/s][A
pytorch_model-00002-of-00004.bin:  72%|███████▏  | 7.22G/9.99G [00:31<00:04, 618MB/s][A
pytorch_model-00002-of-00004.bin:  73%|███████▎  | 7.30G/9.99G [00:31<00:04, 619MB/s][A
pytorch_model-00002-of-00004.bin:  74%|███████▎  | 7.36G/9.99G [00:31<00:04, 615MB/s][A
pytorch_model-00002-of-00004.bin:  74%|███████▍  | 7.42G/9.99G [00:31<00:04, 554MB/s][A
pytorch_model-00002-of-00004.bin:  75%|███████▍  | 7.49G/9.99G [00:31<00:05, 471MB/s][A
pytorch_model-00002-of-00004.bin:  76%|███████▌  | 7.55G/9.99G [00:31<00:04, 503MB/s][A
pytorch_model-00002-of-00004.bin:  76%|███████▋  | 7.62G/9.99G [00:31<00:04, 545MB/s][A
pytorch_model-00002-of-00004.bin:  77%|███████▋  | 7.70G/9.99G [00:31<00:03, 583MB/s][A
pytorch_model-00002-of-00004.bin:  78%|███████▊  | 7.76G/9.99G [00:31<00:03, 592MB/s][A
pytorch_model-00002-of-00004.bin:  78%|███████▊  | 7.83G/9.99G [00:32<00:03, 605MB/s][A
pytorch_model-00002-of-00004.bin:  79%|███████▉  | 7.91G/9.99G [00:32<00:03, 619MB/s][A
pytorch_model-00002-of-00004.bin:  80%|███████▉  | 7.97G/9.99G [00:32<00:03, 600MB/s][A
pytorch_model-00002-of-00004.bin:  80%|████████  | 8.03G/9.99G [00:32<00:03, 604MB/s][A
pytorch_model-00002-of-00004.bin:  81%|████████  | 8.11G/9.99G [00:32<00:03, 627MB/s][A
pytorch_model-00002-of-00004.bin:  82%|████████▏ | 8.18G/9.99G [00:32<00:02, 627MB/s][A
pytorch_model-00002-of-00004.bin:  83%|████████▎ | 8.25G/9.99G [00:32<00:02, 626MB/s][A
pytorch_model-00002-of-00004.bin:  83%|████████▎ | 8.33G/9.99G [00:32<00:02, 629MB/s][A
pytorch_model-00002-of-00004.bin:  84%|████████▍ | 8.40G/9.99G [00:32<00:02, 625MB/s][A
pytorch_model-00002-of-00004.bin:  85%|████████▍ | 8.46G/9.99G [00:33<00:02, 613MB/s][A
pytorch_model-00002-of-00004.bin:  85%|████████▌ | 8.54G/9.99G [00:33<00:02, 622MB/s][A
pytorch_model-00002-of-00004.bin:  86%|████████▌ | 8.61G/9.99G [00:33<00:02, 627MB/s][A
pytorch_model-00002-of-00004.bin:  87%|████████▋ | 8.67G/9.99G [00:33<00:02, 619MB/s][A
pytorch_model-00002-of-00004.bin:  87%|████████▋ | 8.73G/9.99G [00:33<00:02, 621MB/s][A
pytorch_model-00002-of-00004.bin:  88%|████████▊ | 8.80G/9.99G [00:33<00:01, 619MB/s][A
pytorch_model-00002-of-00004.bin:  89%|████████▉ | 8.87G/9.99G [00:33<00:01, 623MB/s][A
pytorch_model-00002-of-00004.bin:  89%|████████▉ | 8.93G/9.99G [00:33<00:01, 614MB/s][A
pytorch_model-00002-of-00004.bin:  90%|█████████ | 9.01G/9.99G [00:33<00:01, 635MB/s][A
pytorch_model-00002-of-00004.bin:  91%|█████████ | 9.08G/9.99G [00:34<00:01, 651MB/s][A
pytorch_model-00002-of-00004.bin:  92%|█████████▏| 9.15G/9.99G [00:34<00:01, 649MB/s][A
pytorch_model-00002-of-00004.bin:  92%|█████████▏| 9.23G/9.99G [00:34<00:01, 638MB/s][A
pytorch_model-00002-of-00004.bin:  93%|█████████▎| 9.30G/9.99G [00:34<00:01, 645MB/s][A
pytorch_model-00002-of-00004.bin:  94%|█████████▍| 9.37G/9.99G [00:34<00:00, 634MB/s][A
pytorch_model-00002-of-00004.bin:  95%|█████████▍| 9.45G/9.99G [00:34<00:00, 621MB/s][A
pytorch_model-00002-of-00004.bin:  95%|█████████▌| 9.52G/9.99G [00:34<00:00, 631MB/s][A
pytorch_model-00002-of-00004.bin:  96%|█████████▌| 9.59G/9.99G [00:34<00:00, 631MB/s][A
pytorch_model-00002-of-00004.bin:  97%|█████████▋| 9.67G/9.99G [00:35<00:00, 631MB/s][A
pytorch_model-00002-of-00004.bin:  97%|█████████▋| 9.74G/9.99G [00:35<00:00, 626MB/s][A
pytorch_model-00002-of-00004.bin:  98%|█████████▊| 9.80G/9.99G [00:35<00:00, 623MB/s][A
pytorch_model-00002-of-00004.bin:  99%|█████████▊| 9.87G/9.99G [00:35<00:00, 619MB/s][A
pytorch_model-00002-of-00004.bin:  99%|█████████▉| 9.94G/9.99G [00:35<00:00, 629MB/s][Apytorch_model-00002-of-00004.bin: 100%|██████████| 9.99G/9.99G [00:35<00:00, 281MB/s]
Downloading shards:  50%|█████     | 2/4 [00:52<00:56, 28.09s/it]Downloading shards:  50%|█████     | 2/4 [00:52<00:56, 28.07s/it]/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 9456.55 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
Downloading shards:  50%|█████     | 2/4 [00:52<00:56, 28.10s/it]/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 9456.55 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(
Downloading shards:  50%|█████     | 2/4 [00:53<00:56, 28.16s/it]
pytorch_model-00003-of-00004.bin:   0%|          | 0.00/9.46G [00:00<?, ?B/s][A
pytorch_model-00003-of-00004.bin:   0%|          | 21.0M/9.46G [00:14<1:50:33, 1.42MB/s][A
pytorch_model-00003-of-00004.bin:   0%|          | 31.5M/9.46G [00:15<1:06:02, 2.38MB/s][A
pytorch_model-00003-of-00004.bin:   1%|          | 62.9M/9.46G [00:15<23:59, 6.53MB/s]  [A
pytorch_model-00003-of-00004.bin:   1%|          | 94.4M/9.46G [00:15<12:44, 12.2MB/s][A
pytorch_model-00003-of-00004.bin:   2%|▏         | 157M/9.46G [00:15<05:30, 28.2MB/s] [A
pytorch_model-00003-of-00004.bin:   2%|▏         | 231M/9.46G [00:15<02:53, 53.1MB/s][A
pytorch_model-00003-of-00004.bin:   3%|▎         | 283M/9.46G [00:15<02:04, 73.4MB/s][A
pytorch_model-00003-of-00004.bin:   4%|▎         | 336M/9.46G [00:15<01:32, 98.6MB/s][A
pytorch_model-00003-of-00004.bin:   4%|▍         | 409M/9.46G [00:15<01:00, 149MB/s] [A
pytorch_model-00003-of-00004.bin:   5%|▌         | 482M/9.46G [00:15<00:43, 206MB/s][A
pytorch_model-00003-of-00004.bin:   6%|▌         | 556M/9.46G [00:16<00:33, 268MB/s][A
pytorch_model-00003-of-00004.bin:   7%|▋         | 619M/9.46G [00:16<00:27, 321MB/s][A
pytorch_model-00003-of-00004.bin:   7%|▋         | 682M/9.46G [00:16<00:23, 374MB/s][A
pytorch_model-00003-of-00004.bin:   8%|▊         | 744M/9.46G [00:16<00:20, 422MB/s][A
pytorch_model-00003-of-00004.bin:   9%|▊         | 807M/9.46G [00:16<00:18, 465MB/s][A
pytorch_model-00003-of-00004.bin:   9%|▉         | 870M/9.46G [00:16<00:17, 485MB/s][A
pytorch_model-00003-of-00004.bin:  10%|▉         | 933M/9.46G [00:16<00:16, 511MB/s][A
pytorch_model-00003-of-00004.bin:  11%|█         | 1.01G/9.46G [00:16<00:15, 552MB/s][A
pytorch_model-00003-of-00004.bin:  11%|█▏        | 1.08G/9.46G [00:16<00:14, 589MB/s][A
pytorch_model-00003-of-00004.bin:  12%|█▏        | 1.15G/9.46G [00:17<00:13, 609MB/s][A
pytorch_model-00003-of-00004.bin:  13%|█▎        | 1.23G/9.46G [00:17<00:13, 615MB/s][A
pytorch_model-00003-of-00004.bin:  14%|█▎        | 1.30G/9.46G [00:17<00:13, 611MB/s][A
pytorch_model-00003-of-00004.bin:  14%|█▍        | 1.36G/9.46G [00:17<00:13, 615MB/s][A
pytorch_model-00003-of-00004.bin:  15%|█▌        | 1.44G/9.46G [00:17<00:12, 622MB/s][A
pytorch_model-00003-of-00004.bin:  16%|█▌        | 1.51G/9.46G [00:17<00:12, 629MB/s][A
pytorch_model-00003-of-00004.bin:  17%|█▋        | 1.58G/9.46G [00:17<00:12, 635MB/s][A
pytorch_model-00003-of-00004.bin:  18%|█▊        | 1.66G/9.46G [00:17<00:12, 624MB/s][A
pytorch_model-00003-of-00004.bin:  18%|█▊        | 1.72G/9.46G [00:17<00:13, 581MB/s][A
pytorch_model-00003-of-00004.bin:  19%|█▉        | 1.78G/9.46G [00:18<00:13, 588MB/s][A
pytorch_model-00003-of-00004.bin:  20%|█▉        | 1.85G/9.46G [00:18<00:13, 576MB/s][A
pytorch_model-00003-of-00004.bin:  20%|██        | 1.91G/9.46G [00:18<00:12, 585MB/s][A
pytorch_model-00003-of-00004.bin:  21%|██        | 1.98G/9.46G [00:18<00:12, 598MB/s][A
pytorch_model-00003-of-00004.bin:  22%|██▏       | 2.06G/9.46G [00:18<00:12, 615MB/s][A
pytorch_model-00003-of-00004.bin:  23%|██▎       | 2.13G/9.46G [00:18<00:11, 624MB/s][A
pytorch_model-00003-of-00004.bin:  23%|██▎       | 2.20G/9.46G [00:18<00:11, 634MB/s][A
pytorch_model-00003-of-00004.bin:  24%|██▍       | 2.28G/9.46G [00:18<00:11, 622MB/s][A
pytorch_model-00003-of-00004.bin:  25%|██▍       | 2.34G/9.46G [00:18<00:11, 616MB/s][A
pytorch_model-00003-of-00004.bin:  26%|██▌       | 2.41G/9.46G [00:19<00:11, 619MB/s][A
pytorch_model-00003-of-00004.bin:  26%|██▌       | 2.47G/9.46G [00:19<00:11, 622MB/s][A
pytorch_model-00003-of-00004.bin:  27%|██▋       | 2.54G/9.46G [00:19<00:11, 622MB/s][A
pytorch_model-00003-of-00004.bin:  27%|██▋       | 2.60G/9.46G [00:19<00:11, 596MB/s][A
pytorch_model-00003-of-00004.bin:  28%|██▊       | 2.66G/9.46G [00:19<00:11, 598MB/s][A
pytorch_model-00003-of-00004.bin:  29%|██▉       | 2.73G/9.46G [00:19<00:11, 600MB/s][A
pytorch_model-00003-of-00004.bin:  30%|██▉       | 2.80G/9.46G [00:19<00:10, 612MB/s][A
pytorch_model-00003-of-00004.bin:  30%|███       | 2.86G/9.46G [00:19<00:10, 615MB/s][A
pytorch_model-00003-of-00004.bin:  31%|███       | 2.94G/9.46G [00:19<00:10, 635MB/s][A
pytorch_model-00003-of-00004.bin:  32%|███▏      | 3.01G/9.46G [00:20<00:10, 618MB/s][A
pytorch_model-00003-of-00004.bin:  32%|███▏      | 3.07G/9.46G [00:20<00:10, 605MB/s][A
pytorch_model-00003-of-00004.bin:  33%|███▎      | 3.14G/9.46G [00:20<00:10, 595MB/s][A
pytorch_model-00003-of-00004.bin:  34%|███▍      | 3.21G/9.46G [00:20<00:10, 613MB/s][A
pytorch_model-00003-of-00004.bin:  35%|███▍      | 3.27G/9.46G [00:20<00:10, 611MB/s][A
pytorch_model-00003-of-00004.bin:  35%|███▌      | 3.33G/9.46G [00:20<00:10, 609MB/s][A
pytorch_model-00003-of-00004.bin:  36%|███▌      | 3.41G/9.46G [00:20<00:09, 625MB/s][A
pytorch_model-00003-of-00004.bin:  37%|███▋      | 3.47G/9.46G [00:20<00:09, 614MB/s][A
pytorch_model-00003-of-00004.bin:  37%|███▋      | 3.54G/9.46G [00:20<00:09, 632MB/s][A
pytorch_model-00003-of-00004.bin:  38%|███▊      | 3.62G/9.46G [00:21<00:09, 618MB/s][A
pytorch_model-00003-of-00004.bin:  39%|███▉      | 3.68G/9.46G [00:21<00:09, 619MB/s][A
pytorch_model-00003-of-00004.bin:  40%|███▉      | 3.75G/9.46G [00:21<00:09, 632MB/s][A
pytorch_model-00003-of-00004.bin:  40%|████      | 3.83G/9.46G [00:21<00:09, 624MB/s][A
pytorch_model-00003-of-00004.bin:  41%|████      | 3.89G/9.46G [00:21<00:08, 621MB/s][A
pytorch_model-00003-of-00004.bin:  42%|████▏     | 3.95G/9.46G [00:21<00:08, 621MB/s][A
pytorch_model-00003-of-00004.bin:  42%|████▏     | 4.02G/9.46G [00:21<00:08, 613MB/s][A
pytorch_model-00003-of-00004.bin:  43%|████▎     | 4.09G/9.46G [00:21<00:08, 622MB/s][A
pytorch_model-00003-of-00004.bin:  44%|████▍     | 4.16G/9.46G [00:21<00:08, 633MB/s][A
pytorch_model-00003-of-00004.bin:  45%|████▍     | 4.24G/9.46G [00:22<00:08, 620MB/s][A
pytorch_model-00003-of-00004.bin:  45%|████▌     | 4.30G/9.46G [00:22<00:08, 596MB/s][A
pytorch_model-00003-of-00004.bin:  46%|████▌     | 4.36G/9.46G [00:23<00:28, 180MB/s][A
pytorch_model-00003-of-00004.bin:  47%|████▋     | 4.42G/9.46G [00:23<00:22, 224MB/s][A
pytorch_model-00003-of-00004.bin:  48%|████▊     | 4.50G/9.46G [00:23<00:17, 287MB/s][A
pytorch_model-00003-of-00004.bin:  48%|████▊     | 4.57G/9.46G [00:23<00:14, 348MB/s][A
pytorch_model-00003-of-00004.bin:  49%|████▉     | 4.63G/9.46G [00:23<00:12, 394MB/s][A
pytorch_model-00003-of-00004.bin:  50%|████▉     | 4.70G/9.46G [00:23<00:10, 434MB/s][A
pytorch_model-00003-of-00004.bin:  50%|█████     | 4.76G/9.46G [00:23<00:09, 471MB/s][A
pytorch_model-00003-of-00004.bin:  51%|█████     | 4.83G/9.46G [00:23<00:08, 514MB/s][A
pytorch_model-00003-of-00004.bin:  52%|█████▏    | 4.91G/9.46G [00:24<00:08, 548MB/s][A
pytorch_model-00003-of-00004.bin:  53%|█████▎    | 4.97G/9.46G [00:24<00:08, 552MB/s][A
pytorch_model-00003-of-00004.bin:  53%|█████▎    | 5.03G/9.46G [00:24<00:07, 557MB/s][A
pytorch_model-00003-of-00004.bin:  54%|█████▍    | 5.10G/9.46G [00:24<00:07, 563MB/s][A
pytorch_model-00003-of-00004.bin:  55%|█████▍    | 5.16G/9.46G [00:24<00:07, 569MB/s][A
pytorch_model-00003-of-00004.bin:  55%|█████▌    | 5.22G/9.46G [00:24<00:07, 577MB/s][A
pytorch_model-00003-of-00004.bin:  56%|█████▌    | 5.28G/9.46G [00:24<00:07, 565MB/s][A
pytorch_model-00003-of-00004.bin:  57%|█████▋    | 5.35G/9.46G [00:24<00:07, 578MB/s][A
pytorch_model-00003-of-00004.bin:  57%|█████▋    | 5.41G/9.46G [00:24<00:07, 573MB/s][A
pytorch_model-00003-of-00004.bin:  58%|█████▊    | 5.47G/9.46G [00:25<00:07, 559MB/s][A
pytorch_model-00003-of-00004.bin:  59%|█████▊    | 5.54G/9.46G [00:25<00:06, 564MB/s][A
pytorch_model-00003-of-00004.bin:  59%|█████▉    | 5.60G/9.46G [00:25<00:06, 580MB/s][A
pytorch_model-00003-of-00004.bin:  60%|█████▉    | 5.66G/9.46G [00:25<00:06, 582MB/s][A
pytorch_model-00003-of-00004.bin:  61%|██████    | 5.74G/9.46G [00:25<00:06, 601MB/s][A
pytorch_model-00003-of-00004.bin:  61%|██████▏   | 5.80G/9.46G [00:25<00:06, 604MB/s][A
pytorch_model-00003-of-00004.bin:  62%|██████▏   | 5.86G/9.46G [00:25<00:06, 590MB/s][A
pytorch_model-00003-of-00004.bin:  63%|██████▎   | 5.92G/9.46G [00:25<00:05, 594MB/s][A
pytorch_model-00003-of-00004.bin:  63%|██████▎   | 5.99G/9.46G [00:25<00:05, 587MB/s][A
pytorch_model-00003-of-00004.bin:  64%|██████▍   | 6.06G/9.46G [00:26<00:05, 597MB/s][A
pytorch_model-00003-of-00004.bin:  65%|██████▍   | 6.12G/9.46G [00:26<00:05, 593MB/s][A
pytorch_model-00003-of-00004.bin:  65%|██████▌   | 6.19G/9.46G [00:26<00:05, 598MB/s][A
pytorch_model-00003-of-00004.bin:  66%|██████▌   | 6.25G/9.46G [00:26<00:05, 606MB/s][A
pytorch_model-00003-of-00004.bin:  67%|██████▋   | 6.31G/9.46G [00:26<00:05, 610MB/s][A
pytorch_model-00003-of-00004.bin:  67%|██████▋   | 6.38G/9.46G [00:26<00:05, 601MB/s][A
pytorch_model-00003-of-00004.bin:  68%|██████▊   | 6.44G/9.46G [00:26<00:05, 599MB/s][A
pytorch_model-00003-of-00004.bin:  69%|██████▊   | 6.50G/9.46G [00:26<00:04, 593MB/s][A
pytorch_model-00003-of-00004.bin:  69%|██████▉   | 6.56G/9.46G [00:26<00:04, 597MB/s][A
pytorch_model-00003-of-00004.bin:  70%|███████   | 6.63G/9.46G [00:26<00:04, 604MB/s][A
pytorch_model-00003-of-00004.bin:  71%|███████   | 6.70G/9.46G [00:27<00:04, 613MB/s][A
pytorch_model-00003-of-00004.bin:  72%|███████▏  | 6.76G/9.46G [00:27<00:04, 609MB/s][A
pytorch_model-00003-of-00004.bin:  72%|███████▏  | 6.83G/9.46G [00:27<00:04, 601MB/s][A
pytorch_model-00003-of-00004.bin:  73%|███████▎  | 6.89G/9.46G [00:27<00:04, 604MB/s][A
pytorch_model-00003-of-00004.bin:  74%|███████▎  | 6.95G/9.46G [00:27<00:04, 596MB/s][A
pytorch_model-00003-of-00004.bin:  74%|███████▍  | 7.01G/9.46G [00:27<00:04, 595MB/s][A
pytorch_model-00003-of-00004.bin:  75%|███████▍  | 7.08G/9.46G [00:27<00:03, 602MB/s][A
pytorch_model-00003-of-00004.bin:  76%|███████▌  | 7.14G/9.46G [00:27<00:03, 606MB/s][A
pytorch_model-00003-of-00004.bin:  76%|███████▌  | 7.20G/9.46G [00:27<00:03, 591MB/s][A
pytorch_model-00003-of-00004.bin:  77%|███████▋  | 7.27G/9.46G [00:28<00:03, 599MB/s][A
pytorch_model-00003-of-00004.bin:  78%|███████▊  | 7.33G/9.46G [00:28<00:03, 600MB/s][A
pytorch_model-00003-of-00004.bin:  78%|███████▊  | 7.40G/9.46G [00:28<00:03, 615MB/s][A
pytorch_model-00003-of-00004.bin:  79%|███████▉  | 7.47G/9.46G [00:28<00:03, 609MB/s][A
pytorch_model-00003-of-00004.bin:  80%|███████▉  | 7.53G/9.46G [00:28<00:03, 611MB/s][A
pytorch_model-00003-of-00004.bin:  80%|████████  | 7.60G/9.46G [00:28<00:02, 623MB/s][A
pytorch_model-00003-of-00004.bin:  81%|████████  | 7.67G/9.46G [00:28<00:02, 603MB/s][A
pytorch_model-00003-of-00004.bin:  82%|████████▏ | 7.73G/9.46G [00:28<00:02, 600MB/s][A
pytorch_model-00003-of-00004.bin:  82%|████████▏ | 7.79G/9.46G [00:28<00:02, 591MB/s][A
pytorch_model-00003-of-00004.bin:  83%|████████▎ | 7.85G/9.46G [00:28<00:02, 581MB/s][A
pytorch_model-00003-of-00004.bin:  84%|████████▎ | 7.92G/9.46G [00:29<00:02, 582MB/s][A
pytorch_model-00003-of-00004.bin:  84%|████████▍ | 7.98G/9.46G [00:29<00:02, 573MB/s][A
pytorch_model-00003-of-00004.bin:  85%|████████▌ | 8.04G/9.46G [00:29<00:02, 582MB/s][A
pytorch_model-00003-of-00004.bin:  86%|████████▌ | 8.11G/9.46G [00:29<00:02, 596MB/s][A
pytorch_model-00003-of-00004.bin:  86%|████████▋ | 8.17G/9.46G [00:29<00:02, 602MB/s][A
pytorch_model-00003-of-00004.bin:  87%|████████▋ | 8.23G/9.46G [00:29<00:02, 597MB/s][A
pytorch_model-00003-of-00004.bin:  88%|████████▊ | 8.29G/9.46G [00:29<00:01, 583MB/s][A
pytorch_model-00003-of-00004.bin:  88%|████████▊ | 8.37G/9.46G [00:29<00:01, 601MB/s][A
pytorch_model-00003-of-00004.bin:  89%|████████▉ | 8.43G/9.46G [00:29<00:01, 587MB/s][A
pytorch_model-00003-of-00004.bin:  90%|████████▉ | 8.49G/9.46G [00:30<00:01, 589MB/s][A
pytorch_model-00003-of-00004.bin:  90%|█████████ | 8.56G/9.46G [00:30<00:01, 589MB/s][A
pytorch_model-00003-of-00004.bin:  91%|█████████ | 8.62G/9.46G [00:30<00:01, 593MB/s][A
pytorch_model-00003-of-00004.bin:  92%|█████████▏| 8.68G/9.46G [00:30<00:01, 584MB/s][A
pytorch_model-00003-of-00004.bin:  92%|█████████▏| 8.75G/9.46G [00:30<00:01, 586MB/s][A
pytorch_model-00003-of-00004.bin:  93%|█████████▎| 8.81G/9.46G [00:30<00:01, 583MB/s][A
pytorch_model-00003-of-00004.bin:  94%|█████████▍| 8.87G/9.46G [00:30<00:00, 587MB/s][A
pytorch_model-00003-of-00004.bin:  94%|█████████▍| 8.93G/9.46G [00:30<00:00, 569MB/s][A
pytorch_model-00003-of-00004.bin:  95%|█████████▌| 9.00G/9.46G [00:30<00:00, 563MB/s][A
pytorch_model-00003-of-00004.bin:  96%|█████████▌| 9.06G/9.46G [00:31<00:00, 568MB/s][A
pytorch_model-00003-of-00004.bin:  96%|█████████▋| 9.12G/9.46G [00:31<00:00, 579MB/s][A
pytorch_model-00003-of-00004.bin:  97%|█████████▋| 9.19G/9.46G [00:31<00:00, 585MB/s][A
pytorch_model-00003-of-00004.bin:  98%|█████████▊| 9.25G/9.46G [00:31<00:00, 589MB/s][A
pytorch_model-00003-of-00004.bin:  98%|█████████▊| 9.31G/9.46G [00:31<00:00, 571MB/s][A
pytorch_model-00003-of-00004.bin:  99%|█████████▉| 9.37G/9.46G [00:31<00:00, 571MB/s][A
pytorch_model-00003-of-00004.bin: 100%|█████████▉| 9.44G/9.46G [00:31<00:00, 558MB/s][Apytorch_model-00003-of-00004.bin: 100%|██████████| 9.46G/9.46G [00:31<00:00, 298MB/s]
Downloading shards:  75%|███████▌  | 3/4 [01:24<00:29, 29.86s/it]Downloading shards:  75%|███████▌  | 3/4 [01:24<00:29, 29.83s/it]Downloading shards:  75%|███████▌  | 3/4 [01:24<00:29, 29.84s/it]Downloading shards:  75%|███████▌  | 3/4 [01:24<00:29, 29.86s/it]/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 805.31 MB. The target location /projects/bbvz/bzd2/hub only has 0.00 MB free disk space.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/file_download.py:992: UserWarning: Not enough free disk space to download the file. The expected file size is: 805.31 MB. The target location /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/blobs only has 0.00 MB free disk space.
  warnings.warn(

pytorch_model-00004-of-00004.bin:   0%|          | 0.00/805M [00:00<?, ?B/s][A
pytorch_model-00004-of-00004.bin:   7%|▋         | 52.4M/805M [00:00<00:01, 460MB/s][A
pytorch_model-00004-of-00004.bin:  16%|█▌        | 126M/805M [00:00<00:01, 569MB/s] [A
pytorch_model-00004-of-00004.bin:  23%|██▎       | 189M/805M [00:00<00:01, 591MB/s][A
pytorch_model-00004-of-00004.bin:  31%|███       | 252M/805M [00:00<00:00, 601MB/s][A
pytorch_model-00004-of-00004.bin:  39%|███▉      | 315M/805M [00:00<00:01, 459MB/s][A
pytorch_model-00004-of-00004.bin:  48%|████▊     | 388M/805M [00:00<00:00, 514MB/s][A
pytorch_model-00004-of-00004.bin:  56%|█████▌    | 451M/805M [00:00<00:00, 521MB/s][A
pytorch_model-00004-of-00004.bin:  64%|██████▍   | 514M/805M [00:00<00:00, 538MB/s][A
pytorch_model-00004-of-00004.bin:  72%|███████▏  | 577M/805M [00:01<00:00, 539MB/s][A
pytorch_model-00004-of-00004.bin:  79%|███████▉  | 640M/805M [00:01<00:00, 541MB/s][A
pytorch_model-00004-of-00004.bin:  87%|████████▋ | 703M/805M [00:01<00:00, 541MB/s][A
pytorch_model-00004-of-00004.bin:  95%|█████████▌| 765M/805M [00:01<00:00, 549MB/s][Apytorch_model-00004-of-00004.bin: 100%|██████████| 805M/805M [00:01<00:00, 536MB/s]
Downloading shards: 100%|██████████| 4/4 [01:26<00:00, 18.74s/it]Downloading shards: 100%|██████████| 4/4 [01:26<00:00, 21.63s/it]
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Downloading shards: 100%|██████████| 4/4 [01:26<00:00, 18.76s/it]Downloading shards: 100%|██████████| 4/4 [01:26<00:00, 21.65s/it]
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Downloading shards: 100%|██████████| 4/4 [01:26<00:00, 18.76s/it]Downloading shards: 100%|██████████| 4/4 [01:26<00:00, 21.66s/it]
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Downloading shards: 100%|██████████| 4/4 [01:26<00:00, 18.80s/it]Downloading shards: 100%|██████████| 4/4 [01:26<00:00, 21.67s/it]
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  25%|██▌       | 1/4 [01:48<05:26, 108.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [01:49<05:28, 109.47s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [01:49<05:28, 109.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [01:49<05:28, 109.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [03:23<03:20, 100.22s/it]Loading checkpoint shards:  50%|█████     | 2/4 [03:24<03:21, 100.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [03:24<03:21, 100.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [03:24<03:21, 100.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [05:18<01:46, 106.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [05:18<01:47, 107.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [05:19<01:47, 107.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [05:19<01:47, 107.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [05:19<00:00, 65.07s/it] Loading checkpoint shards: 100%|██████████| 4/4 [05:19<00:00, 79.82s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]generation_config.json: 100%|██████████| 111/111 [00:00<00:00, 551kB/s]
Loading checkpoint shards: 100%|██████████| 4/4 [05:23<00:00, 66.71s/it] Loading checkpoint shards: 100%|██████████| 4/4 [05:23<00:00, 80.86s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 4/4 [05:23<00:00, 66.65s/it] Loading checkpoint shards: 100%|██████████| 4/4 [05:23<00:00, 80.87s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 4/4 [05:23<00:00, 66.70s/it] Loading checkpoint shards: 100%|██████████| 4/4 [05:23<00:00, 80.87s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
Starting main loop
PyTorch: setting up devices
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
Starting main loop
PyTorch: setting up devices
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
Starting main loop
PyTorch: setting up devices
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
Starting main loop
PyTorch: setting up devices
max_steps is given, it will override any value given in num_train_epochs
Training...Training...

Using auto half precision backend
Training...
Training...
Currently training with a batch size of: 1
***** Running training *****
  Num examples = 416,000
  Num Epochs = 9,223,372,036,854,775,807
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 6,500
  Number of trainable parameters = 24,944,640
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: hetarthvader (complex_dnn). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /projects/bbvz/bzd2/wandb/run-20240124_133240-k3zmqd13
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent
wandb: ⭐️ View project at https://wandb.ai/complex_dnn/huggingface
wandb: 🚀 View run at https://wandb.ai/complex_dnn/huggingface/runs/k3zmqd13
  0%|          | 0/6500 [00:00<?, ?it/s]/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9448, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 1.9273, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 1.9691, 'learning_rate': 9.999999415640199e-05, 'epoch': 0.0}
{'loss': 1.8761, 'learning_rate': 9.999997662560938e-05, 'epoch': 0.0}
{'loss': 1.7331, 'learning_rate': 9.999994740762622e-05, 'epoch': 0.0}
{'loss': 1.7399, 'learning_rate': 9.999990650245936e-05, 'epoch': 0.0}
  0%|          | 1/6500 [00:29<53:13:06, 29.48s/it]                                                     0%|          | 1/6500 [00:29<53:13:06, 29.48s/it]  0%|          | 2/6500 [00:47<41:07:42, 22.79s/it]                                                     0%|          | 2/6500 [00:47<41:07:42, 22.79s/it]  0%|          | 3/6500 [01:05<37:04:06, 20.54s/it]                                                     0%|          | 3/6500 [01:05<37:04:06, 20.54s/it]  0%|          | 4/6500 [01:23<35:12:15, 19.51s/it]                                                     0%|          | 4/6500 [01:23<35:12:15, 19.51s/it]  0%|          | 5/6500 [01:41<34:11:47, 18.95s/it]                                                     0%|          | 5/6500 [01:41<34:11:47, 18.95s/it]  0%|          | 6/6500 [01:59<33:36:38, 18.63s/it]                                                     0%|          | 6/6500 [01:59<33:36:38, 18.63s/it]  0%|          | 7/6500 [02:17<33:14:53, 18.43s/it]                             {'loss': 1.6301, 'learning_rate': 9.999985391011837e-05, 'epoch': 0.0}
{'loss': 1.6834, 'learning_rate': 9.999978963061551e-05, 'epoch': 0.0}
{'loss': 1.5957, 'learning_rate': 9.999971366396584e-05, 'epoch': 0.0}
{'loss': 1.475, 'learning_rate': 9.99996260101871e-05, 'epoch': 0.0}
                        0%|          | 7/6500 [02:17<33:14:53, 18.43s/it]  0%|          | 8/6500 [02:35<33:00:36, 18.31s/it]                                                     0%|          | 8/6500 [02:35<33:00:36, 18.31s/it]  0%|          | 9/6500 [02:53<32:53:16, 18.24s/it]                                                     0%|          | 9/6500 [02:53<32:53:16, 18.24s/it]  0%|          | 10/6500 [03:11<32:48:32, 18.20s/it]                                                      0%|          | 10/6500 [03:11<32:48:32, 18.20s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.4390507936477661, 'eval_runtime': 5.432, 'eval_samples_per_second': 4.234, 'eval_steps_per_second': 1.105, 'epoch': 0.0}
                                                      0%|          | 10/6500 [03:17<32:48:32, 18.20s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-10
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-10
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-10/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-10


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-10
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-10/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-10/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.5462, 'learning_rate': 9.99995266692998e-05, 'epoch': 0.0}
{'loss': 1.905, 'learning_rate': 9.999941564132713e-05, 'epoch': 0.0}
{'loss': 1.3922, 'learning_rate': 9.999929292629505e-05, 'epoch': 0.0}
{'loss': 1.3594, 'learning_rate': 9.999915852423225e-05, 'epoch': 0.0}
{'loss': 1.3233, 'learning_rate': 9.999901243517017e-05, 'epoch': 0.0}
{'loss': 1.2842, 'learning_rate': 9.99988546591429e-05, 'epoch': 0.0}
  0%|          | 11/6500 [03:36<36:31:18, 20.26s/it]                                                      0%|          | 11/6500 [03:36<36:31:18, 20.26s/it]  0%|          | 12/6500 [03:54<35:21:30, 19.62s/it]                                                      0%|          | 12/6500 [03:54<35:21:30, 19.62s/it]  0%|          | 13/6500 [04:12<34:33:20, 19.18s/it]                                                      0%|          | 13/6500 [04:12<34:33:20, 19.18s/it]  0%|          | 14/6500 [04:31<33:59:38, 18.87s/it]                                                      0%|          | 14/6500 [04:31<33:59:38, 18.87s/it]  0%|          | 15/6500 [04:49<33:36:37, 18.66s/it]                                                      0%|          | 15/6500 [04:49<33:36:37, 18.66s/it]  0%|          | 16/6500 [05:07<33:19:55, 18.51s/it]                                                      0%|          | 16/6500 [05:07<33:19:55, 18.51s/it]  0%|          | 17/6500 [05:25<33:20:23, 18.51s/it]          {'loss': 1.3038, 'learning_rate': 9.999868519618736e-05, 'epoch': 0.0}
{'loss': 1.2925, 'learning_rate': 9.999850404634316e-05, 'epoch': 0.0}
{'loss': 1.2458, 'learning_rate': 9.999831120965261e-05, 'epoch': 0.0}
{'loss': 1.2611, 'learning_rate': 9.999810668616086e-05, 'epoch': 0.0}
                                            0%|          | 17/6500 [05:25<33:20:23, 18.51s/it]  0%|          | 18/6500 [05:44<33:09:59, 18.42s/it]                                                      0%|          | 18/6500 [05:44<33:09:59, 18.42s/it]  0%|          | 19/6500 [06:02<33:02:21, 18.35s/it]                                                      0%|          | 19/6500 [06:02<33:02:21, 18.35s/it]  0%|          | 20/6500 [06:20<32:57:34, 18.31s/it]                                                      0%|          | 20/6500 [06:20<32:57:34, 18.31s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.2068217992782593, 'eval_runtime': 5.3131, 'eval_samples_per_second': 4.329, 'eval_steps_per_second': 1.129, 'epoch': 0.0}
                                                      0%|          | 20/6500 [06:25<32:57:34, 18.31s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-20
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-20

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-20
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-20

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-20
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-20/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2267, 'learning_rate': 9.999789047591562e-05, 'epoch': 0.0}
{'loss': 1.2164, 'learning_rate': 9.999766257896749e-05, 'epoch': 0.0}
{'loss': 1.2215, 'learning_rate': 9.999742299536971e-05, 'epoch': 0.0}
{'loss': 1.228, 'learning_rate': 9.999717172517832e-05, 'epoch': 0.0}
{'loss': 1.2641, 'learning_rate': 9.999690876845202e-05, 'epoch': 0.0}
{'loss': 1.2256, 'learning_rate': 9.999663412525226e-05, 'epoch': 0.0}
  0%|          | 21/6500 [07:39<65:52:40, 36.60s/it]                                                      0%|          | 21/6500 [07:39<65:52:40, 36.60s/it]  0%|          | 22/6500 [07:57<55:50:37, 31.03s/it]                                                      0%|          | 22/6500 [07:57<55:50:37, 31.03s/it]  0%|          | 23/6500 [08:15<48:50:40, 27.15s/it]                                                      0%|          | 23/6500 [08:15<48:50:40, 27.15s/it]  0%|          | 24/6500 [08:33<43:56:33, 24.43s/it]                                                      0%|          | 24/6500 [08:33<43:56:33, 24.43s/it]  0%|          | 25/6500 [08:52<40:31:41, 22.53s/it]                                                      0%|          | 25/6500 [08:52<40:31:41, 22.53s/it]  0%|          | 26/6500 [09:10<38:10:20, 21.23s/it]                                                      0%|          | 26/6500 [09:10<38:10:20, 21.23s/it]  0%|          | 27/6500 [09:28<36:29:39, 20.30s/it]          {'loss': 1.1991, 'learning_rate': 9.999634779564329e-05, 'epoch': 0.0}
{'loss': 1.2332, 'learning_rate': 9.999604977969197e-05, 'epoch': 0.0}
{'loss': 1.1923, 'learning_rate': 9.999574007746801e-05, 'epoch': 0.0}
{'loss': 1.1834, 'learning_rate': 9.99954186890438e-05, 'epoch': 0.0}
                                            0%|          | 27/6500 [09:28<36:29:39, 20.30s/it]  0%|          | 28/6500 [09:46<35:19:28, 19.65s/it]                                                      0%|          | 28/6500 [09:46<35:19:28, 19.65s/it]  0%|          | 29/6500 [10:04<34:30:10, 19.19s/it]                                                      0%|          | 29/6500 [10:04<34:30:10, 19.19s/it]  0%|          | 30/6500 [10:22<33:56:43, 18.89s/it]                                                      0%|          | 30/6500 [10:22<33:56:43, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1399738788604736, 'eval_runtime': 5.3179, 'eval_samples_per_second': 4.325, 'eval_steps_per_second': 1.128, 'epoch': 0.0}
                                                      0%|          | 30/6500 [10:28<33:56:43, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-30
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-30the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-30
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-30

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-30
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-30/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-30/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1532, 'learning_rate': 9.999508561449444e-05, 'epoch': 0.0}
{'loss': 1.1245, 'learning_rate': 9.999474085389778e-05, 'epoch': 0.0}
{'loss': 1.202, 'learning_rate': 9.999438440733445e-05, 'epoch': 0.01}
{'loss': 1.146, 'learning_rate': 9.999401627488769e-05, 'epoch': 0.01}
{'loss': 1.1399, 'learning_rate': 9.999363645664363e-05, 'epoch': 0.01}
{'loss': 1.126, 'learning_rate': 9.9993244952691e-05, 'epoch': 0.01}
  0%|          | 31/6500 [11:22<56:12:04, 31.28s/it]                                                      0%|          | 31/6500 [11:22<56:12:04, 31.28s/it]  0%|          | 32/6500 [11:41<49:03:19, 27.30s/it]                                                      0%|          | 32/6500 [11:41<49:03:19, 27.30s/it]  1%|          | 33/6500 [11:59<44:10:41, 24.59s/it]                                                      1%|          | 33/6500 [11:59<44:10:41, 24.59s/it]  1%|          | 34/6500 [12:17<40:39:03, 22.63s/it]                                                      1%|          | 34/6500 [12:17<40:39:03, 22.63s/it]  1%|          | 35/6500 [12:35<38:11:03, 21.26s/it]                                                      1%|          | 35/6500 [12:35<38:11:03, 21.26s/it]  1%|          | 36/6500 [12:53<36:28:29, 20.31s/it]                                                      1%|          | 36/6500 [12:53<36:28:29, 20.31s/it]  1%|          | 37/6500 [13:11<35:17:51, 19.66s/it]          {'loss': 1.1192, 'learning_rate': 9.999284176312134e-05, 'epoch': 0.01}
{'loss': 1.1643, 'learning_rate': 9.999242688802886e-05, 'epoch': 0.01}
{'loss': 1.1766, 'learning_rate': 9.999200032751056e-05, 'epoch': 0.01}
{'loss': 1.0865, 'learning_rate': 9.999156208166614e-05, 'epoch': 0.01}
                                            1%|          | 37/6500 [13:11<35:17:51, 19.66s/it]  1%|          | 38/6500 [13:29<34:28:03, 19.20s/it]                                                      1%|          | 38/6500 [13:29<34:28:03, 19.20s/it]  1%|          | 39/6500 [13:47<33:53:07, 18.88s/it]                                                      1%|          | 39/6500 [13:47<33:53:07, 18.88s/it]  1%|          | 40/6500 [14:06<33:29:15, 18.66s/it]                                                      1%|          | 40/6500 [14:06<33:29:15, 18.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0979751348495483, 'eval_runtime': 5.3134, 'eval_samples_per_second': 4.329, 'eval_steps_per_second': 1.129, 'epoch': 0.01}
                                                      1%|          | 40/6500 [14:11<33:29:15, 18.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-40
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-40

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-40
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-40

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-40
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-40/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1695, 'learning_rate': 9.999111215059804e-05, 'epoch': 0.01}
{'loss': 1.6204, 'learning_rate': 9.999065053441144e-05, 'epoch': 0.01}
{'loss': 1.1302, 'learning_rate': 9.99901772332142e-05, 'epoch': 0.01}
{'loss': 1.0975, 'learning_rate': 9.998969224711698e-05, 'epoch': 0.01}
{'loss': 1.0741, 'learning_rate': 9.998919557623315e-05, 'epoch': 0.01}
{'loss': 1.1113, 'learning_rate': 9.99886872206788e-05, 'epoch': 0.01}
  1%|          | 41/6500 [15:07<56:20:24, 31.40s/it]                                                      1%|          | 41/6500 [15:07<56:20:24, 31.40s/it]  1%|          | 42/6500 [15:25<49:08:39, 27.40s/it]                                                      1%|          | 42/6500 [15:25<49:08:39, 27.40s/it]  1%|          | 43/6500 [15:43<44:07:46, 24.60s/it]                                                      1%|          | 43/6500 [15:43<44:07:46, 24.60s/it]  1%|          | 44/6500 [16:01<40:35:25, 22.63s/it]                                                      1%|          | 44/6500 [16:01<40:35:25, 22.63s/it]  1%|          | 45/6500 [16:19<38:06:37, 21.25s/it]                                                      1%|          | 45/6500 [16:19<38:06:37, 21.25s/it]  1%|          | 46/6500 [16:37<36:22:49, 20.29s/it]                                                      1%|          | 46/6500 [16:37<36:22:49, 20.29s/it]  1%|          | 47/6500 [16:55<35:09:21, 19.61s/it]          {'loss': 1.1321, 'learning_rate': 9.998816718057274e-05, 'epoch': 0.01}
{'loss': 1.0838, 'learning_rate': 9.998763545603654e-05, 'epoch': 0.01}
{'loss': 1.101, 'learning_rate': 9.998709204719447e-05, 'epoch': 0.01}
{'loss': 1.0785, 'learning_rate': 9.998653695417356e-05, 'epoch': 0.01}
                                            1%|          | 47/6500 [16:55<35:09:21, 19.61s/it]  1%|          | 48/6500 [17:13<34:18:55, 19.15s/it]                                                      1%|          | 48/6500 [17:13<34:18:55, 19.15s/it]  1%|          | 49/6500 [17:32<33:56:55, 18.95s/it]                                                      1%|          | 49/6500 [17:32<33:56:55, 18.95s/it]  1%|          | 50/6500 [17:50<33:28:43, 18.69s/it]                                                      1%|          | 50/6500 [17:50<33:28:43, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0663001537322998, 'eval_runtime': 5.2918, 'eval_samples_per_second': 4.346, 'eval_steps_per_second': 1.134, 'epoch': 0.01}
                                                      1%|          | 50/6500 [17:55<33:28:43, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-50
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-50
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-50

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-50

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-50
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-50/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-50/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0802, 'learning_rate': 9.998597017710356e-05, 'epoch': 0.01}
{'loss': 1.0462, 'learning_rate': 9.998539171611697e-05, 'epoch': 0.01}
{'loss': 1.0968, 'learning_rate': 9.998480157134897e-05, 'epoch': 0.01}
{'loss': 1.1304, 'learning_rate': 9.998419974293752e-05, 'epoch': 0.01}
{'loss': 1.0918, 'learning_rate': 9.998358623102327e-05, 'epoch': 0.01}
{'loss': 1.1012, 'learning_rate': 9.998296103574967e-05, 'epoch': 0.01}
  1%|          | 51/6500 [19:26<75:22:39, 42.08s/it]                                                      1%|          | 51/6500 [19:26<75:22:39, 42.08s/it]  1%|          | 52/6500 [19:44<62:24:10, 34.84s/it]                                                      1%|          | 52/6500 [19:44<62:24:10, 34.84s/it]  1%|          | 53/6500 [20:02<53:20:08, 29.78s/it]                                                      1%|          | 53/6500 [20:02<53:20:08, 29.78s/it]  1%|          | 54/6500 [20:20<47:00:06, 26.25s/it]                                                      1%|          | 54/6500 [20:20<47:00:06, 26.25s/it]  1%|          | 55/6500 [20:38<42:35:28, 23.79s/it]                                                      1%|          | 55/6500 [20:38<42:35:28, 23.79s/it]  1%|          | 56/6500 [20:56<39:30:23, 22.07s/it]                                                      1%|          | 56/6500 [20:56<39:30:23, 22.07s/it]  1%|          | 57/6500 [21:14<37:21:46, 20.88s/it]          {'loss': 1.0861, 'learning_rate': 9.99823241572628e-05, 'epoch': 0.01}
{'loss': 1.104, 'learning_rate': 9.998167559571158e-05, 'epoch': 0.01}
{'loss': 1.0914, 'learning_rate': 9.998101535124758e-05, 'epoch': 0.01}
{'loss': 1.074, 'learning_rate': 9.998034342402513e-05, 'epoch': 0.01}
                                            1%|          | 57/6500 [21:14<37:21:46, 20.88s/it]  1%|          | 58/6500 [21:32<35:51:12, 20.04s/it]                                                      1%|          | 58/6500 [21:32<35:51:12, 20.04s/it]  1%|          | 59/6500 [21:51<34:48:10, 19.45s/it]                                                      1%|          | 59/6500 [21:51<34:48:10, 19.45s/it]  1%|          | 60/6500 [22:09<34:04:25, 19.05s/it]                                                      1%|          | 60/6500 [22:09<34:04:25, 19.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0437326431274414, 'eval_runtime': 5.4499, 'eval_samples_per_second': 4.22, 'eval_steps_per_second': 1.101, 'epoch': 0.01}
                                                      1%|          | 60/6500 [22:14<34:04:25, 19.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-60
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-60

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-60
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-60

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-60
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-60/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-60/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0132, 'learning_rate': 9.997965981420128e-05, 'epoch': 0.01}
{'loss': 1.088, 'learning_rate': 9.997896452193584e-05, 'epoch': 0.01}
{'loss': 1.0728, 'learning_rate': 9.997825754739132e-05, 'epoch': 0.01}
{'loss': 1.0308, 'learning_rate': 9.997753889073296e-05, 'epoch': 0.01}
{'loss': 1.0589, 'learning_rate': 9.997680855212877e-05, 'epoch': 0.01}
{'loss': 1.0104, 'learning_rate': 9.997606653174942e-05, 'epoch': 0.01}
  1%|          | 61/6500 [23:19<61:48:14, 34.55s/it]                                                      1%|          | 61/6500 [23:19<61:48:14, 34.55s/it]  1%|          | 62/6500 [23:37<52:53:42, 29.58s/it]                                                      1%|          | 62/6500 [23:37<52:53:42, 29.58s/it]  1%|          | 63/6500 [23:55<46:39:30, 26.09s/it]                                                      1%|          | 63/6500 [23:55<46:39:30, 26.09s/it]  1%|          | 64/6500 [24:13<42:18:05, 23.66s/it]                                                      1%|          | 64/6500 [24:13<42:18:05, 23.66s/it]  1%|          | 65/6500 [24:32<39:21:49, 22.02s/it]                                                      1%|          | 65/6500 [24:32<39:21:49, 22.02s/it]  1%|          | 66/6500 [24:50<37:12:30, 20.82s/it]                                                      1%|          | 66/6500 [24:50<37:12:30, 20.82s/it]  1%|          | 67/6500 [25:08<35:42:17, 19.98s/it]          {'loss': 1.073, 'learning_rate': 9.99753128297684e-05, 'epoch': 0.01}
{'loss': 1.0803, 'learning_rate': 9.997454744636186e-05, 'epoch': 0.01}
{'loss': 1.0422, 'learning_rate': 9.99737703817087e-05, 'epoch': 0.01}
{'loss': 1.0771, 'learning_rate': 9.997298163599056e-05, 'epoch': 0.01}
                                            1%|          | 67/6500 [25:08<35:42:17, 19.98s/it]  1%|          | 68/6500 [25:26<34:39:41, 19.40s/it]                                                      1%|          | 68/6500 [25:26<34:39:41, 19.40s/it]  1%|          | 69/6500 [25:44<33:56:03, 19.00s/it]                                                      1%|          | 69/6500 [25:44<33:56:03, 19.00s/it]  1%|          | 70/6500 [26:02<33:31:30, 18.77s/it]                                                      1%|          | 70/6500 [26:02<33:31:30, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0247374773025513, 'eval_runtime': 5.3099, 'eval_samples_per_second': 4.331, 'eval_steps_per_second': 1.13, 'epoch': 0.01}
                                                      1%|          | 70/6500 [26:07<33:31:30, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-70
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-70
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-70

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-70
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-70/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-70/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0541, 'learning_rate': 9.99721812093918e-05, 'epoch': 0.01}
{'loss': 1.5246, 'learning_rate': 9.99713691020995e-05, 'epoch': 0.01}
{'loss': 1.0225, 'learning_rate': 9.997054531430353e-05, 'epoch': 0.01}
{'loss': 1.0481, 'learning_rate': 9.996970984619641e-05, 'epoch': 0.01}
{'loss': 1.0027, 'learning_rate': 9.996886269797344e-05, 'epoch': 0.01}
{'loss': 1.0466, 'learning_rate': 9.996800386983263e-05, 'epoch': 0.01}
  1%|          | 71/6500 [27:40<75:56:26, 42.52s/it]                                                      1%|          | 71/6500 [27:40<75:56:26, 42.52s/it]  1%|          | 72/6500 [27:58<62:44:31, 35.14s/it]                                                      1%|          | 72/6500 [27:58<62:44:31, 35.14s/it]  1%|          | 73/6500 [28:16<53:29:59, 29.97s/it]                                                      1%|          | 73/6500 [28:16<53:29:59, 29.97s/it]  1%|          | 74/6500 [28:34<47:02:50, 26.36s/it]                                                      1%|          | 74/6500 [28:34<47:02:50, 26.36s/it]  1%|          | 75/6500 [28:52<42:31:35, 23.83s/it]                                                      1%|          | 75/6500 [28:52<42:31:35, 23.83s/it]  1%|          | 76/6500 [29:09<39:22:27, 22.07s/it]                                                      1%|          | 76/6500 [29:09<39:22:27, 22.07s/it]  1%|          | 77/6500 [29:27<37:10:52, 20.84s/it]          {'loss': 1.0401, 'learning_rate': 9.996713336197472e-05, 'epoch': 0.01}
{'loss': 1.0081, 'learning_rate': 9.996625117460318e-05, 'epoch': 0.01}
{'loss': 1.0221, 'learning_rate': 9.996535730792425e-05, 'epoch': 0.01}
{'loss': 1.0205, 'learning_rate': 9.996445176214684e-05, 'epoch': 0.01}
                                            1%|          | 77/6500 [29:27<37:10:52, 20.84s/it]  1%|          | 78/6500 [29:45<35:39:10, 19.99s/it]                                                      1%|          | 78/6500 [29:45<35:39:10, 19.99s/it]  1%|          | 79/6500 [30:03<34:36:46, 19.41s/it]                                                      1%|          | 79/6500 [30:03<34:36:46, 19.41s/it]  1%|          | 80/6500 [30:22<33:53:14, 19.00s/it]                                                      1%|          | 80/6500 [30:22<33:53:14, 19.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0114480257034302, 'eval_runtime': 5.4371, 'eval_samples_per_second': 4.23, 'eval_steps_per_second': 1.104, 'epoch': 0.01}
                                                      1%|          | 80/6500 [30:27<33:53:14, 19.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-80
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-80the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-80
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-80

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-80
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-80/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0041, 'learning_rate': 9.996353453748261e-05, 'epoch': 0.01}
{'loss': 1.0076, 'learning_rate': 9.996260563414597e-05, 'epoch': 0.01}
{'loss': 1.0435, 'learning_rate': 9.996166505235404e-05, 'epoch': 0.01}
{'loss': 1.0491, 'learning_rate': 9.996071279232667e-05, 'epoch': 0.01}
{'loss': 1.0021, 'learning_rate': 9.995974885428647e-05, 'epoch': 0.01}
{'loss': 1.0608, 'learning_rate': 9.995877323845872e-05, 'epoch': 0.01}
  1%|          | 81/6500 [31:39<65:18:05, 36.62s/it]                                                      1%|          | 81/6500 [31:39<65:18:05, 36.62s/it]  1%|▏         | 82/6500 [31:57<55:17:15, 31.01s/it]                                                      1%|▏         | 82/6500 [31:57<55:17:15, 31.01s/it]  1%|▏         | 83/6500 [32:15<48:15:42, 27.08s/it]                                                      1%|▏         | 83/6500 [32:15<48:15:42, 27.08s/it]  1%|▏         | 84/6500 [32:33<43:22:35, 24.34s/it]                                                      1%|▏         | 84/6500 [32:33<43:22:35, 24.34s/it]  1%|▏         | 85/6500 [32:51<39:57:34, 22.42s/it]                                                      1%|▏         | 85/6500 [32:51<39:57:34, 22.42s/it]  1%|▏         | 86/6500 [33:09<37:35:36, 21.10s/it]                                                      1%|▏         | 86/6500 [33:09<37:35:36, 21.10s/it]  1%|▏         | 87/6500 [33:27<35:56:37,{'loss': 1.0429, 'learning_rate': 9.995778594507148e-05, 'epoch': 0.01}
{'loss': 1.041, 'learning_rate': 9.995678697435552e-05, 'epoch': 0.01}
{'loss': 1.027, 'learning_rate': 9.995577632654436e-05, 'epoch': 0.01}
{'loss': 0.9668, 'learning_rate': 9.99547540018742e-05, 'epoch': 0.01}
 20.18s/it]                                                      1%|▏         | 87/6500 [33:27<35:56:37, 20.18s/it]  1%|▏         | 88/6500 [33:45<34:47:27, 19.53s/it]                                                      1%|▏         | 88/6500 [33:45<34:47:27, 19.53s/it]  1%|▏         | 89/6500 [34:03<33:59:41, 19.09s/it]                                                      1%|▏         | 89/6500 [34:03<33:59:41, 19.09s/it]  1%|▏         | 90/6500 [34:21<33:27:55, 18.79s/it]                                                      1%|▏         | 90/6500 [34:21<33:27:55, 18.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9990336298942566, 'eval_runtime': 5.4385, 'eval_samples_per_second': 4.229, 'eval_steps_per_second': 1.103, 'epoch': 0.01}
                                                      1%|▏         | 90/6500 [34:27<33:27:55, 18.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-90
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-90
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-90 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-90

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-90
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-90/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-90/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9723, 'learning_rate': 9.995372000058404e-05, 'epoch': 0.01}
{'loss': 1.0467, 'learning_rate': 9.995267432291555e-05, 'epoch': 0.01}
{'loss': 1.0191, 'learning_rate': 9.995161696911315e-05, 'epoch': 0.01}
{'loss': 0.976, 'learning_rate': 9.9950547939424e-05, 'epoch': 0.01}
{'loss': 1.0053, 'learning_rate': 9.994946723409797e-05, 'epoch': 0.01}
{'loss': 0.9748, 'learning_rate': 9.994837485338766e-05, 'epoch': 0.01}
  1%|▏         | 91/6500 [35:27<58:44:42, 33.00s/it]                                                      1%|▏         | 91/6500 [35:27<58:44:42, 33.00s/it]  1%|▏         | 92/6500 [35:45<50:40:58, 28.47s/it]                                                      1%|▏         | 92/6500 [35:45<50:40:58, 28.47s/it]  1%|▏         | 93/6500 [36:03<45:03:31, 25.32s/it]                                                      1%|▏         | 93/6500 [36:03<45:03:31, 25.32s/it]  1%|▏         | 94/6500 [36:21<41:08:18, 23.12s/it]                                                      1%|▏         | 94/6500 [36:21<41:08:18, 23.12s/it]  1%|▏         | 95/6500 [36:39<38:23:40, 21.58s/it]                                                      1%|▏         | 95/6500 [36:39<38:23:40, 21.58s/it]  1%|▏         | 96/6500 [36:57<36:29:39, 20.52s/it]                                                      1%|▏         | 96/6500 [36:57<36:29:39, 20.52s/it]  1%|▏         | 97/6500 [37:16<35:18{'loss': 1.0584, 'learning_rate': 9.994727079754844e-05, 'epoch': 0.01}
{'loss': 0.9996, 'learning_rate': 9.994615506683834e-05, 'epoch': 0.02}
{'loss': 0.9835, 'learning_rate': 9.994502766151818e-05, 'epoch': 0.02}
{'loss': 1.0287, 'learning_rate': 9.994388858185147e-05, 'epoch': 0.02}
:01, 19.85s/it]                                                      1%|▏         | 97/6500 [37:16<35:18:01, 19.85s/it]  2%|▏         | 98/6500 [37:34<34:20:37, 19.31s/it]                                                      2%|▏         | 98/6500 [37:34<34:20:37, 19.31s/it]  2%|▏         | 99/6500 [37:52<33:41:28, 18.95s/it]                                                      2%|▏         | 99/6500 [37:52<33:41:28, 18.95s/it]  2%|▏         | 100/6500 [38:10<33:14:11, 18.70s/it]                                                       2%|▏         | 100/6500 [38:10<33:14:11, 18.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9892976880073547, 'eval_runtime': 5.3036, 'eval_samples_per_second': 4.337, 'eval_steps_per_second': 1.131, 'epoch': 0.02}
                                                       2%|▏         | 100/6500 [38:15<33:14:11, 18.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-100
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-100
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-100

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-100/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4946, 'learning_rate': 9.994273782810447e-05, 'epoch': 0.02}
{'loss': 0.9922, 'learning_rate': 9.994157540054616e-05, 'epoch': 0.02}
{'loss': 0.9788, 'learning_rate': 9.994040129944824e-05, 'epoch': 0.02}
{'loss': 0.9826, 'learning_rate': 9.993921552508518e-05, 'epoch': 0.02}
{'loss': 0.9585, 'learning_rate': 9.993801807773411e-05, 'epoch': 0.02}
{'loss': 1.0058, 'learning_rate': 9.993680895767495e-05, 'epoch': 0.02}
  2%|▏         | 101/6500 [39:34<67:53:29, 38.19s/it]                                                       2%|▏         | 101/6500 [39:34<67:53:29, 38.19s/it]  2%|▏         | 102/6500 [39:51<57:04:29, 32.11s/it]                                                       2%|▏         | 102/6500 [39:51<57:04:29, 32.11s/it]  2%|▏         | 103/6500 [40:09<49:30:25, 27.86s/it]                                                       2%|▏         | 103/6500 [40:09<49:30:25, 27.86s/it]  2%|▏         | 104/6500 [40:27<44:13:14, 24.89s/it]                                                       2%|▏         | 104/6500 [40:27<44:13:14, 24.89s/it]  2%|▏         | 105/6500 [40:45<40:31:42, 22.82s/it]                                                       2%|▏         | 105/6500 [40:45<40:31:42, 22.82s/it]  2%|▏         | 106/6500 [41:03<37:57:20, 21.37s/it]                                                       2%|▏         | 106/6500 [41:03<37:57:20, 21.37s/it]  2%|▏         | 10{'loss': 0.9873, 'learning_rate': 9.993558816519031e-05, 'epoch': 0.02}
{'loss': 0.9694, 'learning_rate': 9.993435570056556e-05, 'epoch': 0.02}
{'loss': 0.9831, 'learning_rate': 9.993311156408876e-05, 'epoch': 0.02}
{'loss': 0.9598, 'learning_rate': 9.993185575605073e-05, 'epoch': 0.02}
7/6500 [41:21<36:09:51, 20.36s/it]                                                       2%|▏         | 107/6500 [41:21<36:09:51, 20.36s/it]  2%|▏         | 108/6500 [41:39<34:55:39, 19.67s/it]                                                       2%|▏         | 108/6500 [41:39<34:55:39, 19.67s/it]  2%|▏         | 109/6500 [41:57<34:04:44, 19.20s/it]                                                       2%|▏         | 109/6500 [41:57<34:04:44, 19.20s/it]  2%|▏         | 110/6500 [42:16<33:29:04, 18.86s/it]                                                       2%|▏         | 110/6500 [42:16<33:29:04, 18.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.97999507188797, 'eval_runtime': 5.4481, 'eval_samples_per_second': 4.222, 'eval_steps_per_second': 1.101, 'epoch': 0.02}
                                                       2%|▏         | 110/6500 [42:21<33:29:04, 18.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-110
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-110
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-110/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-110/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.973, 'learning_rate': 9.993058827674501e-05, 'epoch': 0.02}
{'loss': 0.9683, 'learning_rate': 9.992930912646787e-05, 'epoch': 0.02}
{'loss': 1.0207, 'learning_rate': 9.99280183055183e-05, 'epoch': 0.02}
{'loss': 1.0081, 'learning_rate': 9.9926715814198e-05, 'epoch': 0.02}
{'loss': 1.0015, 'learning_rate': 9.992540165281145e-05, 'epoch': 0.02}
{'loss': 0.989, 'learning_rate': 9.992407582166581e-05, 'epoch': 0.02}
  2%|▏         | 111/6500 [43:08<51:16:41, 28.89s/it]                                                       2%|▏         | 111/6500 [43:08<51:16:41, 28.89s/it]  2%|▏         | 112/6500 [43:26<45:28:37, 25.63s/it]                                                       2%|▏         | 112/6500 [43:26<45:28:37, 25.63s/it]  2%|▏         | 113/6500 [43:44<41:24:13, 23.34s/it]                                                       2%|▏         | 113/6500 [43:44<41:24:13, 23.34s/it]  2%|▏         | 114/6500 [44:02<38:45:27, 21.85s/it]                                                       2%|▏         | 114/6500 [44:02<38:45:27, 21.85s/it]  2%|▏         | 115/6500 [44:20<36:42:08, 20.69s/it]                                                       2%|▏         | 115/6500 [44:20<36:42:08, 20.69s/it]  2%|▏         | 116/6500 [44:38<35:16:12, 19.89s/it]                                                       2%|▏         | 116/6500 [44:38<35:16:12, 19.89s/it]  2%|▏         | 11{'loss': 1.0042, 'learning_rate': 9.9922738321071e-05, 'epoch': 0.02}
{'loss': 0.9838, 'learning_rate': 9.992138915133965e-05, 'epoch': 0.02}
{'loss': 0.9868, 'learning_rate': 9.992002831278708e-05, 'epoch': 0.02}
{'loss': 0.9401, 'learning_rate': 9.991865580573143e-05, 'epoch': 0.02}
7/6500 [44:56<34:16:32, 19.33s/it]                                                       2%|▏         | 117/6500 [44:56<34:16:32, 19.33s/it]  2%|▏         | 118/6500 [45:14<33:35:42, 18.95s/it]                                                       2%|▏         | 118/6500 [45:14<33:35:42, 18.95s/it]  2%|▏         | 119/6500 [45:32<33:07:36, 18.69s/it]                                                       2%|▏         | 119/6500 [45:32<33:07:36, 18.69s/it]  2%|▏         | 120/6500 [45:50<32:48:17, 18.51s/it]                                                       2%|▏         | 120/6500 [45:50<32:48:17, 18.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.971781849861145, 'eval_runtime': 5.3132, 'eval_samples_per_second': 4.329, 'eval_steps_per_second': 1.129, 'epoch': 0.02}
                                                       2%|▏         | 120/6500 [45:56<32:48:17, 18.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-120
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-120
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-120/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-120


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-120/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-120/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9447, 'learning_rate': 9.99172716304935e-05, 'epoch': 0.02}
{'loss': 1.007, 'learning_rate': 9.991587578739684e-05, 'epoch': 0.02}
{'loss': 0.9674, 'learning_rate': 9.991446827676767e-05, 'epoch': 0.02}
{'loss': 0.9745, 'learning_rate': 9.991304909893506e-05, 'epoch': 0.02}
{'loss': 0.9338, 'learning_rate': 9.991161825423067e-05, 'epoch': 0.02}
{'loss': 0.9863, 'learning_rate': 9.9910175742989e-05, 'epoch': 0.02}
  2%|▏         | 121/6500 [47:11<65:38:38, 37.05s/it]                                                       2%|▏         | 121/6500 [47:11<65:38:38, 37.05s/it]  2%|▏         | 122/6500 [47:29<55:29:13, 31.32s/it]                                                       2%|▏         | 122/6500 [47:29<55:29:13, 31.32s/it]  2%|▏         | 123/6500 [47:47<48:22:58, 27.31s/it]                                                       2%|▏         | 123/6500 [47:47<48:22:58, 27.31s/it]  2%|▏         | 124/6500 [48:05<43:27:16, 24.54s/it]                                                       2%|▏         | 124/6500 [48:05<43:27:16, 24.54s/it]  2%|▏         | 125/6500 [48:23<39:58:07, 22.57s/it]                                                       2%|▏         | 125/6500 [48:23<39:58:07, 22.57s/it]  2%|▏         | 126/6500 [48:41<37:31:58, 21.20s/it]                                                       2%|▏         | 126/6500 [48:41<37:31:58, 21.20s/it]  2%|▏         | 12{'loss': 1.0122, 'learning_rate': 9.990872156554721e-05, 'epoch': 0.02}
{'loss': 0.983, 'learning_rate': 9.990725572224521e-05, 'epoch': 0.02}
{'loss': 0.9459, 'learning_rate': 9.990577821342561e-05, 'epoch': 0.02}
{'loss': 0.9878, 'learning_rate': 9.99042890394338e-05, 'epoch': 0.02}
7/6500 [48:59<35:50:20, 20.24s/it]                                                       2%|▏         | 127/6500 [48:59<35:50:20, 20.24s/it]  2%|▏         | 128/6500 [49:17<34:40:34, 19.59s/it]                                                       2%|▏         | 128/6500 [49:17<34:40:34, 19.59s/it]  2%|▏         | 129/6500 [49:35<33:52:18, 19.14s/it]                                                       2%|▏         | 129/6500 [49:35<33:52:18, 19.14s/it]  2%|▏         | 130/6500 [49:53<33:25:50, 18.89s/it]                                                       2%|▏         | 130/6500 [49:53<33:25:50, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9641287326812744, 'eval_runtime': 5.3053, 'eval_samples_per_second': 4.335, 'eval_steps_per_second': 1.131, 'epoch': 0.02}
                                                       2%|▏         | 130/6500 [49:59<33:25:50, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-130
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-130I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-130
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-130
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-130/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-130/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4522, 'learning_rate': 9.990278820061783e-05, 'epoch': 0.02}
{'loss': 0.9653, 'learning_rate': 9.990127569732855e-05, 'epoch': 0.02}
{'loss': 0.947, 'learning_rate': 9.989975152991947e-05, 'epoch': 0.02}
{'loss': 0.9095, 'learning_rate': 9.989821569874687e-05, 'epoch': 0.02}
{'loss': 0.9339, 'learning_rate': 9.989666820416974e-05, 'epoch': 0.02}
{'loss': 1.0065, 'learning_rate': 9.989510904654979e-05, 'epoch': 0.02}
  2%|▏         | 131/6500 [51:01<59:21:40, 33.55s/it]                                                       2%|▏         | 131/6500 [51:01<59:21:40, 33.55s/it]  2%|▏         | 132/6500 [51:19<51:05:16, 28.88s/it]                                                       2%|▏         | 132/6500 [51:19<51:05:16, 28.88s/it]  2%|▏         | 133/6500 [51:37<45:17:40, 25.61s/it]                                                       2%|▏         | 133/6500 [51:37<45:17:40, 25.61s/it]  2%|▏         | 134/6500 [51:55<41:14:39, 23.32s/it]                                                       2%|▏         | 134/6500 [51:55<41:14:39, 23.32s/it]  2%|▏         | 135/6500 [52:13<38:24:32, 21.72s/it]                                                       2%|▏         | 135/6500 [52:13<38:24:32, 21.72s/it]  2%|▏         | 136/6500 [52:31<36:26:01, 20.61s/it]                                                       2%|▏         | 136/6500 [52:31<36:26:01, 20.61s/it]  2%|▏         | 13{'loss': 0.9382, 'learning_rate': 9.989353822625146e-05, 'epoch': 0.02}
{'loss': 0.9334, 'learning_rate': 9.989195574364194e-05, 'epoch': 0.02}
{'loss': 0.9454, 'learning_rate': 9.98903615990911e-05, 'epoch': 0.02}
{'loss': 0.9289, 'learning_rate': 9.988875579297159e-05, 'epoch': 0.02}
7/6500 [52:49<35:03:43, 19.84s/it]                                                       2%|▏         | 137/6500 [52:49<35:03:43, 19.84s/it]  2%|▏         | 138/6500 [53:07<34:06:43, 19.30s/it]                                                       2%|▏         | 138/6500 [53:07<34:06:43, 19.30s/it]  2%|▏         | 139/6500 [53:25<33:27:57, 18.94s/it]                                                       2%|▏         | 139/6500 [53:25<33:27:57, 18.94s/it]  2%|▏         | 140/6500 [53:43<33:01:21, 18.69s/it]                                                       2%|▏         | 140/6500 [53:43<33:01:21, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.957303524017334, 'eval_runtime': 5.3239, 'eval_samples_per_second': 4.32, 'eval_steps_per_second': 1.127, 'epoch': 0.02}
                                                       2%|▏         | 140/6500 [53:49<33:01:21, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-140the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-140
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-140/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-140/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9252, 'learning_rate': 9.988713832565874e-05, 'epoch': 0.02}
{'loss': 0.9551, 'learning_rate': 9.988550919753061e-05, 'epoch': 0.02}
{'loss': 0.9953, 'learning_rate': 9.988386840896803e-05, 'epoch': 0.02}
{'loss': 0.9558, 'learning_rate': 9.98822159603545e-05, 'epoch': 0.02}
{'loss': 0.9813, 'learning_rate': 9.988055185207628e-05, 'epoch': 0.02}
{'loss': 0.979, 'learning_rate': 9.987887608452235e-05, 'epoch': 0.02}
  2%|▏         | 141/6500 [55:08<67:57:06, 38.47s/it]                                                       2%|▏         | 141/6500 [55:08<67:57:06, 38.47s/it]  2%|▏         | 142/6500 [55:26<57:08:03, 32.35s/it]                                                       2%|▏         | 142/6500 [55:26<57:08:03, 32.35s/it]  2%|▏         | 143/6500 [55:44<49:29:49, 28.03s/it]                                                       2%|▏         | 143/6500 [55:44<49:29:49, 28.03s/it]  2%|▏         | 144/6500 [56:02<44:09:36, 25.01s/it]                                                       2%|▏         | 144/6500 [56:02<44:09:36, 25.01s/it]  2%|▏         | 145/6500 [56:20<40:26:37, 22.91s/it]                                                       2%|▏         | 145/6500 [56:20<40:26:37, 22.91s/it]  2%|▏         | 146/6500 [56:38<38:04:52, 21.58s/it]                                                       2%|▏         | 146/6500 [56:38<38:04:52, 21.58s/it]  2%|▏         | 14{'loss': 0.9659, 'learning_rate': 9.98771886580844e-05, 'epoch': 0.02}
{'loss': 0.9664, 'learning_rate': 9.987548957315685e-05, 'epoch': 0.02}
{'loss': 0.9503, 'learning_rate': 9.987377883013687e-05, 'epoch': 0.02}
{'loss': 0.9155, 'learning_rate': 9.987205642942432e-05, 'epoch': 0.02}
7/6500 [56:56<36:13:04, 20.52s/it]                                                       2%|▏         | 147/6500 [56:56<36:13:04, 20.52s/it]  2%|▏         | 148/6500 [57:14<34:54:40, 19.79s/it]                                                       2%|▏         | 148/6500 [57:14<34:54:40, 19.79s/it]  2%|▏         | 149/6500 [57:33<34:00:46, 19.28s/it]                                                       2%|▏         | 149/6500 [57:33<34:00:46, 19.28s/it]  2%|▏         | 150/6500 [57:51<33:23:12, 18.93s/it]                                                       2%|▏         | 150/6500 [57:51<33:23:12, 18.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9508768916130066, 'eval_runtime': 5.308, 'eval_samples_per_second': 4.333, 'eval_steps_per_second': 1.13, 'epoch': 0.02}
                                                       2%|▏         | 150/6500 [57:56<33:23:12, 18.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-150/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-150


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-150/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9616, 'learning_rate': 9.98703223714218e-05, 'epoch': 0.02}
{'loss': 0.9585, 'learning_rate': 9.986857665653466e-05, 'epoch': 0.02}
{'loss': 0.9251, 'learning_rate': 9.986681928517092e-05, 'epoch': 0.02}
{'loss': 0.9447, 'learning_rate': 9.986505025774138e-05, 'epoch': 0.02}
{'loss': 0.9008, 'learning_rate': 9.986326957465951e-05, 'epoch': 0.02}
{'loss': 0.9728, 'learning_rate': 9.986147723634156e-05, 'epoch': 0.02}
  2%|▏         | 151/6500 [58:56<58:01:06, 32.90s/it]                                                       2%|▏         | 151/6500 [58:56<58:01:06, 32.90s/it]  2%|▏         | 152/6500 [59:14<50:06:20, 28.42s/it]                                                       2%|▏         | 152/6500 [59:14<50:06:20, 28.42s/it]  2%|▏         | 153/6500 [59:32<44:34:22, 25.28s/it]                                                       2%|▏         | 153/6500 [59:32<44:34:22, 25.28s/it]  2%|▏         | 154/6500 [59:50<40:45:53, 23.13s/it]                                                       2%|▏         | 154/6500 [59:50<40:45:53, 23.13s/it]  2%|▏         | 155/6500 [1:00:08<38:04:04, 21.60s/it]                                                         2%|▏         | 155/6500 [1:00:08<38:04:04, 21.60s/it]  2%|▏         | 156/6500 [1:00:26<36:11:28, 20.54s/it]                                                         2%|▏         | 156/6500 [1:00:26<36:11:28, 20.54s/it]  2%|▏ {'loss': 0.9873, 'learning_rate': 9.985967324320646e-05, 'epoch': 0.02}
{'loss': 0.9211, 'learning_rate': 9.985785759567591e-05, 'epoch': 0.02}
{'loss': 0.9649, 'learning_rate': 9.985603029417427e-05, 'epoch': 0.02}
{'loss': 0.9383, 'learning_rate': 9.985419133912869e-05, 'epoch': 0.02}
        | 157/6500 [1:00:44<34:52:45, 19.80s/it]                                                         2%|▏         | 157/6500 [1:00:44<34:52:45, 19.80s/it]  2%|▏         | 158/6500 [1:01:02<33:57:46, 19.28s/it]                                                         2%|▏         | 158/6500 [1:01:02<33:57:46, 19.28s/it]  2%|▏         | 159/6500 [1:01:21<33:20:21, 18.93s/it]                                                         2%|▏         | 159/6500 [1:01:21<33:20:21, 18.93s/it]  2%|▏         | 160/6500 [1:01:39<32:54:08, 18.68s/it]                                                         2%|▏         | 160/6500 [1:01:39<32:54:08, 18.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9433439373970032, 'eval_runtime': 5.3095, 'eval_samples_per_second': 4.332, 'eval_steps_per_second': 1.13, 'epoch': 0.02}
                                                         2%|▏         | 160/6500 [1:01:44<32:54:08, 18.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-160
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-160I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-160
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-160
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-160/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4169, 'learning_rate': 9.9852340730969e-05, 'epoch': 0.02}
{'loss': 0.9209, 'learning_rate': 9.985047847012776e-05, 'epoch': 0.02}
{'loss': 0.9294, 'learning_rate': 9.984860455704028e-05, 'epoch': 0.03}
{'loss': 0.8846, 'learning_rate': 9.984671899214457e-05, 'epoch': 0.03}
{'loss': 0.9435, 'learning_rate': 9.984482177588138e-05, 'epoch': 0.03}
{'loss': 0.966, 'learning_rate': 9.984291290869415e-05, 'epoch': 0.03}
  2%|▏         | 161/6500 [1:03:05<68:44:25, 39.04s/it]                                                         2%|▏         | 161/6500 [1:03:05<68:44:25, 39.04s/it]  2%|▏         | 162/6500 [1:03:23<57:47:19, 32.82s/it]                                                         2%|▏         | 162/6500 [1:03:23<57:47:19, 32.82s/it]  3%|▎         | 163/6500 [1:03:42<49:57:24, 28.38s/it]                                                         3%|▎         | 163/6500 [1:03:42<49:57:24, 28.38s/it]  3%|▎         | 164/6500 [1:04:00<44:35:02, 25.33s/it]                                                         3%|▎         | 164/6500 [1:04:00<44:35:02, 25.33s/it]  3%|▎         | 165/6500 [1:04:18<40:44:40, 23.15s/it]                                                         3%|▎         | 165/6500 [1:04:18<40:44:40, 23.15s/it]  3%|▎         | 166/6500 [1:04:36<38:00:58, 21.61s/it]                                                         3%|▎         | 166/6500 [1:04:36<38:00:{'loss': 0.8878, 'learning_rate': 9.984099239102909e-05, 'epoch': 0.03}
{'loss': 0.9289, 'learning_rate': 9.983906022333507e-05, 'epoch': 0.03}
{'loss': 0.9021, 'learning_rate': 9.983711640606377e-05, 'epoch': 0.03}
{'loss': 0.9116, 'learning_rate': 9.983516093966952e-05, 'epoch': 0.03}
58, 21.61s/it]  3%|▎         | 167/6500 [1:04:54<36:21:09, 20.66s/it]                                                         3%|▎         | 167/6500 [1:04:54<36:21:09, 20.66s/it]  3%|▎         | 168/6500 [1:05:13<35:11:21, 20.01s/it]                                                         3%|▎         | 168/6500 [1:05:13<35:11:21, 20.01s/it]  3%|▎         | 169/6500 [1:05:31<34:22:46, 19.55s/it]                                                         3%|▎         | 169/6500 [1:05:31<34:22:46, 19.55s/it]  3%|▎         | 170/6500 [1:05:49<33:42:26, 19.17s/it]                                                         3%|▎         | 170/6500 [1:05:49<33:42:26, 19.17s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.939308762550354, 'eval_runtime': 5.7123, 'eval_samples_per_second': 4.026, 'eval_steps_per_second': 1.05, 'epoch': 0.03}
                                                         3%|▎         | 170/6500 [1:05:55<33:42:26, 19.17s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-170
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-170/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-170/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9079, 'learning_rate': 9.98331938246094e-05, 'epoch': 0.03}
{'loss': 0.9465, 'learning_rate': 9.983121506134322e-05, 'epoch': 0.03}
{'loss': 0.9505, 'learning_rate': 9.98292246503335e-05, 'epoch': 0.03}
{'loss': 0.9363, 'learning_rate': 9.982722259204548e-05, 'epoch': 0.03}
{'loss': 0.9496, 'learning_rate': 9.982520888694713e-05, 'epoch': 0.03}
{'loss': 0.974, 'learning_rate': 9.982318353550915e-05, 'epoch': 0.03}
  3%|▎         | 171/6500 [1:06:57<59:25:22, 33.80s/it]                                                         3%|▎         | 171/6500 [1:06:57<59:25:22, 33.80s/it]  3%|▎         | 172/6500 [1:07:15<51:03:36, 29.05s/it]                                                         3%|▎         | 172/6500 [1:07:15<51:03:36, 29.05s/it]  3%|▎         | 173/6500 [1:07:33<45:12:13, 25.72s/it]                                                         3%|▎         | 173/6500 [1:07:33<45:12:13, 25.72s/it]  3%|▎         | 174/6500 [1:07:51<41:06:32, 23.39s/it]                                                         3%|▎         | 174/6500 [1:07:51<41:06:32, 23.39s/it]  3%|▎         | 175/6500 [1:08:09<38:17:47, 21.80s/it]                                                         3%|▎         | 175/6500 [1:08:09<38:17:47, 21.80s/it]  3%|▎         | 176/6500 [1:08:27<36:18:37, 20.67s/it]                                                         3%|▎         | 176/6500 [1:08:27<36:18:{'loss': 0.9315, 'learning_rate': 9.982114653820494e-05, 'epoch': 0.03}
{'loss': 0.932, 'learning_rate': 9.981909789551065e-05, 'epoch': 0.03}
{'loss': 0.8945, 'learning_rate': 9.981703760790515e-05, 'epoch': 0.03}
{'loss': 0.8985, 'learning_rate': 9.981496567586997e-05, 'epoch': 0.03}
37, 20.67s/it]  3%|▎         | 177/6500 [1:08:45<34:55:24, 19.88s/it]                                                         3%|▎         | 177/6500 [1:08:45<34:55:24, 19.88s/it]  3%|▎         | 178/6500 [1:09:04<34:10:51, 19.46s/it]                                                         3%|▎         | 178/6500 [1:09:04<34:10:51, 19.46s/it]  3%|▎         | 179/6500 [1:09:22<33:27:41, 19.06s/it]                                                         3%|▎         | 179/6500 [1:09:22<33:27:41, 19.06s/it]  3%|▎         | 180/6500 [1:09:40<32:58:07, 18.78s/it]                                                         3%|▎         | 180/6500 [1:09:40<32:58:07, 18.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9333817958831787, 'eval_runtime': 5.3204, 'eval_samples_per_second': 4.323, 'eval_steps_per_second': 1.128, 'epoch': 0.03}
                                                         3%|▎         | 180/6500 [1:09:46<32:58:07, 18.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-180
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-180
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-180

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-180/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9487, 'learning_rate': 9.981288209988946e-05, 'epoch': 0.03}
{'loss': 0.9314, 'learning_rate': 9.981078688045062e-05, 'epoch': 0.03}
{'loss': 0.923, 'learning_rate': 9.980868001804322e-05, 'epoch': 0.03}
{'loss': 0.8999, 'learning_rate': 9.980656151315969e-05, 'epoch': 0.03}
{'loss': 0.9131, 'learning_rate': 9.980443136629525e-05, 'epoch': 0.03}
{'loss': 0.9634, 'learning_rate': 9.980228957794777e-05, 'epoch': 0.03}
  3%|▎         | 181/6500 [1:10:23<45:31:55, 25.94s/it]                                                         3%|▎         | 181/6500 [1:10:23<45:31:55, 25.94s/it]  3%|▎         | 182/6500 [1:10:41<41:22:34, 23.58s/it]                                                         3%|▎         | 182/6500 [1:10:41<41:22:34, 23.58s/it]  3%|▎         | 183/6500 [1:10:59<38:27:23, 21.92s/it]                                                         3%|▎         | 183/6500 [1:10:59<38:27:23, 21.92s/it]  3%|▎         | 184/6500 [1:11:17<36:23:58, 20.75s/it]                                                         3%|▎         | 184/6500 [1:11:17<36:23:58, 20.75s/it]  3%|▎         | 185/6500 [1:11:35<34:58:19, 19.94s/it]                                                         3%|▎         | 185/6500 [1:11:35<34:58:19, 19.94s/it]  3%|▎         | 186/6500 [1:11:53<33:59:03, 19.38s/it]                                                         3%|▎         | 186/6500 [1:11:53<33:59:{'loss': 0.9286, 'learning_rate': 9.980013614861792e-05, 'epoch': 0.03}
{'loss': 0.8886, 'learning_rate': 9.979797107880903e-05, 'epoch': 0.03}
{'loss': 0.9362, 'learning_rate': 9.979579436902717e-05, 'epoch': 0.03}
{'loss': 1.4119, 'learning_rate': 9.979360601978116e-05, 'epoch': 0.03}
03, 19.38s/it]  3%|▎         | 187/6500 [1:12:11<33:18:19, 18.99s/it]                                                         3%|▎         | 187/6500 [1:12:11<33:18:19, 18.99s/it]  3%|▎         | 188/6500 [1:12:29<32:49:55, 18.73s/it]                                                         3%|▎         | 188/6500 [1:12:29<32:49:55, 18.73s/it]  3%|▎         | 189/6500 [1:12:47<32:31:06, 18.55s/it]                                                         3%|▎         | 189/6500 [1:12:47<32:31:06, 18.55s/it]  3%|▎         | 190/6500 [1:13:06<32:17:39, 18.42s/it]                                                         3%|▎         | 190/6500 [1:13:06<32:17:39, 18.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9281346797943115, 'eval_runtime': 5.3166, 'eval_samples_per_second': 4.326, 'eval_steps_per_second': 1.129, 'epoch': 0.03}
                                                         3%|▎         | 190/6500 [1:13:11<32:17:39, 18.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-190
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-190
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-190
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-190
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-190/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9202, 'learning_rate': 9.979140603158248e-05, 'epoch': 0.03}
{'loss': 0.8812, 'learning_rate': 9.978919440494539e-05, 'epoch': 0.03}
{'loss': 0.9119, 'learning_rate': 9.978697114038681e-05, 'epoch': 0.03}
{'loss': 0.8688, 'learning_rate': 9.978473623842644e-05, 'epoch': 0.03}
{'loss': 0.9309, 'learning_rate': 9.978248969958668e-05, 'epoch': 0.03}
{'loss': 0.9112, 'learning_rate': 9.978023152439263e-05, 'epoch': 0.03}
  3%|▎         | 191/6500 [1:14:45<74:38:51, 42.60s/it]                                                         3%|▎         | 191/6500 [1:14:45<74:38:51, 42.60s/it]  3%|▎         | 192/6500 [1:15:03<61:41:07, 35.20s/it]                                                         3%|▎         | 192/6500 [1:15:03<61:41:07, 35.20s/it]  3%|▎         | 193/6500 [1:15:20<52:36:09, 30.03s/it]                                                         3%|▎         | 193/6500 [1:15:20<52:36:09, 30.03s/it]  3%|▎         | 194/6500 [1:15:39<46:41:14, 26.65s/it]                                                         3%|▎         | 194/6500 [1:15:39<46:41:14, 26.65s/it]  3%|▎         | 195/6500 [1:15:57<42:08:23, 24.06s/it]                                                         3%|▎         | 195/6500 [1:15:57<42:08:23, 24.06s/it]  3%|▎         | 196/6500 [1:16:15<38:58:27, 22.26s/it]                                                         3%|▎         | 196/6500 [1:16:15<38:58:{'loss': 0.887, 'learning_rate': 9.977796171337212e-05, 'epoch': 0.03}
{'loss': 0.8888, 'learning_rate': 9.977568026705574e-05, 'epoch': 0.03}
{'loss': 0.8878, 'learning_rate': 9.977338718597672e-05, 'epoch': 0.03}
{'loss': 0.8832, 'learning_rate': 9.977108247067108e-05, 'epoch': 0.03}
27, 22.26s/it]  3%|▎         | 197/6500 [1:16:33<36:46:26, 21.00s/it]                                                         3%|▎         | 197/6500 [1:16:33<36:46:26, 21.00s/it]  3%|▎         | 198/6500 [1:16:51<35:14:11, 20.13s/it]                                                         3%|▎         | 198/6500 [1:16:51<35:14:11, 20.13s/it]  3%|▎         | 199/6500 [1:17:10<34:10:42, 19.53s/it]                                                         3%|▎         | 199/6500 [1:17:10<34:10:42, 19.53s/it]  3%|▎         | 200/6500 [1:17:28<33:26:49, 19.11s/it]                                                         3%|▎         | 200/6500 [1:17:28<33:26:49, 19.11s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9252139925956726, 'eval_runtime': 5.3275, 'eval_samples_per_second': 4.317, 'eval_steps_per_second': 1.126, 'epoch': 0.03}
                                                         3%|▎         | 200/6500 [1:17:33<33:26:49, 19.11s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-200/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-200

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-200/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-200/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8918, 'learning_rate': 9.976876612167752e-05, 'epoch': 0.03}
{'loss': 0.9455, 'learning_rate': 9.976643813953747e-05, 'epoch': 0.03}
{'loss': 0.9171, 'learning_rate': 9.976409852479511e-05, 'epoch': 0.03}
{'loss': 0.9266, 'learning_rate': 9.976174727799728e-05, 'epoch': 0.03}
{'loss': 0.9262, 'learning_rate': 9.975938439969357e-05, 'epoch': 0.03}
{'loss': 0.9148, 'learning_rate': 9.975700989043633e-05, 'epoch': 0.03}
  3%|▎         | 201/6500 [1:18:19<50:32:42, 28.89s/it]                                                         3%|▎         | 201/6500 [1:18:19<50:32:42, 28.89s/it]  3%|▎         | 202/6500 [1:18:37<44:50:26, 25.63s/it]                                                         3%|▎         | 202/6500 [1:18:37<44:50:26, 25.63s/it]  3%|▎         | 203/6500 [1:18:55<40:50:44, 23.35s/it]                                                         3%|▎         | 203/6500 [1:18:55<40:50:44, 23.35s/it]  3%|▎         | 204/6500 [1:19:14<38:03:23, 21.76s/it]                                                         3%|▎         | 204/6500 [1:19:14<38:03:23, 21.76s/it]  3%|▎         | 205/6500 [1:19:32<36:06:54, 20.65s/it]                                                         3%|▎         | 205/6500 [1:19:32<36:06:54, 20.65s/it]  3%|▎         | 206/6500 [1:19:50<34:45:25, 19.88s/it]                                                         3%|▎         | 206/6500 [1:19:50<34:45:{'loss': 0.9179, 'learning_rate': 9.975462375078053e-05, 'epoch': 0.03}
{'loss': 0.9112, 'learning_rate': 9.975222598128394e-05, 'epoch': 0.03}
{'loss': 0.8526, 'learning_rate': 9.974981658250704e-05, 'epoch': 0.03}
{'loss': 0.9086, 'learning_rate': 9.974739555501298e-05, 'epoch': 0.03}
25, 19.88s/it]  3%|▎         | 207/6500 [1:20:08<33:49:01, 19.35s/it]                                                         3%|▎         | 207/6500 [1:20:08<33:49:01, 19.35s/it]  3%|▎         | 208/6500 [1:20:26<33:10:30, 18.98s/it]                                                         3%|▎         | 208/6500 [1:20:26<33:10:30, 18.98s/it]  3%|▎         | 209/6500 [1:20:44<32:43:31, 18.73s/it]                                                         3%|▎         | 209/6500 [1:20:44<32:43:31, 18.73s/it]  3%|▎         | 210/6500 [1:21:02<32:25:01, 18.55s/it]                                                         3%|▎         | 210/6500 [1:21:02<32:25:01, 18.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9211086630821228, 'eval_runtime': 5.321, 'eval_samples_per_second': 4.323, 'eval_steps_per_second': 1.128, 'epoch': 0.03}
                                                         3%|▎         | 210/6500 [1:21:08<32:25:01, 18.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-210 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-210/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9292, 'learning_rate': 9.974496289936769e-05, 'epoch': 0.03}
{'loss': 0.891, 'learning_rate': 9.974251861613977e-05, 'epoch': 0.03}
{'loss': 0.9009, 'learning_rate': 9.974006270590058e-05, 'epoch': 0.03}
{'loss': 0.8662, 'learning_rate': 9.973759516922414e-05, 'epoch': 0.03}
{'loss': 0.9387, 'learning_rate': 9.973511600668724e-05, 'epoch': 0.03}
{'loss': 0.9356, 'learning_rate': 9.973262521886937e-05, 'epoch': 0.03}
  3%|▎         | 211/6500 [1:22:11<58:39:08, 33.57s/it]                                                         3%|▎         | 211/6500 [1:22:11<58:39:08, 33.57s/it]  3%|▎         | 212/6500 [1:22:29<50:32:20, 28.93s/it]                                                         3%|▎         | 212/6500 [1:22:29<50:32:20, 28.93s/it]  3%|▎         | 213/6500 [1:22:47<44:53:40, 25.71s/it]                                                         3%|▎         | 213/6500 [1:22:47<44:53:40, 25.71s/it]  3%|▎         | 214/6500 [1:23:05<40:51:04, 23.40s/it]                                                         3%|▎         | 214/6500 [1:23:05<40:51:04, 23.40s/it]  3%|▎         | 215/6500 [1:23:23<38:02:15, 21.79s/it]                                                         3%|▎         | 215/6500 [1:23:23<38:02:15, 21.79s/it]  3%|▎         | 216/6500 [1:23:41<36:04:46, 20.67s/it]                                                         3%|▎         | 216/6500 [1:23:41<36:04:{'loss': 0.898, 'learning_rate': 9.973012280635273e-05, 'epoch': 0.03}
{'loss': 0.9044, 'learning_rate': 9.972760876972226e-05, 'epoch': 0.03}
{'loss': 0.8921, 'learning_rate': 9.972508310956557e-05, 'epoch': 0.03}
{'loss': 1.3793, 'learning_rate': 9.972254582647305e-05, 'epoch': 0.03}
46, 20.67s/it]  3%|▎         | 217/6500 [1:23:59<34:43:28, 19.90s/it]                                                         3%|▎         | 217/6500 [1:23:59<34:43:28, 19.90s/it]  3%|▎         | 218/6500 [1:24:18<33:50:55, 19.40s/it]                                                         3%|▎         | 218/6500 [1:24:18<33:50:55, 19.40s/it]  3%|▎         | 219/6500 [1:24:36<33:10:55, 19.02s/it]                                                         3%|▎         | 219/6500 [1:24:36<33:10:55, 19.02s/it]  3%|▎         | 220/6500 [1:24:54<32:42:04, 18.75s/it]                                                         3%|▎         | 220/6500 [1:24:54<32:42:04, 18.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.914512574672699, 'eval_runtime': 5.422, 'eval_samples_per_second': 4.242, 'eval_steps_per_second': 1.107, 'epoch': 0.03}
                                                         3%|▎         | 220/6500 [1:24:59<32:42:04, 18.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-220
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-220
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-220/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8903, 'learning_rate': 9.971999692103777e-05, 'epoch': 0.03}
{'loss': 0.8909, 'learning_rate': 9.971743639385551e-05, 'epoch': 0.03}
{'loss': 0.8354, 'learning_rate': 9.971486424552477e-05, 'epoch': 0.03}
{'loss': 0.8783, 'learning_rate': 9.971228047664677e-05, 'epoch': 0.03}
{'loss': 0.9475, 'learning_rate': 9.970968508782549e-05, 'epoch': 0.03}
{'loss': 0.8578, 'learning_rate': 9.970707807966755e-05, 'epoch': 0.03}
  3%|▎         | 221/6500 [1:26:27<71:54:38, 41.23s/it]                                                         3%|▎         | 221/6500 [1:26:27<71:54:38, 41.23s/it]  3%|▎         | 222/6500 [1:26:45<59:44:03, 34.25s/it]                                                         3%|▎         | 222/6500 [1:26:45<59:44:03, 34.25s/it]  3%|▎         | 223/6500 [1:27:03<51:12:01, 29.36s/it]                                                         3%|▎         | 223/6500 [1:27:03<51:12:01, 29.36s/it]  3%|▎         | 224/6500 [1:27:21<45:14:16, 25.95s/it]                                                         3%|▎         | 224/6500 [1:27:21<45:14:16, 25.95s/it]  3%|▎         | 225/6500 [1:27:39<41:04:33, 23.57s/it]                                                         3%|▎         | 225/6500 [1:27:39<41:04:33, 23.57s/it]  3%|▎         | 226/6500 [1:27:57<38:10:59, 21.91s/it]                                                         3%|▎         | 226/6500 [1:27:57<38:10:{'loss': 0.8727, 'learning_rate': 9.970445945278233e-05, 'epoch': 0.03}
{'loss': 0.8654, 'learning_rate': 9.970182920778193e-05, 'epoch': 0.04}
{'loss': 0.865, 'learning_rate': 9.969918734528114e-05, 'epoch': 0.04}
{'loss': 0.8715, 'learning_rate': 9.969653386589748e-05, 'epoch': 0.04}
59, 21.91s/it]  3%|▎         | 227/6500 [1:28:16<36:22:28, 20.87s/it]                                                         3%|▎         | 227/6500 [1:28:16<36:22:28, 20.87s/it]  4%|▎         | 228/6500 [1:28:34<34:54:56, 20.04s/it]                                                         4%|▎         | 228/6500 [1:28:34<34:54:56, 20.04s/it]  4%|▎         | 229/6500 [1:28:52<33:54:06, 19.46s/it]                                                         4%|▎         | 229/6500 [1:28:52<33:54:06, 19.46s/it]  4%|▎         | 230/6500 [1:29:10<33:12:30, 19.07s/it]                                                         4%|▎         | 230/6500 [1:29:10<33:12:30, 19.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9119396805763245, 'eval_runtime': 5.4695, 'eval_samples_per_second': 4.205, 'eval_steps_per_second': 1.097, 'epoch': 0.04}
                                                         4%|▎         | 230/6500 [1:29:16<33:12:30, 19.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-230/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-230

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-230/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8896, 'learning_rate': 9.96938687702512e-05, 'epoch': 0.04}
{'loss': 0.9091, 'learning_rate': 9.969119205896523e-05, 'epoch': 0.04}
{'loss': 0.8653, 'learning_rate': 9.968850373266522e-05, 'epoch': 0.04}
{'loss': 0.9275, 'learning_rate': 9.968580379197961e-05, 'epoch': 0.04}
{'loss': 0.9063, 'learning_rate': 9.968309223753944e-05, 'epoch': 0.04}
{'loss': 0.8877, 'learning_rate': 9.968036906997855e-05, 'epoch': 0.04}
  4%|▎         | 231/6500 [1:30:01<49:59:35, 28.71s/it]                                                         4%|▎         | 231/6500 [1:30:01<49:59:35, 28.71s/it]  4%|▎         | 232/6500 [1:30:20<44:25:31, 25.52s/it]                                                         4%|▎         | 232/6500 [1:30:20<44:25:31, 25.52s/it]  4%|▎         | 233/6500 [1:30:38<40:32:32, 23.29s/it]                                                         4%|▎         | 233/6500 [1:30:38<40:32:32, 23.29s/it]  4%|▎         | 234/6500 [1:30:56<37:49:20, 21.73s/it]                                                         4%|▎         | 234/6500 [1:30:56<37:49:20, 21.73s/it]  4%|▎         | 235/6500 [1:31:14<35:55:11, 20.64s/it]                                                         4%|▎         | 235/6500 [1:31:14<35:55:11, 20.64s/it]  4%|▎         | 236/6500 [1:31:32<34:35:59, 19.88s/it]                                                         4%|▎         | 236/6500 [1:31:32<34:35:{'loss': 0.9006, 'learning_rate': 9.967763428993344e-05, 'epoch': 0.04}
{'loss': 0.8649, 'learning_rate': 9.967488789804337e-05, 'epoch': 0.04}
{'loss': 0.8684, 'learning_rate': 9.967212989495028e-05, 'epoch': 0.04}
{'loss': 0.8977, 'learning_rate': 9.966936028129882e-05, 'epoch': 0.04}
59, 19.88s/it]  4%|▎         | 237/6500 [1:31:50<33:40:51, 19.36s/it]                                                         4%|▎         | 237/6500 [1:31:50<33:40:51, 19.36s/it]  4%|▎         | 238/6500 [1:32:08<33:03:36, 19.01s/it]                                                         4%|▎         | 238/6500 [1:32:08<33:03:36, 19.01s/it]  4%|▎         | 239/6500 [1:32:26<32:36:58, 18.75s/it]                                                         4%|▎         | 239/6500 [1:32:26<32:36:58, 18.75s/it]  4%|▎         | 240/6500 [1:32:45<32:19:36, 18.59s/it]                                                         4%|▎         | 240/6500 [1:32:45<32:19:36, 18.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9059955477714539, 'eval_runtime': 5.3228, 'eval_samples_per_second': 4.321, 'eval_steps_per_second': 1.127, 'epoch': 0.04}
                                                         4%|▎         | 240/6500 [1:32:50<32:19:36, 18.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-240
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-240
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-240
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-240/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-240/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-240/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8936, 'learning_rate': 9.966657905773642e-05, 'epoch': 0.04}
{'loss': 0.8697, 'learning_rate': 9.966378622491312e-05, 'epoch': 0.04}
{'loss': 0.8753, 'learning_rate': 9.966098178348176e-05, 'epoch': 0.04}
{'loss': 0.8528, 'learning_rate': 9.965816573409785e-05, 'epoch': 0.04}
{'loss': 0.9614, 'learning_rate': 9.965533807741964e-05, 'epoch': 0.04}
{'loss': 0.8812, 'learning_rate': 9.965249881410805e-05, 'epoch': 0.04}
  4%|▎         | 241/6500 [1:34:23<74:07:59, 42.64s/it]                                                         4%|▎         | 241/6500 [1:34:23<74:07:59, 42.64s/it]  4%|▎         | 242/6500 [1:34:41<61:14:48, 35.23s/it]                                                         4%|▎         | 242/6500 [1:34:41<61:14:48, 35.23s/it]  4%|▎         | 243/6500 [1:35:00<52:21:27, 30.12s/it]                                                         4%|▎         | 243/6500 [1:35:00<52:21:27, 30.12s/it]  4%|▍         | 244/6500 [1:35:17<46:00:33, 26.48s/it]                                                         4%|▍         | 244/6500 [1:35:17<46:00:33, 26.48s/it]  4%|▍         | 245/6500 [1:35:35<41:34:53, 23.93s/it]                                                         4%|▍         | 245/6500 [1:35:35<41:34:53, 23.93s/it]  4%|▍         | 246/6500 [1:35:54<38:30:05, 22.16s/it]                                                         4%|▍         | 246/6500 [1:35:54<38:30:{'loss': 0.8542, 'learning_rate': 9.964964794482675e-05, 'epoch': 0.04}
{'loss': 0.9091, 'learning_rate': 9.964678547024213e-05, 'epoch': 0.04}
{'loss': 1.369, 'learning_rate': 9.964391139102325e-05, 'epoch': 0.04}
{'loss': 0.8716, 'learning_rate': 9.964102570784193e-05, 'epoch': 0.04}
05, 22.16s/it]  4%|▍         | 247/6500 [1:36:12<36:21:08, 20.93s/it]                                                         4%|▍         | 247/6500 [1:36:12<36:21:08, 20.93s/it]  4%|▍         | 248/6500 [1:36:30<34:51:44, 20.07s/it]                                                         4%|▍         | 248/6500 [1:36:30<34:51:44, 20.07s/it]  4%|▍         | 249/6500 [1:36:48<33:49:33, 19.48s/it]                                                         4%|▍         | 249/6500 [1:36:48<33:49:33, 19.48s/it]  4%|▍         | 250/6500 [1:37:06<33:06:48, 19.07s/it]                                                         4%|▍         | 250/6500 [1:37:06<33:06:48, 19.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9013875722885132, 'eval_runtime': 5.4616, 'eval_samples_per_second': 4.211, 'eval_steps_per_second': 1.099, 'epoch': 0.04}
                                                         4%|▍         | 250/6500 [1:37:11<33:06:48, 19.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-250
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-250/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-250


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-250/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-250/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8723, 'learning_rate': 9.963812842137267e-05, 'epoch': 0.04}
{'loss': 0.8564, 'learning_rate': 9.963521953229268e-05, 'epoch': 0.04}
{'loss': 0.8228, 'learning_rate': 9.963229904128196e-05, 'epoch': 0.04}
{'loss': 0.8934, 'learning_rate': 9.962936694902307e-05, 'epoch': 0.04}
{'loss': 0.9023, 'learning_rate': 9.96264232562014e-05, 'epoch': 0.04}
{'loss': 0.8263, 'learning_rate': 9.962346796350504e-05, 'epoch': 0.04}
  4%|▍         | 251/6500 [1:38:03<52:49:30, 30.43s/it]                                                         4%|▍         | 251/6500 [1:38:03<52:49:30, 30.43s/it]  4%|▍         | 252/6500 [1:38:21<46:20:59, 26.71s/it]                                                         4%|▍         | 252/6500 [1:38:21<46:20:59, 26.71s/it]  4%|▍         | 253/6500 [1:38:39<41:49:32, 24.10s/it]                                                         4%|▍         | 253/6500 [1:38:39<41:49:32, 24.10s/it]  4%|▍         | 254/6500 [1:38:57<38:39:42, 22.28s/it]                                                         4%|▍         | 254/6500 [1:38:57<38:39:42, 22.28s/it]  4%|▍         | 255/6500 [1:39:15<36:28:24, 21.03s/it]                                                         4%|▍         | 255/6500 [1:39:15<36:28:24, 21.03s/it]  4%|▍         | 256/6500 [1:39:33<34:55:51, 20.14s/it]                                                         4%|▍         | 256/6500 [1:39:33<34:55:{'loss': 0.8462, 'learning_rate': 9.962050107162477e-05, 'epoch': 0.04}
{'loss': 0.8407, 'learning_rate': 9.961752258125406e-05, 'epoch': 0.04}
{'loss': 0.8555, 'learning_rate': 9.961453249308914e-05, 'epoch': 0.04}
{'loss': 0.8394, 'learning_rate': 9.96115308078289e-05, 'epoch': 0.04}
51, 20.14s/it]  4%|▍         | 257/6500 [1:39:51<33:51:55, 19.53s/it]                                                         4%|▍         | 257/6500 [1:39:51<33:51:55, 19.53s/it]  4%|▍         | 258/6500 [1:40:09<33:07:57, 19.11s/it]                                                         4%|▍         | 258/6500 [1:40:09<33:07:57, 19.11s/it]  4%|▍         | 259/6500 [1:40:28<32:48:53, 18.93s/it]                                                         4%|▍         | 259/6500 [1:40:28<32:48:53, 18.93s/it]  4%|▍         | 260/6500 [1:40:46<32:24:34, 18.70s/it]                                                         4%|▍         | 260/6500 [1:40:46<32:24:34, 18.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9002916812896729, 'eval_runtime': 5.3152, 'eval_samples_per_second': 4.327, 'eval_steps_per_second': 1.129, 'epoch': 0.04}
                                                         4%|▍         | 260/6500 [1:40:51<32:24:34, 18.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-260 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-260 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-260

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-260/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-260/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8795, 'learning_rate': 9.960851752617498e-05, 'epoch': 0.04}
{'loss': 0.8728, 'learning_rate': 9.96054926488317e-05, 'epoch': 0.04}
{'loss': 0.8883, 'learning_rate': 9.960245617650613e-05, 'epoch': 0.04}
{'loss': 0.8764, 'learning_rate': 9.959940810990802e-05, 'epoch': 0.04}
{'loss': 0.9178, 'learning_rate': 9.959634844974983e-05, 'epoch': 0.04}
{'loss': 0.8612, 'learning_rate': 9.959327719674674e-05, 'epoch': 0.04}
  4%|▍         | 261/6500 [1:42:04<63:26:03, 36.60s/it]                                                         4%|▍         | 261/6500 [1:42:04<63:26:03, 36.60s/it]  4%|▍         | 262/6500 [1:42:22<53:44:37, 31.02s/it]                                                         4%|▍         | 262/6500 [1:42:22<53:44:37, 31.02s/it]  4%|▍         | 263/6500 [1:42:40<47:01:30, 27.14s/it]                                                         4%|▍         | 263/6500 [1:42:40<47:01:30, 27.14s/it]  4%|▍         | 264/6500 [1:42:58<42:16:06, 24.40s/it]                                                         4%|▍         | 264/6500 [1:42:58<42:16:06, 24.40s/it]  4%|▍         | 265/6500 [1:43:16<38:57:06, 22.49s/it]                                                         4%|▍         | 265/6500 [1:43:16<38:57:06, 22.49s/it]  4%|▍         | 266/6500 [1:43:34<36:38:07, 21.16s/it]                                                         4%|▍         | 266/6500 [1:43:34<36:38:{'loss': 0.8699, 'learning_rate': 9.959019435161664e-05, 'epoch': 0.04}
{'loss': 0.8306, 'learning_rate': 9.958709991508012e-05, 'epoch': 0.04}
{'loss': 0.8235, 'learning_rate': 9.958399388786049e-05, 'epoch': 0.04}
{'loss': 0.9068, 'learning_rate': 9.958087627068376e-05, 'epoch': 0.04}
07, 21.16s/it]  4%|▍         | 267/6500 [1:43:53<35:01:40, 20.23s/it]                                                         4%|▍         | 267/6500 [1:43:53<35:01:40, 20.23s/it]  4%|▍         | 268/6500 [1:44:11<33:54:26, 19.59s/it]                                                         4%|▍         | 268/6500 [1:44:11<33:54:26, 19.59s/it]  4%|▍         | 269/6500 [1:44:29<33:08:10, 19.14s/it]                                                         4%|▍         | 269/6500 [1:44:29<33:08:10, 19.14s/it]  4%|▍         | 270/6500 [1:44:47<32:36:30, 18.84s/it]                                                         4%|▍         | 270/6500 [1:44:47<32:36:30, 18.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8938393592834473, 'eval_runtime': 5.3305, 'eval_samples_per_second': 4.315, 'eval_steps_per_second': 1.126, 'epoch': 0.04}
                                                         4%|▍         | 270/6500 [1:44:52<32:36:30, 18.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-270
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-270
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-270/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-270/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8624, 'learning_rate': 9.957774706427867e-05, 'epoch': 0.04}
{'loss': 0.8565, 'learning_rate': 9.957460626937664e-05, 'epoch': 0.04}
{'loss': 0.8408, 'learning_rate': 9.957145388671181e-05, 'epoch': 0.04}
{'loss': 0.886, 'learning_rate': 9.956828991702103e-05, 'epoch': 0.04}
{'loss': 0.8974, 'learning_rate': 9.956511436104385e-05, 'epoch': 0.04}
{'loss': 0.8776, 'learning_rate': 9.956192721952257e-05, 'epoch': 0.04}
  4%|▍         | 271/6500 [1:45:48<54:27:00, 31.47s/it]                                                         4%|▍         | 271/6500 [1:45:48<54:27:00, 31.47s/it]  4%|▍         | 272/6500 [1:46:06<47:28:07, 27.44s/it]                                                         4%|▍         | 272/6500 [1:46:06<47:28:07, 27.44s/it]  4%|▍         | 273/6500 [1:46:24<42:34:24, 24.61s/it]                                                         4%|▍         | 273/6500 [1:46:24<42:34:24, 24.61s/it]  4%|▍         | 274/6500 [1:46:42<39:08:13, 22.63s/it]                                                         4%|▍         | 274/6500 [1:46:42<39:08:13, 22.63s/it]  4%|▍         | 275/6500 [1:47:00<36:51:01, 21.31s/it]                                                         4%|▍         | 275/6500 [1:47:00<36:51:01, 21.31s/it]  4%|▍         | 276/6500 [1:47:18<35:09:37, 20.34s/it]                                                         4%|▍         | 276/6500 [1:47:18<35:09:{'loss': 0.8479, 'learning_rate': 9.955872849320213e-05, 'epoch': 0.04}
{'loss': 0.8603, 'learning_rate': 9.955551818283024e-05, 'epoch': 0.04}
{'loss': 1.357, 'learning_rate': 9.955229628915727e-05, 'epoch': 0.04}
{'loss': 0.8628, 'learning_rate': 9.954906281293634e-05, 'epoch': 0.04}
37, 20.34s/it]  4%|▍         | 277/6500 [1:47:36<33:59:06, 19.66s/it]                                                         4%|▍         | 277/6500 [1:47:36<33:59:06, 19.66s/it]  4%|▍         | 278/6500 [1:47:54<33:09:35, 19.19s/it]                                                         4%|▍         | 278/6500 [1:47:54<33:09:35, 19.19s/it]  4%|▍         | 279/6500 [1:48:12<32:35:18, 18.86s/it]                                                         4%|▍         | 279/6500 [1:48:12<32:35:18, 18.86s/it]  4%|▍         | 280/6500 [1:48:31<32:11:58, 18.64s/it]                                                         4%|▍         | 280/6500 [1:48:31<32:11:58, 18.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8910583257675171, 'eval_runtime': 5.4611, 'eval_samples_per_second': 4.212, 'eval_steps_per_second': 1.099, 'epoch': 0.04}
                                                         4%|▍         | 280/6500 [1:48:36<32:11:58, 18.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-280
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-280
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-280
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-280
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-280/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8335, 'learning_rate': 9.954581775492322e-05, 'epoch': 0.04}
{'loss': 0.8098, 'learning_rate': 9.954256111587645e-05, 'epoch': 0.04}
{'loss': 0.8363, 'learning_rate': 9.953929289655724e-05, 'epoch': 0.04}
{'loss': 0.8936, 'learning_rate': 9.953601309772953e-05, 'epoch': 0.04}
{'loss': 0.8386, 'learning_rate': 9.953272172015992e-05, 'epoch': 0.04}
{'loss': 0.8518, 'learning_rate': 9.952941876461779e-05, 'epoch': 0.04}
  4%|▍         | 281/6500 [1:49:28<52:07:02, 30.17s/it]                                                         4%|▍         | 281/6500 [1:49:28<52:07:02, 30.17s/it]  4%|▍         | 282/6500 [1:49:46<45:51:10, 26.55s/it]                                                         4%|▍         | 282/6500 [1:49:46<45:51:10, 26.55s/it]  4%|▍         | 283/6500 [1:50:04<41:29:15, 24.02s/it]                                                         4%|▍         | 283/6500 [1:50:04<41:29:15, 24.02s/it]  4%|▍         | 284/6500 [1:50:22<38:23:59, 22.24s/it]                                                         4%|▍         | 284/6500 [1:50:22<38:23:59, 22.24s/it]  4%|▍         | 285/6500 [1:50:40<36:13:32, 20.98s/it]                                                         4%|▍         | 285/6500 [1:50:40<36:13:32, 20.98s/it]  4%|▍         | 286/6500 [1:50:58<34:42:50, 20.11s/it]                                                         4%|▍         | 286/6500 [1:50:58<34:42:{'loss': 0.8131, 'learning_rate': 9.952610423187516e-05, 'epoch': 0.04}
{'loss': 0.8407, 'learning_rate': 9.952277812270681e-05, 'epoch': 0.04}
{'loss': 0.8194, 'learning_rate': 9.951944043789016e-05, 'epoch': 0.04}
{'loss': 0.8467, 'learning_rate': 9.951609117820538e-05, 'epoch': 0.04}
50, 20.11s/it]  4%|▍         | 287/6500 [1:51:16<33:41:51, 19.53s/it]                                                         4%|▍         | 287/6500 [1:51:16<33:41:51, 19.53s/it]  4%|▍         | 288/6500 [1:51:34<32:57:29, 19.10s/it]                                                         4%|▍         | 288/6500 [1:51:34<32:57:29, 19.10s/it]  4%|▍         | 289/6500 [1:51:52<32:27:07, 18.81s/it]                                                         4%|▍         | 289/6500 [1:51:52<32:27:07, 18.81s/it]  4%|▍         | 290/6500 [1:52:11<32:05:57, 18.61s/it]                                                         4%|▍         | 290/6500 [1:52:11<32:05:57, 18.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8876444101333618, 'eval_runtime': 5.328, 'eval_samples_per_second': 4.317, 'eval_steps_per_second': 1.126, 'epoch': 0.04}
                                                         4%|▍         | 290/6500 [1:52:16<32:05:57, 18.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-290
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-290

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-290/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9017, 'learning_rate': 9.951273034443537e-05, 'epoch': 0.04}
{'loss': 0.8534, 'learning_rate': 9.950935793736567e-05, 'epoch': 0.04}
{'loss': 0.8659, 'learning_rate': 9.950597395778458e-05, 'epoch': 0.05}
{'loss': 0.8618, 'learning_rate': 9.950257840648307e-05, 'epoch': 0.05}
{'loss': 0.8515, 'learning_rate': 9.949917128425485e-05, 'epoch': 0.05}
{'loss': 0.8567, 'learning_rate': 9.94957525918963e-05, 'epoch': 0.05}
  4%|▍         | 291/6500 [1:53:03<49:29:49, 28.70s/it]                                                         4%|▍         | 291/6500 [1:53:03<49:29:49, 28.70s/it]  4%|▍         | 292/6500 [1:53:21<43:58:29, 25.50s/it]                                                         4%|▍         | 292/6500 [1:53:21<43:58:29, 25.50s/it]  5%|▍         | 293/6500 [1:53:39<40:06:34, 23.26s/it]                                                         5%|▍         | 293/6500 [1:53:39<40:06:34, 23.26s/it]  5%|▍         | 294/6500 [1:53:57<37:24:20, 21.70s/it]                                                         5%|▍         | 294/6500 [1:53:57<37:24:20, 21.70s/it]  5%|▍         | 295/6500 [1:54:15<35:31:44, 20.61s/it]                                                         5%|▍         | 295/6500 [1:54:15<35:31:44, 20.61s/it]  5%|▍         | 296/6500 [1:54:33<34:12:36, 19.85s/it]                                                         5%|▍         | 296/6500 [1:54:33<34:12:{'loss': 0.8422, 'learning_rate': 9.949232233020653e-05, 'epoch': 0.05}
{'loss': 0.7972, 'learning_rate': 9.948888049998731e-05, 'epoch': 0.05}
{'loss': 0.8621, 'learning_rate': 9.948542710204319e-05, 'epoch': 0.05}
{'loss': 0.8717, 'learning_rate': 9.948196213718135e-05, 'epoch': 0.05}
36, 19.85s/it]  5%|▍         | 297/6500 [1:54:52<33:27:34, 19.42s/it]                                                         5%|▍         | 297/6500 [1:54:52<33:27:34, 19.42s/it]  5%|▍         | 298/6500 [1:55:10<32:46:15, 19.02s/it]                                                         5%|▍         | 298/6500 [1:55:10<32:46:15, 19.02s/it]  5%|▍         | 299/6500 [1:55:28<32:18:22, 18.76s/it]                                                         5%|▍         | 299/6500 [1:55:28<32:18:22, 18.76s/it]  5%|▍         | 300/6500 [1:55:46<31:58:55, 18.57s/it]                                                         5%|▍         | 300/6500 [1:55:46<31:58:55, 18.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8831104636192322, 'eval_runtime': 5.3198, 'eval_samples_per_second': 4.323, 'eval_steps_per_second': 1.128, 'epoch': 0.05}
                                                         5%|▍         | 300/6500 [1:55:51<31:58:55, 18.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-300the checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-300
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-300

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-300/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8288, 'learning_rate': 9.947848560621172e-05, 'epoch': 0.05}
{'loss': 0.8579, 'learning_rate': 9.94749975099469e-05, 'epoch': 0.05}
{'loss': 0.7943, 'learning_rate': 9.947149784920225e-05, 'epoch': 0.05}
{'loss': 0.896, 'learning_rate': 9.946798662479577e-05, 'epoch': 0.05}
{'loss': 0.8675, 'learning_rate': 9.946446383754817e-05, 'epoch': 0.05}
{'loss': 0.8434, 'learning_rate': 9.946092948828289e-05, 'epoch': 0.05}
  5%|▍         | 301/6500 [1:56:24<41:50:14, 24.30s/it]                                                         5%|▍         | 301/6500 [1:56:24<41:50:14, 24.30s/it]  5%|▍         | 302/6500 [1:56:42<38:35:54, 22.42s/it]                                                         5%|▍         | 302/6500 [1:56:42<38:35:54, 22.42s/it]  5%|▍         | 303/6500 [1:57:00<36:20:04, 21.11s/it]                                                         5%|▍         | 303/6500 [1:57:00<36:20:04, 21.11s/it]  5%|▍         | 304/6500 [1:57:18<34:45:12, 20.19s/it]                                                         5%|▍         | 304/6500 [1:57:18<34:45:12, 20.19s/it]  5%|▍         | 305/6500 [1:57:36<33:40:13, 19.57s/it]                                                         5%|▍         | 305/6500 [1:57:36<33:40:13, 19.57s/it]  5%|▍         | 306/6500 [1:57:54<32:55:02, 19.13s/it]                                                         5%|▍         | 306/6500 [1:57:54<32:55:{'loss': 0.8477, 'learning_rate': 9.94573835778261e-05, 'epoch': 0.05}
{'loss': 0.8403, 'learning_rate': 9.945382610700657e-05, 'epoch': 0.05}
{'loss': 1.3138, 'learning_rate': 9.94502570766559e-05, 'epoch': 0.05}
{'loss': 0.8278, 'learning_rate': 9.944667648760828e-05, 'epoch': 0.05}
02, 19.13s/it]  5%|▍         | 307/6500 [1:58:12<32:33:50, 18.93s/it]                                                         5%|▍         | 307/6500 [1:58:12<32:33:50, 18.93s/it]  5%|▍         | 308/6500 [1:58:31<32:12:13, 18.72s/it]                                                         5%|▍         | 308/6500 [1:58:31<32:12:13, 18.72s/it]  5%|▍         | 309/6500 [1:58:49<31:54:58, 18.56s/it]                                                         5%|▍         | 309/6500 [1:58:49<31:54:58, 18.56s/it]  5%|▍         | 310/6500 [1:59:09<32:48:03, 19.08s/it]                                                         5%|▍         | 310/6500 [1:59:09<32:48:03, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8799161314964294, 'eval_runtime': 6.5298, 'eval_samples_per_second': 3.522, 'eval_steps_per_second': 0.919, 'epoch': 0.05}
                                                         5%|▍         | 310/6500 [1:59:16<32:48:03, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-310
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-310
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-310
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-310
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-310/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8487, 'learning_rate': 9.944308434070069e-05, 'epoch': 0.05}
{'loss': 0.7833, 'learning_rate': 9.943948063677274e-05, 'epoch': 0.05}
{'loss': 0.8508, 'learning_rate': 9.94358653766668e-05, 'epoch': 0.05}
{'loss': 0.8525, 'learning_rate': 9.943223856122788e-05, 'epoch': 0.05}
{'loss': 0.7989, 'learning_rate': 9.942860019130377e-05, 'epoch': 0.05}
{'loss': 0.8172, 'learning_rate': 9.942495026774489e-05, 'epoch': 0.05}
  5%|▍         | 311/6500 [2:00:44<71:46:34, 41.75s/it]                                                         5%|▍         | 311/6500 [2:00:44<71:46:34, 41.75s/it]  5%|▍         | 312/6500 [2:01:02<59:29:47, 34.61s/it]                                                         5%|▍         | 312/6500 [2:01:02<59:29:47, 34.61s/it]  5%|▍         | 313/6500 [2:01:20<50:53:24, 29.61s/it]                                                         5%|▍         | 313/6500 [2:01:20<50:53:24, 29.61s/it]  5%|▍         | 314/6500 [2:01:39<45:28:42, 26.47s/it]                                                         5%|▍         | 314/6500 [2:01:39<45:28:42, 26.47s/it]  5%|▍         | 315/6500 [2:01:57<41:06:19, 23.93s/it]                                                         5%|▍         | 315/6500 [2:01:57<41:06:19, 23.93s/it]  5%|▍         | 316/6500 [2:02:15<38:03:40, 22.16s/it]                                                         5%|▍         | 316/6500 [2:02:15<38:03:{'loss': 0.8107, 'learning_rate': 9.94212887914044e-05, 'epoch': 0.05}
{'loss': 0.8147, 'learning_rate': 9.941761576313812e-05, 'epoch': 0.05}
{'loss': 0.8102, 'learning_rate': 9.941393118380466e-05, 'epoch': 0.05}
{'loss': 0.8407, 'learning_rate': 9.94102350542652e-05, 'epoch': 0.05}
40, 22.16s/it]  5%|▍         | 317/6500 [2:02:33<35:56:48, 20.93s/it]                                                         5%|▍         | 317/6500 [2:02:33<35:56:48, 20.93s/it]  5%|▍         | 318/6500 [2:02:51<34:28:28, 20.08s/it]                                                         5%|▍         | 318/6500 [2:02:51<34:28:28, 20.08s/it]  5%|▍         | 319/6500 [2:03:09<33:27:14, 19.48s/it]                                                         5%|▍         | 319/6500 [2:03:09<33:27:14, 19.48s/it]  5%|▍         | 320/6500 [2:03:27<32:45:39, 19.08s/it]                                                         5%|▍         | 320/6500 [2:03:27<32:45:39, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8796353340148926, 'eval_runtime': 5.3425, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.05}
                                                         5%|▍         | 320/6500 [2:03:33<32:45:39, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-320/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-320

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-320

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-320/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-320/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.841, 'learning_rate': 9.940652737538372e-05, 'epoch': 0.05}
{'loss': 0.8167, 'learning_rate': 9.940280814802685e-05, 'epoch': 0.05}
{'loss': 0.8803, 'learning_rate': 9.939907737306397e-05, 'epoch': 0.05}
{'loss': 0.8522, 'learning_rate': 9.939533505136708e-05, 'epoch': 0.05}
{'loss': 0.8575, 'learning_rate': 9.939158118381098e-05, 'epoch': 0.05}
{'loss': 0.853, 'learning_rate': 9.938781577127306e-05, 'epoch': 0.05}
  5%|▍         | 321/6500 [2:04:38<59:15:06, 34.52s/it]                                                         5%|▍         | 321/6500 [2:04:38<59:15:06, 34.52s/it]  5%|▍         | 322/6500 [2:04:56<50:43:33, 29.56s/it]                                                         5%|▍         | 322/6500 [2:04:56<50:43:33, 29.56s/it]  5%|▍         | 323/6500 [2:05:14<44:45:43, 26.09s/it]                                                         5%|▍         | 323/6500 [2:05:14<44:45:43, 26.09s/it]  5%|▍         | 324/6500 [2:05:32<40:47:47, 23.78s/it]                                                         5%|▍         | 324/6500 [2:05:32<40:47:47, 23.78s/it]  5%|▌         | 325/6500 [2:05:50<37:50:12, 22.06s/it]                                                         5%|▌         | 325/6500 [2:05:50<37:50:12, 22.06s/it]  5%|▌         | 326/6500 [2:06:08<35:46:36, 20.86s/it]                                                         5%|▌         | 326/6500 [2:06:08<35:46:{'loss': 0.8105, 'learning_rate': 9.93840388146335e-05, 'epoch': 0.05}
{'loss': 0.7992, 'learning_rate': 9.938025031477512e-05, 'epoch': 0.05}
{'loss': 0.8541, 'learning_rate': 9.937645027258347e-05, 'epoch': 0.05}
{'loss': 0.845, 'learning_rate': 9.937263868894678e-05, 'epoch': 0.05}
36, 20.86s/it]  5%|▌         | 327/6500 [2:06:26<34:24:53, 20.07s/it]                                                         5%|▌         | 327/6500 [2:06:26<34:24:53, 20.07s/it]  5%|▌         | 328/6500 [2:06:45<33:24:02, 19.48s/it]                                                         5%|▌         | 328/6500 [2:06:45<33:24:02, 19.48s/it]  5%|▌         | 329/6500 [2:07:03<32:42:25, 19.08s/it]                                                         5%|▌         | 329/6500 [2:07:03<32:42:25, 19.08s/it]  5%|▌         | 330/6500 [2:07:21<32:13:12, 18.80s/it]                                                         5%|▌         | 330/6500 [2:07:21<32:13:12, 18.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8738104104995728, 'eval_runtime': 5.3386, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.05}
                                                         5%|▌         | 330/6500 [2:07:26<32:13:12, 18.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-330/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-330
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-330/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8091, 'learning_rate': 9.936881556475599e-05, 'epoch': 0.05}
{'loss': 0.817, 'learning_rate': 9.936498090090474e-05, 'epoch': 0.05}
{'loss': 0.797, 'learning_rate': 9.936113469828933e-05, 'epoch': 0.05}
{'loss': 0.9061, 'learning_rate': 9.935727695780881e-05, 'epoch': 0.05}
{'loss': 0.8191, 'learning_rate': 9.93534076803649e-05, 'epoch': 0.05}
{'loss': 0.8157, 'learning_rate': 9.934952686686201e-05, 'epoch': 0.05}
  5%|▌         | 331/6500 [2:07:59<41:59:38, 24.51s/it]                                                         5%|▌         | 331/6500 [2:07:59<41:59:38, 24.51s/it]  5%|▌         | 332/6500 [2:08:17<38:40:51, 22.58s/it]                                                         5%|▌         | 332/6500 [2:08:17<38:40:51, 22.58s/it]  5%|▌         | 333/6500 [2:08:35<36:32:30, 21.33s/it]                                                         5%|▌         | 333/6500 [2:08:35<36:32:30, 21.33s/it]  5%|▌         | 334/6500 [2:08:55<35:59:01, 21.01s/it]                                                         5%|▌         | 334/6500 [2:08:56<35:59:01, 21.01s/it]  5%|▌         | 335/6500 [2:09:14<34:32:13, 20.17s/it]                                                         5%|▌         | 335/6500 [2:09:14<34:32:13, 20.17s/it]  5%|▌         | 336/6500 [2:09:32<33:26:47, 19.53s/it]                                                         5%|▌         | 336/6500 [2:09:32<33:26:{'loss': 0.8388, 'learning_rate': 9.934563451820728e-05, 'epoch': 0.05}
{'loss': 1.3246, 'learning_rate': 9.93417306353105e-05, 'epoch': 0.05}
{'loss': 0.8176, 'learning_rate': 9.933781521908419e-05, 'epoch': 0.05}
{'loss': 0.802, 'learning_rate': 9.933388827044355e-05, 'epoch': 0.05}
47, 19.53s/it]  5%|▌         | 337/6500 [2:09:50<32:42:24, 19.11s/it]                                                         5%|▌         | 337/6500 [2:09:50<32:42:24, 19.11s/it]  5%|▌         | 338/6500 [2:10:08<32:22:02, 18.91s/it]                                                         5%|▌         | 338/6500 [2:10:08<32:22:02, 18.91s/it]  5%|▌         | 339/6500 [2:10:26<31:57:24, 18.67s/it]                                                         5%|▌         | 339/6500 [2:10:26<31:57:24, 18.67s/it]  5%|▌         | 340/6500 [2:10:46<32:30:33, 19.00s/it]                                                         5%|▌         | 340/6500 [2:10:46<32:30:33, 19.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718916177749634, 'eval_runtime': 5.9684, 'eval_samples_per_second': 3.854, 'eval_steps_per_second': 1.005, 'epoch': 0.05}
                                                         5%|▌         | 340/6500 [2:10:52<32:30:33, 19.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-340
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-340
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-340/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8115, 'learning_rate': 9.932994979030647e-05, 'epoch': 0.05}
{'loss': 0.7749, 'learning_rate': 9.932599977959356e-05, 'epoch': 0.05}
{'loss': 0.8513, 'learning_rate': 9.932203823922812e-05, 'epoch': 0.05}
{'loss': 0.8198, 'learning_rate': 9.931806517013612e-05, 'epoch': 0.05}
{'loss': 0.7982, 'learning_rate': 9.931408057324625e-05, 'epoch': 0.05}
{'loss': 0.783, 'learning_rate': 9.931008444948988e-05, 'epoch': 0.05}
  5%|▌         | 341/6500 [2:11:39<49:40:58, 29.04s/it]                                                         5%|▌         | 341/6500 [2:11:39<49:40:58, 29.04s/it]  5%|▌         | 342/6500 [2:11:57<44:00:49, 25.73s/it]                                                         5%|▌         | 342/6500 [2:11:57<44:00:49, 25.73s/it]  5%|▌         | 343/6500 [2:12:15<40:01:55, 23.41s/it]                                                         5%|▌         | 343/6500 [2:12:15<40:01:55, 23.41s/it]  5%|▌         | 344/6500 [2:12:33<37:15:32, 21.79s/it]                                                         5%|▌         | 344/6500 [2:12:33<37:15:32, 21.79s/it]  5%|▌         | 345/6500 [2:12:51<35:19:47, 20.66s/it]                                                         5%|▌         | 345/6500 [2:12:51<35:19:47, 20.66s/it]  5%|▌         | 346/6500 [2:13:09<33:58:52, 19.88s/it]                                                         5%|▌         | 346/6500 [2:13:09<33:58:{'loss': 0.8111, 'learning_rate': 9.930607679980107e-05, 'epoch': 0.05}
{'loss': 0.7921, 'learning_rate': 9.93020576251166e-05, 'epoch': 0.05}
{'loss': 0.7839, 'learning_rate': 9.929802692637593e-05, 'epoch': 0.05}
{'loss': 0.854, 'learning_rate': 9.929398470452118e-05, 'epoch': 0.05}
52, 19.88s/it]  5%|▌         | 347/6500 [2:13:27<33:03:06, 19.34s/it]                                                         5%|▌         | 347/6500 [2:13:27<33:03:06, 19.34s/it]  5%|▌         | 348/6500 [2:13:45<32:24:22, 18.96s/it]                                                         5%|▌         | 348/6500 [2:13:45<32:24:22, 18.96s/it]  5%|▌         | 349/6500 [2:14:03<31:58:10, 18.71s/it]                                                         5%|▌         | 349/6500 [2:14:03<31:58:10, 18.71s/it]  5%|▌         | 350/6500 [2:14:21<31:40:24, 18.54s/it]                                                         5%|▌         | 350/6500 [2:14:21<31:40:24, 18.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8692601323127747, 'eval_runtime': 5.3777, 'eval_samples_per_second': 4.277, 'eval_steps_per_second': 1.116, 'epoch': 0.05}
                                                         5%|▌         | 350/6500 [2:14:26<31:40:24, 18.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-350/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-350

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-350/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-350/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.819, 'learning_rate': 9.928993096049724e-05, 'epoch': 0.05}
{'loss': 0.8376, 'learning_rate': 9.928586569525162e-05, 'epoch': 0.05}
{'loss': 0.8315, 'learning_rate': 9.928178890973455e-05, 'epoch': 0.05}
{'loss': 0.8424, 'learning_rate': 9.927770060489897e-05, 'epoch': 0.05}
{'loss': 0.8127, 'learning_rate': 9.927360078170048e-05, 'epoch': 0.05}
{'loss': 0.8197, 'learning_rate': 9.92694894410974e-05, 'epoch': 0.05}
  5%|▌         | 351/6500 [2:15:27<56:07:10, 32.86s/it]                                                         5%|▌         | 351/6500 [2:15:27<56:07:10, 32.86s/it]  5%|▌         | 352/6500 [2:15:45<48:30:27, 28.40s/it]                                                         5%|▌         | 352/6500 [2:15:45<48:30:27, 28.40s/it]  5%|▌         | 353/6500 [2:16:03<43:13:47, 25.32s/it]                                                         5%|▌         | 353/6500 [2:16:03<43:13:47, 25.32s/it]  5%|▌         | 354/6500 [2:16:22<39:32:32, 23.16s/it]                                                         5%|▌         | 354/6500 [2:16:22<39:32:32, 23.16s/it]  5%|▌         | 355/6500 [2:16:40<36:54:22, 21.62s/it]                                                         5%|▌         | 355/6500 [2:16:40<36:54:22, 21.62s/it]  5%|▌         | 356/6500 [2:16:58<35:17:35, 20.68s/it]                                                         5%|▌         | 356/6500 [2:16:58<35:17:{'loss': 0.7965, 'learning_rate': 9.926536658405072e-05, 'epoch': 0.05}
{'loss': 0.7767, 'learning_rate': 9.926123221152415e-05, 'epoch': 0.06}
{'loss': 0.8569, 'learning_rate': 9.925708632448405e-05, 'epoch': 0.06}
{'loss': 0.8043, 'learning_rate': 9.925292892389953e-05, 'epoch': 0.06}
35, 20.68s/it]  5%|▌         | 357/6500 [2:17:16<33:57:37, 19.90s/it]                                                         5%|▌         | 357/6500 [2:17:16<33:57:37, 19.90s/it]  6%|▌         | 358/6500 [2:17:34<33:01:44, 19.36s/it]                                                         6%|▌         | 358/6500 [2:17:34<33:01:44, 19.36s/it]  6%|▌         | 359/6500 [2:17:52<32:23:26, 18.99s/it]                                                         6%|▌         | 359/6500 [2:17:52<32:23:26, 18.99s/it]  6%|▌         | 360/6500 [2:18:11<31:56:56, 18.73s/it]                                                         6%|▌         | 360/6500 [2:18:11<31:56:56, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8649398684501648, 'eval_runtime': 5.3352, 'eval_samples_per_second': 4.311, 'eval_steps_per_second': 1.125, 'epoch': 0.06}
                                                         6%|▌         | 360/6500 [2:18:16<31:56:56, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-360
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-360

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-360/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8189, 'learning_rate': 9.924876001074231e-05, 'epoch': 0.06}
{'loss': 0.7789, 'learning_rate': 9.92445795859869e-05, 'epoch': 0.06}
{'loss': 0.8331, 'learning_rate': 9.924038765061042e-05, 'epoch': 0.06}
{'loss': 0.8609, 'learning_rate': 9.923618420559268e-05, 'epoch': 0.06}
{'loss': 0.8119, 'learning_rate': 9.923196925191629e-05, 'epoch': 0.06}
{'loss': 0.8007, 'learning_rate': 9.922774279056639e-05, 'epoch': 0.06}
  6%|▌         | 361/6500 [2:19:21<58:30:46, 34.31s/it]                                                         6%|▌         | 361/6500 [2:19:21<58:30:46, 34.31s/it]  6%|▌         | 362/6500 [2:19:39<50:09:30, 29.42s/it]                                                         6%|▌         | 362/6500 [2:19:39<50:09:30, 29.42s/it]  6%|▌         | 363/6500 [2:19:57<44:18:56, 26.00s/it]                                                         6%|▌         | 363/6500 [2:19:57<44:18:56, 26.00s/it]  6%|▌         | 364/6500 [2:20:15<40:13:45, 23.60s/it]                                                         6%|▌         | 364/6500 [2:20:15<40:13:45, 23.60s/it]  6%|▌         | 365/6500 [2:20:33<37:22:27, 21.93s/it]                                                         6%|▌         | 365/6500 [2:20:33<37:22:27, 21.93s/it]  6%|▌         | 366/6500 [2:20:51<35:22:55, 20.77s/it]                                                         6%|▌         | 366/6500 [2:20:51<35:22:{'loss': 0.8005, 'learning_rate': 9.922350482253093e-05, 'epoch': 0.06}
{'loss': 1.3041, 'learning_rate': 9.921925534880051e-05, 'epoch': 0.06}
{'loss': 0.8163, 'learning_rate': 9.921499437036841e-05, 'epoch': 0.06}
{'loss': 0.7808, 'learning_rate': 9.92107218882306e-05, 'epoch': 0.06}
55, 20.77s/it]  6%|▌         | 367/6500 [2:21:09<34:00:06, 19.96s/it]                                                         6%|▌         | 367/6500 [2:21:09<34:00:06, 19.96s/it]  6%|▌         | 368/6500 [2:21:27<33:01:55, 19.39s/it]                                                         6%|▌         | 368/6500 [2:21:27<33:01:55, 19.39s/it]  6%|▌         | 369/6500 [2:21:46<32:23:13, 19.02s/it]                                                         6%|▌         | 369/6500 [2:21:46<32:23:13, 19.02s/it]  6%|▌         | 370/6500 [2:22:04<31:55:23, 18.75s/it]                                                         6%|▌         | 370/6500 [2:22:04<31:55:23, 18.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8632839918136597, 'eval_runtime': 5.3339, 'eval_samples_per_second': 4.312, 'eval_steps_per_second': 1.125, 'epoch': 0.06}
                                                         6%|▌         | 370/6500 [2:22:09<31:55:23, 18.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-370
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-370 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-370
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-370
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-370/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.764, 'learning_rate': 9.920643790338575e-05, 'epoch': 0.06}
{'loss': 0.7936, 'learning_rate': 9.920214241683523e-05, 'epoch': 0.06}
{'loss': 0.8437, 'learning_rate': 9.919783542958308e-05, 'epoch': 0.06}
{'loss': 0.8, 'learning_rate': 9.9193516942636e-05, 'epoch': 0.06}
{'loss': 0.7839, 'learning_rate': 9.918918695700348e-05, 'epoch': 0.06}
{'loss': 0.7652, 'learning_rate': 9.918484547369755e-05, 'epoch': 0.06}
  6%|▌         | 371/6500 [2:23:18<60:25:04, 35.49s/it]                                                         6%|▌         | 371/6500 [2:23:18<60:25:04, 35.49s/it]  6%|▌         | 372/6500 [2:23:37<51:45:14, 30.40s/it]                                                         6%|▌         | 372/6500 [2:23:37<51:45:14, 30.40s/it]  6%|▌         | 373/6500 [2:23:55<45:25:09, 26.69s/it]                                                         6%|▌         | 373/6500 [2:23:55<45:25:09, 26.69s/it]  6%|▌         | 374/6500 [2:24:13<40:59:51, 24.09s/it]                                                         6%|▌         | 374/6500 [2:24:13<40:59:51, 24.09s/it]  6%|▌         | 375/6500 [2:24:31<37:54:43, 22.28s/it]                                                         6%|▌         | 375/6500 [2:24:31<37:54:43, 22.28s/it]  6%|▌         | 376/6500 [2:24:49<35:46:06, 21.03s/it]                                                         6%|▌         | 376/6500 [2:24:49<35:46:{'loss': 0.7848, 'learning_rate': 9.918049249373305e-05, 'epoch': 0.06}
{'loss': 0.7747, 'learning_rate': 9.917612801812744e-05, 'epoch': 0.06}
{'loss': 0.7906, 'learning_rate': 9.917175204790093e-05, 'epoch': 0.06}
{'loss': 0.8384, 'learning_rate': 9.916736458407632e-05, 'epoch': 0.06}
06, 21.03s/it]  6%|▌         | 377/6500 [2:25:07<34:16:36, 20.15s/it]                                                         6%|▌         | 377/6500 [2:25:07<34:16:36, 20.15s/it]  6%|▌         | 378/6500 [2:25:25<33:14:28, 19.55s/it]                                                         6%|▌         | 378/6500 [2:25:25<33:14:28, 19.55s/it]  6%|▌         | 379/6500 [2:25:43<32:30:56, 19.12s/it]                                                         6%|▌         | 379/6500 [2:25:43<32:30:56, 19.12s/it]  6%|▌         | 380/6500 [2:26:02<32:00:32, 18.83s/it]                                                         6%|▌         | 380/6500 [2:26:02<32:00:32, 18.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8613506555557251, 'eval_runtime': 5.3411, 'eval_samples_per_second': 4.306, 'eval_steps_per_second': 1.123, 'epoch': 0.06}
                                                         6%|▌         | 380/6500 [2:26:07<32:00:32, 18.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-380 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-380
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-380

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-380/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-380/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7886, 'learning_rate': 9.91629656276792e-05, 'epoch': 0.06}
{'loss': 0.8302, 'learning_rate': 9.915855517973776e-05, 'epoch': 0.06}
{'loss': 0.8194, 'learning_rate': 9.915413324128295e-05, 'epoch': 0.06}
{'loss': 0.7972, 'learning_rate': 9.914969981334834e-05, 'epoch': 0.06}
{'loss': 0.816, 'learning_rate': 9.914525489697026e-05, 'epoch': 0.06}
{'loss': 0.7967, 'learning_rate': 9.914079849318764e-05, 'epoch': 0.06}
  6%|▌         | 381/6500 [2:27:46<75:53:36, 44.65s/it]                                                         6%|▌         | 381/6500 [2:27:46<75:53:36, 44.65s/it]  6%|▌         | 382/6500 [2:28:04<62:16:02, 36.64s/it]                                                         6%|▌         | 382/6500 [2:28:04<62:16:02, 36.64s/it]  6%|▌         | 383/6500 [2:28:22<52:43:37, 31.03s/it]                                                         6%|▌         | 383/6500 [2:28:22<52:43:37, 31.03s/it]  6%|▌         | 384/6500 [2:28:40<46:02:40, 27.10s/it]                                                         6%|▌         | 384/6500 [2:28:40<46:02:40, 27.10s/it]  6%|▌         | 385/6500 [2:28:58<41:23:00, 24.36s/it]                                                         6%|▌         | 385/6500 [2:28:58<41:23:00, 24.36s/it]  6%|▌         | 386/6500 [2:29:16<38:08:26, 22.46s/it]                                                         6%|▌         | 386/6500 [2:29:16<38:08:{'loss': 0.779, 'learning_rate': 9.913633060304214e-05, 'epoch': 0.06}
{'loss': 0.8028, 'learning_rate': 9.913185122757814e-05, 'epoch': 0.06}
{'loss': 0.8126, 'learning_rate': 9.912736036784264e-05, 'epoch': 0.06}
{'loss': 0.7873, 'learning_rate': 9.912285802488534e-05, 'epoch': 0.06}
26, 22.46s/it]  6%|▌         | 387/6500 [2:29:34<35:52:53, 21.13s/it]                                                         6%|▌         | 387/6500 [2:29:34<35:52:53, 21.13s/it]  6%|▌         | 388/6500 [2:29:53<34:27:07, 20.29s/it]                                                         6%|▌         | 388/6500 [2:29:53<34:27:07, 20.29s/it]  6%|▌         | 389/6500 [2:30:11<33:20:25, 19.64s/it]                                                         6%|▌         | 389/6500 [2:30:11<33:20:25, 19.64s/it]  6%|▌         | 390/6500 [2:30:29<32:34:12, 19.19s/it]                                                         6%|▌         | 390/6500 [2:30:29<32:34:12, 19.19s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8569175601005554, 'eval_runtime': 5.3285, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.06}
                                                         6%|▌         | 390/6500 [2:30:34<32:34:12, 19.19s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-390
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-390
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-390/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8018, 'learning_rate': 9.911834419975866e-05, 'epoch': 0.06}
{'loss': 0.7592, 'learning_rate': 9.911381889351765e-05, 'epoch': 0.06}
{'loss': 0.8511, 'learning_rate': 9.91092821072201e-05, 'epoch': 0.06}
{'loss': 0.8188, 'learning_rate': 9.910473384192647e-05, 'epoch': 0.06}
{'loss': 0.7785, 'learning_rate': 9.910017409869984e-05, 'epoch': 0.06}
{'loss': 0.8102, 'learning_rate': 9.909560287860606e-05, 'epoch': 0.06}
  6%|▌         | 391/6500 [2:31:36<56:45:35, 33.45s/it]                                                         6%|▌         | 391/6500 [2:31:36<56:45:35, 33.45s/it]  6%|▌         | 392/6500 [2:31:54<48:57:24, 28.85s/it]                                                         6%|▌         | 392/6500 [2:31:54<48:57:24, 28.85s/it]  6%|▌         | 393/6500 [2:32:12<43:24:32, 25.59s/it]                                                         6%|▌         | 393/6500 [2:32:12<43:24:32, 25.59s/it]  6%|▌         | 394/6500 [2:32:30<39:32:00, 23.31s/it]                                                         6%|▌         | 394/6500 [2:32:30<39:32:00, 23.31s/it]  6%|▌         | 395/6500 [2:32:48<36:50:03, 21.72s/it]                                                         6%|▌         | 395/6500 [2:32:48<36:50:03, 21.72s/it]  6%|▌         | 396/6500 [2:33:06<34:57:18, 20.62s/it]                                                         6%|▌         | 396/6500 [2:33:06<34:57:{'loss': 0.7847, 'learning_rate': 9.90910201827136e-05, 'epoch': 0.06}
{'loss': 1.3074, 'learning_rate': 9.908642601209366e-05, 'epoch': 0.06}
{'loss': 0.7833, 'learning_rate': 9.908182036782009e-05, 'epoch': 0.06}
{'loss': 0.7832, 'learning_rate': 9.907720325096943e-05, 'epoch': 0.06}
18, 20.62s/it]  6%|▌         | 397/6500 [2:33:24<33:39:05, 19.85s/it]                                                         6%|▌         | 397/6500 [2:33:24<33:39:05, 19.85s/it]  6%|▌         | 398/6500 [2:33:42<32:44:38, 19.32s/it]                                                         6%|▌         | 398/6500 [2:33:42<32:44:38, 19.32s/it]  6%|▌         | 399/6500 [2:34:00<32:07:20, 18.95s/it]                                                         6%|▌         | 399/6500 [2:34:00<32:07:20, 18.95s/it]  6%|▌         | 400/6500 [2:34:18<31:42:11, 18.71s/it]                                                         6%|▌         | 400/6500 [2:34:18<31:42:11, 18.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8546104431152344, 'eval_runtime': 5.3208, 'eval_samples_per_second': 4.323, 'eval_steps_per_second': 1.128, 'epoch': 0.06}
                                                         6%|▌         | 400/6500 [2:34:23<31:42:11, 18.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-400
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-400
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-400/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-400/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7446, 'learning_rate': 9.90725746626209e-05, 'epoch': 0.06}
{'loss': 0.7992, 'learning_rate': 9.906793460385642e-05, 'epoch': 0.06}
{'loss': 0.8252, 'learning_rate': 9.906328307576056e-05, 'epoch': 0.06}
{'loss': 0.7404, 'learning_rate': 9.905862007942058e-05, 'epoch': 0.06}
{'loss': 0.7824, 'learning_rate': 9.905394561592645e-05, 'epoch': 0.06}
{'loss': 0.7584, 'learning_rate': 9.904925968637078e-05, 'epoch': 0.06}
  6%|▌         | 401/6500 [2:35:52<69:43:45, 41.16s/it]                                                         6%|▌         | 401/6500 [2:35:52<69:43:45, 41.16s/it]  6%|▌         | 402/6500 [2:36:10<57:55:06, 34.19s/it]                                                         6%|▌         | 402/6500 [2:36:10<57:55:06, 34.19s/it]  6%|▌         | 403/6500 [2:36:28<49:40:37, 29.33s/it]                                                         6%|▌         | 403/6500 [2:36:28<49:40:37, 29.33s/it]  6%|▌         | 404/6500 [2:36:46<44:06:09, 26.04s/it]                                                         6%|▌         | 404/6500 [2:36:46<44:06:09, 26.04s/it]  6%|▌         | 405/6500 [2:37:04<40:01:57, 23.65s/it]                                                         6%|▌         | 405/6500 [2:37:04<40:01:57, 23.65s/it]  6%|▌         | 406/6500 [2:37:22<37:15:23, 22.01s/it]                                                         6%|▌         | 406/6500 [2:37:22<37:15:{'loss': 0.7661, 'learning_rate': 9.904456229184887e-05, 'epoch': 0.06}
{'loss': 0.7813, 'learning_rate': 9.903985343345873e-05, 'epoch': 0.06}
{'loss': 0.7864, 'learning_rate': 9.903513311230104e-05, 'epoch': 0.06}
{'loss': 0.7792, 'learning_rate': 9.90304013294791e-05, 'epoch': 0.06}
23, 22.01s/it]  6%|▋         | 407/6500 [2:37:40<35:15:50, 20.84s/it]                                                         6%|▋         | 407/6500 [2:37:40<35:15:50, 20.84s/it]  6%|▋         | 408/6500 [2:37:58<33:52:44, 20.02s/it]                                                         6%|▋         | 408/6500 [2:37:58<33:52:44, 20.02s/it]  6%|▋         | 409/6500 [2:38:17<32:54:53, 19.45s/it]                                                         6%|▋         | 409/6500 [2:38:17<32:54:53, 19.45s/it]  6%|▋         | 410/6500 [2:38:35<32:14:49, 19.06s/it]                                                         6%|▋         | 410/6500 [2:38:35<32:14:49, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.851826548576355, 'eval_runtime': 5.3435, 'eval_samples_per_second': 4.304, 'eval_steps_per_second': 1.123, 'epoch': 0.06}
                                                         6%|▋         | 410/6500 [2:38:40<32:14:49, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-410
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-410
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-410

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-410/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7802, 'learning_rate': 9.902565808609896e-05, 'epoch': 0.06}
{'loss': 0.8225, 'learning_rate': 9.902090338326932e-05, 'epoch': 0.06}
{'loss': 0.8089, 'learning_rate': 9.901613722210158e-05, 'epoch': 0.06}
{'loss': 0.7849, 'learning_rate': 9.901135960370977e-05, 'epoch': 0.06}
{'loss': 0.8033, 'learning_rate': 9.900657052921066e-05, 'epoch': 0.06}
{'loss': 0.7647, 'learning_rate': 9.900176999972366e-05, 'epoch': 0.06}
  6%|▋         | 411/6500 [2:39:17<43:48:27, 25.90s/it]                                                         6%|▋         | 411/6500 [2:39:17<43:48:27, 25.90s/it]  6%|▋         | 412/6500 [2:39:35<39:49:09, 23.55s/it]                                                         6%|▋         | 412/6500 [2:39:35<39:49:09, 23.55s/it]  6%|▋         | 413/6500 [2:39:53<37:01:58, 21.90s/it]                                                         6%|▋         | 413/6500 [2:39:53<37:01:58, 21.90s/it]  6%|▋         | 414/6500 [2:40:11<35:05:22, 20.76s/it]                                                         6%|▋         | 414/6500 [2:40:11<35:05:22, 20.76s/it]  6%|▋         | 415/6500 [2:40:29<33:44:17, 19.96s/it]                                                         6%|▋         | 415/6500 [2:40:29<33:44:17, 19.96s/it]  6%|▋         | 416/6500 [2:40:47<32:48:11, 19.41s/it]                                                         6%|▋         | 416/6500 [2:40:47<32:48:{'loss': 0.7591, 'learning_rate': 9.899695801637085e-05, 'epoch': 0.06}
{'loss': 0.8093, 'learning_rate': 9.899213458027701e-05, 'epoch': 0.06}
{'loss': 0.7907, 'learning_rate': 9.898729969256958e-05, 'epoch': 0.06}
{'loss': 0.7759, 'learning_rate': 9.89824533543787e-05, 'epoch': 0.06}
11, 19.41s/it]  6%|▋         | 417/6500 [2:41:05<32:09:12, 19.03s/it]                                                         6%|▋         | 417/6500 [2:41:05<32:09:12, 19.03s/it]  6%|▋         | 418/6500 [2:41:23<31:41:49, 18.76s/it]                                                         6%|▋         | 418/6500 [2:41:23<31:41:49, 18.76s/it]  6%|▋         | 419/6500 [2:41:41<31:23:09, 18.58s/it]                                                         6%|▋         | 419/6500 [2:41:41<31:23:09, 18.58s/it]  6%|▋         | 420/6500 [2:42:00<31:10:21, 18.46s/it]                                                         6%|▋         | 420/6500 [2:42:00<31:10:21, 18.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8480896949768066, 'eval_runtime': 5.3406, 'eval_samples_per_second': 4.307, 'eval_steps_per_second': 1.123, 'epoch': 0.06}
                                                         6%|▋         | 420/6500 [2:42:05<31:10:21, 18.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-420
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-420
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-420

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-420/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7621, 'learning_rate': 9.897759556683716e-05, 'epoch': 0.06}
{'loss': 0.7834, 'learning_rate': 9.897272633108046e-05, 'epoch': 0.06}
{'loss': 0.8459, 'learning_rate': 9.896784564824673e-05, 'epoch': 0.07}
{'loss': 0.7692, 'learning_rate': 9.896295351947681e-05, 'epoch': 0.07}
{'loss': 0.767, 'learning_rate': 9.895804994591421e-05, 'epoch': 0.07}
{'loss': 0.7943, 'learning_rate': 9.89531349287051e-05, 'epoch': 0.07}
  6%|▋         | 421/6500 [2:43:15<60:10:44, 35.64s/it]                                                         6%|▋         | 421/6500 [2:43:15<60:10:44, 35.64s/it]  6%|▋         | 422/6500 [2:43:33<51:14:54, 30.35s/it]                                                         6%|▋         | 422/6500 [2:43:33<51:14:54, 30.35s/it]  7%|▋         | 423/6500 [2:43:51<44:59:15, 26.65s/it]                                                         7%|▋         | 423/6500 [2:43:51<44:59:15, 26.65s/it]  7%|▋         | 424/6500 [2:44:10<40:39:09, 24.09s/it]                                                         7%|▋         | 424/6500 [2:44:10<40:39:09, 24.09s/it]  7%|▋         | 425/6500 [2:44:28<37:34:42, 22.27s/it]                                                         7%|▋         | 425/6500 [2:44:28<37:34:42, 22.27s/it]  7%|▋         | 426/6500 [2:44:46<35:29:37, 21.04s/it]                                                         7%|▋         | 426/6500 [2:44:46<35:29:{'loss': 1.2777, 'learning_rate': 9.894820846899835e-05, 'epoch': 0.07}
{'loss': 0.7863, 'learning_rate': 9.894327056794547e-05, 'epoch': 0.07}
{'loss': 0.7576, 'learning_rate': 9.893832122670068e-05, 'epoch': 0.07}
{'loss': 0.7719, 'learning_rate': 9.893336044642085e-05, 'epoch': 0.07}
37, 21.04s/it]  7%|▋         | 427/6500 [2:45:04<34:01:05, 20.17s/it]                                                         7%|▋         | 427/6500 [2:45:04<34:01:05, 20.17s/it]  7%|▋         | 428/6500 [2:45:22<32:57:35, 19.54s/it]                                                         7%|▋         | 428/6500 [2:45:22<32:57:35, 19.54s/it]  7%|▋         | 429/6500 [2:45:40<32:14:10, 19.12s/it]                                                         7%|▋         | 429/6500 [2:45:40<32:14:10, 19.12s/it]  7%|▋         | 430/6500 [2:45:58<31:44:14, 18.82s/it]                                                         7%|▋         | 430/6500 [2:45:58<31:44:14, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8481532335281372, 'eval_runtime': 5.3347, 'eval_samples_per_second': 4.311, 'eval_steps_per_second': 1.125, 'epoch': 0.07}
                                                         7%|▋         | 430/6500 [2:46:04<31:44:14, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-430
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-430
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7282, 'learning_rate': 9.892838822826553e-05, 'epoch': 0.07}
{'loss': 0.7965, 'learning_rate': 9.892340457339695e-05, 'epoch': 0.07}
{'loss': 0.7761, 'learning_rate': 9.891840948298003e-05, 'epoch': 0.07}
{'loss': 0.752, 'learning_rate': 9.89134029581823e-05, 'epoch': 0.07}
{'loss': 0.7372, 'learning_rate': 9.890838500017403e-05, 'epoch': 0.07}
{'loss': 0.7587, 'learning_rate': 9.890335561012815e-05, 'epoch': 0.07}
  7%|▋         | 431/6500 [2:47:08<57:36:01, 34.17s/it]                                                         7%|▋         | 431/6500 [2:47:08<57:36:01, 34.17s/it]  7%|▋         | 432/6500 [2:47:26<49:25:46, 29.33s/it]                                                         7%|▋         | 432/6500 [2:47:26<49:25:46, 29.33s/it]  7%|▋         | 433/6500 [2:47:44<43:41:01, 25.92s/it]                                                         7%|▋         | 433/6500 [2:47:44<43:41:01, 25.92s/it]  7%|▋         | 434/6500 [2:48:02<39:39:57, 23.54s/it]                                                         7%|▋         | 434/6500 [2:48:02<39:39:57, 23.54s/it]  7%|▋         | 435/6500 [2:48:20<36:52:28, 21.89s/it]                                                         7%|▋         | 435/6500 [2:48:20<36:52:28, 21.89s/it]  7%|▋         | 436/6500 [2:48:38<34:56:08, 20.74s/it]                                                         7%|▋         | 436/6500 [2:48:38<34:56:{'loss': 0.7593, 'learning_rate': 9.889831478922023e-05, 'epoch': 0.07}
{'loss': 0.7434, 'learning_rate': 9.889326253862852e-05, 'epoch': 0.07}
{'loss': 0.8101, 'learning_rate': 9.888819885953398e-05, 'epoch': 0.07}
{'loss': 0.7621, 'learning_rate': 9.888312375312019e-05, 'epoch': 0.07}
08, 20.74s/it]  7%|▋         | 437/6500 [2:48:57<33:51:28, 20.10s/it]                                                         7%|▋         | 437/6500 [2:48:57<33:51:28, 20.10s/it]  7%|▋         | 438/6500 [2:49:15<32:50:13, 19.50s/it]                                                         7%|▋         | 438/6500 [2:49:15<32:50:13, 19.50s/it]  7%|▋         | 439/6500 [2:49:33<32:08:23, 19.09s/it]                                                         7%|▋         | 439/6500 [2:49:33<32:08:23, 19.09s/it]  7%|▋         | 440/6500 [2:49:51<31:39:37, 18.81s/it]                                                         7%|▋         | 440/6500 [2:49:51<31:39:37, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.843244194984436, 'eval_runtime': 5.3429, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.07}
                                                         7%|▋         | 440/6500 [2:49:57<31:39:37, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-440
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-440
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-440/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-440/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7868, 'learning_rate': 9.887803722057344e-05, 'epoch': 0.07}
{'loss': 0.7995, 'learning_rate': 9.887293926308267e-05, 'epoch': 0.07}
{'loss': 0.7913, 'learning_rate': 9.886782988183952e-05, 'epoch': 0.07}
{'loss': 0.7739, 'learning_rate': 9.886270907803823e-05, 'epoch': 0.07}
{'loss': 0.7651, 'learning_rate': 9.88575768528758e-05, 'epoch': 0.07}
{'loss': 0.7333, 'learning_rate': 9.885243320755184e-05, 'epoch': 0.07}
  7%|▋         | 441/6500 [2:51:15<64:33:46, 38.36s/it]                                                         7%|▋         | 441/6500 [2:51:15<64:33:46, 38.36s/it]  7%|▋         | 442/6500 [2:51:33<54:15:26, 32.24s/it]                                                         7%|▋         | 442/6500 [2:51:33<54:15:26, 32.24s/it]  7%|▋         | 443/6500 [2:51:51<47:02:54, 27.96s/it]                                                         7%|▋         | 443/6500 [2:51:51<47:02:54, 27.96s/it]  7%|▋         | 444/6500 [2:52:09<42:00:31, 24.97s/it]                                                         7%|▋         | 444/6500 [2:52:09<42:00:31, 24.97s/it]  7%|▋         | 445/6500 [2:52:27<38:29:40, 22.89s/it]                                                         7%|▋         | 445/6500 [2:52:27<38:29:40, 22.89s/it]  7%|▋         | 446/6500 [2:52:45<36:02:47, 21.43s/it]                                                         7%|▋         | 446/6500 [2:52:45<36:02:{'loss': 0.7717, 'learning_rate': 9.884727814326864e-05, 'epoch': 0.07}
{'loss': 0.7887, 'learning_rate': 9.884211166123116e-05, 'epoch': 0.07}
{'loss': 0.7459, 'learning_rate': 9.883693376264707e-05, 'epoch': 0.07}
{'loss': 0.7652, 'learning_rate': 9.883174444872663e-05, 'epoch': 0.07}
47, 21.43s/it]  7%|▋         | 447/6500 [2:53:03<34:20:44, 20.43s/it]                                                         7%|▋         | 447/6500 [2:53:03<34:20:44, 20.43s/it]  7%|▋         | 448/6500 [2:53:21<33:10:18, 19.73s/it]                                                         7%|▋         | 448/6500 [2:53:21<33:10:18, 19.73s/it]  7%|▋         | 449/6500 [2:53:40<32:21:37, 19.25s/it]                                                         7%|▋         | 449/6500 [2:53:40<32:21:37, 19.25s/it]  7%|▋         | 450/6500 [2:53:58<31:47:55, 18.92s/it]                                                         7%|▋         | 450/6500 [2:53:58<31:47:55, 18.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8409216403961182, 'eval_runtime': 5.3372, 'eval_samples_per_second': 4.309, 'eval_steps_per_second': 1.124, 'epoch': 0.07}
                                                         7%|▋         | 450/6500 [2:54:03<31:47:55, 18.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-450
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-450
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-450

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-450/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-450/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7419, 'learning_rate': 9.882654372068284e-05, 'epoch': 0.07}
{'loss': 0.8044, 'learning_rate': 9.88213315797313e-05, 'epoch': 0.07}
{'loss': 0.8113, 'learning_rate': 9.881610802709036e-05, 'epoch': 0.07}
{'loss': 0.762, 'learning_rate': 9.881087306398097e-05, 'epoch': 0.07}
{'loss': 0.7743, 'learning_rate': 9.880562669162677e-05, 'epoch': 0.07}
{'loss': 0.746, 'learning_rate': 9.880036891125409e-05, 'epoch': 0.07}
  7%|▋         | 451/6500 [2:55:12<59:34:05, 35.45s/it]                                                         7%|▋         | 451/6500 [2:55:12<59:34:05, 35.45s/it]  7%|▋         | 452/6500 [2:55:30<50:44:50, 30.21s/it]                                                         7%|▋         | 452/6500 [2:55:30<50:44:50, 30.21s/it]  7%|▋         | 453/6500 [2:55:48<44:41:39, 26.61s/it]                                                         7%|▋         | 453/6500 [2:55:48<44:41:39, 26.61s/it]  7%|▋         | 454/6500 [2:56:06<40:20:03, 24.02s/it]                                                         7%|▋         | 454/6500 [2:56:06<40:20:03, 24.02s/it]  7%|▋         | 455/6500 [2:56:24<37:17:57, 22.21s/it]                                                         7%|▋         | 455/6500 [2:56:24<37:17:57, 22.21s/it]  7%|▋         | 456/6500 [2:56:42<35:11:35, 20.96s/it]                                                         7%|▋         | 456/6500 [2:56:42<35:11:{'loss': 1.2597, 'learning_rate': 9.879509972409188e-05, 'epoch': 0.07}
{'loss': 0.7654, 'learning_rate': 9.878981913137179e-05, 'epoch': 0.07}
{'loss': 0.7594, 'learning_rate': 9.878452713432813e-05, 'epoch': 0.07}
{'loss': 0.7016, 'learning_rate': 9.877922373419786e-05, 'epoch': 0.07}
35, 20.96s/it]  7%|▋         | 457/6500 [2:57:00<33:43:19, 20.09s/it]                                                         7%|▋         | 457/6500 [2:57:00<33:43:19, 20.09s/it]  7%|▋         | 458/6500 [2:57:18<32:45:44, 19.52s/it]                                                         7%|▋         | 458/6500 [2:57:18<32:45:44, 19.52s/it]  7%|▋         | 459/6500 [2:57:36<32:02:35, 19.10s/it]                                                         7%|▋         | 459/6500 [2:57:36<32:02:35, 19.10s/it]  7%|▋         | 460/6500 [2:57:54<31:32:58, 18.80s/it]                                                         7%|▋         | 460/6500 [2:57:54<31:32:58, 18.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8402305245399475, 'eval_runtime': 5.3342, 'eval_samples_per_second': 4.312, 'eval_steps_per_second': 1.125, 'epoch': 0.07}
                                                         7%|▋         | 460/6500 [2:58:00<31:32:58, 18.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-460

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-460the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-460

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-460/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-460/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7577, 'learning_rate': 9.877390893222061e-05, 'epoch': 0.07}
{'loss': 0.8041, 'learning_rate': 9.876858272963871e-05, 'epoch': 0.07}
{'loss': 0.7393, 'learning_rate': 9.876324512769713e-05, 'epoch': 0.07}
{'loss': 0.747, 'learning_rate': 9.875789612764346e-05, 'epoch': 0.07}
{'loss': 0.7253, 'learning_rate': 9.875253573072804e-05, 'epoch': 0.07}
{'loss': 0.7428, 'learning_rate': 9.874716393820383e-05, 'epoch': 0.07}
  7%|▋         | 461/6500 [2:59:05<57:31:45, 34.29s/it]                                                         7%|▋         | 461/6500 [2:59:05<57:31:45, 34.29s/it]  7%|▋         | 462/6500 [2:59:23<49:18:36, 29.40s/it]                                                         7%|▋         | 462/6500 [2:59:23<49:18:36, 29.40s/it]  7%|▋         | 463/6500 [2:59:41<43:33:57, 25.98s/it]                                                         7%|▋         | 463/6500 [2:59:41<43:33:57, 25.98s/it]  7%|▋         | 464/6500 [2:59:59<39:38:17, 23.64s/it]                                                         7%|▋         | 464/6500 [2:59:59<39:38:17, 23.64s/it]  7%|▋         | 465/6500 [3:00:17<36:49:53, 21.97s/it]                                                         7%|▋         | 465/6500 [3:00:17<36:49:53, 21.97s/it]  7%|▋         | 466/6500 [3:00:35<34:52:01, 20.80s/it]                                                         7%|▋         | 466/6500 [3:00:35<34:52:{'loss': 0.7386, 'learning_rate': 9.87417807513264e-05, 'epoch': 0.07}
{'loss': 0.747, 'learning_rate': 9.87363861713541e-05, 'epoch': 0.07}
{'loss': 0.7922, 'learning_rate': 9.873098019954786e-05, 'epoch': 0.07}
{'loss': 0.736, 'learning_rate': 9.872556283717125e-05, 'epoch': 0.07}
01, 20.80s/it]  7%|▋         | 467/6500 [3:00:53<33:30:02, 19.99s/it]                                                         7%|▋         | 467/6500 [3:00:53<33:30:02, 19.99s/it]  7%|▋         | 468/6500 [3:01:11<32:33:17, 19.43s/it]                                                         7%|▋         | 468/6500 [3:01:11<32:33:17, 19.43s/it]  7%|▋         | 469/6500 [3:01:31<32:32:14, 19.42s/it]                                                         7%|▋         | 469/6500 [3:01:31<32:32:14, 19.42s/it]  7%|▋         | 470/6500 [3:01:49<31:53:26, 19.04s/it]                                                         7%|▋         | 470/6500 [3:01:49<31:53:26, 19.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8388095498085022, 'eval_runtime': 5.8616, 'eval_samples_per_second': 3.924, 'eval_steps_per_second': 1.024, 'epoch': 0.07}
                                                         7%|▋         | 470/6500 [3:01:55<31:53:26, 19.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-470
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-470

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-470
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-470/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-470/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-470/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8036, 'learning_rate': 9.872013408549061e-05, 'epoch': 0.07}
{'loss': 0.7632, 'learning_rate': 9.871469394577484e-05, 'epoch': 0.07}
{'loss': 0.7504, 'learning_rate': 9.870924241929558e-05, 'epoch': 0.07}
{'loss': 0.7726, 'learning_rate': 9.870377950732703e-05, 'epoch': 0.07}
{'loss': 0.7345, 'learning_rate': 9.869830521114616e-05, 'epoch': 0.07}
{'loss': 0.7473, 'learning_rate': 9.869281953203254e-05, 'epoch': 0.07}
  7%|▋         | 471/6500 [3:03:27<71:33:48, 42.73s/it]                                                         7%|▋         | 471/6500 [3:03:27<71:33:48, 42.73s/it]  7%|▋         | 472/6500 [3:03:45<59:07:05, 35.31s/it]                                                         7%|▋         | 472/6500 [3:03:45<59:07:05, 35.31s/it]  7%|▋         | 473/6500 [3:04:03<50:24:22, 30.11s/it]                                                         7%|▋         | 473/6500 [3:04:03<50:24:22, 30.11s/it]  7%|▋         | 474/6500 [3:04:21<44:19:05, 26.48s/it]                                                         7%|▋         | 474/6500 [3:04:21<44:19:05, 26.48s/it]  7%|▋         | 475/6500 [3:04:39<40:04:33, 23.95s/it]                                                         7%|▋         | 475/6500 [3:04:39<40:04:33, 23.95s/it]  7%|▋         | 476/6500 [3:04:57<37:07:03, 22.18s/it]                                                         7%|▋         | 476/6500 [3:04:57<37:07:{'loss': 0.745, 'learning_rate': 9.86873224712684e-05, 'epoch': 0.07}
{'loss': 0.7752, 'learning_rate': 9.868181403013865e-05, 'epoch': 0.07}
{'loss': 0.7495, 'learning_rate': 9.867629420993086e-05, 'epoch': 0.07}
{'loss': 0.7499, 'learning_rate': 9.867076301193528e-05, 'epoch': 0.07}
03, 22.18s/it]  7%|▋         | 477/6500 [3:05:15<35:03:27, 20.95s/it]                                                         7%|▋         | 477/6500 [3:05:15<35:03:27, 20.95s/it]  7%|▋         | 478/6500 [3:05:33<33:37:37, 20.10s/it]                                                         7%|▋         | 478/6500 [3:05:33<33:37:37, 20.10s/it]  7%|▋         | 479/6500 [3:05:51<32:37:49, 19.51s/it]                                                         7%|▋         | 479/6500 [3:05:51<32:37:49, 19.51s/it]  7%|▋         | 480/6500 [3:06:09<31:56:27, 19.10s/it]                                                         7%|▋         | 480/6500 [3:06:09<31:56:27, 19.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8332237601280212, 'eval_runtime': 5.672, 'eval_samples_per_second': 4.055, 'eval_steps_per_second': 1.058, 'epoch': 0.07}
                                                         7%|▋         | 480/6500 [3:06:15<31:56:27, 19.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-480
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-480
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-480
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-480

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-480/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-480/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-480/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7313, 'learning_rate': 9.866522043744475e-05, 'epoch': 0.07}
{'loss': 0.8411, 'learning_rate': 9.865966648775483e-05, 'epoch': 0.07}
{'loss': 0.7433, 'learning_rate': 9.865410116416374e-05, 'epoch': 0.07}
{'loss': 0.7285, 'learning_rate': 9.86485244679723e-05, 'epoch': 0.07}
{'loss': 0.7597, 'learning_rate': 9.864293640048407e-05, 'epoch': 0.07}
{'loss': 1.2407, 'learning_rate': 9.863733696300521e-05, 'epoch': 0.07}
  7%|▋         | 481/6500 [3:07:29<62:26:13, 37.34s/it]                                                         7%|▋         | 481/6500 [3:07:29<62:26:13, 37.34s/it]  7%|▋         | 482/6500 [3:07:47<52:43:26, 31.54s/it]                                                         7%|▋         | 482/6500 [3:07:47<52:43:26, 31.54s/it]  7%|▋         | 483/6500 [3:08:05<45:55:53, 27.48s/it]                                                         7%|▋         | 483/6500 [3:08:05<45:55:53, 27.48s/it]  7%|▋         | 484/6500 [3:08:23<41:11:03, 24.64s/it]                                                         7%|▋         | 484/6500 [3:08:23<41:11:03, 24.64s/it]  7%|▋         | 485/6500 [3:08:42<38:02:36, 22.77s/it]                                                         7%|▋         | 485/6500 [3:08:42<38:02:36, 22.77s/it]  7%|▋         | 486/6500 [3:09:00<35:40:21, 21.35s/it]                                                         7%|▋         | 486/6500 [3:09:00<35:40:{'loss': 0.7598, 'learning_rate': 9.863172615684455e-05, 'epoch': 0.07}
{'loss': 0.7509, 'learning_rate': 9.86261039833136e-05, 'epoch': 0.08}
{'loss': 0.7345, 'learning_rate': 9.862047044372648e-05, 'epoch': 0.08}
{'loss': 0.6975, 'learning_rate': 9.861482553940003e-05, 'epoch': 0.08}
21, 21.35s/it]  7%|▋         | 487/6500 [3:09:18<34:02:04, 20.38s/it]                                                         7%|▋         | 487/6500 [3:09:18<34:02:04, 20.38s/it]  8%|▊         | 488/6500 [3:09:36<32:54:01, 19.70s/it]                                                         8%|▊         | 488/6500 [3:09:36<32:54:01, 19.70s/it]  8%|▊         | 489/6500 [3:09:54<32:06:52, 19.23s/it]                                                         8%|▊         | 489/6500 [3:09:54<32:06:52, 19.23s/it]  8%|▊         | 490/6500 [3:10:12<31:34:15, 18.91s/it]                                                         8%|▊         | 490/6500 [3:10:12<31:34:15, 18.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8336156010627747, 'eval_runtime': 6.4962, 'eval_samples_per_second': 3.541, 'eval_steps_per_second': 0.924, 'epoch': 0.08}
                                                         8%|▊         | 490/6500 [3:10:19<31:34:15, 18.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-490
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-490
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-490

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-490/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-490/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7765, 'learning_rate': 9.860916927165366e-05, 'epoch': 0.08}
{'loss': 0.762, 'learning_rate': 9.860350164180954e-05, 'epoch': 0.08}
{'loss': 0.7027, 'learning_rate': 9.859782265119244e-05, 'epoch': 0.08}
{'loss': 0.7315, 'learning_rate': 9.859213230112976e-05, 'epoch': 0.08}
{'loss': 0.7112, 'learning_rate': 9.85864305929516e-05, 'epoch': 0.08}
{'loss': 0.7366, 'learning_rate': 9.85807175279907e-05, 'epoch': 0.08}
  8%|▊         | 491/6500 [3:11:29<60:27:17, 36.22s/it]                                                         8%|▊         | 491/6500 [3:11:29<60:27:17, 36.22s/it]  8%|▊         | 492/6500 [3:11:47<51:18:57, 30.75s/it]                                                         8%|▊         | 492/6500 [3:11:47<51:18:57, 30.75s/it]  8%|▊         | 493/6500 [3:12:05<44:56:07, 26.93s/it]                                                         8%|▊         | 493/6500 [3:12:05<44:56:07, 26.93s/it]  8%|▊         | 494/6500 [3:12:23<40:28:37, 24.26s/it]                                                         8%|▊         | 494/6500 [3:12:23<40:28:37, 24.26s/it]  8%|▊         | 495/6500 [3:12:41<37:25:09, 22.43s/it]                                                         8%|▊         | 495/6500 [3:12:41<37:25:09, 22.43s/it]  8%|▊         | 496/6500 [3:12:59<35:14:26, 21.13s/it]                                                         8%|▊         | 496/6500 [3:12:59<35:14:{'loss': 0.719, 'learning_rate': 9.857499310758245e-05, 'epoch': 0.08}
{'loss': 0.7498, 'learning_rate': 9.85692573330649e-05, 'epoch': 0.08}
{'loss': 0.7337, 'learning_rate': 9.856351020577876e-05, 'epoch': 0.08}
{'loss': 0.7437, 'learning_rate': 9.855775172706738e-05, 'epoch': 0.08}
26, 21.13s/it]  8%|▊         | 497/6500 [3:13:17<33:46:27, 20.25s/it]                                                         8%|▊         | 497/6500 [3:13:17<33:46:27, 20.25s/it]  8%|▊         | 498/6500 [3:13:36<32:44:27, 19.64s/it]                                                         8%|▊         | 498/6500 [3:13:36<32:44:27, 19.64s/it]  8%|▊         | 499/6500 [3:13:54<31:59:38, 19.19s/it]                                                         8%|▊         | 499/6500 [3:13:54<31:59:38, 19.19s/it]  8%|▊         | 500/6500 [3:14:12<31:31:40, 18.92s/it]                                                         8%|▊         | 500/6500 [3:14:12<31:31:40, 18.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8300136923789978, 'eval_runtime': 5.7616, 'eval_samples_per_second': 3.992, 'eval_steps_per_second': 1.041, 'epoch': 0.08}
                                                         8%|▊         | 500/6500 [3:14:18<31:31:40, 18.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-500
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-500
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-500/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-500/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7815, 'learning_rate': 9.855198189827677e-05, 'epoch': 0.08}
{'loss': 0.7788, 'learning_rate': 9.85462007207556e-05, 'epoch': 0.08}
{'loss': 0.7464, 'learning_rate': 9.854040819585517e-05, 'epoch': 0.08}
{'loss': 0.7565, 'learning_rate': 9.853460432492944e-05, 'epoch': 0.08}
{'loss': 0.7287, 'learning_rate': 9.852878910933507e-05, 'epoch': 0.08}
{'loss': 0.707, 'learning_rate': 9.852296255043129e-05, 'epoch': 0.08}
  8%|▊         | 501/6500 [3:15:57<74:26:22, 44.67s/it]                                                         8%|▊         | 501/6500 [3:15:57<74:26:22, 44.67s/it]  8%|▊         | 502/6500 [3:16:15<61:14:46, 36.76s/it]                                                         8%|▊         | 502/6500 [3:16:15<61:14:46, 36.76s/it]  8%|▊         | 503/6500 [3:16:33<51:52:11, 31.14s/it]                                                         8%|▊         | 503/6500 [3:16:33<51:52:11, 31.14s/it]  8%|▊         | 504/6500 [3:16:51<45:19:33, 27.21s/it]                                                         8%|▊         | 504/6500 [3:16:51<45:19:33, 27.21s/it]  8%|▊         | 505/6500 [3:17:09<40:45:43, 24.48s/it]                                                         8%|▊         | 505/6500 [3:17:09<40:45:43, 24.48s/it]  8%|▊         | 506/6500 [3:17:27<37:34:38, 22.57s/it]                                                         8%|▊         | 506/6500 [3:17:27<37:34:{'loss': 0.7666, 'learning_rate': 9.851712464958005e-05, 'epoch': 0.08}
{'loss': 0.7477, 'learning_rate': 9.85112754081459e-05, 'epoch': 0.08}
{'loss': 0.7333, 'learning_rate': 9.850541482749608e-05, 'epoch': 0.08}
{'loss': 0.7126, 'learning_rate': 9.849954290900046e-05, 'epoch': 0.08}
38, 22.57s/it]  8%|▊         | 507/6500 [3:17:46<35:20:14, 21.23s/it]                                                         8%|▊         | 507/6500 [3:17:46<35:20:14, 21.23s/it]  8%|▊         | 508/6500 [3:18:04<33:47:24, 20.30s/it]                                                         8%|▊         | 508/6500 [3:18:04<33:47:24, 20.30s/it]  8%|▊         | 509/6500 [3:18:22<32:42:21, 19.65s/it]                                                         8%|▊         | 509/6500 [3:18:22<32:42:21, 19.65s/it]  8%|▊         | 510/6500 [3:18:40<31:56:59, 19.20s/it]                                                         8%|▊         | 510/6500 [3:18:40<31:56:59, 19.20s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8285696506500244, 'eval_runtime': 5.4873, 'eval_samples_per_second': 4.192, 'eval_steps_per_second': 1.093, 'epoch': 0.08}
                                                         8%|▊         | 510/6500 [3:18:45<31:56:59, 19.20s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-510
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-510the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-510
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-510

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-510
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-510/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-510/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7545, 'learning_rate': 9.849365965403157e-05, 'epoch': 0.08}
{'loss': 0.7807, 'learning_rate': 9.848776506396458e-05, 'epoch': 0.08}
{'loss': 0.7559, 'learning_rate': 9.848185914017733e-05, 'epoch': 0.08}
{'loss': 0.7195, 'learning_rate': 9.847594188405027e-05, 'epoch': 0.08}
{'loss': 0.7434, 'learning_rate': 9.847001329696653e-05, 'epoch': 0.08}
{'loss': 1.2335, 'learning_rate': 9.846407338031189e-05, 'epoch': 0.08}
  8%|▊         | 511/6500 [3:19:37<50:41:33, 30.47s/it]                                                         8%|▊         | 511/6500 [3:19:37<50:41:33, 30.47s/it]  8%|▊         | 512/6500 [3:19:55<44:29:46, 26.75s/it]                                                         8%|▊         | 512/6500 [3:19:55<44:29:46, 26.75s/it]  8%|▊         | 513/6500 [3:20:13<40:08:46, 24.14s/it]                                                         8%|▊         | 513/6500 [3:20:13<40:08:46, 24.14s/it]  8%|▊         | 514/6500 [3:20:31<37:07:21, 22.33s/it]                                                         8%|▊         | 514/6500 [3:20:31<37:07:21, 22.33s/it]  8%|▊         | 515/6500 [3:20:49<34:59:30, 21.05s/it]                                                         8%|▊         | 515/6500 [3:20:49<34:59:30, 21.05s/it]  8%|▊         | 516/6500 [3:21:07<33:29:42, 20.15s/it]                                                         8%|▊         | 516/6500 [3:21:07<33:29:{'loss': 0.7477, 'learning_rate': 9.845812213547475e-05, 'epoch': 0.08}
{'loss': 0.7038, 'learning_rate': 9.84521595638462e-05, 'epoch': 0.08}
{'loss': 0.6998, 'learning_rate': 9.844618566681996e-05, 'epoch': 0.08}
{'loss': 0.7138, 'learning_rate': 9.844020044579237e-05, 'epoch': 0.08}
42, 20.15s/it]  8%|▊         | 517/6500 [3:21:25<32:28:08, 19.54s/it]                                                         8%|▊         | 517/6500 [3:21:25<32:28:08, 19.54s/it]  8%|▊         | 518/6500 [3:21:44<31:57:41, 19.23s/it]                                                         8%|▊         | 518/6500 [3:21:44<31:57:41, 19.23s/it]  8%|▊         | 519/6500 [3:22:02<31:30:30, 18.97s/it]                                                         8%|▊         | 519/6500 [3:22:02<31:30:30, 18.97s/it]  8%|▊         | 520/6500 [3:22:20<31:06:11, 18.72s/it]                                                         8%|▊         | 520/6500 [3:22:20<31:06:11, 18.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8280140161514282, 'eval_runtime': 5.3217, 'eval_samples_per_second': 4.322, 'eval_steps_per_second': 1.127, 'epoch': 0.08}
                                                         8%|▊         | 520/6500 [3:22:26<31:06:11, 18.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-520
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-520
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-520
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-520

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-520
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-520/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.774, 'learning_rate': 9.843420390216242e-05, 'epoch': 0.08}
{'loss': 0.7262, 'learning_rate': 9.842819603733182e-05, 'epoch': 0.08}
{'loss': 0.7257, 'learning_rate': 9.842217685270484e-05, 'epoch': 0.08}
{'loss': 0.6859, 'learning_rate': 9.841614634968843e-05, 'epoch': 0.08}
{'loss': 0.7267, 'learning_rate': 9.84101045296922e-05, 'epoch': 0.08}
{'loss': 0.7106, 'learning_rate': 9.840405139412836e-05, 'epoch': 0.08}
  8%|▊         | 521/6500 [3:23:12<47:45:09, 28.75s/it]                                                         8%|▊         | 521/6500 [3:23:12<47:45:09, 28.75s/it]  8%|▊         | 522/6500 [3:23:30<42:24:03, 25.53s/it]                                                         8%|▊         | 522/6500 [3:23:30<42:24:03, 25.53s/it]  8%|▊         | 523/6500 [3:23:48<38:39:22, 23.28s/it]                                                         8%|▊         | 523/6500 [3:23:48<38:39:22, 23.28s/it]  8%|▊         | 524/6500 [3:24:06<36:02:34, 21.71s/it]                                                         8%|▊         | 524/6500 [3:24:06<36:02:34, 21.71s/it]  8%|▊         | 525/6500 [3:24:25<34:13:23, 20.62s/it]                                                         8%|▊         | 525/6500 [3:24:25<34:13:23, 20.62s/it]  8%|▊         | 526/6500 [3:24:43<32:57:13, 19.86s/it]                                                         8%|▊         | 526/6500 [3:24:43<32:57:{'loss': 0.7152, 'learning_rate': 9.83979869444118e-05, 'epoch': 0.08}
{'loss': 0.7747, 'learning_rate': 9.839191118196007e-05, 'epoch': 0.08}
{'loss': 0.7235, 'learning_rate': 9.838582410819332e-05, 'epoch': 0.08}
{'loss': 0.7486, 'learning_rate': 9.83797257245344e-05, 'epoch': 0.08}
13, 19.86s/it]  8%|▊         | 527/6500 [3:25:01<32:04:13, 19.33s/it]                                                         8%|▊         | 527/6500 [3:25:01<32:04:13, 19.33s/it]  8%|▊         | 528/6500 [3:25:19<31:28:19, 18.97s/it]                                                         8%|▊         | 528/6500 [3:25:19<31:28:19, 18.97s/it]  8%|▊         | 529/6500 [3:25:37<31:03:10, 18.72s/it]                                                         8%|▊         | 529/6500 [3:25:37<31:03:10, 18.72s/it]  8%|▊         | 530/6500 [3:25:55<30:45:52, 18.55s/it]                                                         8%|▊         | 530/6500 [3:25:55<30:45:52, 18.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8255618810653687, 'eval_runtime': 5.3196, 'eval_samples_per_second': 4.324, 'eval_steps_per_second': 1.128, 'epoch': 0.08}
                                                         8%|▊         | 530/6500 [3:26:00<30:45:52, 18.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-530
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-530
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-530/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-530


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-530
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-530/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7466, 'learning_rate': 9.837361603240872e-05, 'epoch': 0.08}
{'loss': 0.7284, 'learning_rate': 9.836749503324442e-05, 'epoch': 0.08}
{'loss': 0.7347, 'learning_rate': 9.836136272847223e-05, 'epoch': 0.08}
{'loss': 0.7273, 'learning_rate': 9.835521911952555e-05, 'epoch': 0.08}
{'loss': 0.6882, 'learning_rate': 9.83490642078404e-05, 'epoch': 0.08}
{'loss': 0.7359, 'learning_rate': 9.834289799485545e-05, 'epoch': 0.08}
  8%|▊         | 531/6500 [3:27:32<69:51:35, 42.13s/it]                                                         8%|▊         | 531/6500 [3:27:32<69:51:35, 42.13s/it]  8%|▊         | 532/6500 [3:27:50<57:49:45, 34.88s/it]                                                         8%|▊         | 532/6500 [3:27:50<57:49:45, 34.88s/it]  8%|▊         | 533/6500 [3:28:08<49:24:35, 29.81s/it]                                                         8%|▊         | 533/6500 [3:28:08<49:24:35, 29.81s/it]  8%|▊         | 534/6500 [3:28:27<43:43:23, 26.38s/it]                                                         8%|▊         | 534/6500 [3:28:27<43:43:23, 26.38s/it]  8%|▊         | 535/6500 [3:28:45<39:32:38, 23.87s/it]                                                         8%|▊         | 535/6500 [3:28:45<39:32:38, 23.87s/it]  8%|▊         | 536/6500 [3:29:03<36:38:39, 22.12s/it]                                                         8%|▊         | 536/6500 [3:29:03<36:38:{'loss': 0.7643, 'learning_rate': 9.833672048201204e-05, 'epoch': 0.08}
{'loss': 0.6984, 'learning_rate': 9.83305316707541e-05, 'epoch': 0.08}
{'loss': 0.7405, 'learning_rate': 9.832433156252822e-05, 'epoch': 0.08}
{'loss': 0.6845, 'learning_rate': 9.831812015878368e-05, 'epoch': 0.08}
39, 22.12s/it]  8%|▊         | 537/6500 [3:29:21<34:37:12, 20.90s/it]                                                         8%|▊         | 537/6500 [3:29:21<34:37:12, 20.90s/it]  8%|▊         | 538/6500 [3:29:39<33:12:50, 20.06s/it]                                                         8%|▊         | 538/6500 [3:29:39<33:12:50, 20.06s/it]  8%|▊         | 539/6500 [3:29:57<32:14:02, 19.47s/it]                                                         8%|▊         | 539/6500 [3:29:57<32:14:02, 19.47s/it]  8%|▊         | 540/6500 [3:30:15<31:33:43, 19.06s/it]                                                         8%|▊         | 540/6500 [3:30:15<31:33:43, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8229918479919434, 'eval_runtime': 5.33, 'eval_samples_per_second': 4.315, 'eval_steps_per_second': 1.126, 'epoch': 0.08}
                                                         8%|▊         | 540/6500 [3:30:20<31:33:43, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-540
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-540

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-540
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-540

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-540
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-540/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-540/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7713, 'learning_rate': 9.831189746097232e-05, 'epoch': 0.08}
{'loss': 0.7572, 'learning_rate': 9.830566347054868e-05, 'epoch': 0.08}
{'loss': 0.719, 'learning_rate': 9.82994181889699e-05, 'epoch': 0.08}
{'loss': 0.7366, 'learning_rate': 9.829316161769578e-05, 'epoch': 0.08}
{'loss': 0.7069, 'learning_rate': 9.828689375818877e-05, 'epoch': 0.08}
{'loss': 1.2192, 'learning_rate': 9.828061461191392e-05, 'epoch': 0.08}
  8%|▊         | 541/6500 [3:32:05<76:33:48, 46.25s/it]                                                         8%|▊         | 541/6500 [3:32:05<76:33:48, 46.25s/it]  8%|▊         | 542/6500 [3:32:23<62:29:56, 37.76s/it]                                                         8%|▊         | 542/6500 [3:32:23<62:29:56, 37.76s/it]  8%|▊         | 543/6500 [3:32:41<52:38:51, 31.82s/it]                                                         8%|▊         | 543/6500 [3:32:41<52:38:51, 31.82s/it]  8%|▊         | 544/6500 [3:32:59<45:46:39, 27.67s/it]                                                         8%|▊         | 544/6500 [3:32:59<45:46:39, 27.67s/it]  8%|▊         | 545/6500 [3:33:17<40:58:36, 24.77s/it]                                                         8%|▊         | 545/6500 [3:33:17<40:58:36, 24.77s/it]  8%|▊         | 546/6500 [3:33:35<37:37:33, 22.75s/it]                                                         8%|▊         | 546/6500 [3:33:35<37:37:{'loss': 0.7225, 'learning_rate': 9.827432418033897e-05, 'epoch': 0.08}
{'loss': 0.7208, 'learning_rate': 9.826802246493425e-05, 'epoch': 0.08}
{'loss': 0.6751, 'learning_rate': 9.826170946717274e-05, 'epoch': 0.08}
{'loss': 0.7411, 'learning_rate': 9.825538518853009e-05, 'epoch': 0.08}
33, 22.75s/it]  8%|▊         | 547/6500 [3:33:53<35:17:27, 21.34s/it]                                                         8%|▊         | 547/6500 [3:33:53<35:17:27, 21.34s/it]  8%|▊         | 548/6500 [3:34:11<33:40:02, 20.36s/it]                                                         8%|▊         | 548/6500 [3:34:11<33:40:02, 20.36s/it]  8%|▊         | 549/6500 [3:34:29<32:32:59, 19.69s/it]                                                         8%|▊         | 549/6500 [3:34:29<32:32:59, 19.69s/it]  8%|▊         | 550/6500 [3:34:47<31:58:09, 19.34s/it]                                                         8%|▊         | 550/6500 [3:34:47<31:58:09, 19.34s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8225523233413696, 'eval_runtime': 5.4685, 'eval_samples_per_second': 4.206, 'eval_steps_per_second': 1.097, 'epoch': 0.08}
                                                         8%|▊         | 550/6500 [3:34:53<31:58:09, 19.34s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-550
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-550
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-550
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-550

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-550
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-550/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-550/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7437, 'learning_rate': 9.824904963048455e-05, 'epoch': 0.08}
{'loss': 0.6847, 'learning_rate': 9.824270279451701e-05, 'epoch': 0.08}
{'loss': 0.6987, 'learning_rate': 9.823634468211103e-05, 'epoch': 0.09}
{'loss': 0.6871, 'learning_rate': 9.822997529475275e-05, 'epoch': 0.09}
{'loss': 0.7096, 'learning_rate': 9.822359463393099e-05, 'epoch': 0.09}
{'loss': 0.6947, 'learning_rate': 9.821720270113718e-05, 'epoch': 0.09}
  8%|▊         | 551/6500 [3:35:58<57:18:07, 34.68s/it]                                                         8%|▊         | 551/6500 [3:35:58<57:18:07, 34.68s/it]  8%|▊         | 552/6500 [3:36:16<49:01:07, 29.67s/it]                                                         8%|▊         | 552/6500 [3:36:16<49:01:07, 29.67s/it]  9%|▊         | 553/6500 [3:36:34<43:13:31, 26.17s/it]                                                         9%|▊         | 553/6500 [3:36:34<43:13:31, 26.17s/it]  9%|▊         | 554/6500 [3:36:52<39:10:02, 23.71s/it]                                                         9%|▊         | 554/6500 [3:36:52<39:10:02, 23.71s/it]  9%|▊         | 555/6500 [3:37:10<36:20:24, 22.01s/it]                                                         9%|▊         | 555/6500 [3:37:10<36:20:24, 22.01s/it]  9%|▊         | 556/6500 [3:37:28<34:22:42, 20.82s/it]                                                         9%|▊         | 556/6500 [3:37:28<34:22:{'loss': 0.7247, 'learning_rate': 9.821079949786541e-05, 'epoch': 0.09}
{'loss': 0.7275, 'learning_rate': 9.820438502561238e-05, 'epoch': 0.09}
{'loss': 0.6907, 'learning_rate': 9.819795928587745e-05, 'epoch': 0.09}
{'loss': 0.7596, 'learning_rate': 9.819152228016257e-05, 'epoch': 0.09}
42, 20.82s/it]  9%|▊         | 557/6500 [3:37:46<33:00:34, 20.00s/it]                                                         9%|▊         | 557/6500 [3:37:46<33:00:34, 20.00s/it]  9%|▊         | 558/6500 [3:38:04<32:03:40, 19.42s/it]                                                         9%|▊         | 558/6500 [3:38:04<32:03:40, 19.42s/it]  9%|▊         | 559/6500 [3:38:22<31:23:56, 19.03s/it]                                                         9%|▊         | 559/6500 [3:38:22<31:23:56, 19.03s/it]  9%|▊         | 560/6500 [3:38:40<30:56:34, 18.75s/it]                                                         9%|▊         | 560/6500 [3:38:40<30:56:34, 18.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8214421272277832, 'eval_runtime': 5.3316, 'eval_samples_per_second': 4.314, 'eval_steps_per_second': 1.125, 'epoch': 0.09}
                                                         9%|▊         | 560/6500 [3:38:46<30:56:34, 18.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-560
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-560
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-560
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-560

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-560
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-560/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-560/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7244, 'learning_rate': 9.81850740099724e-05, 'epoch': 0.09}
{'loss': 0.7288, 'learning_rate': 9.817861447681411e-05, 'epoch': 0.09}
{'loss': 0.7474, 'learning_rate': 9.817214368219763e-05, 'epoch': 0.09}
{'loss': 0.6891, 'learning_rate': 9.816566162763546e-05, 'epoch': 0.09}
{'loss': 0.6916, 'learning_rate': 9.815916831464273e-05, 'epoch': 0.09}
{'loss': 0.7376, 'learning_rate': 9.815266374473721e-05, 'epoch': 0.09}
  9%|▊         | 561/6500 [3:40:27<74:24:55, 45.11s/it]                                                         9%|▊         | 561/6500 [3:40:27<74:24:55, 45.11s/it]  9%|▊         | 562/6500 [3:40:45<60:57:45, 36.96s/it]                                                         9%|▊         | 562/6500 [3:40:45<60:57:45, 36.96s/it]  9%|▊         | 563/6500 [3:41:03<51:33:53, 31.27s/it]                                                         9%|▊         | 563/6500 [3:41:03<51:33:53, 31.27s/it]  9%|▊         | 564/6500 [3:41:21<44:59:35, 27.29s/it]                                                         9%|▊         | 564/6500 [3:41:21<44:59:35, 27.29s/it]  9%|▊         | 565/6500 [3:41:39<40:24:44, 24.51s/it]                                                         9%|▊         | 565/6500 [3:41:39<40:24:44, 24.51s/it]  9%|▊         | 566/6500 [3:41:57<37:18:53, 22.64s/it]                                                         9%|▊         | 566/6500 [3:41:57<37:18:{'loss': 0.7326, 'learning_rate': 9.814614791943933e-05, 'epoch': 0.09}
{'loss': 0.7032, 'learning_rate': 9.813962084027211e-05, 'epoch': 0.09}
{'loss': 0.7056, 'learning_rate': 9.813308250876121e-05, 'epoch': 0.09}
{'loss': 0.6946, 'learning_rate': 9.812653292643492e-05, 'epoch': 0.09}
53, 22.64s/it]  9%|▊         | 567/6500 [3:42:15<35:05:58, 21.30s/it]                                                         9%|▊         | 567/6500 [3:42:15<35:05:58, 21.30s/it]  9%|▊         | 568/6500 [3:42:34<33:34:09, 20.37s/it]                                                         9%|▊         | 568/6500 [3:42:34<33:34:09, 20.37s/it]  9%|▉         | 569/6500 [3:42:52<32:28:10, 19.71s/it]                                                         9%|▉         | 569/6500 [3:42:52<32:28:10, 19.71s/it]  9%|▉         | 570/6500 [3:43:10<31:41:15, 19.24s/it]                                                         9%|▉         | 570/6500 [3:43:10<31:41:15, 19.24s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8164215683937073, 'eval_runtime': 5.4585, 'eval_samples_per_second': 4.214, 'eval_steps_per_second': 1.099, 'epoch': 0.09}
                                                         9%|▉         | 570/6500 [3:43:15<31:41:15, 19.24s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-570
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-570I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-570
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-570

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-570
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-570/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-570/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8019, 'learning_rate': 9.811997209482418e-05, 'epoch': 0.09}
{'loss': 0.6989, 'learning_rate': 9.811340001546251e-05, 'epoch': 0.09}
{'loss': 0.698, 'learning_rate': 9.810681668988615e-05, 'epoch': 0.09}
{'loss': 0.7186, 'learning_rate': 9.810022211963388e-05, 'epoch': 0.09}
{'loss': 1.2154, 'learning_rate': 9.809361630624714e-05, 'epoch': 0.09}
{'loss': 0.7217, 'learning_rate': 9.808699925127001e-05, 'epoch': 0.09}
  9%|▉         | 571/6500 [3:44:20<57:01:04, 34.62s/it]                                                         9%|▉         | 571/6500 [3:44:20<57:01:04, 34.62s/it]  9%|▉         | 572/6500 [3:44:38<48:47:18, 29.63s/it]                                                         9%|▉         | 572/6500 [3:44:38<48:47:18, 29.63s/it]  9%|▉         | 573/6500 [3:44:56<43:01:53, 26.14s/it]                                                         9%|▉         | 573/6500 [3:44:56<43:01:53, 26.14s/it]  9%|▉         | 574/6500 [3:45:14<39:00:06, 23.69s/it]                                                         9%|▉         | 574/6500 [3:45:14<39:00:06, 23.69s/it]  9%|▉         | 575/6500 [3:45:32<36:10:19, 21.98s/it]                                                         9%|▉         | 575/6500 [3:45:32<36:10:19, 21.98s/it]  9%|▉         | 576/6500 [3:45:50<34:13:44, 20.80s/it]                                                         9%|▉         | 576/6500 [3:45:50<34:13:{'loss': 0.6957, 'learning_rate': 9.808037095624917e-05, 'epoch': 0.09}
{'loss': 0.7055, 'learning_rate': 9.807373142273395e-05, 'epoch': 0.09}
{'loss': 0.6684, 'learning_rate': 9.80670806522763e-05, 'epoch': 0.09}
{'loss': 0.7453, 'learning_rate': 9.80604186464308e-05, 'epoch': 0.09}
44, 20.80s/it]  9%|▉         | 577/6500 [3:46:08<32:52:24, 19.98s/it]                                                         9%|▉         | 577/6500 [3:46:08<32:52:24, 19.98s/it]  9%|▉         | 578/6500 [3:46:26<31:56:03, 19.41s/it]                                                         9%|▉         | 578/6500 [3:46:26<31:56:03, 19.41s/it]  9%|▉         | 579/6500 [3:46:45<31:16:24, 19.01s/it]                                                         9%|▉         | 579/6500 [3:46:45<31:16:24, 19.01s/it]  9%|▉         | 580/6500 [3:47:03<30:49:57, 18.75s/it]                                                         9%|▉         | 580/6500 [3:47:03<30:49:57, 18.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.818230926990509, 'eval_runtime': 5.3154, 'eval_samples_per_second': 4.327, 'eval_steps_per_second': 1.129, 'epoch': 0.09}
                                                         9%|▉         | 580/6500 [3:47:08<30:49:57, 18.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-580
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-580 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-580
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-580

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-580/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-580/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-580/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.704, 'learning_rate': 9.805374540675468e-05, 'epoch': 0.09}
{'loss': 0.6899, 'learning_rate': 9.804706093480771e-05, 'epoch': 0.09}
{'loss': 0.6737, 'learning_rate': 9.804036523215239e-05, 'epoch': 0.09}
{'loss': 0.6975, 'learning_rate': 9.803365830035379e-05, 'epoch': 0.09}
{'loss': 0.6883, 'learning_rate': 9.80269401409796e-05, 'epoch': 0.09}
{'loss': 0.69, 'learning_rate': 9.802021075560017e-05, 'epoch': 0.09}
  9%|▉         | 581/6500 [3:47:48<43:46:30, 26.62s/it]                                                         9%|▉         | 581/6500 [3:47:48<43:46:30, 26.62s/it]  9%|▉         | 582/6500 [3:48:06<39:47:37, 24.21s/it]                                                         9%|▉         | 582/6500 [3:48:06<39:47:37, 24.21s/it]  9%|▉         | 583/6500 [3:48:24<36:45:04, 22.36s/it]                                                         9%|▉         | 583/6500 [3:48:24<36:45:04, 22.36s/it]  9%|▉         | 584/6500 [3:48:42<34:36:41, 21.06s/it]                                                         9%|▉         | 584/6500 [3:48:42<34:36:41, 21.06s/it]  9%|▉         | 585/6500 [3:49:00<33:07:21, 20.16s/it]                                                         9%|▉         | 585/6500 [3:49:00<33:07:21, 20.16s/it]  9%|▉         | 586/6500 [3:49:18<32:05:17, 19.53s/it]                                                         9%|▉         | 586/6500 [3:49:18<32:05:{'loss': 0.7258, 'learning_rate': 9.801347014578846e-05, 'epoch': 0.09}
{'loss': 0.686, 'learning_rate': 9.800671831312e-05, 'epoch': 0.09}
{'loss': 0.7237, 'learning_rate': 9.799995525917304e-05, 'epoch': 0.09}
{'loss': 0.7174, 'learning_rate': 9.799318098552837e-05, 'epoch': 0.09}
17, 19.53s/it]  9%|▉         | 587/6500 [3:49:37<31:22:09, 19.10s/it]                                                         9%|▉         | 587/6500 [3:49:37<31:22:09, 19.10s/it]  9%|▉         | 588/6500 [3:49:55<30:52:17, 18.80s/it]                                                         9%|▉         | 588/6500 [3:49:55<30:52:17, 18.80s/it]  9%|▉         | 589/6500 [3:50:13<30:31:53, 18.59s/it]                                                         9%|▉         | 589/6500 [3:50:13<30:31:53, 18.59s/it]  9%|▉         | 590/6500 [3:50:31<30:17:55, 18.46s/it]                                                         9%|▉         | 590/6500 [3:50:31<30:17:55, 18.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8125140070915222, 'eval_runtime': 5.3293, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.09}
                                                         9%|▉         | 590/6500 [3:50:36<30:17:55, 18.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-590
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-590
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-590
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-590

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-590/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-590/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-590/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.727, 'learning_rate': 9.798639549376945e-05, 'epoch': 0.09}
{'loss': 0.6974, 'learning_rate': 9.797959878548236e-05, 'epoch': 0.09}
{'loss': 0.7233, 'learning_rate': 9.797279086225576e-05, 'epoch': 0.09}
{'loss': 0.7005, 'learning_rate': 9.796597172568099e-05, 'epoch': 0.09}
{'loss': 0.6759, 'learning_rate': 9.795914137735194e-05, 'epoch': 0.09}
{'loss': 0.7411, 'learning_rate': 9.795229981886521e-05, 'epoch': 0.09}
  9%|▉         | 591/6500 [3:52:21<75:33:47, 46.04s/it]                                                         9%|▉         | 591/6500 [3:52:21<75:33:47, 46.04s/it]  9%|▉         | 592/6500 [3:52:39<61:43:14, 37.61s/it]                                                         9%|▉         | 592/6500 [3:52:39<61:43:14, 37.61s/it]  9%|▉         | 593/6500 [3:52:57<52:02:12, 31.71s/it]                                                         9%|▉         | 593/6500 [3:52:57<52:02:12, 31.71s/it]  9%|▉         | 594/6500 [3:53:15<45:16:07, 27.59s/it]                                                         9%|▉         | 594/6500 [3:53:15<45:16:07, 27.59s/it]  9%|▉         | 595/6500 [3:53:33<40:33:05, 24.72s/it]                                                         9%|▉         | 595/6500 [3:53:33<40:33:05, 24.72s/it]  9%|▉         | 596/6500 [3:53:51<37:15:38, 22.72s/it]                                                         9%|▉         | 596/6500 [3:53:51<37:15:{'loss': 0.6986, 'learning_rate': 9.794544705181995e-05, 'epoch': 0.09}
{'loss': 0.7022, 'learning_rate': 9.793858307781796e-05, 'epoch': 0.09}
{'loss': 0.6818, 'learning_rate': 9.793170789846364e-05, 'epoch': 0.09}
{'loss': 0.7213, 'learning_rate': 9.792482151536402e-05, 'epoch': 0.09}
38, 22.72s/it]  9%|▉         | 597/6500 [3:54:09<34:57:37, 21.32s/it]                                                         9%|▉         | 597/6500 [3:54:09<34:57:37, 21.32s/it]  9%|▉         | 598/6500 [3:54:28<33:34:25, 20.48s/it]                                                         9%|▉         | 598/6500 [3:54:28<33:34:25, 20.48s/it]  9%|▉         | 599/6500 [3:54:46<32:24:25, 19.77s/it]                                                         9%|▉         | 599/6500 [3:54:46<32:24:25, 19.77s/it]  9%|▉         | 600/6500 [3:55:04<31:36:06, 19.28s/it]                                                         9%|▉         | 600/6500 [3:55:04<31:36:06, 19.28s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8121168613433838, 'eval_runtime': 5.5384, 'eval_samples_per_second': 4.153, 'eval_steps_per_second': 1.083, 'epoch': 0.09}
                                                         9%|▉         | 600/6500 [3:55:10<31:36:06, 19.28s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-600
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-600 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-600the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-600
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-600/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7665, 'learning_rate': 9.791792393012877e-05, 'epoch': 0.09}
{'loss': 0.6923, 'learning_rate': 9.791101514437014e-05, 'epoch': 0.09}
{'loss': 0.6971, 'learning_rate': 9.790409515970302e-05, 'epoch': 0.09}
{'loss': 0.6944, 'learning_rate': 9.789716397774493e-05, 'epoch': 0.09}
{'loss': 1.198, 'learning_rate': 9.789022160011597e-05, 'epoch': 0.09}
{'loss': 0.7185, 'learning_rate': 9.78832680284389e-05, 'epoch': 0.09}
  9%|▉         | 601/6500 [3:56:31<64:36:42, 39.43s/it]                                                         9%|▉         | 601/6500 [3:56:31<64:36:42, 39.43s/it]  9%|▉         | 602/6500 [3:56:49<54:02:55, 32.99s/it]                                                         9%|▉         | 602/6500 [3:56:49<54:02:55, 32.99s/it]  9%|▉         | 603/6500 [3:57:06<46:39:25, 28.48s/it]                                                         9%|▉         | 603/6500 [3:57:06<46:39:25, 28.48s/it]  9%|▉         | 604/6500 [3:57:24<41:29:09, 25.33s/it]                                                         9%|▉         | 604/6500 [3:57:24<41:29:09, 25.33s/it]  9%|▉         | 605/6500 [3:57:42<37:52:18, 23.13s/it]                                                         9%|▉         | 605/6500 [3:57:42<37:52:18, 23.13s/it]  9%|▉         | 606/6500 [3:58:00<35:22:00, 21.60s/it]                                                         9%|▉         | 606/6500 [3:58:00<35:22:{'loss': 0.6801, 'learning_rate': 9.787630326433905e-05, 'epoch': 0.09}
{'loss': 0.652, 'learning_rate': 9.786932730944441e-05, 'epoch': 0.09}
{'loss': 0.6841, 'learning_rate': 9.786234016538557e-05, 'epoch': 0.09}
{'loss': 0.7247, 'learning_rate': 9.785534183379572e-05, 'epoch': 0.09}
00, 21.60s/it]  9%|▉         | 607/6500 [3:58:19<33:37:00, 20.54s/it]                                                         9%|▉         | 607/6500 [3:58:19<33:37:00, 20.54s/it]  9%|▉         | 608/6500 [3:58:37<32:24:13, 19.80s/it]                                                         9%|▉         | 608/6500 [3:58:37<32:24:13, 19.80s/it]  9%|▉         | 609/6500 [3:58:55<31:34:30, 19.30s/it]                                                         9%|▉         | 609/6500 [3:58:55<31:34:30, 19.30s/it]  9%|▉         | 610/6500 [3:59:13<31:05:12, 19.00s/it]                                                         9%|▉         | 610/6500 [3:59:13<31:05:12, 19.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8132233023643494, 'eval_runtime': 5.3296, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.09}
                                                         9%|▉         | 610/6500 [3:59:18<31:05:12, 19.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-610
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-610
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-610 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-610

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-610
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-610/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.695, 'learning_rate': 9.784833231631068e-05, 'epoch': 0.09}
{'loss': 0.6771, 'learning_rate': 9.784131161456888e-05, 'epoch': 0.09}
{'loss': 0.6513, 'learning_rate': 9.783427973021136e-05, 'epoch': 0.09}
{'loss': 0.6851, 'learning_rate': 9.782723666488181e-05, 'epoch': 0.09}
{'loss': 0.6758, 'learning_rate': 9.782018242022648e-05, 'epoch': 0.09}
{'loss': 0.6764, 'learning_rate': 9.781311699789426e-05, 'epoch': 0.09}
  9%|▉         | 611/6500 [4:00:05<47:20:10, 28.94s/it]                                                         9%|▉         | 611/6500 [4:00:05<47:20:10, 28.94s/it]  9%|▉         | 612/6500 [4:00:23<42:00:42, 25.69s/it]                                                         9%|▉         | 612/6500 [4:00:23<42:00:42, 25.69s/it]  9%|▉         | 613/6500 [4:00:41<38:16:08, 23.40s/it]                                                         9%|▉         | 613/6500 [4:00:41<38:16:08, 23.40s/it]  9%|▉         | 614/6500 [4:00:59<35:38:44, 21.80s/it]                                                         9%|▉         | 614/6500 [4:00:59<35:38:44, 21.80s/it]  9%|▉         | 615/6500 [4:01:18<34:06:45, 20.87s/it]                                                         9%|▉         | 615/6500 [4:01:18<34:06:45, 20.87s/it]  9%|▉         | 616/6500 [4:01:36<32:44:47, 20.04s/it]                                                         9%|▉         | 616/6500 [4:01:36<32:44:{'loss': 0.7332, 'learning_rate': 9.780604039953665e-05, 'epoch': 0.09}
{'loss': 0.6768, 'learning_rate': 9.779895262680775e-05, 'epoch': 0.1}
{'loss': 0.7286, 'learning_rate': 9.77918536813643e-05, 'epoch': 0.1}
{'loss': 0.7183, 'learning_rate': 9.778474356486564e-05, 'epoch': 0.1}
47, 20.04s/it]  9%|▉         | 617/6500 [4:01:54<31:47:30, 19.45s/it]                                                         9%|▉         | 617/6500 [4:01:54<31:47:30, 19.45s/it] 10%|▉         | 618/6500 [4:02:12<31:07:46, 19.05s/it]                                                        10%|▉         | 618/6500 [4:02:12<31:07:46, 19.05s/it] 10%|▉         | 619/6500 [4:02:31<30:40:05, 18.77s/it]                                                        10%|▉         | 619/6500 [4:02:31<30:40:05, 18.77s/it] 10%|▉         | 620/6500 [4:02:49<30:22:05, 18.59s/it]                                                        10%|▉         | 620/6500 [4:02:49<30:22:05, 18.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8097134232521057, 'eval_runtime': 5.3253, 'eval_samples_per_second': 4.319, 'eval_steps_per_second': 1.127, 'epoch': 0.1}
                                                        10%|▉         | 620/6500 [4:02:54<30:22:05, 18.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-620
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-620
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-620

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-620

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-620
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-620/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-620/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6947, 'learning_rate': 9.777762227897371e-05, 'epoch': 0.1}
{'loss': 0.6986, 'learning_rate': 9.777048982535306e-05, 'epoch': 0.1}
{'loss': 0.6899, 'learning_rate': 9.776334620567085e-05, 'epoch': 0.1}
{'loss': 0.6693, 'learning_rate': 9.77561914215969e-05, 'epoch': 0.1}
{'loss': 0.6953, 'learning_rate': 9.774902547480353e-05, 'epoch': 0.1}
{'loss': 0.716, 'learning_rate': 9.77418483669658e-05, 'epoch': 0.1}
 10%|▉         | 621/6500 [4:04:04<57:55:10, 35.47s/it]                                                        10%|▉         | 621/6500 [4:04:04<57:55:10, 35.47s/it] 10%|▉         | 622/6500 [4:04:22<49:21:38, 30.23s/it]                                                        10%|▉         | 622/6500 [4:04:22<49:21:38, 30.23s/it] 10%|▉         | 623/6500 [4:04:40<43:23:17, 26.58s/it]                                                        10%|▉         | 623/6500 [4:04:40<43:23:17, 26.58s/it] 10%|▉         | 624/6500 [4:04:58<39:13:00, 24.03s/it]                                                        10%|▉         | 624/6500 [4:04:58<39:13:00, 24.03s/it] 10%|▉         | 625/6500 [4:05:16<36:18:55, 22.25s/it]                                                        10%|▉         | 625/6500 [4:05:16<36:18:55, 22.25s/it] 10%|▉         | 626/6500 [4:05:34<34:16:45, 21.01s/it]                                                        10%|▉         | 626/6500 [4:05:34<34:16:{'loss': 0.6706, 'learning_rate': 9.773466009976129e-05, 'epoch': 0.1}
{'loss': 0.7032, 'learning_rate': 9.77274606748702e-05, 'epoch': 0.1}
{'loss': 0.6629, 'learning_rate': 9.772025009397537e-05, 'epoch': 0.1}
{'loss': 0.7307, 'learning_rate': 9.771302835876224e-05, 'epoch': 0.1}
45, 21.01s/it] 10%|▉         | 627/6500 [4:05:52<32:51:34, 20.14s/it]                                                        10%|▉         | 627/6500 [4:05:52<32:51:34, 20.14s/it] 10%|▉         | 628/6500 [4:06:10<31:52:39, 19.54s/it]                                                        10%|▉         | 628/6500 [4:06:10<31:52:39, 19.54s/it] 10%|▉         | 629/6500 [4:06:28<31:12:05, 19.13s/it]                                                        10%|▉         | 629/6500 [4:06:28<31:12:05, 19.13s/it] 10%|▉         | 630/6500 [4:06:47<30:44:12, 18.85s/it]                                                        10%|▉         | 630/6500 [4:06:47<30:44:12, 18.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8088764548301697, 'eval_runtime': 5.3403, 'eval_samples_per_second': 4.307, 'eval_steps_per_second': 1.124, 'epoch': 0.1}
                                                        10%|▉         | 630/6500 [4:06:52<30:44:12, 18.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-630
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-630the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-630
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-630
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-630/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.717, 'learning_rate': 9.77057954709188e-05, 'epoch': 0.1}
{'loss': 0.6736, 'learning_rate': 9.769855143213575e-05, 'epoch': 0.1}
{'loss': 0.6952, 'learning_rate': 9.769129624410631e-05, 'epoch': 0.1}
{'loss': 0.679, 'learning_rate': 9.768402990852635e-05, 'epoch': 0.1}
{'loss': 1.1973, 'learning_rate': 9.76767524270943e-05, 'epoch': 0.1}
{'loss': 0.6879, 'learning_rate': 9.766946380151125e-05, 'epoch': 0.1}
 10%|▉         | 631/6500 [4:08:15<64:41:37, 39.68s/it]                                                        10%|▉         | 631/6500 [4:08:15<64:41:37, 39.68s/it] 10%|▉         | 632/6500 [4:08:33<54:05:20, 33.18s/it]                                                        10%|▉         | 632/6500 [4:08:33<54:05:20, 33.18s/it] 10%|▉         | 633/6500 [4:08:51<46:39:53, 28.63s/it]                                                        10%|▉         | 633/6500 [4:08:51<46:39:53, 28.63s/it] 10%|▉         | 634/6500 [4:09:09<41:27:35, 25.44s/it]                                                        10%|▉         | 634/6500 [4:09:09<41:27:35, 25.44s/it] 10%|▉         | 635/6500 [4:09:27<38:02:02, 23.35s/it]                                                        10%|▉         | 635/6500 [4:09:27<38:02:02, 23.35s/it] 10%|▉         | 636/6500 [4:09:45<35:25:51, 21.75s/it]                                                        10%|▉         | 636/6500 [4:09:45<35:25:{'loss': 0.6781, 'learning_rate': 9.766216403348089e-05, 'epoch': 0.1}
{'loss': 0.639, 'learning_rate': 9.765485312470946e-05, 'epoch': 0.1}
{'loss': 0.7039, 'learning_rate': 9.764753107690588e-05, 'epoch': 0.1}
{'loss': 0.7031, 'learning_rate': 9.76401978917816e-05, 'epoch': 0.1}
51, 21.75s/it] 10%|▉         | 637/6500 [4:10:03<33:37:45, 20.65s/it]                                                        10%|▉         | 637/6500 [4:10:03<33:37:45, 20.65s/it] 10%|▉         | 638/6500 [4:10:22<32:24:20, 19.90s/it]                                                        10%|▉         | 638/6500 [4:10:22<32:24:20, 19.90s/it] 10%|▉         | 639/6500 [4:10:40<31:32:06, 19.37s/it]                                                        10%|▉         | 639/6500 [4:10:40<31:32:06, 19.37s/it] 10%|▉         | 640/6500 [4:10:58<30:59:48, 19.04s/it]                                                        10%|▉         | 640/6500 [4:10:58<30:59:48, 19.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8077807426452637, 'eval_runtime': 5.3772, 'eval_samples_per_second': 4.277, 'eval_steps_per_second': 1.116, 'epoch': 0.1}
                                                        10%|▉         | 640/6500 [4:11:03<30:59:48, 19.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-640
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-640
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-640

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-640
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-640
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-640/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6487, 'learning_rate': 9.763285357105072e-05, 'epoch': 0.1}
{'loss': 0.6714, 'learning_rate': 9.762549811642991e-05, 'epoch': 0.1}
{'loss': 0.6571, 'learning_rate': 9.761813152963853e-05, 'epoch': 0.1}
{'loss': 0.6715, 'learning_rate': 9.761075381239839e-05, 'epoch': 0.1}
{'loss': 0.6705, 'learning_rate': 9.760336496643403e-05, 'epoch': 0.1}
{'loss': 0.6848, 'learning_rate': 9.759596499347254e-05, 'epoch': 0.1}
 10%|▉         | 641/6500 [4:12:01<52:40:47, 32.37s/it]                                                        10%|▉         | 641/6500 [4:12:01<52:40:47, 32.37s/it] 10%|▉         | 642/6500 [4:12:19<45:38:30, 28.05s/it]                                                        10%|▉         | 642/6500 [4:12:19<45:38:30, 28.05s/it] 10%|▉         | 643/6500 [4:12:37<40:42:18, 25.02s/it]                                                        10%|▉         | 643/6500 [4:12:37<40:42:18, 25.02s/it] 10%|▉         | 644/6500 [4:12:55<37:15:18, 22.90s/it]                                                        10%|▉         | 644/6500 [4:12:55<37:15:18, 22.90s/it] 10%|▉         | 645/6500 [4:13:13<34:51:36, 21.43s/it]                                                        10%|▉         | 645/6500 [4:13:13<34:51:36, 21.43s/it] 10%|▉         | 646/6500 [4:13:31<33:11:08, 20.41s/it]                                                        10%|▉         | 646/6500 [4:13:31<33:11:{'loss': 0.6862, 'learning_rate': 9.758855389524364e-05, 'epoch': 0.1}
{'loss': 0.6629, 'learning_rate': 9.75811316734796e-05, 'epoch': 0.1}
{'loss': 0.7202, 'learning_rate': 9.757369832991532e-05, 'epoch': 0.1}
{'loss': 0.6911, 'learning_rate': 9.756625386628832e-05, 'epoch': 0.1}
08, 20.41s/it] 10%|▉         | 647/6500 [4:13:50<32:13:19, 19.82s/it]                                                        10%|▉         | 647/6500 [4:13:50<32:13:19, 19.82s/it] 10%|▉         | 648/6500 [4:14:08<31:21:37, 19.29s/it]                                                        10%|▉         | 648/6500 [4:14:08<31:21:37, 19.29s/it] 10%|▉         | 649/6500 [4:14:26<30:46:08, 18.93s/it]                                                        10%|▉         | 649/6500 [4:14:26<30:46:08, 18.93s/it] 10%|█         | 650/6500 [4:14:44<30:25:38, 18.72s/it]                                                        10%|█         | 650/6500 [4:14:44<30:25:38, 18.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8055773377418518, 'eval_runtime': 5.3321, 'eval_samples_per_second': 4.313, 'eval_steps_per_second': 1.125, 'epoch': 0.1}
                                                        10%|█         | 650/6500 [4:14:50<30:25:38, 18.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-650
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-650

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-650
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-650
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-650/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-650/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6761, 'learning_rate': 9.755879828433869e-05, 'epoch': 0.1}
{'loss': 0.7089, 'learning_rate': 9.755133158580912e-05, 'epoch': 0.1}
{'loss': 0.6509, 'learning_rate': 9.75438537724449e-05, 'epoch': 0.1}
{'loss': 0.6595, 'learning_rate': 9.753636484599393e-05, 'epoch': 0.1}
{'loss': 0.7009, 'learning_rate': 9.752886480820671e-05, 'epoch': 0.1}
{'loss': 0.6913, 'learning_rate': 9.752135366083632e-05, 'epoch': 0.1}
 10%|█         | 651/6500 [4:16:22<69:12:18, 42.60s/it]                                                        10%|█         | 651/6500 [4:16:22<69:12:18, 42.60s/it] 10%|█         | 652/6500 [4:16:40<57:11:39, 35.21s/it]                                                        10%|█         | 652/6500 [4:16:40<57:11:39, 35.21s/it] 10%|█         | 653/6500 [4:16:58<48:47:04, 30.04s/it]                                                        10%|█         | 653/6500 [4:16:58<48:47:04, 30.04s/it] 10%|█         | 654/6500 [4:17:16<42:54:04, 26.42s/it]                                                        10%|█         | 654/6500 [4:17:16<42:54:04, 26.42s/it] 10%|█         | 655/6500 [4:17:34<38:47:48, 23.90s/it]                                                        10%|█         | 655/6500 [4:17:34<38:47:48, 23.90s/it] 10%|█         | 656/6500 [4:17:52<35:55:53, 22.13s/it]                                                        10%|█         | 656/6500 [4:17:52<35:55:{'loss': 0.6757, 'learning_rate': 9.751383140563845e-05, 'epoch': 0.1}
{'loss': 0.6577, 'learning_rate': 9.750629804437137e-05, 'epoch': 0.1}
{'loss': 0.6966, 'learning_rate': 9.749875357879597e-05, 'epoch': 0.1}
{'loss': 0.7324, 'learning_rate': 9.749119801067572e-05, 'epoch': 0.1}
53, 22.13s/it] 10%|█         | 657/6500 [4:18:10<33:56:18, 20.91s/it]                                                        10%|█         | 657/6500 [4:18:10<33:56:18, 20.91s/it] 10%|█         | 658/6500 [4:18:29<32:33:09, 20.06s/it]                                                        10%|█         | 658/6500 [4:18:29<32:33:09, 20.06s/it] 10%|█         | 659/6500 [4:18:47<31:35:58, 19.48s/it]                                                        10%|█         | 659/6500 [4:18:47<31:35:58, 19.48s/it] 10%|█         | 660/6500 [4:19:05<30:58:21, 19.09s/it]                                                        10%|█         | 660/6500 [4:19:05<30:58:21, 19.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8037922978401184, 'eval_runtime': 6.1321, 'eval_samples_per_second': 3.751, 'eval_steps_per_second': 0.978, 'epoch': 0.1}
                                                        10%|█         | 660/6500 [4:19:11<30:58:21, 19.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-660
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-660/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-660

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-660

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-660
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-660/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6633, 'learning_rate': 9.74836313417767e-05, 'epoch': 0.1}
{'loss': 0.654, 'learning_rate': 9.747605357386754e-05, 'epoch': 0.1}
{'loss': 0.6916, 'learning_rate': 9.746846470871951e-05, 'epoch': 0.1}
{'loss': 1.1819, 'learning_rate': 9.746086474810649e-05, 'epoch': 0.1}
{'loss': 0.6806, 'learning_rate': 9.745325369380489e-05, 'epoch': 0.1}
{'loss': 0.6655, 'learning_rate': 9.744563154759375e-05, 'epoch': 0.1}
 10%|█         | 661/6500 [4:19:48<42:53:57, 26.45s/it]                                                        10%|█         | 661/6500 [4:19:48<42:53:57, 26.45s/it] 10%|█         | 662/6500 [4:20:07<38:48:54, 23.94s/it]                                                        10%|█         | 662/6500 [4:20:07<38:48:54, 23.94s/it] 10%|█         | 663/6500 [4:20:27<37:04:48, 22.87s/it]                                                        10%|█         | 663/6500 [4:20:27<37:04:48, 22.87s/it] 10%|█         | 664/6500 [4:20:45<34:45:56, 21.45s/it]                                                        10%|█         | 664/6500 [4:20:45<34:45:56, 21.45s/it] 10%|█         | 665/6500 [4:21:03<33:08:36, 20.45s/it]                                                        10%|█         | 665/6500 [4:21:03<33:08:36, 20.45s/it] 10%|█         | 666/6500 [4:21:21<31:59:15, 19.74s/it]                                                        10%|█         | 666/6500 [4:21:21<31:59:{'loss': 0.657, 'learning_rate': 9.743799831125472e-05, 'epoch': 0.1}
{'loss': 0.6401, 'learning_rate': 9.743035398657201e-05, 'epoch': 0.1}
{'loss': 0.6935, 'learning_rate': 9.742269857533244e-05, 'epoch': 0.1}
{'loss': 0.6785, 'learning_rate': 9.74150320793254e-05, 'epoch': 0.1}
15, 19.74s/it] 10%|█         | 667/6500 [4:21:39<31:10:27, 19.24s/it]                                                        10%|█         | 667/6500 [4:21:39<31:10:27, 19.24s/it] 10%|█         | 668/6500 [4:21:57<30:36:54, 18.90s/it]                                                        10%|█         | 668/6500 [4:21:57<30:36:54, 18.90s/it] 10%|█         | 669/6500 [4:22:17<30:45:42, 18.99s/it]                                                        10%|█         | 669/6500 [4:22:17<30:45:42, 18.99s/it] 10%|█         | 670/6500 [4:22:35<30:19:40, 18.73s/it]                                                        10%|█         | 670/6500 [4:22:35<30:19:40, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8046067357063293, 'eval_runtime': 6.2831, 'eval_samples_per_second': 3.661, 'eval_steps_per_second': 0.955, 'epoch': 0.1}
                                                        10%|█         | 670/6500 [4:22:41<30:19:40, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-670
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-670/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-670/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-670



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-670
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-670/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6523, 'learning_rate': 9.740735450034292e-05, 'epoch': 0.1}
{'loss': 0.6424, 'learning_rate': 9.739966584017956e-05, 'epoch': 0.1}
{'loss': 0.6522, 'learning_rate': 9.739196610063251e-05, 'epoch': 0.1}
{'loss': 0.6613, 'learning_rate': 9.738425528350152e-05, 'epoch': 0.1}
{'loss': 0.6393, 'learning_rate': 9.737653339058896e-05, 'epoch': 0.1}
{'loss': 0.6972, 'learning_rate': 9.736880042369978e-05, 'epoch': 0.1}
 10%|█         | 671/6500 [4:23:37<51:27:27, 31.78s/it]                                                        10%|█         | 671/6500 [4:23:37<51:27:27, 31.78s/it] 10%|█         | 672/6500 [4:23:57<45:43:33, 28.25s/it]                                                        10%|█         | 672/6500 [4:23:57<45:43:33, 28.25s/it] 10%|█         | 673/6500 [4:24:15<40:48:34, 25.21s/it]                                                        10%|█         | 673/6500 [4:24:15<40:48:34, 25.21s/it] 10%|█         | 674/6500 [4:24:33<37:18:10, 23.05s/it]                                                        10%|█         | 674/6500 [4:24:33<37:18:10, 23.05s/it] 10%|█         | 675/6500 [4:24:51<34:51:08, 21.54s/it]                                                        10%|█         | 675/6500 [4:24:51<34:51:08, 21.54s/it] 10%|█         | 676/6500 [4:25:09<33:08:50, 20.49s/it]                                                        10%|█         | 676/6500 [4:25:09<33:08:{'loss': 0.6528, 'learning_rate': 9.736105638464151e-05, 'epoch': 0.1}
{'loss': 0.6851, 'learning_rate': 9.735330127522425e-05, 'epoch': 0.1}
{'loss': 0.6964, 'learning_rate': 9.734553509726074e-05, 'epoch': 0.1}
{'loss': 0.6807, 'learning_rate': 9.733775785256629e-05, 'epoch': 0.1}
50, 20.49s/it] 10%|█         | 677/6500 [4:25:27<31:57:47, 19.76s/it]                                                        10%|█         | 677/6500 [4:25:27<31:57:47, 19.76s/it] 10%|█         | 678/6500 [4:25:45<31:08:38, 19.26s/it]                                                        10%|█         | 678/6500 [4:25:45<31:08:38, 19.26s/it] 10%|█         | 679/6500 [4:26:05<31:27:18, 19.45s/it]                                                        10%|█         | 679/6500 [4:26:05<31:27:18, 19.45s/it] 10%|█         | 680/6500 [4:26:23<30:47:55, 19.05s/it]                                                        10%|█         | 680/6500 [4:26:23<30:47:55, 19.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7989833950996399, 'eval_runtime': 5.6635, 'eval_samples_per_second': 4.061, 'eval_steps_per_second': 1.059, 'epoch': 0.1}
                                                        10%|█         | 680/6500 [4:26:29<30:47:55, 19.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-680
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-680
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-680the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-680

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-680
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-680/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6751, 'learning_rate': 9.732996954295874e-05, 'epoch': 0.1}
{'loss': 0.6746, 'learning_rate': 9.732217017025858e-05, 'epoch': 0.1}
{'loss': 0.6549, 'learning_rate': 9.731435973628886e-05, 'epoch': 0.11}
{'loss': 0.6665, 'learning_rate': 9.730653824287523e-05, 'epoch': 0.11}
{'loss': 0.6872, 'learning_rate': 9.729870569184593e-05, 'epoch': 0.11}
{'loss': 0.6531, 'learning_rate': 9.729086208503174e-05, 'epoch': 0.11}
 10%|█         | 681/6500 [4:27:21<49:22:30, 30.55s/it]                                                        10%|█         | 681/6500 [4:27:21<49:22:30, 30.55s/it] 10%|█         | 682/6500 [4:27:39<43:16:09, 26.77s/it]                                                        10%|█         | 682/6500 [4:27:39<43:16:09, 26.77s/it] 11%|█         | 683/6500 [4:27:57<38:59:10, 24.13s/it]                                                        11%|█         | 683/6500 [4:27:57<38:59:10, 24.13s/it] 11%|█         | 684/6500 [4:28:15<36:00:51, 22.29s/it]                                                        11%|█         | 684/6500 [4:28:15<36:00:51, 22.29s/it] 11%|█         | 685/6500 [4:28:33<33:55:38, 21.00s/it]                                                        11%|█         | 685/6500 [4:28:33<33:55:38, 21.00s/it] 11%|█         | 686/6500 [4:28:51<32:28:14, 20.11s/it]                                                        11%|█         | 686/6500 [4:28:51<32:28:{'loss': 0.6648, 'learning_rate': 9.728300742426609e-05, 'epoch': 0.11}
{'loss': 0.638, 'learning_rate': 9.727514171138492e-05, 'epoch': 0.11}
{'loss': 0.6993, 'learning_rate': 9.726726494822681e-05, 'epoch': 0.11}
{'loss': 0.7203, 'learning_rate': 9.725937713663292e-05, 'epoch': 0.11}
14, 20.11s/it] 11%|█         | 687/6500 [4:29:09<31:28:07, 19.49s/it]                                                        11%|█         | 687/6500 [4:29:09<31:28:07, 19.49s/it] 11%|█         | 688/6500 [4:29:27<30:46:28, 19.06s/it]                                                        11%|█         | 688/6500 [4:29:27<30:46:28, 19.06s/it] 11%|█         | 689/6500 [4:29:45<30:17:58, 18.77s/it]                                                        11%|█         | 689/6500 [4:29:45<30:17:58, 18.77s/it] 11%|█         | 690/6500 [4:30:03<29:59:19, 18.58s/it]                                                        11%|█         | 690/6500 [4:30:03<29:59:19, 18.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7979978322982788, 'eval_runtime': 5.3269, 'eval_samples_per_second': 4.318, 'eval_steps_per_second': 1.126, 'epoch': 0.11}
                                                        11%|█         | 690/6500 [4:30:08<29:59:19, 18.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-690
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-690
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-690
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-690/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6657, 'learning_rate': 9.725147827844697e-05, 'epoch': 0.11}
{'loss': 0.6499, 'learning_rate': 9.724356837551525e-05, 'epoch': 0.11}
{'loss': 0.6664, 'learning_rate': 9.723564742968667e-05, 'epoch': 0.11}
{'loss': 1.1705, 'learning_rate': 9.722771544281271e-05, 'epoch': 0.11}
{'loss': 0.6641, 'learning_rate': 9.721977241674742e-05, 'epoch': 0.11}
{'loss': 0.6587, 'learning_rate': 9.721181835334741e-05, 'epoch': 0.11}
 11%|█         | 691/6500 [4:31:27<61:28:11, 38.09s/it]                                                        11%|█         | 691/6500 [4:31:27<61:28:11, 38.09s/it] 11%|█         | 692/6500 [4:31:45<51:42:44, 32.05s/it]                                                        11%|█         | 692/6500 [4:31:45<51:42:44, 32.05s/it] 11%|█         | 693/6500 [4:32:03<44:52:26, 27.82s/it]                                                        11%|█         | 693/6500 [4:32:03<44:52:26, 27.82s/it] 11%|█         | 694/6500 [4:32:20<40:04:56, 24.85s/it]                                                        11%|█         | 694/6500 [4:32:20<40:04:56, 24.85s/it] 11%|█         | 695/6500 [4:32:39<37:01:02, 22.96s/it]                                                        11%|█         | 695/6500 [4:32:39<37:01:02, 22.96s/it] 11%|█         | 696/6500 [4:32:57<34:36:35, 21.47s/it]                                                        11%|█         | 696/6500 [4:32:57<34:36:{'loss': 0.6039, 'learning_rate': 9.720385325447192e-05, 'epoch': 0.11}
{'loss': 0.6565, 'learning_rate': 9.719587712198275e-05, 'epoch': 0.11}
{'loss': 0.7076, 'learning_rate': 9.718788995774423e-05, 'epoch': 0.11}
{'loss': 0.6395, 'learning_rate': 9.717989176362337e-05, 'epoch': 0.11}
35, 21.47s/it] 11%|█         | 697/6500 [4:33:15<32:56:29, 20.44s/it]                                                        11%|█         | 697/6500 [4:33:15<32:56:29, 20.44s/it] 11%|█         | 698/6500 [4:33:33<31:47:20, 19.72s/it]                                                        11%|█         | 698/6500 [4:33:33<31:47:20, 19.72s/it] 11%|█         | 699/6500 [4:33:51<30:59:37, 19.23s/it]                                                        11%|█         | 699/6500 [4:33:51<30:59:37, 19.23s/it] 11%|█         | 700/6500 [4:34:09<30:26:29, 18.89s/it]                                                        11%|█         | 700/6500 [4:34:09<30:26:29, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7995375990867615, 'eval_runtime': 5.3286, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.11}
                                                        11%|█         | 700/6500 [4:34:15<30:26:29, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-700
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-700the checkpoint model will be saved in 
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-700/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-700


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-700
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-700/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-700/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6458, 'learning_rate': 9.717188254148966e-05, 'epoch': 0.11}
{'loss': 0.626, 'learning_rate': 9.71638622932152e-05, 'epoch': 0.11}
{'loss': 0.651, 'learning_rate': 9.715583102067469e-05, 'epoch': 0.11}
{'loss': 0.6363, 'learning_rate': 9.714778872574541e-05, 'epoch': 0.11}
{'loss': 0.6467, 'learning_rate': 9.713973541030716e-05, 'epoch': 0.11}
{'loss': 0.6903, 'learning_rate': 9.713167107624239e-05, 'epoch': 0.11}
 11%|█         | 701/6500 [4:35:35<62:35:52, 38.86s/it]                                                        11%|█         | 701/6500 [4:35:35<62:35:52, 38.86s/it] 11%|█         | 702/6500 [4:35:53<52:28:48, 32.59s/it]                                                        11%|█         | 702/6500 [4:35:53<52:28:48, 32.59s/it] 11%|█         | 703/6500 [4:36:11<45:24:59, 28.20s/it]                                                        11%|█         | 703/6500 [4:36:11<45:24:59, 28.20s/it] 11%|█         | 704/6500 [4:36:29<40:29:51, 25.15s/it]                                                        11%|█         | 704/6500 [4:36:29<40:29:51, 25.15s/it] 11%|█         | 705/6500 [4:36:47<37:02:52, 23.02s/it]                                                        11%|█         | 705/6500 [4:36:47<37:02:52, 23.02s/it] 11%|█         | 706/6500 [4:37:05<34:38:05, 21.52s/it]                                                        11%|█         | 706/6500 [4:37:05<34:38:{'loss': 0.6367, 'learning_rate': 9.712359572543606e-05, 'epoch': 0.11}
{'loss': 0.7074, 'learning_rate': 9.711550935977576e-05, 'epoch': 0.11}
{'loss': 0.6711, 'learning_rate': 9.71074119811516e-05, 'epoch': 0.11}
{'loss': 0.6516, 'learning_rate': 9.709930359145631e-05, 'epoch': 0.11}
05, 21.52s/it] 11%|█         | 707/6500 [4:37:23<32:57:44, 20.48s/it]                                                        11%|█         | 707/6500 [4:37:23<32:57:44, 20.48s/it] 11%|█         | 708/6500 [4:37:41<31:48:17, 19.77s/it]                                                        11%|█         | 708/6500 [4:37:41<31:48:17, 19.77s/it] 11%|█         | 709/6500 [4:37:59<31:00:37, 19.28s/it]                                                        11%|█         | 709/6500 [4:37:59<31:00:37, 19.28s/it] 11%|█         | 710/6500 [4:38:17<30:30:10, 18.97s/it]                                                        11%|█         | 710/6500 [4:38:17<30:30:10, 18.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7938210964202881, 'eval_runtime': 5.3111, 'eval_samples_per_second': 4.331, 'eval_steps_per_second': 1.13, 'epoch': 0.11}
                                                        11%|█         | 710/6500 [4:38:23<30:30:10, 18.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-710
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-710
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-710
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-710

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-710/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-710/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-710/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6781, 'learning_rate': 9.709118419258518e-05, 'epoch': 0.11}
{'loss': 0.6382, 'learning_rate': 9.708305378643604e-05, 'epoch': 0.11}
{'loss': 0.6511, 'learning_rate': 9.707491237490937e-05, 'epoch': 0.11}
{'loss': 0.6478, 'learning_rate': 9.706675995990815e-05, 'epoch': 0.11}
{'loss': 0.6759, 'learning_rate': 9.705859654333797e-05, 'epoch': 0.11}
{'loss': 0.6439, 'learning_rate': 9.705042212710695e-05, 'epoch': 0.11}
 11%|█         | 711/6500 [4:39:44<63:20:28, 39.39s/it]                                                        11%|█         | 711/6500 [4:39:44<63:20:28, 39.39s/it] 11%|█         | 712/6500 [4:40:02<53:02:03, 32.99s/it]                                                        11%|█         | 712/6500 [4:40:02<53:02:03, 32.99s/it] 11%|█         | 713/6500 [4:40:20<45:46:02, 28.47s/it]                                                        11%|█         | 713/6500 [4:40:20<45:46:02, 28.47s/it] 11%|█         | 714/6500 [4:40:38<40:41:07, 25.31s/it]                                                        11%|█         | 714/6500 [4:40:38<40:41:07, 25.31s/it] 11%|█         | 715/6500 [4:40:56<37:07:50, 23.11s/it]                                                        11%|█         | 715/6500 [4:40:56<37:07:50, 23.11s/it] 11%|█         | 716/6500 [4:41:14<34:40:04, 21.58s/it]                                                        11%|█         | 716/6500 [4:41:14<34:40:{'loss': 0.6637, 'learning_rate': 9.704223671312584e-05, 'epoch': 0.11}
{'loss': 0.6376, 'learning_rate': 9.703404030330791e-05, 'epoch': 0.11}
{'loss': 0.7407, 'learning_rate': 9.702583289956903e-05, 'epoch': 0.11}
{'loss': 0.6392, 'learning_rate': 9.701761450382765e-05, 'epoch': 0.11}
04, 21.58s/it] 11%|█         | 717/6500 [4:41:32<32:56:55, 20.51s/it]                                                        11%|█         | 717/6500 [4:41:32<32:56:55, 20.51s/it] 11%|█         | 718/6500 [4:41:50<31:45:03, 19.77s/it]                                                        11%|█         | 718/6500 [4:41:50<31:45:03, 19.77s/it] 11%|█         | 719/6500 [4:42:08<30:55:53, 19.26s/it]                                                        11%|█         | 719/6500 [4:42:08<30:55:53, 19.26s/it] 11%|█         | 720/6500 [4:42:26<30:21:55, 18.91s/it]                                                        11%|█         | 720/6500 [4:42:26<30:21:55, 18.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7961171269416809, 'eval_runtime': 5.3163, 'eval_samples_per_second': 4.326, 'eval_steps_per_second': 1.129, 'epoch': 0.11}
                                                        11%|█         | 720/6500 [4:42:32<30:21:55, 18.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-720
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-720
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-720/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-720


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-720
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-720/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-720/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6348, 'learning_rate': 9.700938511800474e-05, 'epoch': 0.11}
{'loss': 0.6528, 'learning_rate': 9.700114474402387e-05, 'epoch': 0.11}
{'loss': 1.0761, 'learning_rate': 9.69928933838112e-05, 'epoch': 0.11}
{'loss': 0.7354, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.11}
{'loss': 0.6623, 'learning_rate': 9.69763577124078e-05, 'epoch': 0.11}
{'loss': 0.6357, 'learning_rate': 9.696807340508221e-05, 'epoch': 0.11}
 11%|█         | 721/6500 [4:44:01<66:48:20, 41.62s/it]                                                        11%|█         | 721/6500 [4:44:01<66:48:20, 41.62s/it] 11%|█         | 722/6500 [4:44:19<55:23:49, 34.52s/it]                                                        11%|█         | 722/6500 [4:44:19<55:23:49, 34.52s/it] 11%|█         | 723/6500 [4:44:37<47:23:55, 29.54s/it]                                                        11%|█         | 723/6500 [4:44:37<47:23:55, 29.54s/it] 11%|█         | 724/6500 [4:44:55<41:48:59, 26.06s/it]                                                        11%|█         | 724/6500 [4:44:55<41:48:59, 26.06s/it] 11%|█         | 725/6500 [4:45:13<37:54:48, 23.63s/it]                                                        11%|█         | 725/6500 [4:45:13<37:54:48, 23.63s/it] 11%|█         | 726/6500 [4:45:31<35:12:28, 21.95s/it]                                                        11%|█         | 726/6500 [4:45:31<35:12:{'loss': 0.6098, 'learning_rate': 9.6959778119255e-05, 'epoch': 0.11}
{'loss': 0.6743, 'learning_rate': 9.69514718568652e-05, 'epoch': 0.11}
{'loss': 0.6647, 'learning_rate': 9.69431546198543e-05, 'epoch': 0.11}
{'loss': 0.6078, 'learning_rate': 9.693482641016645e-05, 'epoch': 0.11}
28, 21.95s/it] 11%|█         | 727/6500 [4:45:49<33:18:53, 20.77s/it]                                                        11%|█         | 727/6500 [4:45:49<33:18:53, 20.77s/it] 11%|█         | 728/6500 [4:46:07<32:11:43, 20.08s/it]                                                        11%|█         | 728/6500 [4:46:07<32:11:43, 20.08s/it] 11%|█         | 729/6500 [4:46:25<31:13:30, 19.48s/it]                                                        11%|█         | 729/6500 [4:46:25<31:13:30, 19.48s/it] 11%|█         | 730/6500 [4:46:44<30:33:04, 19.06s/it]                                                        11%|█         | 730/6500 [4:46:44<30:33:04, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7954713702201843, 'eval_runtime': 5.468, 'eval_samples_per_second': 4.206, 'eval_steps_per_second': 1.097, 'epoch': 0.11}
                                                        11%|█         | 730/6500 [4:46:49<30:33:04, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-730
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-730
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-730/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-730


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-730
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-730/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-730/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6363, 'learning_rate': 9.692648722974829e-05, 'epoch': 0.11}
{'loss': 0.6153, 'learning_rate': 9.691813708054904e-05, 'epoch': 0.11}
{'loss': 0.6413, 'learning_rate': 9.690977596452053e-05, 'epoch': 0.11}
{'loss': 0.6252, 'learning_rate': 9.69014038836171e-05, 'epoch': 0.11}
{'loss': 0.6494, 'learning_rate': 9.689302083979568e-05, 'epoch': 0.11}
{'loss': 0.6466, 'learning_rate': 9.688462683501574e-05, 'epoch': 0.11}
 11%|█         | 731/6500 [4:48:07<61:24:00, 38.32s/it]                                                        11%|█         | 731/6500 [4:48:07<61:24:00, 38.32s/it] 11%|█▏        | 732/6500 [4:48:25<51:35:59, 32.21s/it]                                                        11%|█▏        | 732/6500 [4:48:25<51:35:59, 32.21s/it] 11%|█▏        | 733/6500 [4:48:43<44:45:28, 27.94s/it]                                                        11%|█▏        | 733/6500 [4:48:43<44:45:28, 27.94s/it] 11%|█▏        | 734/6500 [4:49:01<39:58:50, 24.96s/it]                                                        11%|█▏        | 734/6500 [4:49:01<39:58:50, 24.96s/it] 11%|█▏        | 735/6500 [4:49:19<36:38:17, 22.88s/it]                                                        11%|█▏        | 735/6500 [4:49:19<36:38:17, 22.88s/it] 11%|█▏        | 736/6500 [4:49:37<34:18:42, 21.43s/it]                                                        11%|█▏        | 736/{'loss': 0.6319, 'learning_rate': 9.687622187123936e-05, 'epoch': 0.11}
{'loss': 0.6794, 'learning_rate': 9.686780595043113e-05, 'epoch': 0.11}
{'loss': 0.6653, 'learning_rate': 9.68593790745582e-05, 'epoch': 0.11}
{'loss': 0.6522, 'learning_rate': 9.685094124559034e-05, 'epoch': 0.11}
6500 [4:49:37<34:18:42, 21.43s/it] 11%|█▏        | 737/6500 [4:49:55<32:41:48, 20.42s/it]                                                        11%|█▏        | 737/6500 [4:49:55<32:41:48, 20.42s/it] 11%|█▏        | 738/6500 [4:50:13<31:34:29, 19.73s/it]                                                        11%|█▏        | 738/6500 [4:50:13<31:34:29, 19.73s/it] 11%|█▏        | 739/6500 [4:50:31<30:47:58, 19.25s/it]                                                        11%|█▏        | 739/6500 [4:50:31<30:47:58, 19.25s/it] 11%|█▏        | 740/6500 [4:50:49<30:16:07, 18.92s/it]                                                        11%|█▏        | 740/6500 [4:50:49<30:16:07, 18.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.792510986328125, 'eval_runtime': 5.32, 'eval_samples_per_second': 4.323, 'eval_steps_per_second': 1.128, 'epoch': 0.11}
                                                        11%|█▏        | 740/6500 [4:50:55<30:16:07, 18.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-740
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-740
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-740
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-740

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-740
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-740/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6706, 'learning_rate': 9.684249246549981e-05, 'epoch': 0.11}
{'loss': 0.6243, 'learning_rate': 9.683403273626148e-05, 'epoch': 0.11}
{'loss': 0.6283, 'learning_rate': 9.682556205985274e-05, 'epoch': 0.11}
{'loss': 0.6688, 'learning_rate': 9.68170804382536e-05, 'epoch': 0.11}
{'loss': 0.6525, 'learning_rate': 9.680858787344654e-05, 'epoch': 0.11}
{'loss': 0.6363, 'learning_rate': 9.680008436741665e-05, 'epoch': 0.11}
 11%|█▏        | 741/6500 [4:51:41<46:08:09, 28.84s/it]                                                        11%|█▏        | 741/6500 [4:51:41<46:08:09, 28.84s/it] 11%|█▏        | 742/6500 [4:51:59<40:56:38, 25.60s/it]                                                        11%|█▏        | 742/6500 [4:51:59<40:56:38, 25.60s/it] 11%|█▏        | 743/6500 [4:52:17<37:17:43, 23.32s/it]                                                        11%|█▏        | 743/6500 [4:52:17<37:17:43, 23.32s/it] 11%|█▏        | 744/6500 [4:52:36<34:51:42, 21.80s/it]                                                        11%|█▏        | 744/6500 [4:52:36<34:51:42, 21.80s/it] 11%|█▏        | 745/6500 [4:52:54<33:02:37, 20.67s/it]                                                        11%|█▏        | 745/6500 [4:52:54<33:02:37, 20.67s/it] 11%|█▏        | 746/6500 [4:53:12<31:46:35, 19.88s/it]                                                        11%|█▏        | {'loss': 0.6217, 'learning_rate': 9.679156992215162e-05, 'epoch': 0.11}
{'loss': 0.6624, 'learning_rate': 9.67830445396416e-05, 'epoch': 0.12}
{'loss': 0.7005, 'learning_rate': 9.677450822187937e-05, 'epoch': 0.12}
{'loss': 0.6422, 'learning_rate': 9.676596097086023e-05, 'epoch': 0.12}
746/6500 [4:53:12<31:46:35, 19.88s/it] 11%|█▏        | 747/6500 [4:53:30<30:56:06, 19.36s/it]                                                        11%|█▏        | 747/6500 [4:53:30<30:56:06, 19.36s/it] 12%|█▏        | 748/6500 [4:53:48<30:19:27, 18.98s/it]                                                        12%|█▏        | 748/6500 [4:53:48<30:19:27, 18.98s/it] 12%|█▏        | 749/6500 [4:54:06<29:54:45, 18.72s/it]                                                        12%|█▏        | 749/6500 [4:54:06<29:54:45, 18.72s/it] 12%|█▏        | 750/6500 [4:54:24<29:37:22, 18.55s/it]                                                        12%|█▏        | 750/6500 [4:54:24<29:37:22, 18.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7920599579811096, 'eval_runtime': 5.9912, 'eval_samples_per_second': 3.839, 'eval_steps_per_second': 1.001, 'epoch': 0.12}
                                                        12%|█▏        | 750/6500 [4:54:30<29:37:22, 18.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-750
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-750/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-750/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-750



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-750
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-750/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6256, 'learning_rate': 9.675740278858208e-05, 'epoch': 0.12}
{'loss': 0.6457, 'learning_rate': 9.674883367704529e-05, 'epoch': 0.12}
{'loss': 1.1457, 'learning_rate': 9.674025363825287e-05, 'epoch': 0.12}
{'loss': 0.6636, 'learning_rate': 9.673166267421037e-05, 'epoch': 0.12}
{'loss': 0.6296, 'learning_rate': 9.672306078692583e-05, 'epoch': 0.12}
{'loss': 0.6165, 'learning_rate': 9.671444797840991e-05, 'epoch': 0.12}
 12%|█▏        | 751/6500 [4:55:21<47:53:25, 29.99s/it]                                                        12%|█▏        | 751/6500 [4:55:21<47:53:25, 29.99s/it] 12%|█▏        | 752/6500 [4:55:39<42:07:47, 26.39s/it]                                                        12%|█▏        | 752/6500 [4:55:39<42:07:47, 26.39s/it] 12%|█▏        | 753/6500 [4:55:57<38:05:56, 23.87s/it]                                                        12%|█▏        | 753/6500 [4:55:57<38:05:56, 23.87s/it] 12%|█▏        | 754/6500 [4:56:15<35:17:15, 22.11s/it]                                                        12%|█▏        | 754/6500 [4:56:15<35:17:15, 22.11s/it] 12%|█▏        | 755/6500 [4:56:33<33:19:44, 20.89s/it]                                                        12%|█▏        | 755/6500 [4:56:33<33:19:44, 20.89s/it] 12%|█▏        | 756/6500 [4:56:51<31:57:03, 20.02s/it]                                                        12%|█▏        | {'loss': 0.6189, 'learning_rate': 9.670582425067581e-05, 'epoch': 0.12}
{'loss': 0.6783, 'learning_rate': 9.669718960573927e-05, 'epoch': 0.12}
{'loss': 0.6293, 'learning_rate': 9.668854404561858e-05, 'epoch': 0.12}
{'loss': 0.6293, 'learning_rate': 9.66798875723346e-05, 'epoch': 0.12}
756/6500 [4:56:51<31:57:03, 20.02s/it] 12%|█▏        | 757/6500 [4:57:09<31:00:38, 19.44s/it]                                                        12%|█▏        | 757/6500 [4:57:09<31:00:38, 19.44s/it] 12%|█▏        | 758/6500 [4:57:27<30:21:59, 19.04s/it]                                                        12%|█▏        | 758/6500 [4:57:27<30:21:59, 19.04s/it] 12%|█▏        | 759/6500 [4:57:45<29:54:57, 18.76s/it]                                                        12%|█▏        | 759/6500 [4:57:45<29:54:57, 18.76s/it] 12%|█▏        | 760/6500 [4:58:04<29:47:48, 18.69s/it]                                                        12%|█▏        | 760/6500 [4:58:04<29:47:48, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7915827631950378, 'eval_runtime': 5.3237, 'eval_samples_per_second': 4.32, 'eval_steps_per_second': 1.127, 'epoch': 0.12}
                                                        12%|█▏        | 760/6500 [4:58:09<29:47:48, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-760
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-760
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-760
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-760

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-760
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-760/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-760/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5996, 'learning_rate': 9.667122018791071e-05, 'epoch': 0.12}
{'loss': 0.6342, 'learning_rate': 9.666254189437286e-05, 'epoch': 0.12}
{'loss': 0.6278, 'learning_rate': 9.665385269374956e-05, 'epoch': 0.12}
{'loss': 0.6276, 'learning_rate': 9.664515258807185e-05, 'epoch': 0.12}
{'loss': 0.6735, 'learning_rate': 9.663644157937336e-05, 'epoch': 0.12}
{'loss': 0.6186, 'learning_rate': 9.662771966969017e-05, 'epoch': 0.12}
 12%|█▏        | 761/6500 [4:59:19<56:54:55, 35.70s/it]                                                        12%|█▏        | 761/6500 [4:59:19<56:54:55, 35.70s/it] 12%|█▏        | 762/6500 [4:59:37<48:26:17, 30.39s/it]                                                        12%|█▏        | 762/6500 [4:59:37<48:26:17, 30.39s/it] 12%|█▏        | 763/6500 [4:59:55<42:30:05, 26.67s/it]                                                        12%|█▏        | 763/6500 [4:59:55<42:30:05, 26.67s/it] 12%|█▏        | 764/6500 [5:00:13<38:22:21, 24.08s/it]                                                        12%|█▏        | 764/6500 [5:00:13<38:22:21, 24.08s/it] 12%|█▏        | 765/6500 [5:00:31<35:28:14, 22.27s/it]                                                        12%|█▏        | 765/6500 [5:00:31<35:28:14, 22.27s/it] 12%|█▏        | 766/6500 [5:00:49<33:26:41, 21.00s/it]                                                        12%|█▏        | {'loss': 0.6623, 'learning_rate': 9.661898686106101e-05, 'epoch': 0.12}
{'loss': 0.656, 'learning_rate': 9.661024315552714e-05, 'epoch': 0.12}
{'loss': 0.6275, 'learning_rate': 9.66014885551323e-05, 'epoch': 0.12}
{'loss': 0.6414, 'learning_rate': 9.659272306192286e-05, 'epoch': 0.12}
766/6500 [5:00:49<33:26:41, 21.00s/it] 12%|█▏        | 767/6500 [5:01:07<32:08:54, 20.19s/it]                                                        12%|█▏        | 767/6500 [5:01:07<32:08:54, 20.19s/it] 12%|█▏        | 768/6500 [5:01:25<31:07:56, 19.55s/it]                                                        12%|█▏        | 768/6500 [5:01:25<31:07:56, 19.55s/it] 12%|█▏        | 769/6500 [5:01:44<30:25:52, 19.12s/it]                                                        12%|█▏        | 769/6500 [5:01:44<30:25:52, 19.12s/it] 12%|█▏        | 770/6500 [5:02:02<29:56:52, 18.82s/it]                                                        12%|█▏        | 770/6500 [5:02:02<29:56:52, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7881096601486206, 'eval_runtime': 5.3174, 'eval_samples_per_second': 4.325, 'eval_steps_per_second': 1.128, 'epoch': 0.12}
                                                        12%|█▏        | 770/6500 [5:02:07<29:56:52, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-770
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-770/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-770

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-770

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-770
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-770/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6534, 'learning_rate': 9.658394667794771e-05, 'epoch': 0.12}
{'loss': 0.6054, 'learning_rate': 9.657515940525826e-05, 'epoch': 0.12}
{'loss': 0.6425, 'learning_rate': 9.656636124590845e-05, 'epoch': 0.12}
{'loss': 0.6732, 'learning_rate': 9.655755220195486e-05, 'epoch': 0.12}
{'loss': 0.6108, 'learning_rate': 9.65487322754565e-05, 'epoch': 0.12}
{'loss': 0.6455, 'learning_rate': 9.653990146847499e-05, 'epoch': 0.12}
 12%|█▏        | 771/6500 [5:03:12<54:41:19, 34.37s/it]                                                        12%|█▏        | 771/6500 [5:03:12<54:41:19, 34.37s/it] 12%|█▏        | 772/6500 [5:03:30<46:51:21, 29.45s/it]                                                        12%|█▏        | 772/6500 [5:03:30<46:51:21, 29.45s/it] 12%|█▏        | 773/6500 [5:03:48<41:22:40, 26.01s/it]                                                        12%|█▏        | 773/6500 [5:03:48<41:22:40, 26.01s/it] 12%|█▏        | 774/6500 [5:04:06<37:32:39, 23.60s/it]                                                        12%|█▏        | 774/6500 [5:04:06<37:32:39, 23.60s/it] 12%|█▏        | 775/6500 [5:04:24<34:52:19, 21.93s/it]                                                        12%|█▏        | 775/6500 [5:04:24<34:52:19, 21.93s/it] 12%|█▏        | 776/6500 [5:04:43<33:07:09, 20.83s/it]                                                        12%|█▏        | {'loss': 0.604, 'learning_rate': 9.653105978307449e-05, 'epoch': 0.12}
{'loss': 0.6744, 'learning_rate': 9.652220722132167e-05, 'epoch': 0.12}
{'loss': 0.6731, 'learning_rate': 9.651334378528578e-05, 'epoch': 0.12}
{'loss': 0.621, 'learning_rate': 9.650446947703857e-05, 'epoch': 0.12}
776/6500 [5:04:43<33:07:09, 20.83s/it] 12%|█▏        | 777/6500 [5:05:01<31:47:56, 20.00s/it]                                                        12%|█▏        | 777/6500 [5:05:01<31:47:56, 20.00s/it] 12%|█▏        | 778/6500 [5:05:19<30:53:00, 19.43s/it]                                                        12%|█▏        | 778/6500 [5:05:19<30:53:00, 19.43s/it] 12%|█▏        | 779/6500 [5:05:37<30:14:42, 19.03s/it]                                                        12%|█▏        | 779/6500 [5:05:37<30:14:42, 19.03s/it] 12%|█▏        | 780/6500 [5:05:55<29:50:46, 18.78s/it]                                                        12%|█▏        | 780/6500 [5:05:55<29:50:46, 18.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7889187932014465, 'eval_runtime': 5.3211, 'eval_samples_per_second': 4.322, 'eval_steps_per_second': 1.128, 'epoch': 0.12}
                                                        12%|█▏        | 780/6500 [5:06:00<29:50:46, 18.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-780
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-780
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-780 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-780

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-780
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-780/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-780/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6437, 'learning_rate': 9.64955842986544e-05, 'epoch': 0.12}
{'loss': 0.616, 'learning_rate': 9.648668825221006e-05, 'epoch': 0.12}
{'loss': 1.1416, 'learning_rate': 9.647778133978502e-05, 'epoch': 0.12}
{'loss': 0.6347, 'learning_rate': 9.646886356346116e-05, 'epoch': 0.12}
{'loss': 0.6395, 'learning_rate': 9.645993492532298e-05, 'epoch': 0.12}
{'loss': 0.5826, 'learning_rate': 9.64509954274575e-05, 'epoch': 0.12}
 12%|█▏        | 781/6500 [5:07:45<73:09:42, 46.05s/it]                                                        12%|█▏        | 781/6500 [5:07:45<73:09:42, 46.05s/it] 12%|█▏        | 782/6500 [5:08:03<59:48:12, 37.65s/it]                                                        12%|█▏        | 782/6500 [5:08:03<59:48:12, 37.65s/it] 12%|█▏        | 783/6500 [5:08:21<50:25:45, 31.76s/it]                                                        12%|█▏        | 783/6500 [5:08:21<50:25:45, 31.76s/it] 12%|█▏        | 784/6500 [5:08:39<43:51:47, 27.63s/it]                                                        12%|█▏        | 784/6500 [5:08:39<43:51:47, 27.63s/it] 12%|█▏        | 785/6500 [5:08:57<39:17:22, 24.75s/it]                                                        12%|█▏        | 785/6500 [5:08:57<39:17:22, 24.75s/it] 12%|█▏        | 786/6500 [5:09:15<36:05:29, 22.74s/it]                                                        12%|█▏        | {'loss': 0.6507, 'learning_rate': 9.644204507195426e-05, 'epoch': 0.12}
{'loss': 0.6411, 'learning_rate': 9.643308386090537e-05, 'epoch': 0.12}
{'loss': 0.5896, 'learning_rate': 9.642411179640542e-05, 'epoch': 0.12}
{'loss': 0.6185, 'learning_rate': 9.641512888055162e-05, 'epoch': 0.12}
786/6500 [5:09:15<36:05:29, 22.74s/it] 12%|█▏        | 787/6500 [5:09:33<33:51:52, 21.34s/it]                                                        12%|█▏        | 787/6500 [5:09:33<33:51:52, 21.34s/it] 12%|█▏        | 788/6500 [5:09:51<32:18:54, 20.37s/it]                                                        12%|█▏        | 788/6500 [5:09:51<32:18:54, 20.37s/it] 12%|█▏        | 789/6500 [5:10:09<31:14:16, 19.69s/it]                                                        12%|█▏        | 789/6500 [5:10:09<31:14:16, 19.69s/it] 12%|█▏        | 790/6500 [5:10:27<30:29:50, 19.23s/it]                                                        12%|█▏        | 790/6500 [5:10:27<30:29:50, 19.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7913035750389099, 'eval_runtime': 5.3255, 'eval_samples_per_second': 4.319, 'eval_steps_per_second': 1.127, 'epoch': 0.12}
                                                        12%|█▏        | 790/6500 [5:10:33<30:29:50, 19.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-790
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-790
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-790
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-790

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-790
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-790/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-790/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5977, 'learning_rate': 9.640613511544365e-05, 'epoch': 0.12}
{'loss': 0.6178, 'learning_rate': 9.639713050318375e-05, 'epoch': 0.12}
{'loss': 0.6092, 'learning_rate': 9.638811504587669e-05, 'epoch': 0.12}
{'loss': 0.6301, 'learning_rate': 9.637908874562978e-05, 'epoch': 0.12}
{'loss': 0.6414, 'learning_rate': 9.637005160455287e-05, 'epoch': 0.12}
{'loss': 0.6088, 'learning_rate': 9.636100362475832e-05, 'epoch': 0.12}
 12%|█▏        | 791/6500 [5:12:25<77:28:48, 48.86s/it]                                                        12%|█▏        | 791/6500 [5:12:25<77:28:48, 48.86s/it] 12%|█▏        | 792/6500 [5:12:44<62:56:44, 39.70s/it]                                                        12%|█▏        | 792/6500 [5:12:44<62:56:44, 39.70s/it] 12%|█▏        | 793/6500 [5:13:02<52:35:35, 33.18s/it]                                                        12%|█▏        | 793/6500 [5:13:02<52:35:35, 33.18s/it] 12%|█▏        | 794/6500 [5:13:20<45:20:55, 28.61s/it]                                                        12%|█▏        | 794/6500 [5:13:20<45:20:55, 28.61s/it] 12%|█▏        | 795/6500 [5:13:37<40:17:27, 25.42s/it]                                                        12%|█▏        | 795/6500 [5:13:37<40:17:27, 25.42s/it] 12%|█▏        | 796/6500 [5:13:56<36:49:01, 23.24s/it]                                                        12%|█▏        | {'loss': 0.6825, 'learning_rate': 9.635194480836108e-05, 'epoch': 0.12}
{'loss': 0.6436, 'learning_rate': 9.634287515747856e-05, 'epoch': 0.12}
{'loss': 0.6369, 'learning_rate': 9.633379467423072e-05, 'epoch': 0.12}
{'loss': 0.6519, 'learning_rate': 9.632470336074009e-05, 'epoch': 0.12}
796/6500 [5:13:56<36:49:01, 23.24s/it] 12%|█▏        | 797/6500 [5:14:14<34:20:57, 21.68s/it]                                                        12%|█▏        | 797/6500 [5:14:14<34:20:57, 21.68s/it] 12%|█▏        | 798/6500 [5:14:32<32:39:43, 20.62s/it]                                                        12%|█▏        | 798/6500 [5:14:32<32:39:43, 20.62s/it] 12%|█▏        | 799/6500 [5:14:50<31:27:39, 19.87s/it]                                                        12%|█▏        | 799/6500 [5:14:50<31:27:39, 19.87s/it] 12%|█▏        | 800/6500 [5:15:08<30:38:07, 19.35s/it]                                                        12%|█▏        | 800/6500 [5:15:08<30:38:07, 19.35s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7852601408958435, 'eval_runtime': 6.267, 'eval_samples_per_second': 3.67, 'eval_steps_per_second': 0.957, 'epoch': 0.12}
                                                        12%|█▏        | 800/6500 [5:15:14<30:38:07, 19.35s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-800
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-800
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-800

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-800
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-800/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-800/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5981, 'learning_rate': 9.631560121913172e-05, 'epoch': 0.12}
{'loss': 0.6085, 'learning_rate': 9.630648825153317e-05, 'epoch': 0.12}
{'loss': 0.6483, 'learning_rate': 9.629736446007454e-05, 'epoch': 0.12}
{'loss': 0.6349, 'learning_rate': 9.628822984688845e-05, 'epoch': 0.12}
{'loss': 0.6074, 'learning_rate': 9.627908441411008e-05, 'epoch': 0.12}
{'loss': 0.6241, 'learning_rate': 9.62699281638771e-05, 'epoch': 0.12}
 12%|█▏        | 801/6500 [5:16:41<65:34:13, 41.42s/it]                                                        12%|█▏        | 801/6500 [5:16:41<65:34:13, 41.42s/it] 12%|█▏        | 802/6500 [5:16:59<54:24:51, 34.38s/it]                                                        12%|█▏        | 802/6500 [5:16:59<54:24:51, 34.38s/it] 12%|█▏        | 803/6500 [5:17:17<46:36:26, 29.45s/it]                                                        12%|█▏        | 803/6500 [5:17:17<46:36:26, 29.45s/it] 12%|█▏        | 804/6500 [5:17:35<41:09:35, 26.01s/it]                                                        12%|█▏        | 804/6500 [5:17:35<41:09:35, 26.01s/it] 12%|█▏        | 805/6500 [5:17:53<37:22:02, 23.62s/it]                                                        12%|█▏        | 805/6500 [5:17:53<37:22:02, 23.62s/it] 12%|█▏        | 806/6500 [5:18:11<34:42:45, 21.95s/it]                                                        12%|█▏        | {'loss': 0.61, 'learning_rate': 9.626076109832975e-05, 'epoch': 0.12}
{'loss': 0.7182, 'learning_rate': 9.625158321961075e-05, 'epoch': 0.12}
{'loss': 0.6008, 'learning_rate': 9.624239452986539e-05, 'epoch': 0.12}
{'loss': 0.6122, 'learning_rate': 9.623319503124148e-05, 'epoch': 0.12}
806/6500 [5:18:11<34:42:45, 21.95s/it] 12%|█▏        | 807/6500 [5:18:29<32:52:42, 20.79s/it]                                                        12%|█▏        | 807/6500 [5:18:29<32:52:42, 20.79s/it] 12%|█▏        | 808/6500 [5:18:47<31:41:47, 20.05s/it]                                                        12%|█▏        | 808/6500 [5:18:47<31:41:47, 20.05s/it] 12%|█▏        | 809/6500 [5:19:06<30:47:10, 19.47s/it]                                                        12%|█▏        | 809/6500 [5:19:06<30:47:10, 19.47s/it] 12%|█▏        | 810/6500 [5:19:24<30:09:05, 19.08s/it]                                                        12%|█▏        | 810/6500 [5:19:24<30:09:05, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.789171576499939, 'eval_runtime': 5.3293, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.12}
                                                        12%|█▏        | 810/6500 [5:19:29<30:09:05, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-810
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-810/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-810


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-810
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-810/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-810/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6254, 'learning_rate': 9.622398472588932e-05, 'epoch': 0.12}
{'loss': 1.1323, 'learning_rate': 9.621476361596177e-05, 'epoch': 0.12}
{'loss': 0.6246, 'learning_rate': 9.620553170361423e-05, 'epoch': 0.13}
{'loss': 0.6168, 'learning_rate': 9.619628899100459e-05, 'epoch': 0.13}
{'loss': 0.6054, 'learning_rate': 9.618703548029327e-05, 'epoch': 0.13}
{'loss': 0.5883, 'learning_rate': 9.617777117364322e-05, 'epoch': 0.13}
 12%|█▏        | 811/6500 [5:20:34<54:37:27, 34.57s/it]                                                        12%|█▏        | 811/6500 [5:20:34<54:37:27, 34.57s/it] 12%|█▏        | 812/6500 [5:20:52<46:45:26, 29.59s/it]                                                        12%|█▏        | 812/6500 [5:20:52<46:45:26, 29.59s/it] 13%|█▎        | 813/6500 [5:21:10<41:14:47, 26.11s/it]                                                        13%|█▎        | 813/6500 [5:21:10<41:14:47, 26.11s/it] 13%|█▎        | 814/6500 [5:21:28<37:23:30, 23.67s/it]                                                        13%|█▎        | 814/6500 [5:21:28<37:23:30, 23.67s/it] 13%|█▎        | 815/6500 [5:21:46<34:43:20, 21.99s/it]                                                        13%|█▎        | 815/6500 [5:21:46<34:43:20, 21.99s/it] 13%|█▎        | 816/6500 [5:22:04<32:51:30, 20.81s/it]                                                        13%|█▎        | {'loss': 0.6481, 'learning_rate': 9.616849607321994e-05, 'epoch': 0.13}
{'loss': 0.6188, 'learning_rate': 9.61592101811914e-05, 'epoch': 0.13}
{'loss': 0.5949, 'learning_rate': 9.614991349972815e-05, 'epoch': 0.13}
{'loss': 0.5959, 'learning_rate': 9.614060603100318e-05, 'epoch': 0.13}
816/6500 [5:22:04<32:51:30, 20.81s/it] 13%|█▎        | 817/6500 [5:22:23<31:33:14, 19.99s/it]                                                        13%|█▎        | 817/6500 [5:22:23<31:33:14, 19.99s/it] 13%|█▎        | 818/6500 [5:22:41<30:38:55, 19.42s/it]                                                        13%|█▎        | 818/6500 [5:22:41<30:38:55, 19.42s/it] 13%|█▎        | 819/6500 [5:22:59<30:01:11, 19.02s/it]                                                        13%|█▎        | 819/6500 [5:22:59<30:01:11, 19.02s/it] 13%|█▎        | 820/6500 [5:23:17<29:35:57, 18.76s/it]                                                        13%|█▎        | 820/6500 [5:23:17<29:35:57, 18.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7878454327583313, 'eval_runtime': 5.3378, 'eval_samples_per_second': 4.309, 'eval_steps_per_second': 1.124, 'epoch': 0.13}
                                                        13%|█▎        | 820/6500 [5:23:22<29:35:57, 18.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-820
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-820
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-820the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-820

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-820/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-820/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-820/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5956, 'learning_rate': 9.613128777719214e-05, 'epoch': 0.13}
{'loss': 0.6201, 'learning_rate': 9.612195874047302e-05, 'epoch': 0.13}
{'loss': 0.5923, 'learning_rate': 9.61126189230265e-05, 'epoch': 0.13}
{'loss': 0.6397, 'learning_rate': 9.610326832703565e-05, 'epoch': 0.13}
{'loss': 0.5942, 'learning_rate': 9.609390695468616e-05, 'epoch': 0.13}
{'loss': 0.6245, 'learning_rate': 9.608453480816617e-05, 'epoch': 0.13}
 13%|█▎        | 821/6500 [5:24:49<64:12:55, 40.71s/it]                                                        13%|█▎        | 821/6500 [5:24:49<64:12:55, 40.71s/it] 13%|█▎        | 822/6500 [5:25:07<53:26:58, 33.89s/it]                                                        13%|█▎        | 822/6500 [5:25:07<53:26:58, 33.89s/it] 13%|█▎        | 823/6500 [5:25:25<46:01:11, 29.18s/it]                                                        13%|█▎        | 823/6500 [5:25:25<46:01:11, 29.18s/it] 13%|█▎        | 824/6500 [5:25:43<40:44:57, 25.85s/it]                                                        13%|█▎        | 824/6500 [5:25:43<40:44:57, 25.85s/it] 13%|█▎        | 825/6500 [5:26:02<37:23:46, 23.72s/it]                                                        13%|█▎        | 825/6500 [5:26:02<37:23:46, 23.72s/it] 13%|█▎        | 826/6500 [5:26:20<34:43:10, 22.03s/it]                                                        13%|█▎        | {'loss': 0.6408, 'learning_rate': 9.607515188966638e-05, 'epoch': 0.13}
{'loss': 0.6223, 'learning_rate': 9.606575820137996e-05, 'epoch': 0.13}
{'loss': 0.6152, 'learning_rate': 9.605635374550263e-05, 'epoch': 0.13}
{'loss': 0.6207, 'learning_rate': 9.604693852423268e-05, 'epoch': 0.13}
826/6500 [5:26:20<34:43:10, 22.03s/it] 13%|█▎        | 827/6500 [5:26:38<32:50:29, 20.84s/it]                                                        13%|█▎        | 827/6500 [5:26:38<32:50:29, 20.84s/it] 13%|█▎        | 828/6500 [5:26:56<31:31:56, 20.01s/it]                                                        13%|█▎        | 828/6500 [5:26:56<31:31:56, 20.01s/it] 13%|█▎        | 829/6500 [5:27:16<31:28:24, 19.98s/it]                                                        13%|█▎        | 829/6500 [5:27:16<31:28:24, 19.98s/it] 13%|█▎        | 830/6500 [5:27:34<30:36:18, 19.43s/it]                                                        13%|█▎        | 830/6500 [5:27:34<30:36:18, 19.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7838425636291504, 'eval_runtime': 6.7984, 'eval_samples_per_second': 3.383, 'eval_steps_per_second': 0.883, 'epoch': 0.13}
                                                        13%|█▎        | 830/6500 [5:27:41<30:36:18, 19.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-830
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-830/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-830
 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-830

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-830
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-830/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-830/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.599, 'learning_rate': 9.603751253977079e-05, 'epoch': 0.13}
{'loss': 0.5942, 'learning_rate': 9.602807579432027e-05, 'epoch': 0.13}
{'loss': 0.653, 'learning_rate': 9.601862829008688e-05, 'epoch': 0.13}
{'loss': 0.6078, 'learning_rate': 9.600917002927893e-05, 'epoch': 0.13}
{'loss': 0.6232, 'learning_rate': 9.599970101410722e-05, 'epoch': 0.13}
{'loss': 0.5993, 'learning_rate': 9.59902212467851e-05, 'epoch': 0.13}
 13%|█▎        | 831/6500 [5:29:14<68:35:11, 43.55s/it]                                                        13%|█▎        | 831/6500 [5:29:14<68:35:11, 43.55s/it] 13%|█▎        | 832/6500 [5:29:32<56:29:01, 35.88s/it]                                                        13%|█▎        | 832/6500 [5:29:32<56:29:01, 35.88s/it] 13%|█▎        | 833/6500 [5:29:50<48:00:36, 30.50s/it]                                                        13%|█▎        | 833/6500 [5:29:50<48:00:36, 30.50s/it] 13%|█▎        | 834/6500 [5:30:08<42:04:11, 26.73s/it]                                                        13%|█▎        | 834/6500 [5:30:08<42:04:11, 26.73s/it] 13%|█▎        | 835/6500 [5:30:26<37:55:39, 24.10s/it]                                                        13%|█▎        | 835/6500 [5:30:26<37:55:39, 24.10s/it] 13%|█▎        | 836/6500 [5:30:44<35:02:29, 22.27s/it]                                                        13%|█▎        | {'loss': 0.6222, 'learning_rate': 9.598073072952836e-05, 'epoch': 0.13}
{'loss': 0.6715, 'learning_rate': 9.59712294645554e-05, 'epoch': 0.13}
{'loss': 0.611, 'learning_rate': 9.596171745408705e-05, 'epoch': 0.13}
{'loss': 0.5852, 'learning_rate': 9.595219470034671e-05, 'epoch': 0.13}
836/6500 [5:30:44<35:02:29, 22.27s/it] 13%|█▎        | 837/6500 [5:31:02<33:02:16, 21.00s/it]                                                        13%|█▎        | 837/6500 [5:31:02<33:02:16, 21.00s/it] 13%|█▎        | 838/6500 [5:31:20<31:38:17, 20.12s/it]                                                        13%|█▎        | 838/6500 [5:31:20<31:38:17, 20.12s/it] 13%|█▎        | 839/6500 [5:31:38<30:40:11, 19.50s/it]                                                        13%|█▎        | 839/6500 [5:31:38<30:40:11, 19.50s/it] 13%|█▎        | 840/6500 [5:31:56<30:00:56, 19.09s/it]                                                        13%|█▎        | 840/6500 [5:31:56<30:00:56, 19.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7847349047660828, 'eval_runtime': 5.5684, 'eval_samples_per_second': 4.13, 'eval_steps_per_second': 1.078, 'epoch': 0.13}
                                                        13%|█▎        | 840/6500 [5:32:02<30:00:56, 19.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-840
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-840/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-840
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-840/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6207, 'learning_rate': 9.594266120556023e-05, 'epoch': 0.13}
{'loss': 1.1114, 'learning_rate': 9.593311697195605e-05, 'epoch': 0.13}
{'loss': 0.6343, 'learning_rate': 9.592356200176504e-05, 'epoch': 0.13}
{'loss': 0.6007, 'learning_rate': 9.591399629722066e-05, 'epoch': 0.13}
{'loss': 0.5722, 'learning_rate': 9.590441986055878e-05, 'epoch': 0.13}
{'loss': 0.6004, 'learning_rate': 9.589483269401786e-05, 'epoch': 0.13}
 13%|█▎        | 841/6500 [5:33:07<54:15:57, 34.52s/it]                                                        13%|█▎        | 841/6500 [5:33:07<54:15:57, 34.52s/it] 13%|█▎        | 842/6500 [5:33:25<46:27:21, 29.56s/it]                                                        13%|█▎        | 842/6500 [5:33:25<46:27:21, 29.56s/it] 13%|█▎        | 843/6500 [5:33:43<40:59:22, 26.08s/it]                                                        13%|█▎        | 843/6500 [5:33:43<40:59:22, 26.08s/it] 13%|█▎        | 844/6500 [5:34:00<37:09:51, 23.65s/it]                                                        13%|█▎        | 844/6500 [5:34:00<37:09:51, 23.65s/it] 13%|█▎        | 845/6500 [5:34:19<34:30:15, 21.97s/it]                                                        13%|█▎        | 845/6500 [5:34:19<34:30:15, 21.97s/it] 13%|█▎        | 846/6500 [5:34:37<32:38:43, 20.79s/it]                                                        13%|█▎        | {'loss': 0.6296, 'learning_rate': 9.588523479983887e-05, 'epoch': 0.13}
{'loss': 0.6093, 'learning_rate': 9.58756261802652e-05, 'epoch': 0.13}
{'loss': 0.6016, 'learning_rate': 9.586600683754287e-05, 'epoch': 0.13}
{'loss': 0.5794, 'learning_rate': 9.58563767739203e-05, 'epoch': 0.13}
846/6500 [5:34:37<32:38:43, 20.79s/it] 13%|█▎        | 847/6500 [5:34:55<31:27:33, 20.03s/it]                                                        13%|█▎        | 847/6500 [5:34:55<31:27:33, 20.03s/it] 13%|█▎        | 848/6500 [5:35:13<30:32:51, 19.46s/it]                                                        13%|█▎        | 848/6500 [5:35:13<30:32:51, 19.46s/it] 13%|█▎        | 849/6500 [5:35:31<29:54:28, 19.05s/it]                                                        13%|█▎        | 849/6500 [5:35:31<29:54:28, 19.05s/it] 13%|█▎        | 850/6500 [5:35:49<29:27:40, 18.77s/it]                                                        13%|█▎        | 850/6500 [5:35:49<29:27:40, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7857524752616882, 'eval_runtime': 5.3459, 'eval_samples_per_second': 4.302, 'eval_steps_per_second': 1.122, 'epoch': 0.13}
                                                        13%|█▎        | 850/6500 [5:35:55<29:27:40, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-850
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-850
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-850
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-850

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-850
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-850/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-850/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.597, 'learning_rate': 9.584673599164846e-05, 'epoch': 0.13}
{'loss': 0.5896, 'learning_rate': 9.583708449298083e-05, 'epoch': 0.13}
{'loss': 0.592, 'learning_rate': 9.582742228017342e-05, 'epoch': 0.13}
{'loss': 0.6323, 'learning_rate': 9.581774935548467e-05, 'epoch': 0.13}
{'loss': 0.5794, 'learning_rate': 9.580806572117557e-05, 'epoch': 0.13}
{'loss': 0.6408, 'learning_rate': 9.579837137950966e-05, 'epoch': 0.13}
 13%|█▎        | 851/6500 [5:37:00<53:46:39, 34.27s/it]                                                        13%|█▎        | 851/6500 [5:37:00<53:46:39, 34.27s/it] 13%|█▎        | 852/6500 [5:37:18<46:06:24, 29.39s/it]                                                        13%|█▎        | 852/6500 [5:37:18<46:06:24, 29.39s/it] 13%|█▎        | 853/6500 [5:37:36<40:46:48, 26.00s/it]                                                        13%|█▎        | 853/6500 [5:37:36<40:46:48, 26.00s/it] 13%|█▎        | 854/6500 [5:37:54<37:02:58, 23.62s/it]                                                        13%|█▎        | 854/6500 [5:37:54<37:02:58, 23.62s/it] 13%|█▎        | 855/6500 [5:38:12<34:31:34, 22.02s/it]                                                        13%|█▎        | 855/6500 [5:38:12<34:31:34, 22.02s/it] 13%|█▎        | 856/6500 [5:38:30<32:39:05, 20.83s/it]                                                        13%|█▎        | {'loss': 0.6262, 'learning_rate': 9.578866633275288e-05, 'epoch': 0.13}
{'loss': 0.6067, 'learning_rate': 9.577895058317374e-05, 'epoch': 0.13}
{'loss': 0.6163, 'learning_rate': 9.576922413304326e-05, 'epoch': 0.13}
{'loss': 0.6159, 'learning_rate': 9.575948698463491e-05, 'epoch': 0.13}
856/6500 [5:38:30<32:39:05, 20.83s/it] 13%|█▎        | 857/6500 [5:38:49<31:51:14, 20.32s/it]                                                        13%|█▎        | 857/6500 [5:38:49<31:51:14, 20.32s/it] 13%|█▎        | 858/6500 [5:39:07<30:49:07, 19.66s/it]                                                        13%|█▎        | 858/6500 [5:39:07<30:49:07, 19.66s/it] 13%|█▎        | 859/6500 [5:39:25<30:05:24, 19.20s/it]                                                        13%|█▎        | 859/6500 [5:39:25<30:05:24, 19.20s/it] 13%|█▎        | 860/6500 [5:39:44<29:35:21, 18.89s/it]                                                        13%|█▎        | 860/6500 [5:39:44<29:35:21, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7779276371002197, 'eval_runtime': 5.4792, 'eval_samples_per_second': 4.198, 'eval_steps_per_second': 1.095, 'epoch': 0.13}
                                                        13%|█▎        | 860/6500 [5:39:49<29:35:21, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-860
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-860 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-860
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-860
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-860/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5989, 'learning_rate': 9.574973914022469e-05, 'epoch': 0.13}
{'loss': 0.6017, 'learning_rate': 9.57399806020911e-05, 'epoch': 0.13}
{'loss': 0.6252, 'learning_rate': 9.573021137251516e-05, 'epoch': 0.13}
{'loss': 0.5919, 'learning_rate': 9.572043145378038e-05, 'epoch': 0.13}
{'loss': 0.6104, 'learning_rate': 9.571064084817271e-05, 'epoch': 0.13}
{'loss': 0.577, 'learning_rate': 9.570083955798065e-05, 'epoch': 0.13}
 13%|█▎        | 861/6500 [5:40:40<47:20:51, 30.23s/it]                                                        13%|█▎        | 861/6500 [5:40:40<47:20:51, 30.23s/it] 13%|█▎        | 862/6500 [5:40:58<41:36:40, 26.57s/it]                                                        13%|█▎        | 862/6500 [5:40:58<41:36:40, 26.57s/it] 13%|█▎        | 863/6500 [5:41:16<37:35:34, 24.01s/it]                                                        13%|█▎        | 863/6500 [5:41:16<37:35:34, 24.01s/it] 13%|█▎        | 864/6500 [5:41:34<34:47:03, 22.22s/it]                                                        13%|█▎        | 864/6500 [5:41:34<34:47:03, 22.22s/it] 13%|█▎        | 865/6500 [5:41:52<32:49:06, 20.97s/it]                                                        13%|█▎        | 865/6500 [5:41:52<32:49:06, 20.97s/it] 13%|█▎        | 866/6500 [5:42:11<31:27:34, 20.10s/it]                                                        13%|█▎        | {'loss': 0.6422, 'learning_rate': 9.569102758549524e-05, 'epoch': 0.13}
{'loss': 0.6422, 'learning_rate': 9.568120493300993e-05, 'epoch': 0.13}
{'loss': 0.5993, 'learning_rate': 9.567137160282071e-05, 'epoch': 0.13}
{'loss': 0.6078, 'learning_rate': 9.566152759722606e-05, 'epoch': 0.13}
866/6500 [5:42:11<31:27:34, 20.10s/it] 13%|█▎        | 867/6500 [5:42:29<30:30:54, 19.50s/it]                                                        13%|█▎        | 867/6500 [5:42:29<30:30:54, 19.50s/it] 13%|█▎        | 868/6500 [5:42:47<29:53:16, 19.10s/it]                                                        13%|█▎        | 868/6500 [5:42:47<29:53:16, 19.10s/it] 13%|█▎        | 869/6500 [5:43:05<29:25:30, 18.81s/it]                                                        13%|█▎        | 869/6500 [5:43:05<29:25:30, 18.81s/it] 13%|█▎        | 870/6500 [5:43:23<29:06:25, 18.61s/it]                                                        13%|█▎        | 870/6500 [5:43:23<29:06:25, 18.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.77903813123703, 'eval_runtime': 5.3374, 'eval_samples_per_second': 4.309, 'eval_steps_per_second': 1.124, 'epoch': 0.13}
                                                        13%|█▎        | 870/6500 [5:43:28<29:06:25, 18.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-870
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-870
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-870
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-870

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-870
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-870/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-870/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5999, 'learning_rate': 9.565167291852697e-05, 'epoch': 0.13}
{'loss': 1.1195, 'learning_rate': 9.56418075690269e-05, 'epoch': 0.13}
{'loss': 0.5946, 'learning_rate': 9.563193155103181e-05, 'epoch': 0.13}
{'loss': 0.6105, 'learning_rate': 9.562204486685017e-05, 'epoch': 0.13}
{'loss': 0.5588, 'learning_rate': 9.561214751879292e-05, 'epoch': 0.13}
{'loss': 0.6181, 'learning_rate': 9.560223950917353e-05, 'epoch': 0.13}
 13%|█▎        | 871/6500 [5:45:01<66:08:03, 42.30s/it]                                                        13%|█▎        | 871/6500 [5:45:01<66:08:03, 42.30s/it] 13%|█▎        | 872/6500 [5:45:19<54:42:08, 34.99s/it]                                                        13%|█▎        | 872/6500 [5:45:19<54:42:08, 34.99s/it] 13%|█▎        | 873/6500 [5:45:37<46:54:06, 30.01s/it]                                                        13%|█▎        | 873/6500 [5:45:37<46:54:06, 30.01s/it] 13%|█▎        | 874/6500 [5:45:55<41:16:38, 26.41s/it]                                                        13%|█▎        | 874/6500 [5:45:55<41:16:38, 26.41s/it] 13%|█▎        | 875/6500 [5:46:13<37:20:45, 23.90s/it]                                                        13%|█▎        | 875/6500 [5:46:13<37:20:45, 23.90s/it] 13%|█▎        | 876/6500 [5:46:31<34:36:28, 22.15s/it]                                                        13%|█▎        | {'loss': 0.6146, 'learning_rate': 9.559232084030791e-05, 'epoch': 0.13}
{'loss': 0.5554, 'learning_rate': 9.558239151451451e-05, 'epoch': 0.14}
{'loss': 0.5985, 'learning_rate': 9.557245153411423e-05, 'epoch': 0.14}
{'loss': 0.5706, 'learning_rate': 9.556250090143049e-05, 'epoch': 0.14}
876/6500 [5:46:31<34:36:28, 22.15s/it] 13%|█▎        | 877/6500 [5:46:49<32:45:40, 20.97s/it]                                                        13%|█▎        | 877/6500 [5:46:49<32:45:40, 20.97s/it] 14%|█▎        | 878/6500 [5:47:07<31:24:55, 20.12s/it]                                                        14%|█▎        | 878/6500 [5:47:07<31:24:55, 20.12s/it] 14%|█▎        | 879/6500 [5:47:26<30:29:41, 19.53s/it]                                                        14%|█▎        | 879/6500 [5:47:26<30:29:41, 19.53s/it] 14%|█▎        | 880/6500 [5:47:44<29:50:46, 19.12s/it]                                                        14%|█▎        | 880/6500 [5:47:44<29:50:46, 19.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7814657688140869, 'eval_runtime': 5.3431, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.14}
                                                        14%|█▎        | 880/6500 [5:47:49<29:50:46, 19.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-880
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-880
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-880 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-880

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-880
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-880/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-880/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5931, 'learning_rate': 9.55525396187892e-05, 'epoch': 0.14}
{'loss': 0.5861, 'learning_rate': 9.554256768851873e-05, 'epoch': 0.14}
{'loss': 0.6008, 'learning_rate': 9.553258511294996e-05, 'epoch': 0.14}
{'loss': 0.6042, 'learning_rate': 9.552259189441626e-05, 'epoch': 0.14}
{'loss': 0.5781, 'learning_rate': 9.55125880352535e-05, 'epoch': 0.14}
{'loss': 0.6423, 'learning_rate': 9.55025735378e-05, 'epoch': 0.14}
 14%|█▎        | 881/6500 [5:48:44<48:53:56, 31.33s/it]                                                        14%|█▎        | 881/6500 [5:48:44<48:53:56, 31.33s/it] 14%|█▎        | 882/6500 [5:49:02<42:40:54, 27.35s/it]                                                        14%|█▎        | 882/6500 [5:49:02<42:40:54, 27.35s/it] 14%|█▎        | 883/6500 [5:49:20<38:18:55, 24.56s/it]                                                        14%|█▎        | 883/6500 [5:49:20<38:18:55, 24.56s/it] 14%|█▎        | 884/6500 [5:49:38<35:16:09, 22.61s/it]                                                        14%|█▎        | 884/6500 [5:49:38<35:16:09, 22.61s/it] 14%|█▎        | 885/6500 [5:49:56<33:08:43, 21.25s/it]                                                        14%|█▎        | 885/6500 [5:49:56<33:08:43, 21.25s/it] 14%|█▎        | 886/6500 [5:50:14<31:40:23, 20.31s/it]                                                        14%|█▎        | {'loss': 0.6084, 'learning_rate': 9.549254840439659e-05, 'epoch': 0.14}
{'loss': 0.5935, 'learning_rate': 9.54825126373866e-05, 'epoch': 0.14}
{'loss': 0.6245, 'learning_rate': 9.547246623911582e-05, 'epoch': 0.14}
{'loss': 0.5754, 'learning_rate': 9.546240921193253e-05, 'epoch': 0.14}
886/6500 [5:50:14<31:40:23, 20.31s/it] 14%|█▎        | 887/6500 [5:50:32<30:38:51, 19.66s/it]                                                        14%|█▎        | 887/6500 [5:50:32<30:38:51, 19.66s/it] 14%|█▎        | 888/6500 [5:50:50<29:55:56, 19.20s/it]                                                        14%|█▎        | 888/6500 [5:50:50<29:55:56, 19.20s/it] 14%|█▎        | 889/6500 [5:51:09<29:32:18, 18.95s/it]                                                        14%|█▎        | 889/6500 [5:51:09<29:32:18, 18.95s/it] 14%|█▎        | 890/6500 [5:51:27<29:10:02, 18.72s/it]                                                        14%|█▎        | 890/6500 [5:51:27<29:10:02, 18.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7754690647125244, 'eval_runtime': 5.3415, 'eval_samples_per_second': 4.306, 'eval_steps_per_second': 1.123, 'epoch': 0.14}
                                                        14%|█▎        | 890/6500 [5:51:32<29:10:02, 18.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-890
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-890 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-890 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-890
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-890/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5794, 'learning_rate': 9.54523415581875e-05, 'epoch': 0.14}
{'loss': 0.6177, 'learning_rate': 9.544226328023398e-05, 'epoch': 0.14}
{'loss': 0.6098, 'learning_rate': 9.543217438042773e-05, 'epoch': 0.14}
{'loss': 0.5885, 'learning_rate': 9.542207486112694e-05, 'epoch': 0.14}
{'loss': 0.5781, 'learning_rate': 9.541196472469233e-05, 'epoch': 0.14}
{'loss': 0.615, 'learning_rate': 9.540184397348706e-05, 'epoch': 0.14}
 14%|█▎        | 891/6500 [5:52:51<59:50:52, 38.41s/it]                                                        14%|█▎        | 891/6500 [5:52:51<59:50:52, 38.41s/it] 14%|█▎        | 892/6500 [5:53:09<50:20:56, 32.32s/it]                                                        14%|█▎        | 892/6500 [5:53:09<50:20:56, 32.32s/it] 14%|█▎        | 893/6500 [5:53:27<43:38:26, 28.02s/it]                                                        14%|█▎        | 893/6500 [5:53:27<43:38:26, 28.02s/it] 14%|█▍        | 894/6500 [5:53:45<38:57:04, 25.01s/it]                                                        14%|█▍        | 894/6500 [5:53:45<38:57:04, 25.01s/it] 14%|█▍        | 895/6500 [5:54:03<35:40:58, 22.92s/it]                                                        14%|█▍        | 895/6500 [5:54:03<35:40:58, 22.92s/it] 14%|█▍        | 896/6500 [5:54:21<33:24:39, 21.46s/it]                                                        14%|█▍        | {'loss': 0.6491, 'learning_rate': 9.539171260987681e-05, 'epoch': 0.14}
{'loss': 0.5727, 'learning_rate': 9.538157063622974e-05, 'epoch': 0.14}
{'loss': 0.5798, 'learning_rate': 9.537141805491646e-05, 'epoch': 0.14}
{'loss': 0.5993, 'learning_rate': 9.536125486831005e-05, 'epoch': 0.14}
896/6500 [5:54:21<33:24:39, 21.46s/it] 14%|█▍        | 897/6500 [5:54:39<31:49:00, 20.44s/it]                                                        14%|█▍        | 897/6500 [5:54:39<31:49:00, 20.44s/it] 14%|█▍        | 898/6500 [5:54:58<30:42:54, 19.74s/it]                                                        14%|█▍        | 898/6500 [5:54:58<30:42:54, 19.74s/it] 14%|█▍        | 899/6500 [5:55:16<29:57:35, 19.26s/it]                                                        14%|█▍        | 899/6500 [5:55:16<29:57:35, 19.26s/it] 14%|█▍        | 900/6500 [5:55:34<29:25:58, 18.92s/it]                                                        14%|█▍        | 900/6500 [5:55:34<29:25:58, 18.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7817606925964355, 'eval_runtime': 5.3381, 'eval_samples_per_second': 4.309, 'eval_steps_per_second': 1.124, 'epoch': 0.14}
                                                        14%|█▍        | 900/6500 [5:55:39<29:25:58, 18.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-900
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-900/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-900
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-900/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-900/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1068, 'learning_rate': 9.535108107878612e-05, 'epoch': 0.14}
{'loss': 0.5987, 'learning_rate': 9.534089668872274e-05, 'epoch': 0.14}
{'loss': 0.587, 'learning_rate': 9.533070170050042e-05, 'epoch': 0.14}
{'loss': 0.5722, 'learning_rate': 9.53204961165022e-05, 'epoch': 0.14}
{'loss': 0.5714, 'learning_rate': 9.531027993911356e-05, 'epoch': 0.14}
{'loss': 0.6091, 'learning_rate': 9.53000531707225e-05, 'epoch': 0.14}
 14%|█▍        | 901/6500 [5:56:45<53:53:41, 34.65s/it]                                                        14%|█▍        | 901/6500 [5:56:45<53:53:41, 34.65s/it] 14%|█▍        | 902/6500 [5:57:03<46:06:28, 29.65s/it]                                                        14%|█▍        | 902/6500 [5:57:03<46:06:28, 29.65s/it] 14%|█▍        | 903/6500 [5:57:21<40:39:34, 26.15s/it]                                                        14%|█▍        | 903/6500 [5:57:21<40:39:34, 26.15s/it] 14%|█▍        | 904/6500 [5:57:39<36:50:31, 23.70s/it]                                                        14%|█▍        | 904/6500 [5:57:39<36:50:31, 23.70s/it] 14%|█▍        | 905/6500 [5:57:57<34:21:02, 22.10s/it]                                                        14%|█▍        | 905/6500 [5:57:57<34:21:02, 22.10s/it] 14%|█▍        | 906/6500 [5:58:15<32:26:46, 20.88s/it]                                                        14%|█▍        | {'loss': 0.5929, 'learning_rate': 9.528981581371942e-05, 'epoch': 0.14}
{'loss': 0.5701, 'learning_rate': 9.527956787049727e-05, 'epoch': 0.14}
{'loss': 0.5581, 'learning_rate': 9.526930934345142e-05, 'epoch': 0.14}
{'loss': 0.5688, 'learning_rate': 9.525904023497975e-05, 'epoch': 0.14}
906/6500 [5:58:15<32:26:46, 20.88s/it] 14%|█▍        | 907/6500 [5:58:34<31:08:33, 20.05s/it]                                                        14%|█▍        | 907/6500 [5:58:34<31:08:33, 20.05s/it] 14%|█▍        | 908/6500 [5:58:52<30:21:07, 19.54s/it]                                                        14%|█▍        | 908/6500 [5:58:52<30:21:07, 19.54s/it] 14%|█▍        | 909/6500 [5:59:10<29:40:48, 19.11s/it]                                                        14%|█▍        | 909/6500 [5:59:10<29:40:48, 19.11s/it] 14%|█▍        | 910/6500 [5:59:29<29:32:52, 19.03s/it]                                                        14%|█▍        | 910/6500 [5:59:29<29:32:52, 19.03s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7812359929084778, 'eval_runtime': 5.4099, 'eval_samples_per_second': 4.251, 'eval_steps_per_second': 1.109, 'epoch': 0.14}
                                                        14%|█▍        | 910/6500 [5:59:34<29:32:52, 19.03s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-910
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-910

the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-910
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-910

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-910
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-910/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-910/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5802, 'learning_rate': 9.524876054748262e-05, 'epoch': 0.14}
{'loss': 0.563, 'learning_rate': 9.523847028336283e-05, 'epoch': 0.14}
{'loss': 0.6153, 'learning_rate': 9.522816944502565e-05, 'epoch': 0.14}
{'loss': 0.5789, 'learning_rate': 9.521785803487889e-05, 'epoch': 0.14}
{'loss': 0.5935, 'learning_rate': 9.52075360553327e-05, 'epoch': 0.14}
{'loss': 0.614, 'learning_rate': 9.519720350879985e-05, 'epoch': 0.14}
 14%|█▍        | 911/6500 [6:00:33<50:34:15, 32.57s/it]                                                        14%|█▍        | 911/6500 [6:00:33<50:34:15, 32.57s/it] 14%|█▍        | 912/6500 [6:00:51<43:46:19, 28.20s/it]                                                        14%|█▍        | 912/6500 [6:00:51<43:46:19, 28.20s/it] 14%|█▍        | 913/6500 [6:01:09<39:00:41, 25.14s/it]                                                        14%|█▍        | 913/6500 [6:01:09<39:00:41, 25.14s/it] 14%|█▍        | 914/6500 [6:01:27<35:40:58, 23.00s/it]                                                        14%|█▍        | 914/6500 [6:01:27<35:40:58, 23.00s/it] 14%|█▍        | 915/6500 [6:01:45<33:22:04, 21.51s/it]                                                        14%|█▍        | 915/6500 [6:01:45<33:22:04, 21.51s/it] 14%|█▍        | 916/6500 [6:02:03<31:45:17, 20.47s/it]                                                        14%|█▍        | {'loss': 0.5912, 'learning_rate': 9.518686039769548e-05, 'epoch': 0.14}
{'loss': 0.5992, 'learning_rate': 9.517650672443722e-05, 'epoch': 0.14}
{'loss': 0.5946, 'learning_rate': 9.51661424914452e-05, 'epoch': 0.14}
{'loss': 0.5749, 'learning_rate': 9.515576770114199e-05, 'epoch': 0.14}
916/6500 [6:02:03<31:45:17, 20.47s/it] 14%|█▍        | 917/6500 [6:02:21<30:37:50, 19.75s/it]                                                        14%|█▍        | 917/6500 [6:02:21<30:37:50, 19.75s/it] 14%|█▍        | 918/6500 [6:02:39<29:51:45, 19.26s/it]                                                        14%|█▍        | 918/6500 [6:02:39<29:51:45, 19.26s/it] 14%|█▍        | 919/6500 [6:02:57<29:20:00, 18.92s/it]                                                        14%|█▍        | 919/6500 [6:02:57<29:20:00, 18.92s/it] 14%|█▍        | 920/6500 [6:03:16<28:58:09, 18.69s/it]                                                        14%|█▍        | 920/6500 [6:03:16<28:58:09, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7731558084487915, 'eval_runtime': 5.3493, 'eval_samples_per_second': 4.3, 'eval_steps_per_second': 1.122, 'epoch': 0.14}
                                                        14%|█▍        | 920/6500 [6:03:21<28:58:09, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-920
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-920
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-920
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-920
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-920/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-920/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5893, 'learning_rate': 9.514538235595262e-05, 'epoch': 0.14}
{'loss': 0.6186, 'learning_rate': 9.513498645830462e-05, 'epoch': 0.14}
{'loss': 0.5729, 'learning_rate': 9.512458001062797e-05, 'epoch': 0.14}
{'loss': 0.5784, 'learning_rate': 9.511416301535508e-05, 'epoch': 0.14}
{'loss': 0.5605, 'learning_rate': 9.51037354749209e-05, 'epoch': 0.14}
{'loss': 0.6128, 'learning_rate': 9.509329739176278e-05, 'epoch': 0.14}
 14%|█▍        | 921/6500 [6:04:48<63:26:18, 40.94s/it]                                                        14%|█▍        | 921/6500 [6:04:48<63:26:18, 40.94s/it] 14%|█▍        | 922/6500 [6:05:07<52:59:09, 34.20s/it]                                                        14%|█▍        | 922/6500 [6:05:07<52:59:09, 34.20s/it] 14%|█▍        | 923/6500 [6:05:25<45:30:40, 29.38s/it]                                                        14%|█▍        | 923/6500 [6:05:25<45:30:40, 29.38s/it] 14%|█▍        | 924/6500 [6:05:43<40:13:53, 25.97s/it]                                                        14%|█▍        | 924/6500 [6:05:43<40:13:53, 25.97s/it] 14%|█▍        | 925/6500 [6:06:01<36:36:01, 23.63s/it]                                                        14%|█▍        | 925/6500 [6:06:01<36:36:01, 23.63s/it] 14%|█▍        | 926/6500 [6:06:19<34:00:58, 21.97s/it]                                                        14%|█▍        | {'loss': 0.6441, 'learning_rate': 9.508284876832058e-05, 'epoch': 0.14}
{'loss': 0.5774, 'learning_rate': 9.507238960703659e-05, 'epoch': 0.14}
{'loss': 0.5709, 'learning_rate': 9.506191991035556e-05, 'epoch': 0.14}
{'loss': 0.5799, 'learning_rate': 9.505143968072474e-05, 'epoch': 0.14}
926/6500 [6:06:19<34:00:58, 21.97s/it] 14%|█▍        | 927/6500 [6:06:38<32:20:30, 20.89s/it]                                                        14%|█▍        | 927/6500 [6:06:38<32:20:30, 20.89s/it] 14%|█▍        | 928/6500 [6:06:56<31:03:32, 20.07s/it]                                                        14%|█▍        | 928/6500 [6:06:56<31:03:32, 20.07s/it] 14%|█▍        | 929/6500 [6:07:14<30:09:19, 19.49s/it]                                                        14%|█▍        | 929/6500 [6:07:14<30:09:19, 19.49s/it] 14%|█▍        | 930/6500 [6:07:32<29:31:32, 19.08s/it]                                                        14%|█▍        | 930/6500 [6:07:32<29:31:32, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7793897390365601, 'eval_runtime': 5.6458, 'eval_samples_per_second': 4.074, 'eval_steps_per_second': 1.063, 'epoch': 0.14}
                                                        14%|█▍        | 930/6500 [6:07:38<29:31:32, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-930
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-930the checkpoint model will be saved in 
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-930
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-930

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-930
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-930/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0995, 'learning_rate': 9.50409489205938e-05, 'epoch': 0.14}
{'loss': 0.5949, 'learning_rate': 9.50304476324149e-05, 'epoch': 0.14}
{'loss': 0.575, 'learning_rate': 9.501993581864268e-05, 'epoch': 0.14}
{'loss': 0.543, 'learning_rate': 9.500941348173417e-05, 'epoch': 0.14}
{'loss': 0.5753, 'learning_rate': 9.499888062414893e-05, 'epoch': 0.14}
{'loss': 0.6307, 'learning_rate': 9.498833724834895e-05, 'epoch': 0.14}
 14%|█▍        | 931/6500 [6:08:47<55:25:32, 35.83s/it]                                                        14%|█▍        | 931/6500 [6:08:47<55:25:32, 35.83s/it] 14%|█▍        | 932/6500 [6:09:05<47:07:36, 30.47s/it]                                                        14%|█▍        | 932/6500 [6:09:05<47:07:36, 30.47s/it] 14%|█▍        | 933/6500 [6:09:23<41:19:00, 26.72s/it]                                                        14%|█▍        | 933/6500 [6:09:23<41:19:00, 26.72s/it] 14%|█▍        | 934/6500 [6:09:41<37:15:03, 24.09s/it]                                                        14%|█▍        | 934/6500 [6:09:41<37:15:03, 24.09s/it] 14%|█▍        | 935/6500 [6:09:59<34:25:37, 22.27s/it]                                                        14%|█▍        | 935/6500 [6:09:59<34:25:37, 22.27s/it] 14%|█▍        | 936/6500 [6:10:17<32:27:38, 21.00s/it]                                                        14%|█▍        | {'loss': 0.5587, 'learning_rate': 9.497778335679865e-05, 'epoch': 0.14}
{'loss': 0.5727, 'learning_rate': 9.496721895196497e-05, 'epoch': 0.14}
{'loss': 0.5556, 'learning_rate': 9.495664403631727e-05, 'epoch': 0.14}
{'loss': 0.5771, 'learning_rate': 9.494605861232736e-05, 'epoch': 0.14}
936/6500 [6:10:17<32:27:38, 21.00s/it] 14%|█▍        | 937/6500 [6:10:35<31:05:34, 20.12s/it]                                                        14%|█▍        | 937/6500 [6:10:35<31:05:34, 20.12s/it] 14%|█▍        | 938/6500 [6:10:54<30:19:55, 19.63s/it]                                                        14%|█▍        | 938/6500 [6:10:54<30:19:55, 19.63s/it] 14%|█▍        | 939/6500 [6:11:12<29:40:28, 19.21s/it]                                                        14%|█▍        | 939/6500 [6:11:12<29:40:28, 19.21s/it] 14%|█▍        | 940/6500 [6:11:30<29:09:26, 18.88s/it]                                                        14%|█▍        | 940/6500 [6:11:30<29:09:26, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7760236263275146, 'eval_runtime': 5.3175, 'eval_samples_per_second': 4.325, 'eval_steps_per_second': 1.128, 'epoch': 0.14}
                                                        14%|█▍        | 940/6500 [6:11:35<29:09:26, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-940
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-940
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-940the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-940
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-940/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-940/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5708, 'learning_rate': 9.493546268246954e-05, 'epoch': 0.14}
{'loss': 0.5802, 'learning_rate': 9.492485624922051e-05, 'epoch': 0.14}
{'loss': 0.6021, 'learning_rate': 9.491423931505949e-05, 'epoch': 0.15}
{'loss': 0.5625, 'learning_rate': 9.49036118824681e-05, 'epoch': 0.15}
{'loss': 0.6174, 'learning_rate': 9.489297395393047e-05, 'epoch': 0.15}
{'loss': 0.5915, 'learning_rate': 9.488232553193312e-05, 'epoch': 0.15}
 14%|█▍        | 941/6500 [6:12:23<45:00:27, 29.15s/it]                                                        14%|█▍        | 941/6500 [6:12:23<45:00:27, 29.15s/it] 14%|█▍        | 942/6500 [6:12:41<39:51:15, 25.81s/it]                                                        14%|█▍        | 942/6500 [6:12:41<39:51:15, 25.81s/it] 15%|█▍        | 943/6500 [6:12:59<36:14:00, 23.47s/it]                                                        15%|█▍        | 943/6500 [6:12:59<36:14:00, 23.47s/it] 15%|█▍        | 944/6500 [6:13:17<33:42:06, 21.84s/it]                                                        15%|█▍        | 944/6500 [6:13:17<33:42:06, 21.84s/it] 15%|█▍        | 945/6500 [6:13:35<31:55:50, 20.69s/it]                                                        15%|█▍        | 945/6500 [6:13:35<31:55:50, 20.69s/it] 15%|█▍        | 946/6500 [6:13:53<30:41:40, 19.90s/it]                                                        15%|█▍        | {'loss': 0.5697, 'learning_rate': 9.487166661896507e-05, 'epoch': 0.15}
{'loss': 0.6029, 'learning_rate': 9.486099721751777e-05, 'epoch': 0.15}
{'loss': 0.5733, 'learning_rate': 9.485031733008514e-05, 'epoch': 0.15}
{'loss': 0.5823, 'learning_rate': 9.48396269591635e-05, 'epoch': 0.15}
946/6500 [6:13:53<30:41:40, 19.90s/it] 15%|█▍        | 947/6500 [6:14:11<29:50:36, 19.35s/it]                                                        15%|█▍        | 947/6500 [6:14:11<29:50:36, 19.35s/it] 15%|█▍        | 948/6500 [6:14:29<29:15:07, 18.97s/it]                                                        15%|█▍        | 948/6500 [6:14:29<29:15:07, 18.97s/it] 15%|█▍        | 949/6500 [6:14:47<28:51:20, 18.71s/it]                                                        15%|█▍        | 949/6500 [6:14:47<28:51:20, 18.71s/it] 15%|█▍        | 950/6500 [6:15:05<28:33:55, 18.53s/it]                                                        15%|█▍        | 950/6500 [6:15:05<28:33:55, 18.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7694181203842163, 'eval_runtime': 5.3262, 'eval_samples_per_second': 4.318, 'eval_steps_per_second': 1.127, 'epoch': 0.15}
                                                        15%|█▍        | 950/6500 [6:15:11<28:33:55, 18.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-950
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-950

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-950
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-950

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-950
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-950/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5768, 'learning_rate': 9.482892610725171e-05, 'epoch': 0.15}
{'loss': 0.6036, 'learning_rate': 9.481821477685101e-05, 'epoch': 0.15}
{'loss': 0.5736, 'learning_rate': 9.48074929704651e-05, 'epoch': 0.15}
{'loss': 0.5771, 'learning_rate': 9.479676069060016e-05, 'epoch': 0.15}
{'loss': 0.5627, 'learning_rate': 9.478601793976474e-05, 'epoch': 0.15}
{'loss': 0.6627, 'learning_rate': 9.477526472046995e-05, 'epoch': 0.15}
 15%|█▍        | 951/6500 [6:16:19<53:47:24, 34.90s/it]                                                        15%|█▍        | 951/6500 [6:16:19<53:47:24, 34.90s/it] 15%|█▍        | 952/6500 [6:16:37<45:57:09, 29.82s/it]                                                        15%|█▍        | 952/6500 [6:16:37<45:57:09, 29.82s/it] 15%|█▍        | 953/6500 [6:16:55<40:28:43, 26.27s/it]                                                        15%|█▍        | 953/6500 [6:16:55<40:28:43, 26.27s/it] 15%|█▍        | 954/6500 [6:17:13<36:50:14, 23.91s/it]                                                        15%|█▍        | 954/6500 [6:17:13<36:50:14, 23.91s/it] 15%|█▍        | 955/6500 [6:17:31<34:06:48, 22.15s/it]                                                        15%|█▍        | 955/6500 [6:17:31<34:06:48, 22.15s/it] 15%|█▍        | 956/6500 [6:17:49<32:12:35, 20.92s/it]                                                        15%|█▍        | {'loss': 0.5702, 'learning_rate': 9.476450103522927e-05, 'epoch': 0.15}
{'loss': 0.56, 'learning_rate': 9.475372688655864e-05, 'epoch': 0.15}
{'loss': 0.5775, 'learning_rate': 9.474294227697647e-05, 'epoch': 0.15}
{'loss': 0.8512, 'learning_rate': 9.473214720900356e-05, 'epoch': 0.15}
956/6500 [6:17:49<32:12:35, 20.92s/it] 15%|█▍        | 957/6500 [6:18:07<30:53:12, 20.06s/it]                                                        15%|█▍        | 957/6500 [6:18:07<30:53:12, 20.06s/it] 15%|█▍        | 958/6500 [6:18:25<29:58:28, 19.47s/it]                                                        15%|█▍        | 958/6500 [6:18:25<29:58:28, 19.47s/it] 15%|█▍        | 959/6500 [6:18:43<29:20:18, 19.06s/it]                                                        15%|█▍        | 959/6500 [6:18:43<29:20:18, 19.06s/it] 15%|█▍        | 960/6500 [6:19:01<28:53:09, 18.77s/it]                                                        15%|█▍        | 960/6500 [6:19:01<28:53:09, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7779302000999451, 'eval_runtime': 5.3472, 'eval_samples_per_second': 4.301, 'eval_steps_per_second': 1.122, 'epoch': 0.15}
                                                        15%|█▍        | 960/6500 [6:19:07<28:53:09, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-960
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-960
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-960
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-960

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-960
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-960/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-960/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8251, 'learning_rate': 9.472134168516324e-05, 'epoch': 0.15}
{'loss': 0.59, 'learning_rate': 9.47105257079812e-05, 'epoch': 0.15}
{'loss': 0.5645, 'learning_rate': 9.469969927998564e-05, 'epoch': 0.15}
{'loss': 0.5376, 'learning_rate': 9.468886240370712e-05, 'epoch': 0.15}
{'loss': 0.5846, 'learning_rate': 9.467801508167875e-05, 'epoch': 0.15}
{'loss': 0.5853, 'learning_rate': 9.466715731643598e-05, 'epoch': 0.15}
 15%|█▍        | 961/6500 [6:19:57<46:04:50, 29.95s/it]                                                        15%|█▍        | 961/6500 [6:19:57<46:04:50, 29.95s/it] 15%|█▍        | 962/6500 [6:20:15<40:34:48, 26.38s/it]                                                        15%|█▍        | 962/6500 [6:20:15<40:34:48, 26.38s/it] 15%|█▍        | 963/6500 [6:20:34<36:47:58, 23.93s/it]                                                        15%|█▍        | 963/6500 [6:20:34<36:47:58, 23.93s/it] 15%|█▍        | 964/6500 [6:20:52<34:04:15, 22.16s/it]                                                        15%|█▍        | 964/6500 [6:20:52<34:04:15, 22.16s/it] 15%|█▍        | 965/6500 [6:21:10<32:09:25, 20.92s/it]                                                        15%|█▍        | 965/6500 [6:21:10<32:09:25, 20.92s/it] 15%|█▍        | 966/6500 [6:21:28<30:49:43, 20.05s/it]                                                        15%|█▍        | {'loss': 0.5257, 'learning_rate': 9.465628911051679e-05, 'epoch': 0.15}
{'loss': 0.5763, 'learning_rate': 9.464541046646152e-05, 'epoch': 0.15}
{'loss': 0.5367, 'learning_rate': 9.463452138681301e-05, 'epoch': 0.15}
{'loss': 0.5771, 'learning_rate': 9.462362187411651e-05, 'epoch': 0.15}
966/6500 [6:21:28<30:49:43, 20.05s/it] 15%|█▍        | 967/6500 [6:21:46<29:54:23, 19.46s/it]                                                        15%|█▍        | 967/6500 [6:21:46<29:54:23, 19.46s/it] 15%|█▍        | 968/6500 [6:22:04<29:16:13, 19.05s/it]                                                        15%|█▍        | 968/6500 [6:22:04<29:16:13, 19.05s/it] 15%|█▍        | 969/6500 [6:22:22<28:50:26, 18.77s/it]                                                        15%|█▍        | 969/6500 [6:22:22<28:50:26, 18.77s/it] 15%|█▍        | 970/6500 [6:22:41<28:42:27, 18.69s/it]                                                        15%|█▍        | 970/6500 [6:22:41<28:42:27, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7784439325332642, 'eval_runtime': 5.6347, 'eval_samples_per_second': 4.082, 'eval_steps_per_second': 1.065, 'epoch': 0.15}
                                                        15%|█▍        | 970/6500 [6:22:46<28:42:27, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-970
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-970 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-970
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-970

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-970
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-970/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-970/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5561, 'learning_rate': 9.46127119309197e-05, 'epoch': 0.15}
{'loss': 0.5823, 'learning_rate': 9.460179155977274e-05, 'epoch': 0.15}
{'loss': 0.5627, 'learning_rate': 9.459086076322818e-05, 'epoch': 0.15}
{'loss': 0.5674, 'learning_rate': 9.457991954384105e-05, 'epoch': 0.15}
{'loss': 0.62, 'learning_rate': 9.456896790416875e-05, 'epoch': 0.15}
{'loss': 0.5997, 'learning_rate': 9.455800584677119e-05, 'epoch': 0.15}
 15%|█▍        | 971/6500 [6:24:13<62:49:10, 40.90s/it]                                                        15%|█▍        | 971/6500 [6:24:13<62:49:10, 40.90s/it] 15%|█▍        | 972/6500 [6:24:31<52:14:29, 34.02s/it]                                                        15%|█▍        | 972/6500 [6:24:31<52:14:29, 34.02s/it] 15%|█▍        | 973/6500 [6:24:49<44:50:41, 29.21s/it]                                                        15%|█▍        | 973/6500 [6:24:49<44:50:41, 29.21s/it] 15%|█▍        | 974/6500 [6:25:07<39:41:23, 25.86s/it]                                                        15%|█▍        | 974/6500 [6:25:07<39:41:23, 25.86s/it] 15%|█▌        | 975/6500 [6:25:25<36:04:57, 23.51s/it]                                                        15%|█▌        | 975/6500 [6:25:25<36:04:57, 23.51s/it] 15%|█▌        | 976/6500 [6:25:43<33:33:57, 21.88s/it]                                                        15%|█▌        | {'loss': 0.5809, 'learning_rate': 9.454703337421069e-05, 'epoch': 0.15}
{'loss': 0.5912, 'learning_rate': 9.453605048905199e-05, 'epoch': 0.15}
{'loss': 0.5533, 'learning_rate': 9.452505719386227e-05, 'epoch': 0.15}
{'loss': 0.5628, 'learning_rate': 9.451405349121115e-05, 'epoch': 0.15}
976/6500 [6:25:43<33:33:57, 21.88s/it] 15%|█▌        | 977/6500 [6:26:01<31:48:46, 20.74s/it]                                                        15%|█▌        | 977/6500 [6:26:01<31:48:46, 20.74s/it] 15%|█▌        | 978/6500 [6:26:20<30:36:00, 19.95s/it]                                                        15%|█▌        | 978/6500 [6:26:20<30:36:00, 19.95s/it] 15%|█▌        | 979/6500 [6:26:38<29:45:24, 19.40s/it]                                                        15%|█▌        | 979/6500 [6:26:38<29:45:24, 19.40s/it] 15%|█▌        | 980/6500 [6:26:56<29:09:38, 19.02s/it]                                                        15%|█▌        | 980/6500 [6:26:56<29:09:38, 19.02s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7678369283676147, 'eval_runtime': 5.3217, 'eval_samples_per_second': 4.322, 'eval_steps_per_second': 1.127, 'epoch': 0.15}
                                                        15%|█▌        | 980/6500 [6:27:01<29:09:38, 19.02s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-980
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-980
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-980
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-980

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-980
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-980/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-980/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6032, 'learning_rate': 9.450303938367067e-05, 'epoch': 0.15}
{'loss': 0.5694, 'learning_rate': 9.449201487381532e-05, 'epoch': 0.15}
{'loss': 0.5574, 'learning_rate': 9.448097996422201e-05, 'epoch': 0.15}
{'loss': 0.5575, 'learning_rate': 9.446993465747006e-05, 'epoch': 0.15}
{'loss': 0.5931, 'learning_rate': 9.44588789561413e-05, 'epoch': 0.15}
{'loss': 0.6276, 'learning_rate': 9.44478128628199e-05, 'epoch': 0.15}
 15%|█▌        | 981/6500 [6:28:45<70:42:32, 46.12s/it]                                                        15%|█▌        | 981/6500 [6:28:45<70:42:32, 46.12s/it] 15%|█▌        | 982/6500 [6:29:03<57:45:41, 37.68s/it]                                                        15%|█▌        | 982/6500 [6:29:03<57:45:41, 37.68s/it] 15%|█▌        | 983/6500 [6:29:21<48:42:10, 31.78s/it]                                                        15%|█▌        | 983/6500 [6:29:21<48:42:10, 31.78s/it] 15%|█▌        | 984/6500 [6:29:39<42:21:33, 27.65s/it]                                                        15%|█▌        | 984/6500 [6:29:39<42:21:33, 27.65s/it] 15%|█▌        | 985/6500 [6:29:57<37:56:29, 24.77s/it]                                                        15%|█▌        | 985/6500 [6:29:57<37:56:29, 24.77s/it] 15%|█▌        | 986/6500 [6:30:15<34:56:20, 22.81s/it]                                                        15%|█▌        | {'loss': 0.5609, 'learning_rate': 9.443673638009247e-05, 'epoch': 0.15}
{'loss': 0.5572, 'learning_rate': 9.442564951054809e-05, 'epoch': 0.15}
{'loss': 0.5647, 'learning_rate': 9.441455225677827e-05, 'epoch': 0.15}
{'loss': 1.0759, 'learning_rate': 9.440344462137689e-05, 'epoch': 0.15}
986/6500 [6:30:15<34:56:20, 22.81s/it] 15%|█▌        | 987/6500 [6:30:34<32:46:34, 21.40s/it]                                                        15%|█▌        | 987/6500 [6:30:34<32:46:34, 21.40s/it] 15%|█▌        | 988/6500 [6:30:52<31:21:18, 20.48s/it]                                                        15%|█▌        | 988/6500 [6:30:52<31:21:18, 20.48s/it] 15%|█▌        | 989/6500 [6:31:10<30:17:04, 19.78s/it]                                                        15%|█▌        | 989/6500 [6:31:10<30:17:04, 19.78s/it] 15%|█▌        | 990/6500 [6:31:28<29:31:41, 19.29s/it]                                                        15%|█▌        | 990/6500 [6:31:28<29:31:41, 19.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.779030442237854, 'eval_runtime': 5.5894, 'eval_samples_per_second': 4.115, 'eval_steps_per_second': 1.073, 'epoch': 0.15}
                                                        15%|█▌        | 990/6500 [6:31:34<29:31:41, 19.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-990
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-990
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-990
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-990

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-990
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-990/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-990/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5845, 'learning_rate': 9.43923266069403e-05, 'epoch': 0.15}
{'loss': 0.5593, 'learning_rate': 9.438119821606727e-05, 'epoch': 0.15}
{'loss': 0.5546, 'learning_rate': 9.437005945135903e-05, 'epoch': 0.15}
{'loss': 0.537, 'learning_rate': 9.435891031541915e-05, 'epoch': 0.15}
{'loss': 0.5914, 'learning_rate': 9.434775081085368e-05, 'epoch': 0.15}
{'loss': 0.5571, 'learning_rate': 9.433658094027111e-05, 'epoch': 0.15}
 15%|█▌        | 991/6500 [6:33:11<68:04:02, 44.48s/it]                                                        15%|█▌        | 991/6500 [6:33:11<68:04:02, 44.48s/it] 15%|█▌        | 992/6500 [6:33:29<55:53:53, 36.53s/it]                                                        15%|█▌        | 992/6500 [6:33:29<55:53:53, 36.53s/it] 15%|█▌        | 993/6500 [6:33:48<47:25:14, 31.00s/it]                                                        15%|█▌        | 993/6500 [6:33:48<47:25:14, 31.00s/it] 15%|█▌        | 994/6500 [6:34:06<41:32:57, 27.17s/it]                                                        15%|█▌        | 994/6500 [6:34:06<41:32:57, 27.17s/it] 15%|█▌        | 995/6500 [6:34:24<37:21:31, 24.43s/it]                                                        15%|█▌        | 995/6500 [6:34:24<37:21:31, 24.43s/it] 15%|█▌        | 996/6500 [6:34:42<34:33:40, 22.61s/it]                                                        15%|█▌        | {'loss': 0.5603, 'learning_rate': 9.432540070628231e-05, 'epoch': 0.15}
{'loss': 0.5297, 'learning_rate': 9.431421011150062e-05, 'epoch': 0.15}
{'loss': 0.5465, 'learning_rate': 9.430300915854172e-05, 'epoch': 0.15}
{'loss': 0.5646, 'learning_rate': 9.42917978500238e-05, 'epoch': 0.15}
996/6500 [6:34:42<34:33:40, 22.61s/it] 15%|█▌        | 997/6500 [6:35:00<32:27:41, 21.24s/it]                                                        15%|█▌        | 997/6500 [6:35:00<32:27:41, 21.24s/it] 15%|█▌        | 998/6500 [6:35:18<31:02:31, 20.31s/it]                                                        15%|█▌        | 998/6500 [6:35:18<31:02:31, 20.31s/it] 15%|█▌        | 999/6500 [6:35:36<30:01:07, 19.65s/it]                                                        15%|█▌        | 999/6500 [6:35:36<30:01:07, 19.65s/it] 15%|█▌        | 1000/6500 [6:35:55<29:18:40, 19.19s/it]                                                         15%|█▌        | 1000/6500 [6:35:55<29:18:40, 19.19s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7748785614967346, 'eval_runtime': 5.8407, 'eval_samples_per_second': 3.938, 'eval_steps_per_second': 1.027, 'epoch': 0.15}
                                                         15%|█▌        | 1000/6500 [6:36:00<29:18:40, 19.19s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1000
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1000
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1000
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1000

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1000
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1000/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5451, 'learning_rate': 9.428057618856745e-05, 'epoch': 0.15}
{'loss': 0.5922, 'learning_rate': 9.426934417679563e-05, 'epoch': 0.15}
{'loss': 0.5446, 'learning_rate': 9.425810181733377e-05, 'epoch': 0.15}
{'loss': 0.5934, 'learning_rate': 9.424684911280972e-05, 'epoch': 0.15}
{'loss': 0.5666, 'learning_rate': 9.423558606585369e-05, 'epoch': 0.15}
{'loss': 0.5503, 'learning_rate': 9.42243126790984e-05, 'epoch': 0.15}
 15%|█▌        | 1001/6500 [6:37:10<55:09:05, 36.11s/it]                                                         15%|█▌        | 1001/6500 [6:37:10<55:09:05, 36.11s/it] 15%|█▌        | 1002/6500 [6:37:29<47:01:09, 30.79s/it]                                                         15%|█▌        | 1002/6500 [6:37:29<47:01:09, 30.79s/it] 15%|█▌        | 1003/6500 [6:37:46<41:08:12, 26.94s/it]                                                         15%|█▌        | 1003/6500 [6:37:46<41:08:12, 26.94s/it] 15%|█▌        | 1004/6500 [6:38:05<37:07:57, 24.32s/it]                                                         15%|█▌        | 1004/6500 [6:38:05<37:07:57, 24.32s/it] 15%|█▌        | 1005/6500 [6:38:23<34:14:43, 22.44s/it]                                                         15%|█▌        | 1005/6500 [6:38:23<34:14:43, 22.44s/it] 15%|█▌        | 1006/6500 [6:38:42<32:35:18, 21.35s/it]                                                         15%{'loss': 0.5658, 'learning_rate': 9.42130289551789e-05, 'epoch': 0.15}
{'loss': 0.5637, 'learning_rate': 9.420173489673269e-05, 'epoch': 0.16}
{'loss': 0.5418, 'learning_rate': 9.419043050639973e-05, 'epoch': 0.16}
{'loss': 0.5612, 'learning_rate': 9.417911578682229e-05, 'epoch': 0.16}
|█▌        | 1006/6500 [6:38:42<32:35:18, 21.35s/it] 15%|█▌        | 1007/6500 [6:39:00<31:04:56, 20.37s/it]                                                         15%|█▌        | 1007/6500 [6:39:00<31:04:56, 20.37s/it] 16%|█▌        | 1008/6500 [6:39:18<30:01:42, 19.68s/it]                                                         16%|█▌        | 1008/6500 [6:39:18<30:01:42, 19.68s/it] 16%|█▌        | 1009/6500 [6:39:36<29:17:27, 19.20s/it]                                                         16%|█▌        | 1009/6500 [6:39:36<29:17:27, 19.20s/it] 16%|█▌        | 1010/6500 [6:39:54<28:47:19, 18.88s/it]                                                         16%|█▌        | 1010/6500 [6:39:54<28:47:19, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.768711268901825, 'eval_runtime': 5.3345, 'eval_samples_per_second': 4.312, 'eval_steps_per_second': 1.125, 'epoch': 0.16}
                                                         16%|█▌        | 1010/6500 [6:39:59<28:47:19, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1010
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1010
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1010
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1010

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1010/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1010/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1010/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5964, 'learning_rate': 9.416779074064517e-05, 'epoch': 0.16}
{'loss': 0.5444, 'learning_rate': 9.415645537051549e-05, 'epoch': 0.16}
{'loss': 0.5698, 'learning_rate': 9.414510967908286e-05, 'epoch': 0.16}
{'loss': 0.5331, 'learning_rate': 9.413375366899923e-05, 'epoch': 0.16}
{'loss': 0.5988, 'learning_rate': 9.412238734291903e-05, 'epoch': 0.16}
{'loss': 0.599, 'learning_rate': 9.411101070349905e-05, 'epoch': 0.16}
 16%|█▌        | 1011/6500 [6:41:50<73:12:12, 48.01s/it]                                                         16%|█▌        | 1011/6500 [6:41:50<73:12:12, 48.01s/it] 16%|█▌        | 1012/6500 [6:42:08<59:30:19, 39.03s/it]                                                         16%|█▌        | 1012/6500 [6:42:08<59:30:19, 39.03s/it] 16%|█▌        | 1013/6500 [6:42:26<49:51:03, 32.71s/it]                                                         16%|█▌        | 1013/6500 [6:42:26<49:51:03, 32.71s/it] 16%|█▌        | 1014/6500 [6:42:44<43:05:59, 28.28s/it]                                                         16%|█▌        | 1014/6500 [6:42:44<43:05:59, 28.28s/it] 16%|█▌        | 1015/6500 [6:43:02<38:23:28, 25.20s/it]                                                         16%|█▌        | 1015/6500 [6:43:02<38:23:28, 25.20s/it] 16%|█▌        | 1016/6500 [6:43:21<35:32:14, 23.33s/it]                                                         16%{'loss': 0.5538, 'learning_rate': 9.409962375339851e-05, 'epoch': 0.16}
{'loss': 0.5575, 'learning_rate': 9.408822649527906e-05, 'epoch': 0.16}
{'loss': 0.5531, 'learning_rate': 9.407681893180473e-05, 'epoch': 0.16}
{'loss': 1.0698, 'learning_rate': 9.406540106564196e-05, 'epoch': 0.16}
|█▌        | 1016/6500 [6:43:21<35:32:14, 23.33s/it] 16%|█▌        | 1017/6500 [6:43:39<33:07:15, 21.75s/it]                                                         16%|█▌        | 1017/6500 [6:43:39<33:07:15, 21.75s/it] 16%|█▌        | 1018/6500 [6:43:57<31:26:59, 20.65s/it]                                                         16%|█▌        | 1018/6500 [6:43:57<31:26:59, 20.65s/it] 16%|█▌        | 1019/6500 [6:44:16<30:37:15, 20.11s/it]                                                         16%|█▌        | 1019/6500 [6:44:16<30:37:15, 20.11s/it] 16%|█▌        | 1020/6500 [6:44:34<29:42:09, 19.51s/it]                                                         16%|█▌        | 1020/6500 [6:44:34<29:42:09, 19.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7795907855033875, 'eval_runtime': 6.557, 'eval_samples_per_second': 3.508, 'eval_steps_per_second': 0.915, 'epoch': 0.16}
                                                         16%|█▌        | 1020/6500 [6:44:41<29:42:09, 19.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1020
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1020
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1020
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1020

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1020
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1020/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1020/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5646, 'learning_rate': 9.405397289945963e-05, 'epoch': 0.16}
{'loss': 0.5677, 'learning_rate': 9.404253443592896e-05, 'epoch': 0.16}
{'loss': 0.5138, 'learning_rate': 9.403108567772367e-05, 'epoch': 0.16}
{'loss': 0.5682, 'learning_rate': 9.40196266275198e-05, 'epoch': 0.16}
{'loss': 0.5722, 'learning_rate': 9.400815728799586e-05, 'epoch': 0.16}
{'loss': 0.529, 'learning_rate': 9.399667766183274e-05, 'epoch': 0.16}
 16%|█▌        | 1021/6500 [6:45:45<53:15:45, 35.00s/it]                                                         16%|█▌        | 1021/6500 [6:45:45<53:15:45, 35.00s/it] 16%|█▌        | 1022/6500 [6:46:03<45:28:31, 29.89s/it]                                                         16%|█▌        | 1022/6500 [6:46:03<45:28:31, 29.89s/it] 16%|█▌        | 1023/6500 [6:46:21<40:02:12, 26.32s/it]                                                         16%|█▌        | 1023/6500 [6:46:21<40:02:12, 26.32s/it] 16%|█▌        | 1024/6500 [6:46:39<36:14:30, 23.83s/it]                                                         16%|█▌        | 1024/6500 [6:46:39<36:14:30, 23.83s/it] 16%|█▌        | 1025/6500 [6:46:57<33:35:34, 22.09s/it]                                                         16%|█▌        | 1025/6500 [6:46:57<33:35:34, 22.09s/it] 16%|█▌        | 1026/6500 [6:47:15<31:46:53, 20.90s/it]                                                         16%{'loss': 0.5456, 'learning_rate': 9.39851877517137e-05, 'epoch': 0.16}
{'loss': 0.5394, 'learning_rate': 9.397368756032445e-05, 'epoch': 0.16}
{'loss': 0.5397, 'learning_rate': 9.396217709035312e-05, 'epoch': 0.16}
{'loss': 0.5418, 'learning_rate': 9.395065634449018e-05, 'epoch': 0.16}
|█▌        | 1026/6500 [6:47:15<31:46:53, 20.90s/it] 16%|█▌        | 1027/6500 [6:47:33<30:29:33, 20.06s/it]                                                         16%|█▌        | 1027/6500 [6:47:33<30:29:33, 20.06s/it] 16%|█▌        | 1028/6500 [6:47:51<29:36:03, 19.47s/it]                                                         16%|█▌        | 1028/6500 [6:47:51<29:36:03, 19.47s/it] 16%|█▌        | 1029/6500 [6:48:10<28:58:56, 19.07s/it]                                                         16%|█▌        | 1029/6500 [6:48:10<28:58:56, 19.07s/it] 16%|█▌        | 1030/6500 [6:48:28<28:33:20, 18.79s/it]                                                         16%|█▌        | 1030/6500 [6:48:28<28:33:20, 18.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7741656303405762, 'eval_runtime': 5.3493, 'eval_samples_per_second': 4.3, 'eval_steps_per_second': 1.122, 'epoch': 0.16}
                                                         16%|█▌        | 1030/6500 [6:48:33<28:33:20, 18.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1030
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1030I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1030
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1030

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1030
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1030/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1030/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5508, 'learning_rate': 9.393912532542854e-05, 'epoch': 0.16}
{'loss': 0.5615, 'learning_rate': 9.392758403586352e-05, 'epoch': 0.16}
{'loss': 0.5281, 'learning_rate': 9.391603247849281e-05, 'epoch': 0.16}
{'loss': 0.6091, 'learning_rate': 9.390447065601651e-05, 'epoch': 0.16}
{'loss': 0.5528, 'learning_rate': 9.389289857113715e-05, 'epoch': 0.16}
{'loss': 0.5676, 'learning_rate': 9.388131622655962e-05, 'epoch': 0.16}
 16%|█▌        | 1031/6500 [6:50:46<82:48:20, 54.51s/it]                                                         16%|█▌        | 1031/6500 [6:50:46<82:48:20, 54.51s/it] 16%|█▌        | 1032/6500 [6:51:03<66:07:11, 43.53s/it]                                                         16%|█▌        | 1032/6500 [6:51:03<66:07:11, 43.53s/it] 16%|█▌        | 1033/6500 [6:51:21<54:27:21, 35.86s/it]                                                         16%|█▌        | 1033/6500 [6:51:21<54:27:21, 35.86s/it] 16%|█▌        | 1034/6500 [6:51:39<46:18:19, 30.50s/it]                                                         16%|█▌        | 1034/6500 [6:51:39<46:18:19, 30.50s/it] 16%|█▌        | 1035/6500 [6:51:58<40:53:16, 26.93s/it]                                                         16%|█▌        | 1035/6500 [6:51:58<40:53:16, 26.93s/it] 16%|█▌        | 1036/6500 [6:52:16<36:49:28, 24.26s/it]                                                         16%{'loss': 0.5771, 'learning_rate': 9.386972362499123e-05, 'epoch': 0.16}
{'loss': 0.5411, 'learning_rate': 9.385812076914167e-05, 'epoch': 0.16}
{'loss': 0.5581, 'learning_rate': 9.384650766172305e-05, 'epoch': 0.16}
{'loss': 0.5539, 'learning_rate': 9.383488430544984e-05, 'epoch': 0.16}
|█▌        | 1036/6500 [6:52:16<36:49:28, 24.26s/it] 16%|█▌        | 1037/6500 [6:52:34<33:59:43, 22.40s/it]                                                         16%|█▌        | 1037/6500 [6:52:34<33:59:43, 22.40s/it] 16%|█▌        | 1038/6500 [6:52:52<32:01:27, 21.11s/it]                                                         16%|█▌        | 1038/6500 [6:52:52<32:01:27, 21.11s/it] 16%|█▌        | 1039/6500 [6:53:10<30:39:00, 20.21s/it]                                                         16%|█▌        | 1039/6500 [6:53:10<30:39:00, 20.21s/it] 16%|█▌        | 1040/6500 [6:53:28<29:41:53, 19.58s/it]                                                         16%|█▌        | 1040/6500 [6:53:28<29:41:53, 19.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.767231822013855, 'eval_runtime': 5.3449, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.123, 'epoch': 0.16}
                                                         16%|█▌        | 1040/6500 [6:53:34<29:41:53, 19.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1040
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1040
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1040
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1040
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1040/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1040/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5628, 'learning_rate': 9.382325070303896e-05, 'epoch': 0.16}
{'loss': 0.5432, 'learning_rate': 9.381160685720967e-05, 'epoch': 0.16}
{'loss': 0.5479, 'learning_rate': 9.379995277068365e-05, 'epoch': 0.16}
{'loss': 0.5282, 'learning_rate': 9.378828844618499e-05, 'epoch': 0.16}
{'loss': 0.6386, 'learning_rate': 9.377661388644014e-05, 'epoch': 0.16}
{'loss': 0.5424, 'learning_rate': 9.376492909417795e-05, 'epoch': 0.16}
 16%|█▌        | 1041/6500 [6:54:39<52:43:24, 34.77s/it]                                                         16%|█▌        | 1041/6500 [6:54:39<52:43:24, 34.77s/it] 16%|█▌        | 1042/6500 [6:54:57<45:04:56, 29.74s/it]                                                         16%|█▌        | 1042/6500 [6:54:57<45:04:56, 29.74s/it] 16%|█▌        | 1043/6500 [6:55:15<39:43:46, 26.21s/it]                                                         16%|█▌        | 1043/6500 [6:55:15<39:43:46, 26.21s/it] 16%|█▌        | 1044/6500 [6:55:33<35:59:17, 23.75s/it]                                                         16%|█▌        | 1044/6500 [6:55:33<35:59:17, 23.75s/it] 16%|█▌        | 1045/6500 [6:55:51<33:22:50, 22.03s/it]                                                         16%|█▌        | 1045/6500 [6:55:51<33:22:50, 22.03s/it] 16%|█▌        | 1046/6500 [6:56:09<31:33:49, 20.83s/it]                                                         16%{'loss': 0.5448, 'learning_rate': 9.375323407212969e-05, 'epoch': 0.16}
{'loss': 0.5515, 'learning_rate': 9.374152882302898e-05, 'epoch': 0.16}
{'loss': 1.0741, 'learning_rate': 9.372981334961187e-05, 'epoch': 0.16}
{'loss': 0.5567, 'learning_rate': 9.371808765461677e-05, 'epoch': 0.16}
|█▌        | 1046/6500 [6:56:09<31:33:49, 20.83s/it] 16%|█▌        | 1047/6500 [6:56:27<30:18:41, 20.01s/it]                                                         16%|█▌        | 1047/6500 [6:56:27<30:18:41, 20.01s/it] 16%|█▌        | 1048/6500 [6:56:45<29:26:30, 19.44s/it]                                                         16%|█▌        | 1048/6500 [6:56:45<29:26:30, 19.44s/it] 16%|█▌        | 1049/6500 [6:57:03<28:49:28, 19.04s/it]                                                         16%|█▌        | 1049/6500 [6:57:03<28:49:28, 19.04s/it] 16%|█▌        | 1050/6500 [6:57:21<28:24:39, 18.77s/it]                                                         16%|█▌        | 1050/6500 [6:57:21<28:24:39, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.775328516960144, 'eval_runtime': 5.3426, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.16}
                                                         16%|█▌        | 1050/6500 [6:57:26<28:24:39, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1050
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1050
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1050
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1050/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5475, 'learning_rate': 9.370635174078448e-05, 'epoch': 0.16}
{'loss': 0.5375, 'learning_rate': 9.369460561085823e-05, 'epoch': 0.16}
{'loss': 0.5157, 'learning_rate': 9.368284926758357e-05, 'epoch': 0.16}
{'loss': 0.5751, 'learning_rate': 9.36710827137085e-05, 'epoch': 0.16}
{'loss': 0.5399, 'learning_rate': 9.365930595198336e-05, 'epoch': 0.16}
{'loss': 0.5186, 'learning_rate': 9.364751898516091e-05, 'epoch': 0.16}
 16%|█▌        | 1051/6500 [6:59:01<65:15:52, 43.12s/it]                                                         16%|█▌        | 1051/6500 [6:59:01<65:15:52, 43.12s/it] 16%|█▌        | 1052/6500 [6:59:19<53:49:36, 35.57s/it]                                                         16%|█▌        | 1052/6500 [6:59:19<53:49:36, 35.57s/it] 16%|█▌        | 1053/6500 [6:59:37<45:51:24, 30.31s/it]                                                         16%|█▌        | 1053/6500 [6:59:37<45:51:24, 30.31s/it] 16%|█▌        | 1054/6500 [6:59:55<40:14:45, 26.60s/it]                                                         16%|█▌        | 1054/6500 [6:59:55<40:14:45, 26.60s/it] 16%|█▌        | 1055/6500 [7:00:13<36:20:29, 24.03s/it]                                                         16%|█▌        | 1055/6500 [7:00:13<36:20:29, 24.03s/it] 16%|█▌        | 1056/6500 [7:00:31<33:37:09, 22.23s/it]                                                         16%{'loss': 0.5364, 'learning_rate': 9.363572181599628e-05, 'epoch': 0.16}
{'loss': 0.5259, 'learning_rate': 9.362391444724699e-05, 'epoch': 0.16}
{'loss': 0.5428, 'learning_rate': 9.361209688167292e-05, 'epoch': 0.16}
{'loss': 0.5196, 'learning_rate': 9.36002691220364e-05, 'epoch': 0.16}
|█▌        | 1056/6500 [7:00:31<33:37:09, 22.23s/it] 16%|█▋        | 1057/6500 [7:00:49<31:43:15, 20.98s/it]                                                         16%|█▋        | 1057/6500 [7:00:49<31:43:15, 20.98s/it] 16%|█▋        | 1058/6500 [7:01:07<30:23:53, 20.11s/it]                                                         16%|█▋        | 1058/6500 [7:01:07<30:23:53, 20.11s/it] 16%|█▋        | 1059/6500 [7:01:25<29:29:14, 19.51s/it]                                                         16%|█▋        | 1059/6500 [7:01:25<29:29:14, 19.51s/it] 16%|█▋        | 1060/6500 [7:01:43<28:50:48, 19.09s/it]                                                         16%|█▋        | 1060/6500 [7:01:43<28:50:48, 19.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7708403468132019, 'eval_runtime': 5.3438, 'eval_samples_per_second': 4.304, 'eval_steps_per_second': 1.123, 'epoch': 0.16}
                                                         16%|█▋        | 1060/6500 [7:01:49<28:50:48, 19.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1060
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1060
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1060
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1060

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1060
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1060/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5622, 'learning_rate': 9.358843117110204e-05, 'epoch': 0.16}
{'loss': 0.5248, 'learning_rate': 9.357658303163693e-05, 'epoch': 0.16}
{'loss': 0.5554, 'learning_rate': 9.356472470641047e-05, 'epoch': 0.16}
{'loss': 0.5741, 'learning_rate': 9.35528561981945e-05, 'epoch': 0.16}
{'loss': 0.5543, 'learning_rate': 9.354097750976319e-05, 'epoch': 0.16}
{'loss': 0.5495, 'learning_rate': 9.352908864389312e-05, 'epoch': 0.16}
 16%|█▋        | 1061/6500 [7:02:49<50:08:30, 33.19s/it]                                                         16%|█▋        | 1061/6500 [7:02:49<50:08:30, 33.19s/it] 16%|█▋        | 1062/6500 [7:03:07<43:14:54, 28.63s/it]                                                         16%|█▋        | 1062/6500 [7:03:07<43:14:54, 28.63s/it] 16%|█▋        | 1063/6500 [7:03:26<38:25:42, 25.44s/it]                                                         16%|█▋        | 1063/6500 [7:03:26<38:25:42, 25.44s/it] 16%|█▋        | 1064/6500 [7:03:44<35:05:48, 23.24s/it]                                                         16%|█▋        | 1064/6500 [7:03:44<35:05:48, 23.24s/it] 16%|█▋        | 1065/6500 [7:04:02<32:43:47, 21.68s/it]                                                         16%|█▋        | 1065/6500 [7:04:02<32:43:47, 21.68s/it] 16%|█▋        | 1066/6500 [7:04:20<31:04:58, 20.59s/it]                                                         16%{'loss': 0.5472, 'learning_rate': 9.351718960336325e-05, 'epoch': 0.16}
{'loss': 0.5395, 'learning_rate': 9.35052803909549e-05, 'epoch': 0.16}
{'loss': 0.5216, 'learning_rate': 9.349336100945176e-05, 'epoch': 0.16}
{'loss': 0.5834, 'learning_rate': 9.348143146163994e-05, 'epoch': 0.16}
|█▋        | 1066/6500 [7:04:20<31:04:58, 20.59s/it] 16%|█▋        | 1067/6500 [7:04:38<30:04:59, 19.93s/it]                                                         16%|█▋        | 1067/6500 [7:04:38<30:04:59, 19.93s/it] 16%|█▋        | 1068/6500 [7:04:56<29:17:04, 19.41s/it]                                                         16%|█▋        | 1068/6500 [7:04:56<29:17:04, 19.41s/it] 16%|█▋        | 1069/6500 [7:05:14<28:41:45, 19.02s/it]                                                         16%|█▋        | 1069/6500 [7:05:14<28:41:45, 19.02s/it] 16%|█▋        | 1070/6500 [7:05:33<28:17:05, 18.75s/it]                                                         16%|█▋        | 1070/6500 [7:05:33<28:17:05, 18.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7666715383529663, 'eval_runtime': 5.4759, 'eval_samples_per_second': 4.2, 'eval_steps_per_second': 1.096, 'epoch': 0.16}
                                                         16%|█▋        | 1070/6500 [7:05:38<28:17:05, 18.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1070
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1070
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1070
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1070

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1070
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1070/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1070/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5311, 'learning_rate': 9.346949175030791e-05, 'epoch': 0.16}
{'loss': 0.5455, 'learning_rate': 9.345754187824644e-05, 'epoch': 0.16}
{'loss': 0.5278, 'learning_rate': 9.34455818482488e-05, 'epoch': 0.17}
{'loss': 0.5512, 'learning_rate': 9.343361166311057e-05, 'epoch': 0.17}
{'loss': 0.6001, 'learning_rate': 9.342163132562967e-05, 'epoch': 0.17}
{'loss': 0.5378, 'learning_rate': 9.340964083860648e-05, 'epoch': 0.17}
 16%|█▋        | 1071/6500 [7:06:48<53:57:21, 35.78s/it]                                                         16%|█▋        | 1071/6500 [7:06:48<53:57:21, 35.78s/it] 16%|█▋        | 1072/6500 [7:07:06<45:52:27, 30.43s/it]                                                         16%|█▋        | 1072/6500 [7:07:06<45:52:27, 30.43s/it] 17%|█▋        | 1073/6500 [7:07:24<40:13:49, 26.69s/it]                                                         17%|█▋        | 1073/6500 [7:07:24<40:13:49, 26.69s/it] 17%|█▋        | 1074/6500 [7:07:42<36:17:27, 24.08s/it]                                                         17%|█▋        | 1074/6500 [7:07:42<36:17:27, 24.08s/it] 17%|█▋        | 1075/6500 [7:08:00<33:32:49, 22.26s/it]                                                         17%|█▋        | 1075/6500 [7:08:00<33:32:49, 22.26s/it] 17%|█▋        | 1076/6500 [7:08:18<31:37:55, 20.99s/it]                                                         17%{'loss': 0.5219, 'learning_rate': 9.339764020484366e-05, 'epoch': 0.17}
{'loss': 0.5421, 'learning_rate': 9.338562942714631e-05, 'epoch': 0.17}
{'loss': 1.0575, 'learning_rate': 9.337360850832187e-05, 'epoch': 0.17}
{'loss': 0.5542, 'learning_rate': 9.336157745118016e-05, 'epoch': 0.17}
|█▋        | 1076/6500 [7:08:18<31:37:55, 20.99s/it] 17%|█▋        | 1077/6500 [7:08:36<30:18:35, 20.12s/it]                                                         17%|█▋        | 1077/6500 [7:08:36<30:18:35, 20.12s/it] 17%|█▋        | 1078/6500 [7:08:54<29:23:12, 19.51s/it]                                                         17%|█▋        | 1078/6500 [7:08:54<29:23:12, 19.51s/it] 17%|█▋        | 1079/6500 [7:09:12<28:44:08, 19.08s/it]                                                         17%|█▋        | 1079/6500 [7:09:12<28:44:08, 19.08s/it] 17%|█▋        | 1080/6500 [7:09:30<28:18:04, 18.80s/it]                                                         17%|█▋        | 1080/6500 [7:09:30<28:18:04, 18.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7714324593544006, 'eval_runtime': 5.3299, 'eval_samples_per_second': 4.315, 'eval_steps_per_second': 1.126, 'epoch': 0.17}
                                                         17%|█▋        | 1080/6500 [7:09:36<28:18:04, 18.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1080
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1080the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1080
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1080

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1080
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1080/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5294, 'learning_rate': 9.334953625853335e-05, 'epoch': 0.17}
{'loss': 0.5072, 'learning_rate': 9.333748493319603e-05, 'epoch': 0.17}
{'loss': 0.5318, 'learning_rate': 9.332542347798509e-05, 'epoch': 0.17}
{'loss': 0.5601, 'learning_rate': 9.331335189571984e-05, 'epoch': 0.17}
{'loss': 0.5357, 'learning_rate': 9.330127018922194e-05, 'epoch': 0.17}
{'loss': 0.5302, 'learning_rate': 9.32891783613154e-05, 'epoch': 0.17}
 17%|█▋        | 1081/6500 [7:10:29<46:08:53, 30.66s/it]                                                         17%|█▋        | 1081/6500 [7:10:29<46:08:53, 30.66s/it] 17%|█▋        | 1082/6500 [7:10:47<40:26:49, 26.88s/it]                                                         17%|█▋        | 1082/6500 [7:10:47<40:26:49, 26.88s/it] 17%|█▋        | 1083/6500 [7:11:05<36:43:16, 24.40s/it]                                                         17%|█▋        | 1083/6500 [7:11:05<36:43:16, 24.40s/it] 17%|█▋        | 1084/6500 [7:11:23<33:48:42, 22.47s/it]                                                         17%|█▋        | 1084/6500 [7:11:23<33:48:42, 22.47s/it] 17%|█▋        | 1085/6500 [7:11:41<31:48:17, 21.14s/it]                                                         17%|█▋        | 1085/6500 [7:11:41<31:48:17, 21.14s/it] 17%|█▋        | 1086/6500 [7:11:59<30:23:29, 20.21s/it]                                                         17%{'loss': 0.508, 'learning_rate': 9.327707641482662e-05, 'epoch': 0.17}
{'loss': 0.5285, 'learning_rate': 9.326496435258437e-05, 'epoch': 0.17}
{'loss': 0.5203, 'learning_rate': 9.325284217741974e-05, 'epoch': 0.17}
{'loss': 0.5155, 'learning_rate': 9.324070989216625e-05, 'epoch': 0.17}
|█▋        | 1086/6500 [7:11:59<30:23:29, 20.21s/it] 17%|█▋        | 1087/6500 [7:12:17<29:24:56, 19.56s/it]                                                         17%|█▋        | 1087/6500 [7:12:17<29:24:56, 19.56s/it] 17%|█▋        | 1088/6500 [7:12:36<28:44:49, 19.12s/it]                                                         17%|█▋        | 1088/6500 [7:12:36<28:44:49, 19.12s/it] 17%|█▋        | 1089/6500 [7:12:54<28:17:23, 18.82s/it]                                                         17%|█▋        | 1089/6500 [7:12:54<28:17:23, 18.82s/it] 17%|█▋        | 1090/6500 [7:13:12<27:57:42, 18.61s/it]                                                         17%|█▋        | 1090/6500 [7:13:12<27:57:42, 18.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7745704054832458, 'eval_runtime': 5.457, 'eval_samples_per_second': 4.215, 'eval_steps_per_second': 1.1, 'epoch': 0.17}
                                                         17%|█▋        | 1090/6500 [7:13:17<27:57:42, 18.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1090
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1090


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1090/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1090


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1090
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1090/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5654, 'learning_rate': 9.322856749965971e-05, 'epoch': 0.17}
{'loss': 0.5178, 'learning_rate': 9.321641500273836e-05, 'epoch': 0.17}
{'loss': 0.5709, 'learning_rate': 9.320425240424277e-05, 'epoch': 0.17}
{'loss': 0.552, 'learning_rate': 9.319207970701586e-05, 'epoch': 0.17}
{'loss': 0.5272, 'learning_rate': 9.317989691390291e-05, 'epoch': 0.17}
{'loss': 0.5451, 'learning_rate': 9.316770402775164e-05, 'epoch': 0.17}
 17%|█▋        | 1091/6500 [7:14:22<51:16:01, 34.12s/it]                                                         17%|█▋        | 1091/6500 [7:14:22<51:16:01, 34.12s/it] 17%|█▋        | 1092/6500 [7:14:40<43:58:41, 29.28s/it]                                                         17%|█▋        | 1092/6500 [7:14:40<43:58:41, 29.28s/it] 17%|█▋        | 1093/6500 [7:14:58<38:52:29, 25.88s/it]                                                         17%|█▋        | 1093/6500 [7:14:58<38:52:29, 25.88s/it] 17%|█▋        | 1094/6500 [7:15:16<35:19:01, 23.52s/it]                                                         17%|█▋        | 1094/6500 [7:15:16<35:19:01, 23.52s/it] 17%|█▋        | 1095/6500 [7:15:34<32:50:12, 21.87s/it]                                                         17%|█▋        | 1095/6500 [7:15:34<32:50:12, 21.87s/it] 17%|█▋        | 1096/6500 [7:15:52<31:06:39, 20.73s/it]                                                         17%{'loss': 0.5455, 'learning_rate': 9.315550105141199e-05, 'epoch': 0.17}
{'loss': 0.5293, 'learning_rate': 9.314328798773636e-05, 'epoch': 0.17}
{'loss': 0.534, 'learning_rate': 9.313106483957948e-05, 'epoch': 0.17}
{'loss': 0.5636, 'learning_rate': 9.311883160979844e-05, 'epoch': 0.17}
|█▋        | 1096/6500 [7:15:52<31:06:39, 20.73s/it] 17%|█▋        | 1097/6500 [7:16:10<29:54:16, 19.93s/it]                                                         17%|█▋        | 1097/6500 [7:16:10<29:54:16, 19.93s/it] 17%|█▋        | 1098/6500 [7:16:28<29:04:22, 19.37s/it]                                                         17%|█▋        | 1098/6500 [7:16:28<29:04:22, 19.37s/it] 17%|█▋        | 1099/6500 [7:16:47<28:36:28, 19.07s/it]                                                         17%|█▋        | 1099/6500 [7:16:47<28:36:28, 19.07s/it] 17%|█▋        | 1100/6500 [7:17:05<28:10:36, 18.78s/it]                                                         17%|█▋        | 1100/6500 [7:17:05<28:10:36, 18.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7663103938102722, 'eval_runtime': 5.5346, 'eval_samples_per_second': 4.156, 'eval_steps_per_second': 1.084, 'epoch': 0.17}
                                                         17%|█▋        | 1100/6500 [7:17:10<28:10:36, 18.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1100
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1100/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1100


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1100/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1100/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5195, 'learning_rate': 9.310658830125267e-05, 'epoch': 0.17}
{'loss': 0.5303, 'learning_rate': 9.309433491680398e-05, 'epoch': 0.17}
{'loss': 0.5061, 'learning_rate': 9.308207145931653e-05, 'epoch': 0.17}
{'loss': 0.572, 'learning_rate': 9.306979793165681e-05, 'epoch': 0.17}
{'loss': 0.5695, 'learning_rate': 9.305751433669369e-05, 'epoch': 0.17}
{'loss': 0.5233, 'learning_rate': 9.304522067729839e-05, 'epoch': 0.17}
 17%|█▋        | 1101/6500 [7:18:17<52:06:30, 34.75s/it]                                                         17%|█▋        | 1101/6500 [7:18:17<52:06:30, 34.75s/it] 17%|█▋        | 1102/6500 [7:18:35<44:34:00, 29.72s/it]                                                         17%|█▋        | 1102/6500 [7:18:35<44:34:00, 29.72s/it] 17%|█▋        | 1103/6500 [7:18:53<39:17:55, 26.21s/it]                                                         17%|█▋        | 1103/6500 [7:18:53<39:17:55, 26.21s/it] 17%|█▋        | 1104/6500 [7:19:11<35:36:53, 23.76s/it]                                                         17%|█▋        | 1104/6500 [7:19:11<35:36:53, 23.76s/it] 17%|█▋        | 1105/6500 [7:19:29<33:02:51, 22.05s/it]                                                         17%|█▋        | 1105/6500 [7:19:29<33:02:51, 22.05s/it] 17%|█▋        | 1106/6500 [7:19:47<31:15:35, 20.86s/it]                                                         17%{'loss': 0.5346, 'learning_rate': 9.303291695634449e-05, 'epoch': 0.17}
{'loss': 0.5294, 'learning_rate': 9.302060317670787e-05, 'epoch': 0.17}
{'loss': 1.0609, 'learning_rate': 9.300827934126683e-05, 'epoch': 0.17}
{'loss': 0.5242, 'learning_rate': 9.299594545290202e-05, 'epoch': 0.17}
|█▋        | 1106/6500 [7:19:47<31:15:35, 20.86s/it] 17%|█▋        | 1107/6500 [7:20:05<30:01:07, 20.04s/it]                                                         17%|█▋        | 1107/6500 [7:20:05<30:01:07, 20.04s/it] 17%|█▋        | 1108/6500 [7:20:23<29:09:20, 19.47s/it]                                                         17%|█▋        | 1108/6500 [7:20:23<29:09:20, 19.47s/it] 17%|█▋        | 1109/6500 [7:20:41<28:32:48, 19.06s/it]                                                         17%|█▋        | 1109/6500 [7:20:41<28:32:48, 19.06s/it] 17%|█▋        | 1110/6500 [7:20:59<28:08:12, 18.79s/it]                                                         17%|█▋        | 1110/6500 [7:20:59<28:08:12, 18.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7734110355377197, 'eval_runtime': 5.3298, 'eval_samples_per_second': 4.315, 'eval_steps_per_second': 1.126, 'epoch': 0.17}
                                                         17%|█▋        | 1110/6500 [7:21:05<28:08:12, 18.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1110
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1110/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1110


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1110/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5384, 'learning_rate': 9.298360151449635e-05, 'epoch': 0.17}
{'loss': 0.5012, 'learning_rate': 9.297124752893518e-05, 'epoch': 0.17}
{'loss': 0.5481, 'learning_rate': 9.295888349910618e-05, 'epoch': 0.17}
{'loss': 0.5434, 'learning_rate': 9.294650942789933e-05, 'epoch': 0.17}
{'loss': 0.4921, 'learning_rate': 9.293412531820704e-05, 'epoch': 0.17}
{'loss': 0.529, 'learning_rate': 9.292173117292399e-05, 'epoch': 0.17}
 17%|█▋        | 1111/6500 [7:21:56<45:10:12, 30.17s/it]                                                         17%|█▋        | 1111/6500 [7:21:56<45:10:12, 30.17s/it] 17%|█▋        | 1112/6500 [7:22:14<39:42:41, 26.53s/it]                                                         17%|█▋        | 1112/6500 [7:22:14<39:42:41, 26.53s/it] 17%|█▋        | 1113/6500 [7:22:32<35:52:35, 23.98s/it]                                                         17%|█▋        | 1113/6500 [7:22:32<35:52:35, 23.98s/it] 17%|█▋        | 1114/6500 [7:22:50<33:12:14, 22.19s/it]                                                         17%|█▋        | 1114/6500 [7:22:50<33:12:14, 22.19s/it] 17%|█▋        | 1115/6500 [7:23:08<31:20:02, 20.95s/it]                                                         17%|█▋        | 1115/6500 [7:23:08<31:20:02, 20.95s/it] 17%|█▋        | 1116/6500 [7:23:27<30:12:54, 20.20s/it]                                                         17%{'loss': 0.5067, 'learning_rate': 9.290932699494726e-05, 'epoch': 0.17}
{'loss': 0.5338, 'learning_rate': 9.289691278717623e-05, 'epoch': 0.17}
{'loss': 0.5244, 'learning_rate': 9.288448855251265e-05, 'epoch': 0.17}
{'loss': 0.5385, 'learning_rate': 9.287205429386063e-05, 'epoch': 0.17}
|█▋        | 1116/6500 [7:23:27<30:12:54, 20.20s/it] 17%|█▋        | 1117/6500 [7:23:45<29:15:42, 19.57s/it]                                                         17%|█▋        | 1117/6500 [7:23:45<29:15:42, 19.57s/it] 17%|█▋        | 1118/6500 [7:24:03<28:36:03, 19.13s/it]                                                         17%|█▋        | 1118/6500 [7:24:03<28:36:03, 19.13s/it] 17%|█▋        | 1119/6500 [7:24:21<28:12:04, 18.87s/it]                                                         17%|█▋        | 1119/6500 [7:24:21<28:12:04, 18.87s/it] 17%|█▋        | 1120/6500 [7:24:39<27:51:40, 18.64s/it]                                                         17%|█▋        | 1120/6500 [7:24:39<27:51:40, 18.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7711916565895081, 'eval_runtime': 5.3342, 'eval_samples_per_second': 4.312, 'eval_steps_per_second': 1.125, 'epoch': 0.17}
                                                         17%|█▋        | 1120/6500 [7:24:45<27:51:40, 18.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1120
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1120
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1120
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1120/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.528, 'learning_rate': 9.285961001412657e-05, 'epoch': 0.17}
{'loss': 0.5068, 'learning_rate': 9.284715571621927e-05, 'epoch': 0.17}
{'loss': 0.5848, 'learning_rate': 9.283469140304983e-05, 'epoch': 0.17}
{'loss': 0.535, 'learning_rate': 9.28222170775317e-05, 'epoch': 0.17}
{'loss': 0.5349, 'learning_rate': 9.280973274258071e-05, 'epoch': 0.17}
{'loss': 0.5538, 'learning_rate': 9.279723840111496e-05, 'epoch': 0.17}
 17%|█▋        | 1121/6500 [7:25:35<44:36:18, 29.85s/it]                                                         17%|█▋        | 1121/6500 [7:25:35<44:36:18, 29.85s/it] 17%|█▋        | 1122/6500 [7:25:53<39:16:59, 26.30s/it]                                                         17%|█▋        | 1122/6500 [7:25:53<39:16:59, 26.30s/it] 17%|█▋        | 1123/6500 [7:26:11<35:33:23, 23.81s/it]                                                         17%|█▋        | 1123/6500 [7:26:11<35:33:23, 23.81s/it] 17%|█▋        | 1124/6500 [7:26:29<32:57:40, 22.07s/it]                                                         17%|█▋        | 1124/6500 [7:26:29<32:57:40, 22.07s/it] 17%|█▋        | 1125/6500 [7:26:47<31:09:27, 20.87s/it]                                                         17%|█▋        | 1125/6500 [7:26:47<31:09:27, 20.87s/it] 17%|█▋        | 1126/6500 [7:27:06<29:53:48, 20.03s/it]                                                         17%{'loss': 0.5165, 'learning_rate': 9.278473405605497e-05, 'epoch': 0.17}
{'loss': 0.5182, 'learning_rate': 9.277221971032351e-05, 'epoch': 0.17}
{'loss': 0.5483, 'learning_rate': 9.275969536684577e-05, 'epoch': 0.17}
{'loss': 0.5406, 'learning_rate': 9.274716102854922e-05, 'epoch': 0.17}
|█▋        | 1126/6500 [7:27:06<29:53:48, 20.03s/it] 17%|█▋        | 1127/6500 [7:27:24<29:01:23, 19.45s/it]                                                         17%|█▋        | 1127/6500 [7:27:24<29:01:23, 19.45s/it] 17%|█▋        | 1128/6500 [7:27:42<28:25:06, 19.04s/it]                                                         17%|█▋        | 1128/6500 [7:27:42<28:25:06, 19.04s/it] 17%|█▋        | 1129/6500 [7:28:00<28:04:13, 18.81s/it]                                                         17%|█▋        | 1129/6500 [7:28:00<28:04:13, 18.81s/it] 17%|█▋        | 1130/6500 [7:28:18<27:45:43, 18.61s/it]                                                         17%|█▋        | 1130/6500 [7:28:18<27:45:43, 18.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7676607370376587, 'eval_runtime': 5.3502, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.121, 'epoch': 0.17}
                                                         17%|█▋        | 1130/6500 [7:28:24<27:45:43, 18.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1130
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1130
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1130
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1130/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1130/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.524, 'learning_rate': 9.273461669836366e-05, 'epoch': 0.17}
{'loss': 0.5152, 'learning_rate': 9.272206237922129e-05, 'epoch': 0.17}
{'loss': 0.5105, 'learning_rate': 9.270949807405662e-05, 'epoch': 0.17}
{'loss': 0.6216, 'learning_rate': 9.269692378580642e-05, 'epoch': 0.17}
{'loss': 0.4945, 'learning_rate': 9.26843395174099e-05, 'epoch': 0.17}
{'loss': 0.5216, 'learning_rate': 9.267174527180853e-05, 'epoch': 0.17}
 17%|█▋        | 1131/6500 [7:29:31<52:04:03, 34.91s/it]                                                         17%|█▋        | 1131/6500 [7:29:31<52:04:03, 34.91s/it] 17%|█▋        | 1132/6500 [7:29:49<44:35:24, 29.90s/it]                                                         17%|█▋        | 1132/6500 [7:29:49<44:35:24, 29.90s/it] 17%|█▋        | 1133/6500 [7:30:07<39:15:18, 26.33s/it]                                                         17%|█▋        | 1133/6500 [7:30:07<39:15:18, 26.33s/it] 17%|█▋        | 1134/6500 [7:30:25<35:32:11, 23.84s/it]                                                         17%|█▋        | 1134/6500 [7:30:25<35:32:11, 23.84s/it] 17%|█▋        | 1135/6500 [7:30:44<33:02:37, 22.17s/it]                                                         17%|█▋        | 1135/6500 [7:30:44<33:02:37, 22.17s/it] 17%|█▋        | 1136/6500 [7:31:02<31:14:42, 20.97s/it]                                                         17%{'loss': 0.533, 'learning_rate': 9.265914105194617e-05, 'epoch': 0.17}
{'loss': 1.0481, 'learning_rate': 9.264652686076895e-05, 'epoch': 0.18}
{'loss': 0.5341, 'learning_rate': 9.263390270122538e-05, 'epoch': 0.18}
{'loss': 0.5206, 'learning_rate': 9.262126857626627e-05, 'epoch': 0.18}
|█▋        | 1136/6500 [7:31:02<31:14:42, 20.97s/it] 17%|█▋        | 1137/6500 [7:31:20<29:57:27, 20.11s/it]                                                         17%|█▋        | 1137/6500 [7:31:20<29:57:27, 20.11s/it] 18%|█▊        | 1138/6500 [7:31:38<29:05:34, 19.53s/it]                                                         18%|█▊        | 1138/6500 [7:31:38<29:05:34, 19.53s/it] 18%|█▊        | 1139/6500 [7:31:56<28:27:52, 19.11s/it]                                                         18%|█▊        | 1139/6500 [7:31:56<28:27:52, 19.11s/it] 18%|█▊        | 1140/6500 [7:32:14<28:03:40, 18.85s/it]                                                         18%|█▊        | 1140/6500 [7:32:14<28:03:40, 18.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7728403210639954, 'eval_runtime': 5.3313, 'eval_samples_per_second': 4.314, 'eval_steps_per_second': 1.125, 'epoch': 0.18}
                                                         18%|█▊        | 1140/6500 [7:32:20<28:03:40, 18.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1140/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1140the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1140/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1140/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5144, 'learning_rate': 9.260862448884477e-05, 'epoch': 0.18}
{'loss': 0.4916, 'learning_rate': 9.259597044191636e-05, 'epoch': 0.18}
{'loss': 0.5388, 'learning_rate': 9.258330643843884e-05, 'epoch': 0.18}
{'loss': 0.5111, 'learning_rate': 9.257063248137236e-05, 'epoch': 0.18}
{'loss': 0.4978, 'learning_rate': 9.255794857367936e-05, 'epoch': 0.18}
{'loss': 0.4979, 'learning_rate': 9.254525471832464e-05, 'epoch': 0.18}
 18%|█▊        | 1141/6500 [7:33:24<50:38:14, 34.02s/it]                                                         18%|█▊        | 1141/6500 [7:33:24<50:38:14, 34.02s/it] 18%|█▊        | 1142/6500 [7:33:42<43:28:20, 29.21s/it]                                                         18%|█▊        | 1142/6500 [7:33:42<43:28:20, 29.21s/it] 18%|█▊        | 1143/6500 [7:34:00<38:27:51, 25.85s/it]                                                         18%|█▊        | 1143/6500 [7:34:00<38:27:51, 25.85s/it] 18%|█▊        | 1144/6500 [7:34:18<34:57:37, 23.50s/it]                                                         18%|█▊        | 1144/6500 [7:34:18<34:57:37, 23.50s/it] 18%|█▊        | 1145/6500 [7:34:36<32:30:42, 21.86s/it]                                                         18%|█▊        | 1145/6500 [7:34:36<32:30:42, 21.86s/it] 18%|█▊        | 1146/6500 [7:34:54<30:48:24, 20.71s/it]                                                         18%{'loss': 0.5057, 'learning_rate': 9.253255091827533e-05, 'epoch': 0.18}
{'loss': 0.5197, 'learning_rate': 9.251983717650084e-05, 'epoch': 0.18}
{'loss': 0.4985, 'learning_rate': 9.250711349597291e-05, 'epoch': 0.18}
{'loss': 0.5545, 'learning_rate': 9.249437987966567e-05, 'epoch': 0.18}
|█▊        | 1146/6500 [7:34:54<30:48:24, 20.71s/it] 18%|█▊        | 1147/6500 [7:35:12<29:37:13, 19.92s/it]                                                         18%|█▊        | 1147/6500 [7:35:12<29:37:13, 19.92s/it] 18%|█▊        | 1148/6500 [7:35:30<28:58:47, 19.49s/it]                                                         18%|█▊        | 1148/6500 [7:35:31<28:58:47, 19.49s/it] 18%|█▊        | 1149/6500 [7:35:49<28:22:02, 19.08s/it]                                                         18%|█▊        | 1149/6500 [7:35:49<28:22:02, 19.08s/it] 18%|█▊        | 1150/6500 [7:36:07<27:56:15, 18.80s/it]                                                         18%|█▊        | 1150/6500 [7:36:07<27:56:15, 18.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7689542174339294, 'eval_runtime': 5.482, 'eval_samples_per_second': 4.196, 'eval_steps_per_second': 1.094, 'epoch': 0.18}
                                                         18%|█▊        | 1150/6500 [7:36:12<27:56:15, 18.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1150
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1150 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1150/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5112, 'learning_rate': 9.248163633055549e-05, 'epoch': 0.18}
{'loss': 0.5439, 'learning_rate': 9.246888285162112e-05, 'epoch': 0.18}
{'loss': 0.5488, 'learning_rate': 9.24561194458436e-05, 'epoch': 0.18}
{'loss': 0.5349, 'learning_rate': 9.244334611620629e-05, 'epoch': 0.18}
{'loss': 0.535, 'learning_rate': 9.243056286569488e-05, 'epoch': 0.18}
{'loss': 0.5264, 'learning_rate': 9.241776969729739e-05, 'epoch': 0.18}
 18%|█▊        | 1151/6500 [7:37:51<66:12:55, 44.56s/it]                                                         18%|█▊        | 1151/6500 [7:37:51<66:12:55, 44.56s/it] 18%|█▊        | 1152/6500 [7:38:09<54:20:44, 36.58s/it]                                                         18%|█▊        | 1152/6500 [7:38:09<54:20:44, 36.58s/it] 18%|█▊        | 1153/6500 [7:38:27<46:01:52, 30.99s/it]                                                         18%|█▊        | 1153/6500 [7:38:27<46:01:52, 30.99s/it] 18%|█▊        | 1154/6500 [7:38:45<40:13:40, 27.09s/it]                                                         18%|█▊        | 1154/6500 [7:38:45<40:13:40, 27.09s/it] 18%|█▊        | 1155/6500 [7:39:03<36:10:38, 24.37s/it]                                                         18%|█▊        | 1155/6500 [7:39:03<36:10:38, 24.37s/it] 18%|█▊        | 1156/6500 [7:39:21<33:21:02, 22.47s/it]                                                         18%{'loss': 0.5151, 'learning_rate': 9.240496661400414e-05, 'epoch': 0.18}
{'loss': 0.5204, 'learning_rate': 9.239215361880776e-05, 'epoch': 0.18}
{'loss': 0.5507, 'learning_rate': 9.237933071470323e-05, 'epoch': 0.18}
{'loss': 0.507, 'learning_rate': 9.23664979046878e-05, 'epoch': 0.18}
|█▊        | 1156/6500 [7:39:21<33:21:02, 22.47s/it] 18%|█▊        | 1157/6500 [7:39:39<31:22:47, 21.14s/it]                                                         18%|█▊        | 1157/6500 [7:39:39<31:22:47, 21.14s/it] 18%|█▊        | 1158/6500 [7:39:58<30:06:16, 20.29s/it]                                                         18%|█▊        | 1158/6500 [7:39:58<30:06:16, 20.29s/it] 18%|█▊        | 1159/6500 [7:40:16<29:07:45, 19.63s/it]                                                         18%|█▊        | 1159/6500 [7:40:16<29:07:45, 19.63s/it] 18%|█▊        | 1160/6500 [7:40:34<28:27:03, 19.18s/it]                                                         18%|█▊        | 1160/6500 [7:40:34<28:27:03, 19.18s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7645899653434753, 'eval_runtime': 6.2272, 'eval_samples_per_second': 3.693, 'eval_steps_per_second': 0.964, 'epoch': 0.18}
                                                         18%|█▊        | 1160/6500 [7:40:40<28:27:03, 19.18s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1160
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1160
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1160I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1160
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1160/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1160/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5189, 'learning_rate': 9.23536551917611e-05, 'epoch': 0.18}
{'loss': 0.5025, 'learning_rate': 9.2340802578925e-05, 'epoch': 0.18}
{'loss': 0.5409, 'learning_rate': 9.232794006918375e-05, 'epoch': 0.18}
{'loss': 0.5852, 'learning_rate': 9.231506766554384e-05, 'epoch': 0.18}
{'loss': 0.5085, 'learning_rate': 9.230218537101416e-05, 'epoch': 0.18}
{'loss': 0.5067, 'learning_rate': 9.228929318860584e-05, 'epoch': 0.18}
 18%|█▊        | 1161/6500 [7:41:28<43:54:31, 29.61s/it]                                                         18%|█▊        | 1161/6500 [7:41:28<43:54:31, 29.61s/it] 18%|█▊        | 1162/6500 [7:41:46<38:44:54, 26.13s/it]                                                         18%|█▊        | 1162/6500 [7:41:46<38:44:54, 26.13s/it] 18%|█▊        | 1163/6500 [7:42:04<35:08:00, 23.70s/it]                                                         18%|█▊        | 1163/6500 [7:42:04<35:08:00, 23.70s/it] 18%|█▊        | 1164/6500 [7:42:22<32:44:27, 22.09s/it]                                                         18%|█▊        | 1164/6500 [7:42:22<32:44:27, 22.09s/it] 18%|█▊        | 1165/6500 [7:42:40<30:55:41, 20.87s/it]                                                         18%|█▊        | 1165/6500 [7:42:40<30:55:41, 20.87s/it] 18%|█▊        | 1166/6500 [7:42:58<29:40:22, 20.03s/it]                                                         18%{'loss': 0.5151, 'learning_rate': 9.227639112133238e-05, 'epoch': 0.18}
{'loss': 1.0391, 'learning_rate': 9.226347917220953e-05, 'epoch': 0.18}
{'loss': 0.53, 'learning_rate': 9.225055734425539e-05, 'epoch': 0.18}
{'loss': 0.5056, 'learning_rate': 9.223762564049035e-05, 'epoch': 0.18}
|█▊        | 1166/6500 [7:42:58<29:40:22, 20.03s/it] 18%|█▊        | 1167/6500 [7:43:17<28:56:00, 19.53s/it]                                                         18%|█▊        | 1167/6500 [7:43:17<28:56:00, 19.53s/it] 18%|█▊        | 1168/6500 [7:43:35<28:20:01, 19.13s/it]                                                         18%|█▊        | 1168/6500 [7:43:35<28:20:01, 19.13s/it] 18%|█▊        | 1169/6500 [7:43:53<27:53:00, 18.83s/it]                                                         18%|█▊        | 1169/6500 [7:43:53<27:53:00, 18.83s/it] 18%|█▊        | 1170/6500 [7:44:11<27:34:19, 18.62s/it]                                                         18%|█▊        | 1170/6500 [7:44:11<27:34:19, 18.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7724975347518921, 'eval_runtime': 5.431, 'eval_samples_per_second': 4.235, 'eval_steps_per_second': 1.105, 'epoch': 0.18}
                                                         18%|█▊        | 1170/6500 [7:44:17<27:34:19, 18.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1170/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1170

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1170/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4848, 'learning_rate': 9.222468406393713e-05, 'epoch': 0.18}
{'loss': 0.5107, 'learning_rate': 9.221173261762073e-05, 'epoch': 0.18}
{'loss': 0.5481, 'learning_rate': 9.219877130456851e-05, 'epoch': 0.18}
{'loss': 0.5023, 'learning_rate': 9.218580012781005e-05, 'epoch': 0.18}
{'loss': 0.5068, 'learning_rate': 9.217281909037732e-05, 'epoch': 0.18}
{'loss': 0.4812, 'learning_rate': 9.215982819530451e-05, 'epoch': 0.18}
 18%|█▊        | 1171/6500 [7:44:50<36:40:27, 24.78s/it]                                                         18%|█▊        | 1171/6500 [7:44:50<36:40:27, 24.78s/it] 18%|█▊        | 1172/6500 [7:45:08<33:41:45, 22.77s/it]                                                         18%|█▊        | 1172/6500 [7:45:08<33:41:45, 22.77s/it] 18%|█▊        | 1173/6500 [7:45:26<31:36:41, 21.36s/it]                                                         18%|█▊        | 1173/6500 [7:45:26<31:36:41, 21.36s/it] 18%|█▊        | 1174/6500 [7:45:45<30:09:27, 20.38s/it]                                                         18%|█▊        | 1174/6500 [7:45:45<30:09:27, 20.38s/it] 18%|█▊        | 1175/6500 [7:46:05<30:05:56, 20.35s/it]                                                         18%|█▊        | 1175/6500 [7:46:05<30:05:56, 20.35s/it] 18%|█▊        | 1176/6500 [7:46:23<29:13:05, 19.76s/it]                                                         18%{'loss': 0.5092, 'learning_rate': 9.214682744562823e-05, 'epoch': 0.18}
{'loss': 0.5095, 'learning_rate': 9.213381684438726e-05, 'epoch': 0.18}
{'loss': 0.501, 'learning_rate': 9.212079639462281e-05, 'epoch': 0.18}
{'loss': 0.5326, 'learning_rate': 9.210776609937829e-05, 'epoch': 0.18}
|█▊        | 1176/6500 [7:46:23<29:13:05, 19.76s/it] 18%|█▊        | 1177/6500 [7:46:41<28:29:10, 19.27s/it]                                                         18%|█▊        | 1177/6500 [7:46:41<28:29:10, 19.27s/it] 18%|█▊        | 1178/6500 [7:46:59<27:57:49, 18.92s/it]                                                         18%|█▊        | 1178/6500 [7:46:59<27:57:49, 18.92s/it] 18%|█▊        | 1179/6500 [7:47:18<27:35:52, 18.67s/it]                                                         18%|█▊        | 1179/6500 [7:47:18<27:35:52, 18.67s/it] 18%|█▊        | 1180/6500 [7:47:37<27:45:44, 18.79s/it]                                                         18%|█▊        | 1180/6500 [7:47:37<27:45:44, 18.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7686096429824829, 'eval_runtime': 6.261, 'eval_samples_per_second': 3.674, 'eval_steps_per_second': 0.958, 'epoch': 0.18}
                                                         18%|█▊        | 1180/6500 [7:47:43<27:45:44, 18.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1180

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1180
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1180

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1180/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4957, 'learning_rate': 9.209472596169946e-05, 'epoch': 0.18}
{'loss': 0.5474, 'learning_rate': 9.208167598463439e-05, 'epoch': 0.18}
{'loss': 0.5165, 'learning_rate': 9.206861617123341e-05, 'epoch': 0.18}
{'loss': 0.5031, 'learning_rate': 9.205554652454918e-05, 'epoch': 0.18}
{'loss': 0.5239, 'learning_rate': 9.204246704763665e-05, 'epoch': 0.18}
{'loss': 0.5055, 'learning_rate': 9.202937774355307e-05, 'epoch': 0.18}
 18%|█▊        | 1181/6500 [7:49:10<60:46:30, 41.13s/it]                                                         18%|█▊        | 1181/6500 [7:49:10<60:46:30, 41.13s/it] 18%|█▊        | 1182/6500 [7:49:28<50:30:16, 34.19s/it]                                                         18%|█▊        | 1182/6500 [7:49:28<50:30:16, 34.19s/it] 18%|█▊        | 1183/6500 [7:49:46<43:18:07, 29.32s/it]                                                         18%|█▊        | 1183/6500 [7:49:46<43:18:07, 29.32s/it] 18%|█▊        | 1184/6500 [7:50:04<38:16:05, 25.92s/it]                                                         18%|█▊        | 1184/6500 [7:50:04<38:16:05, 25.92s/it] 18%|█▊        | 1185/6500 [7:50:22<34:45:28, 23.54s/it]                                                         18%|█▊        | 1185/6500 [7:50:22<34:45:28, 23.54s/it] 18%|█▊        | 1186/6500 [7:50:40<32:18:32, 21.89s/it]                                                         18%{'loss': 0.51, 'learning_rate': 9.201627861535799e-05, 'epoch': 0.18}
{'loss': 0.5032, 'learning_rate': 9.200316966611324e-05, 'epoch': 0.18}
{'loss': 0.5388, 'learning_rate': 9.199005089888297e-05, 'epoch': 0.18}
{'loss': 0.5046, 'learning_rate': 9.197692231673361e-05, 'epoch': 0.18}
|█▊        | 1186/6500 [7:50:40<32:18:32, 21.89s/it] 18%|█▊        | 1187/6500 [7:50:58<30:35:56, 20.73s/it]                                                         18%|█▊        | 1187/6500 [7:50:58<30:35:56, 20.73s/it] 18%|█▊        | 1188/6500 [7:51:16<29:24:41, 19.93s/it]                                                         18%|█▊        | 1188/6500 [7:51:16<29:24:41, 19.93s/it] 18%|█▊        | 1189/6500 [7:51:34<28:35:16, 19.38s/it]                                                         18%|█▊        | 1189/6500 [7:51:34<28:35:16, 19.38s/it] 18%|█▊        | 1190/6500 [7:51:52<28:01:06, 19.00s/it]                                                         18%|█▊        | 1190/6500 [7:51:52<28:01:06, 19.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7691440582275391, 'eval_runtime': 5.4978, 'eval_samples_per_second': 4.183, 'eval_steps_per_second': 1.091, 'epoch': 0.18}
                                                         18%|█▊        | 1190/6500 [7:51:58<28:01:06, 19.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1190
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1190
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1190 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1190/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1190/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5104, 'learning_rate': 9.196378392273387e-05, 'epoch': 0.18}
{'loss': 0.4891, 'learning_rate': 9.195063571995479e-05, 'epoch': 0.18}
{'loss': 0.5537, 'learning_rate': 9.193747771146968e-05, 'epoch': 0.18}
{'loss': 0.5518, 'learning_rate': 9.192430990035413e-05, 'epoch': 0.18}
{'loss': 0.4905, 'learning_rate': 9.191113228968604e-05, 'epoch': 0.18}
{'loss': 0.5134, 'learning_rate': 9.189794488254561e-05, 'epoch': 0.18}
 18%|█▊        | 1191/6500 [7:53:15<56:29:13, 38.30s/it]                                                         18%|█▊        | 1191/6500 [7:53:15<56:29:13, 38.30s/it] 18%|█▊        | 1192/6500 [7:53:33<47:28:37, 32.20s/it]                                                         18%|█▊        | 1192/6500 [7:53:33<47:28:37, 32.20s/it] 18%|█▊        | 1193/6500 [7:53:51<41:10:45, 27.93s/it]                                                         18%|█▊        | 1193/6500 [7:53:51<41:10:45, 27.93s/it] 18%|█▊        | 1194/6500 [7:54:09<36:46:34, 24.95s/it]                                                         18%|█▊        | 1194/6500 [7:54:09<36:46:34, 24.95s/it] 18%|█▊        | 1195/6500 [7:54:27<33:42:31, 22.87s/it]                                                         18%|█▊        | 1195/6500 [7:54:27<33:42:31, 22.87s/it] 18%|█▊        | 1196/6500 [7:54:46<31:41:36, 21.51s/it]                                                         18%{'loss': 0.6923, 'learning_rate': 9.188474768201532e-05, 'epoch': 0.18}
{'loss': 0.853, 'learning_rate': 9.18715406911799e-05, 'epoch': 0.18}
{'loss': 0.5267, 'learning_rate': 9.185832391312644e-05, 'epoch': 0.18}
{'loss': 0.5096, 'learning_rate': 9.184509735094427e-05, 'epoch': 0.18}
|█▊        | 1196/6500 [7:54:46<31:41:36, 21.51s/it] 18%|█▊        | 1197/6500 [7:55:04<30:09:36, 20.47s/it]                                                         18%|█▊        | 1197/6500 [7:55:04<30:09:36, 20.47s/it] 18%|█▊        | 1198/6500 [7:55:22<29:05:59, 19.76s/it]                                                         18%|█▊        | 1198/6500 [7:55:22<29:05:59, 19.76s/it] 18%|█▊        | 1199/6500 [7:55:40<28:22:11, 19.27s/it]                                                         18%|█▊        | 1199/6500 [7:55:40<28:22:11, 19.27s/it] 18%|█▊        | 1200/6500 [7:55:58<27:51:53, 18.93s/it]                                                         18%|█▊        | 1200/6500 [7:55:58<27:51:53, 18.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7717649340629578, 'eval_runtime': 5.615, 'eval_samples_per_second': 4.096, 'eval_steps_per_second': 1.069, 'epoch': 0.18}
                                                         18%|█▊        | 1200/6500 [7:56:04<27:51:53, 18.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1200
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1200
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1200/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4671, 'learning_rate': 9.1831861007725e-05, 'epoch': 0.18}
{'loss': 0.5243, 'learning_rate': 9.181861488656256e-05, 'epoch': 0.18}
{'loss': 0.5216, 'learning_rate': 9.180535899055316e-05, 'epoch': 0.19}
{'loss': 0.4713, 'learning_rate': 9.17920933227953e-05, 'epoch': 0.19}
{'loss': 0.5113, 'learning_rate': 9.177881788638969e-05, 'epoch': 0.19}
{'loss': 0.475, 'learning_rate': 9.176553268443943e-05, 'epoch': 0.19}
 18%|█▊        | 1201/6500 [7:57:51<69:09:19, 46.98s/it]                                                         18%|█▊        | 1201/6500 [7:57:51<69:09:19, 46.98s/it] 18%|█▊        | 1202/6500 [7:58:09<56:18:47, 38.26s/it]                                                         18%|█▊        | 1202/6500 [7:58:09<56:18:47, 38.26s/it] 19%|█▊        | 1203/6500 [7:58:26<47:19:48, 32.17s/it]                                                         19%|█▊        | 1203/6500 [7:58:26<47:19:48, 32.17s/it] 19%|█▊        | 1204/6500 [7:58:44<41:03:14, 27.91s/it]                                                         19%|█▊        | 1204/6500 [7:58:44<41:03:14, 27.91s/it] 19%|█▊        | 1205/6500 [7:59:02<36:40:22, 24.93s/it]                                                         19%|█▊        | 1205/6500 [7:59:02<36:40:22, 24.93s/it] 19%|█▊        | 1206/6500 [7:59:20<33:37:48, 22.87s/it]                                                         19%{'loss': 0.4972, 'learning_rate': 9.175223772004986e-05, 'epoch': 0.19}
{'loss': 0.4941, 'learning_rate': 9.173893299632856e-05, 'epoch': 0.19}
{'loss': 0.5097, 'learning_rate': 9.172561851638545e-05, 'epoch': 0.19}
{'loss': 0.4927, 'learning_rate': 9.171229428333272e-05, 'epoch': 0.19}
|█▊        | 1206/6500 [7:59:20<33:37:48, 22.87s/it] 19%|█▊        | 1207/6500 [7:59:39<31:30:19, 21.43s/it]                                                         19%|█▊        | 1207/6500 [7:59:39<31:30:19, 21.43s/it] 19%|█▊        | 1208/6500 [7:59:57<30:04:09, 20.46s/it]                                                         19%|█▊        | 1208/6500 [7:59:57<30:04:09, 20.46s/it] 19%|█▊        | 1209/6500 [8:00:15<29:01:16, 19.75s/it]                                                         19%|█▊        | 1209/6500 [8:00:15<29:01:16, 19.75s/it] 19%|█▊        | 1210/6500 [8:00:33<28:21:02, 19.29s/it]                                                         19%|█▊        | 1210/6500 [8:00:33<28:21:02, 19.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7689603567123413, 'eval_runtime': 5.3441, 'eval_samples_per_second': 4.304, 'eval_steps_per_second': 1.123, 'epoch': 0.19}
                                                         19%|█▊        | 1210/6500 [8:00:38<28:21:02, 19.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1210
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1210/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1210/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5, 'learning_rate': 9.169896030028482e-05, 'epoch': 0.19}
{'loss': 0.555, 'learning_rate': 9.168561657035848e-05, 'epoch': 0.19}
{'loss': 0.5185, 'learning_rate': 9.16722630966727e-05, 'epoch': 0.19}
{'loss': 0.5123, 'learning_rate': 9.165889988234881e-05, 'epoch': 0.19}
{'loss': 0.5205, 'learning_rate': 9.164552693051035e-05, 'epoch': 0.19}
{'loss': 0.498, 'learning_rate': 9.16321442442832e-05, 'epoch': 0.19}
 19%|█▊        | 1211/6500 [8:01:49<53:15:55, 36.26s/it]                                                         19%|█▊        | 1211/6500 [8:01:49<53:15:55, 36.26s/it] 19%|█▊        | 1212/6500 [8:02:07<45:21:57, 30.88s/it]                                                         19%|█▊        | 1212/6500 [8:02:07<45:21:57, 30.88s/it] 19%|█▊        | 1213/6500 [8:02:25<39:44:01, 27.06s/it]                                                         19%|█▊        | 1213/6500 [8:02:25<39:44:01, 27.06s/it] 19%|█▊        | 1214/6500 [8:02:43<35:45:40, 24.36s/it]                                                         19%|█▊        | 1214/6500 [8:02:43<35:45:40, 24.36s/it] 19%|█▊        | 1215/6500 [8:03:02<32:59:19, 22.47s/it]                                                         19%|█▊        | 1215/6500 [8:03:02<32:59:19, 22.47s/it] 19%|█▊        | 1216/6500 [8:03:20<31:02:44, 21.15s/it]                                                         19%{'loss': 0.5043, 'learning_rate': 9.161875182679546e-05, 'epoch': 0.19}
{'loss': 0.521, 'learning_rate': 9.160534968117752e-05, 'epoch': 0.19}
{'loss': 0.5138, 'learning_rate': 9.159193781056203e-05, 'epoch': 0.19}
{'loss': 0.5015, 'learning_rate': 9.1578516218084e-05, 'epoch': 0.19}
|█▊        | 1216/6500 [8:03:20<31:02:44, 21.15s/it] 19%|█▊        | 1217/6500 [8:03:38<29:42:04, 20.24s/it]                                                         19%|█▊        | 1217/6500 [8:03:38<29:42:04, 20.24s/it] 19%|█▊        | 1218/6500 [8:03:56<28:46:14, 19.61s/it]                                                         19%|█▊        | 1218/6500 [8:03:56<28:46:14, 19.61s/it] 19%|█▉        | 1219/6500 [8:04:14<28:07:28, 19.17s/it]                                                         19%|█▉        | 1219/6500 [8:04:14<28:07:28, 19.17s/it] 19%|█▉        | 1220/6500 [8:04:32<27:40:19, 18.87s/it]                                                         19%|█▉        | 1220/6500 [8:04:32<27:40:19, 18.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7632871866226196, 'eval_runtime': 5.3249, 'eval_samples_per_second': 4.319, 'eval_steps_per_second': 1.127, 'epoch': 0.19}
                                                         19%|█▉        | 1220/6500 [8:04:37<27:40:19, 18.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1220
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1220 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1220/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1220/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4831, 'learning_rate': 9.156508490688058e-05, 'epoch': 0.19}
{'loss': 0.5062, 'learning_rate': 9.15516438800913e-05, 'epoch': 0.19}
{'loss': 0.5687, 'learning_rate': 9.153819314085787e-05, 'epoch': 0.19}
{'loss': 0.5038, 'learning_rate': 9.152473269232437e-05, 'epoch': 0.19}
{'loss': 0.4886, 'learning_rate': 9.151126253763708e-05, 'epoch': 0.19}
{'loss': 0.511, 'learning_rate': 9.149778267994457e-05, 'epoch': 0.19}
 19%|█▉        | 1221/6500 [8:05:22<41:14:24, 28.12s/it]                                                         19%|█▉        | 1221/6500 [8:05:22<41:14:24, 28.12s/it] 19%|█▉        | 1222/6500 [8:05:40<36:48:11, 25.10s/it]                                                         19%|█▉        | 1222/6500 [8:05:40<36:48:11, 25.10s/it] 19%|█▉        | 1223/6500 [8:05:58<33:41:08, 22.98s/it]                                                         19%|█▉        | 1223/6500 [8:05:58<33:41:08, 22.98s/it] 19%|█▉        | 1224/6500 [8:06:16<31:29:24, 21.49s/it]                                                         19%|█▉        | 1224/6500 [8:06:16<31:29:24, 21.49s/it] 19%|█▉        | 1225/6500 [8:06:34<29:58:10, 20.45s/it]                                                         19%|█▉        | 1225/6500 [8:06:34<29:58:10, 20.45s/it] 19%|█▉        | 1226/6500 [8:06:52<28:54:47, 19.74s/it]                                                         19%{'loss': 1.0356, 'learning_rate': 9.148429312239767e-05, 'epoch': 0.19}
{'loss': 0.5186, 'learning_rate': 9.147079386814947e-05, 'epoch': 0.19}
{'loss': 0.4884, 'learning_rate': 9.145728492035536e-05, 'epoch': 0.19}
{'loss': 0.5049, 'learning_rate': 9.144376628217295e-05, 'epoch': 0.19}
|█▉        | 1226/6500 [8:06:52<28:54:47, 19.74s/it] 19%|█▉        | 1227/6500 [8:07:10<28:09:43, 19.23s/it]                                                         19%|█▉        | 1227/6500 [8:07:10<28:09:43, 19.23s/it] 19%|█▉        | 1228/6500 [8:07:28<27:39:25, 18.89s/it]                                                         19%|█▉        | 1228/6500 [8:07:28<27:39:25, 18.89s/it] 19%|█▉        | 1229/6500 [8:07:47<27:29:24, 18.78s/it]                                                         19%|█▉        | 1229/6500 [8:07:47<27:29:24, 18.78s/it] 19%|█▉        | 1230/6500 [8:08:05<27:11:51, 18.58s/it]                                                         19%|█▉        | 1230/6500 [8:08:05<27:11:51, 18.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7737638354301453, 'eval_runtime': 5.4656, 'eval_samples_per_second': 4.208, 'eval_steps_per_second': 1.098, 'epoch': 0.19}
                                                         19%|█▉        | 1230/6500 [8:08:10<27:11:51, 18.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1230I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1230

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1230/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4711, 'learning_rate': 9.143023795676217e-05, 'epoch': 0.19}
{'loss': 0.5259, 'learning_rate': 9.141669994728517e-05, 'epoch': 0.19}
{'loss': 0.4902, 'learning_rate': 9.140315225690636e-05, 'epoch': 0.19}
{'loss': 0.4862, 'learning_rate': 9.138959488879244e-05, 'epoch': 0.19}
{'loss': 0.4758, 'learning_rate': 9.137602784611239e-05, 'epoch': 0.19}
{'loss': 0.4938, 'learning_rate': 9.136245113203739e-05, 'epoch': 0.19}
 19%|█▉        | 1231/6500 [8:08:57<42:05:43, 28.76s/it]                                                         19%|█▉        | 1231/6500 [8:08:57<42:05:43, 28.76s/it] 19%|█▉        | 1232/6500 [8:09:15<37:22:14, 25.54s/it]                                                         19%|█▉        | 1232/6500 [8:09:15<37:22:14, 25.54s/it] 19%|█▉        | 1233/6500 [8:09:33<34:03:31, 23.28s/it]                                                         19%|█▉        | 1233/6500 [8:09:33<34:03:31, 23.28s/it] 19%|█▉        | 1234/6500 [8:09:51<31:44:57, 21.70s/it]                                                         19%|█▉        | 1234/6500 [8:09:51<31:44:57, 21.70s/it] 19%|█▉        | 1235/6500 [8:10:09<30:08:04, 20.60s/it]                                                         19%|█▉        | 1235/6500 [8:10:09<30:08:04, 20.60s/it] 19%|█▉        | 1236/6500 [8:10:28<29:01:10, 19.85s/it]                                                         19%{'loss': 0.4916, 'learning_rate': 9.134886474974091e-05, 'epoch': 0.19}
{'loss': 0.4777, 'learning_rate': 9.133526870239873e-05, 'epoch': 0.19}
{'loss': 0.5235, 'learning_rate': 9.132166299318878e-05, 'epoch': 0.19}
{'loss': 0.4819, 'learning_rate': 9.130804762529137e-05, 'epoch': 0.19}
|█▉        | 1236/6500 [8:10:28<29:01:10, 19.85s/it] 19%|█▉        | 1237/6500 [8:10:46<28:14:45, 19.32s/it]                                                         19%|█▉        | 1237/6500 [8:10:46<28:14:45, 19.32s/it] 19%|█▉        | 1238/6500 [8:11:04<27:42:39, 18.96s/it]                                                         19%|█▉        | 1238/6500 [8:11:04<27:42:39, 18.96s/it] 19%|█▉        | 1239/6500 [8:11:22<27:20:44, 18.71s/it]                                                         19%|█▉        | 1239/6500 [8:11:22<27:20:44, 18.71s/it] 19%|█▉        | 1240/6500 [8:11:40<27:05:08, 18.54s/it]                                                         19%|█▉        | 1240/6500 [8:11:40<27:05:08, 18.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7679159641265869, 'eval_runtime': 5.378, 'eval_samples_per_second': 4.277, 'eval_steps_per_second': 1.116, 'epoch': 0.19}
                                                         19%|█▉        | 1240/6500 [8:11:45<27:05:08, 18.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1240
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1240
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1240
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1240/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5335, 'learning_rate': 9.129442260188899e-05, 'epoch': 0.19}
{'loss': 0.5096, 'learning_rate': 9.128078792616638e-05, 'epoch': 0.19}
{'loss': 0.49, 'learning_rate': 9.126714360131059e-05, 'epoch': 0.19}
{'loss': 0.5043, 'learning_rate': 9.12534896305109e-05, 'epoch': 0.19}
{'loss': 0.4975, 'learning_rate': 9.123982601695882e-05, 'epoch': 0.19}
{'loss': 0.4887, 'learning_rate': 9.122615276384816e-05, 'epoch': 0.19}
 19%|█▉        | 1241/6500 [8:12:55<52:00:03, 35.60s/it]                                                         19%|█▉        | 1241/6500 [8:12:55<52:00:03, 35.60s/it] 19%|█▉        | 1242/6500 [8:13:13<44:19:01, 30.34s/it]                                                         19%|█▉        | 1242/6500 [8:13:13<44:19:01, 30.34s/it] 19%|█▉        | 1243/6500 [8:13:32<38:56:44, 26.67s/it]                                                         19%|█▉        | 1243/6500 [8:13:32<38:56:44, 26.67s/it] 19%|█▉        | 1244/6500 [8:13:50<35:10:55, 24.10s/it]                                                         19%|█▉        | 1244/6500 [8:13:50<35:10:55, 24.10s/it] 19%|█▉        | 1245/6500 [8:14:08<32:38:32, 22.36s/it]                                                         19%|█▉        | 1245/6500 [8:14:08<32:38:32, 22.36s/it] 19%|█▉        | 1246/6500 [8:14:26<30:46:30, 21.09s/it]                                                         19%{'loss': 0.4987, 'learning_rate': 9.121246987437496e-05, 'epoch': 0.19}
{'loss': 0.5372, 'learning_rate': 9.119877735173748e-05, 'epoch': 0.19}
{'loss': 0.4761, 'learning_rate': 9.118507519913631e-05, 'epoch': 0.19}
{'loss': 0.5053, 'learning_rate': 9.11713634197742e-05, 'epoch': 0.19}
|█▉        | 1246/6500 [8:14:26<30:46:30, 21.09s/it] 19%|█▉        | 1247/6500 [8:14:44<29:28:48, 20.20s/it]                                                         19%|█▉        | 1247/6500 [8:14:44<29:28:48, 20.20s/it] 19%|█▉        | 1248/6500 [8:15:02<28:35:12, 19.59s/it]                                                         19%|█▉        | 1248/6500 [8:15:02<28:35:12, 19.59s/it] 19%|█▉        | 1249/6500 [8:15:21<27:57:51, 19.17s/it]                                                         19%|█▉        | 1249/6500 [8:15:21<27:57:51, 19.17s/it] 19%|█▉        | 1250/6500 [8:15:39<27:31:59, 18.88s/it]                                                         19%|█▉        | 1250/6500 [8:15:39<27:31:59, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7656430602073669, 'eval_runtime': 5.5985, 'eval_samples_per_second': 4.108, 'eval_steps_per_second': 1.072, 'epoch': 0.19}
                                                         19%|█▉        | 1250/6500 [8:15:44<27:31:59, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1250
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1250
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1250/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4771, 'learning_rate': 9.115764201685623e-05, 'epoch': 0.19}
{'loss': 0.5259, 'learning_rate': 9.114391099358968e-05, 'epoch': 0.19}
{'loss': 0.5458, 'learning_rate': 9.113017035318409e-05, 'epoch': 0.19}
{'loss': 0.4854, 'learning_rate': 9.111642009885127e-05, 'epoch': 0.19}
{'loss': 0.5009, 'learning_rate': 9.110266023380523e-05, 'epoch': 0.19}
{'loss': 0.4772, 'learning_rate': 9.108889076126226e-05, 'epoch': 0.19}
 19%|█▉        | 1251/6500 [8:16:49<50:11:54, 34.43s/it]                                                         19%|█▉        | 1251/6500 [8:16:49<50:11:54, 34.43s/it] 19%|█▉        | 1252/6500 [8:17:08<43:01:15, 29.51s/it]                                                         19%|█▉        | 1252/6500 [8:17:08<43:01:15, 29.51s/it] 19%|█▉        | 1253/6500 [8:17:26<37:59:59, 26.07s/it]                                                         19%|█▉        | 1253/6500 [8:17:26<37:59:59, 26.07s/it] 19%|█▉        | 1254/6500 [8:17:44<34:29:04, 23.66s/it]                                                         19%|█▉        | 1254/6500 [8:17:44<34:29:04, 23.66s/it] 19%|█▉        | 1255/6500 [8:18:02<32:02:12, 21.99s/it]                                                         19%|█▉        | 1255/6500 [8:18:02<32:02:12, 21.99s/it] 19%|█▉        | 1256/6500 [8:18:20<30:19:54, 20.82s/it]                                                         19%{'loss': 1.0268, 'learning_rate': 9.107511168444092e-05, 'epoch': 0.19}
{'loss': 0.504, 'learning_rate': 9.106132300656196e-05, 'epoch': 0.19}
{'loss': 0.4982, 'learning_rate': 9.104752473084838e-05, 'epoch': 0.19}
{'loss': 0.4634, 'learning_rate': 9.103371686052548e-05, 'epoch': 0.19}
|█▉        | 1256/6500 [8:18:20<30:19:54, 20.82s/it] 19%|█▉        | 1257/6500 [8:18:38<29:07:30, 20.00s/it]                                                         19%|█▉        | 1257/6500 [8:18:38<29:07:30, 20.00s/it] 19%|█▉        | 1258/6500 [8:18:56<28:18:03, 19.44s/it]                                                         19%|█▉        | 1258/6500 [8:18:56<28:18:03, 19.44s/it] 19%|█▉        | 1259/6500 [8:19:14<27:43:55, 19.05s/it]                                                         19%|█▉        | 1259/6500 [8:19:14<27:43:55, 19.05s/it] 19%|█▉        | 1260/6500 [8:19:32<27:20:20, 18.78s/it]                                                         19%|█▉        | 1260/6500 [8:19:32<27:20:20, 18.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7822850942611694, 'eval_runtime': 5.3372, 'eval_samples_per_second': 4.309, 'eval_steps_per_second': 1.124, 'epoch': 0.19}
                                                         19%|█▉        | 1260/6500 [8:19:38<27:20:20, 18.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1260
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1260
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1260

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1260/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1260/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4952, 'learning_rate': 9.101989939882076e-05, 'epoch': 0.19}
{'loss': 0.5168, 'learning_rate': 9.100607234896397e-05, 'epoch': 0.19}
{'loss': 0.4701, 'learning_rate': 9.099223571418707e-05, 'epoch': 0.19}
{'loss': 0.4826, 'learning_rate': 9.097838949772432e-05, 'epoch': 0.19}
{'loss': 0.4732, 'learning_rate': 9.096453370281219e-05, 'epoch': 0.19}
{'loss': 0.4852, 'learning_rate': 9.095066833268935e-05, 'epoch': 0.19}
 19%|█▉        | 1261/6500 [8:20:38<47:40:41, 32.76s/it]                                                         19%|█▉        | 1261/6500 [8:20:38<47:40:41, 32.76s/it] 19%|█▉        | 1262/6500 [8:20:56<41:15:10, 28.35s/it]                                                         19%|█▉        | 1262/6500 [8:20:56<41:15:10, 28.35s/it] 19%|█▉        | 1263/6500 [8:21:14<36:45:24, 25.27s/it]                                                         19%|█▉        | 1263/6500 [8:21:14<36:45:24, 25.27s/it] 19%|█▉        | 1264/6500 [8:21:32<33:38:12, 23.13s/it]                                                         19%|█▉        | 1264/6500 [8:21:32<33:38:12, 23.13s/it] 19%|█▉        | 1265/6500 [8:21:50<31:26:44, 21.62s/it]                                                         19%|█▉        | 1265/6500 [8:21:50<31:26:44, 21.62s/it] 19%|█▉        | 1266/6500 [8:22:08<29:54:35, 20.57s/it]                                                         19%{'loss': 0.4875, 'learning_rate': 9.093679339059678e-05, 'epoch': 0.19}
{'loss': 0.4796, 'learning_rate': 9.092290887977765e-05, 'epoch': 0.2}
{'loss': 0.496, 'learning_rate': 9.090901480347739e-05, 'epoch': 0.2}
{'loss': 0.466, 'learning_rate': 9.089511116494367e-05, 'epoch': 0.2}
|█▉        | 1266/6500 [8:22:08<29:54:35, 20.57s/it] 19%|█▉        | 1267/6500 [8:22:26<28:50:31, 19.84s/it]                                                         19%|█▉        | 1267/6500 [8:22:26<28:50:31, 19.84s/it] 20%|█▉        | 1268/6500 [8:22:44<28:06:15, 19.34s/it]                                                         20%|█▉        | 1268/6500 [8:22:44<28:06:15, 19.34s/it] 20%|█▉        | 1269/6500 [8:23:03<27:35:32, 18.99s/it]                                                         20%|█▉        | 1269/6500 [8:23:03<27:35:32, 18.99s/it] 20%|█▉        | 1270/6500 [8:23:23<28:00:16, 19.28s/it]                                                         20%|█▉        | 1270/6500 [8:23:23<28:00:16, 19.28s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.771998941898346, 'eval_runtime': 5.3405, 'eval_samples_per_second': 4.307, 'eval_steps_per_second': 1.123, 'epoch': 0.2}
                                                         20%|█▉        | 1270/6500 [8:23:28<28:00:16, 19.28s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1270
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1270
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1270/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5359, 'learning_rate': 9.088119796742633e-05, 'epoch': 0.2}
{'loss': 0.483, 'learning_rate': 9.086727521417755e-05, 'epoch': 0.2}
{'loss': 0.4993, 'learning_rate': 9.085334290845164e-05, 'epoch': 0.2}
{'loss': 0.5158, 'learning_rate': 9.083940105350524e-05, 'epoch': 0.2}
{'loss': 0.4814, 'learning_rate': 9.082544965259715e-05, 'epoch': 0.2}
{'loss': 0.4964, 'learning_rate': 9.081148870898842e-05, 'epoch': 0.2}
 20%|█▉        | 1271/6500 [8:24:19<43:58:19, 30.27s/it]                                                         20%|█▉        | 1271/6500 [8:24:19<43:58:19, 30.27s/it] 20%|█▉        | 1272/6500 [8:24:37<38:39:37, 26.62s/it]                                                         20%|█▉        | 1272/6500 [8:24:37<38:39:37, 26.62s/it] 20%|█▉        | 1273/6500 [8:24:55<34:56:32, 24.07s/it]                                                         20%|█▉        | 1273/6500 [8:24:55<34:56:32, 24.07s/it] 20%|█▉        | 1274/6500 [8:25:13<32:21:33, 22.29s/it]                                                         20%|█▉        | 1274/6500 [8:25:13<32:21:33, 22.29s/it] 20%|█▉        | 1275/6500 [8:25:31<30:32:35, 21.04s/it]                                                         20%|█▉        | 1275/6500 [8:25:31<30:32:35, 21.04s/it] 20%|█▉        | 1276/6500 [8:25:49<29:17:02, 20.18s/it]                                                         20%{'loss': 0.4929, 'learning_rate': 9.079751822594235e-05, 'epoch': 0.2}
{'loss': 0.5063, 'learning_rate': 9.078353820672443e-05, 'epoch': 0.2}
{'loss': 0.4852, 'learning_rate': 9.076954865460243e-05, 'epoch': 0.2}
{'loss': 0.4828, 'learning_rate': 9.075554957284633e-05, 'epoch': 0.2}
|█▉        | 1276/6500 [8:25:49<29:17:02, 20.18s/it] 20%|█▉        | 1277/6500 [8:26:08<28:29:16, 19.64s/it]                                                         20%|█▉        | 1277/6500 [8:26:08<28:29:16, 19.64s/it] 20%|█▉        | 1278/6500 [8:26:26<27:51:29, 19.21s/it]                                                         20%|█▉        | 1278/6500 [8:26:26<27:51:29, 19.21s/it] 20%|█▉        | 1279/6500 [8:26:44<27:34:10, 19.01s/it]                                                         20%|█▉        | 1279/6500 [8:26:44<27:34:10, 19.01s/it] 20%|█▉        | 1280/6500 [8:27:03<27:12:35, 18.77s/it]                                                         20%|█▉        | 1280/6500 [8:27:03<27:12:35, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7648373246192932, 'eval_runtime': 5.503, 'eval_samples_per_second': 4.18, 'eval_steps_per_second': 1.09, 'epoch': 0.2}
                                                         20%|█▉        | 1280/6500 [8:27:08<27:12:35, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1280
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1280
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1280 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1280
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1280/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1280/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4663, 'learning_rate': 9.07415409647283e-05, 'epoch': 0.2}
{'loss': 0.5791, 'learning_rate': 9.072752283352279e-05, 'epoch': 0.2}
{'loss': 0.4835, 'learning_rate': 9.071349518250643e-05, 'epoch': 0.2}
{'loss': 0.4721, 'learning_rate': 9.069945801495813e-05, 'epoch': 0.2}
{'loss': 0.4919, 'learning_rate': 9.068541133415897e-05, 'epoch': 0.2}
{'loss': 1.0212, 'learning_rate': 9.067135514339229e-05, 'epoch': 0.2}
 20%|█▉        | 1281/6500 [8:27:59<43:45:18, 30.18s/it]                                                         20%|█▉        | 1281/6500 [8:27:59<43:45:18, 30.18s/it] 20%|█▉        | 1282/6500 [8:28:17<38:29:20, 26.55s/it]                                                         20%|█▉        | 1282/6500 [8:28:17<38:29:20, 26.55s/it] 20%|█▉        | 1283/6500 [8:28:36<34:49:23, 24.03s/it]                                                         20%|█▉        | 1283/6500 [8:28:36<34:49:23, 24.03s/it] 20%|█▉        | 1284/6500 [8:28:54<32:13:29, 22.24s/it]                                                         20%|█▉        | 1284/6500 [8:28:54<32:13:29, 22.24s/it] 20%|█▉        | 1285/6500 [8:29:12<30:24:53, 21.00s/it]                                                         20%|█▉        | 1285/6500 [8:29:12<30:24:53, 21.00s/it] 20%|█▉        | 1286/6500 [8:29:30<29:07:57, 20.11s/it]                                                         20%{'loss': 0.5004, 'learning_rate': 9.065728944594362e-05, 'epoch': 0.2}
{'loss': 0.4876, 'learning_rate': 9.064321424510074e-05, 'epoch': 0.2}
{'loss': 0.4911, 'learning_rate': 9.062912954415366e-05, 'epoch': 0.2}
{'loss': 0.4647, 'learning_rate': 9.061503534639457e-05, 'epoch': 0.2}
|█▉        | 1286/6500 [8:29:30<29:07:57, 20.11s/it] 20%|█▉        | 1287/6500 [8:29:48<28:15:40, 19.52s/it]                                                         20%|█▉        | 1287/6500 [8:29:48<28:15:40, 19.52s/it] 20%|█▉        | 1288/6500 [8:30:06<27:39:06, 19.10s/it]                                                         20%|█▉        | 1288/6500 [8:30:06<27:39:06, 19.10s/it] 20%|█▉        | 1289/6500 [8:30:24<27:13:34, 18.81s/it]                                                         20%|█▉        | 1289/6500 [8:30:24<27:13:34, 18.81s/it] 20%|█▉        | 1290/6500 [8:30:42<26:55:41, 18.61s/it]                                                         20%|█▉        | 1290/6500 [8:30:42<26:55:41, 18.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7809686064720154, 'eval_runtime': 5.3392, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.2}
                                                         20%|█▉        | 1290/6500 [8:30:48<26:55:41, 18.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1290
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1290
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1290

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1290/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5132, 'learning_rate': 9.060093165511789e-05, 'epoch': 0.2}
{'loss': 0.4856, 'learning_rate': 9.058681847362032e-05, 'epoch': 0.2}
{'loss': 0.4603, 'learning_rate': 9.05726958052007e-05, 'epoch': 0.2}
{'loss': 0.4835, 'learning_rate': 9.055856365316011e-05, 'epoch': 0.2}
{'loss': 0.4666, 'learning_rate': 9.054442202080188e-05, 'epoch': 0.2}
{'loss': 0.4923, 'learning_rate': 9.053027091143151e-05, 'epoch': 0.2}
 20%|█▉        | 1291/6500 [8:32:07<55:26:56, 38.32s/it]                                                         20%|█▉        | 1291/6500 [8:32:07<55:26:56, 38.32s/it] 20%|█▉        | 1292/6500 [8:32:25<46:36:39, 32.22s/it]                                                         20%|█▉        | 1292/6500 [8:32:25<46:36:39, 32.22s/it] 20%|█▉        | 1293/6500 [8:32:43<40:37:35, 28.09s/it]                                                         20%|█▉        | 1293/6500 [8:32:43<40:37:35, 28.09s/it] 20%|█▉        | 1294/6500 [8:33:01<36:14:07, 25.06s/it]                                                         20%|█▉        | 1294/6500 [8:33:01<36:14:07, 25.06s/it] 20%|█▉        | 1295/6500 [8:33:19<33:10:53, 22.95s/it]                                                         20%|█▉        | 1295/6500 [8:33:19<33:10:53, 22.95s/it] 20%|█▉        | 1296/6500 [8:33:38<31:13:07, 21.60s/it]                                                         20%{'loss': 0.4817, 'learning_rate': 9.051611032835675e-05, 'epoch': 0.2}
{'loss': 0.5039, 'learning_rate': 9.050194027488754e-05, 'epoch': 0.2}
{'loss': 0.4669, 'learning_rate': 9.048776075433604e-05, 'epoch': 0.2}
{'loss': 0.4999, 'learning_rate': 9.047357177001663e-05, 'epoch': 0.2}
|█▉        | 1296/6500 [8:33:38<31:13:07, 21.60s/it] 20%|█▉        | 1297/6500 [8:33:56<29:41:39, 20.55s/it]                                                         20%|█▉        | 1297/6500 [8:33:56<29:41:39, 20.55s/it] 20%|█▉        | 1298/6500 [8:34:14<28:38:10, 19.82s/it]                                                         20%|█▉        | 1298/6500 [8:34:14<28:38:10, 19.82s/it] 20%|█▉        | 1299/6500 [8:34:32<27:53:47, 19.31s/it]                                                         20%|█▉        | 1299/6500 [8:34:32<27:53:47, 19.31s/it] 20%|██        | 1300/6500 [8:34:50<27:22:36, 18.95s/it]                                                         20%|██        | 1300/6500 [8:34:50<27:22:36, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7733023166656494, 'eval_runtime': 5.3535, 'eval_samples_per_second': 4.296, 'eval_steps_per_second': 1.121, 'epoch': 0.2}
                                                         20%|██        | 1300/6500 [8:34:55<27:22:36, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1300
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1300
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1300

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1300/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1300/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5172, 'learning_rate': 9.045937332524592e-05, 'epoch': 0.2}
{'loss': 0.4902, 'learning_rate': 9.044516542334267e-05, 'epoch': 0.2}
{'loss': 0.4937, 'learning_rate': 9.043094806762793e-05, 'epoch': 0.2}
{'loss': 0.4912, 'learning_rate': 9.04167212614249e-05, 'epoch': 0.2}
{'loss': 0.4791, 'learning_rate': 9.0402485008059e-05, 'epoch': 0.2}
{'loss': 0.4767, 'learning_rate': 9.038823931085789e-05, 'epoch': 0.2}
 20%|██        | 1301/6500 [8:36:13<54:55:36, 38.03s/it]                                                         20%|██        | 1301/6500 [8:36:13<54:55:36, 38.03s/it] 20%|██        | 1302/6500 [8:36:31<46:13:53, 32.02s/it]                                                         20%|██        | 1302/6500 [8:36:31<46:13:53, 32.02s/it] 20%|██        | 1303/6500 [8:36:48<40:08:20, 27.80s/it]                                                         20%|██        | 1303/6500 [8:36:48<40:08:20, 27.80s/it] 20%|██        | 1304/6500 [8:37:06<35:53:40, 24.87s/it]                                                         20%|██        | 1304/6500 [8:37:06<35:53:40, 24.87s/it] 20%|██        | 1305/6500 [8:37:25<32:55:39, 22.82s/it]                                                         20%|██        | 1305/6500 [8:37:25<32:55:39, 22.82s/it] 20%|██        | 1306/6500 [8:37:43<30:51:55, 21.39s/it]                                                         20%{'loss': 0.5179, 'learning_rate': 9.037398417315142e-05, 'epoch': 0.2}
{'loss': 0.4837, 'learning_rate': 9.03597195982716e-05, 'epoch': 0.2}
{'loss': 0.4883, 'learning_rate': 9.034544558955274e-05, 'epoch': 0.2}
{'loss': 0.4717, 'learning_rate': 9.033116215033126e-05, 'epoch': 0.2}
|██        | 1306/6500 [8:37:43<30:51:55, 21.39s/it] 20%|██        | 1307/6500 [8:38:01<29:25:33, 20.40s/it]                                                         20%|██        | 1307/6500 [8:38:01<29:25:33, 20.40s/it] 20%|██        | 1308/6500 [8:38:19<28:25:56, 19.71s/it]                                                         20%|██        | 1308/6500 [8:38:19<28:25:56, 19.71s/it] 20%|██        | 1309/6500 [8:38:37<27:51:12, 19.32s/it]                                                         20%|██        | 1309/6500 [8:38:37<27:51:12, 19.32s/it] 20%|██        | 1310/6500 [8:38:55<27:20:19, 18.96s/it]                                                         20%|██        | 1310/6500 [8:38:55<27:20:19, 18.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7641118168830872, 'eval_runtime': 5.5026, 'eval_samples_per_second': 4.18, 'eval_steps_per_second': 1.09, 'epoch': 0.2}
                                                         20%|██        | 1310/6500 [8:39:01<27:20:19, 18.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1310
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1310
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1310
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1310
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1310/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.491, 'learning_rate': 9.031686928394584e-05, 'epoch': 0.2}
{'loss': 0.5449, 'learning_rate': 9.030256699373738e-05, 'epoch': 0.2}
{'loss': 0.4709, 'learning_rate': 9.028825528304892e-05, 'epoch': 0.2}
{'loss': 0.4729, 'learning_rate': 9.027393415522574e-05, 'epoch': 0.2}
{'loss': 0.4855, 'learning_rate': 9.025960361361531e-05, 'epoch': 0.2}
{'loss': 1.0111, 'learning_rate': 9.024526366156732e-05, 'epoch': 0.2}
 20%|██        | 1311/6500 [8:40:18<55:01:15, 38.17s/it]                                                         20%|██        | 1311/6500 [8:40:18<55:01:15, 38.17s/it] 20%|██        | 1312/6500 [8:40:36<46:16:45, 32.11s/it]                                                         20%|██        | 1312/6500 [8:40:36<46:16:45, 32.11s/it] 20%|██        | 1313/6500 [8:40:55<40:25:44, 28.06s/it]                                                         20%|██        | 1313/6500 [8:40:55<40:25:44, 28.06s/it] 20%|██        | 1314/6500 [8:41:13<36:05:40, 25.06s/it]                                                         20%|██        | 1314/6500 [8:41:13<36:05:40, 25.06s/it] 20%|██        | 1315/6500 [8:41:31<33:04:19, 22.96s/it]                                                         20%|██        | 1315/6500 [8:41:31<33:04:19, 22.96s/it] 20%|██        | 1316/6500 [8:41:49<30:57:08, 21.49s/it]                                                         20%{'loss': 0.4948, 'learning_rate': 9.023091430243367e-05, 'epoch': 0.2}
{'loss': 0.4811, 'learning_rate': 9.021655553956839e-05, 'epoch': 0.2}
{'loss': 0.4543, 'learning_rate': 9.020218737632778e-05, 'epoch': 0.2}
{'loss': 0.4599, 'learning_rate': 9.018780981607029e-05, 'epoch': 0.2}
|██        | 1316/6500 [8:41:49<30:57:08, 21.49s/it] 20%|██        | 1317/6500 [8:42:07<29:28:54, 20.48s/it]                                                         20%|██        | 1317/6500 [8:42:07<29:28:54, 20.48s/it] 20%|██        | 1318/6500 [8:42:25<28:27:38, 19.77s/it]                                                         20%|██        | 1318/6500 [8:42:25<28:27:38, 19.77s/it] 20%|██        | 1319/6500 [8:42:43<27:45:16, 19.29s/it]                                                         20%|██        | 1319/6500 [8:42:43<27:45:16, 19.29s/it] 20%|██        | 1320/6500 [8:43:02<27:15:52, 18.95s/it]                                                         20%|██        | 1320/6500 [8:43:02<27:15:52, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7743269205093384, 'eval_runtime': 5.3722, 'eval_samples_per_second': 4.281, 'eval_steps_per_second': 1.117, 'epoch': 0.2}
                                                         20%|██        | 1320/6500 [8:43:07<27:15:52, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1320

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1320
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1320

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1320/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1320/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5083, 'learning_rate': 9.01734228621566e-05, 'epoch': 0.2}
{'loss': 0.4611, 'learning_rate': 9.01590265179496e-05, 'epoch': 0.2}
{'loss': 0.4724, 'learning_rate': 9.014462078681431e-05, 'epoch': 0.2}
{'loss': 0.449, 'learning_rate': 9.013020567211799e-05, 'epoch': 0.2}
{'loss': 0.4813, 'learning_rate': 9.01157811772301e-05, 'epoch': 0.2}
{'loss': 0.4679, 'learning_rate': 9.010134730552224e-05, 'epoch': 0.2}
 20%|██        | 1321/6500 [8:44:31<57:29:56, 39.97s/it]                                                         20%|██        | 1321/6500 [8:44:31<57:29:56, 39.97s/it] 20%|██        | 1322/6500 [8:44:49<48:00:20, 33.38s/it]                                                         20%|██        | 1322/6500 [8:44:49<48:00:20, 33.38s/it] 20%|██        | 1323/6500 [8:45:07<41:23:34, 28.78s/it]                                                         20%|██        | 1323/6500 [8:45:07<41:23:34, 28.78s/it] 20%|██        | 1324/6500 [8:45:25<36:44:42, 25.56s/it]                                                         20%|██        | 1324/6500 [8:45:25<36:44:42, 25.56s/it] 20%|██        | 1325/6500 [8:45:43<33:30:29, 23.31s/it]                                                         20%|██        | 1325/6500 [8:45:43<33:30:29, 23.31s/it] 20%|██        | 1326/6500 [8:46:02<31:38:18, 22.01s/it]                                                         20%{'loss': 0.4661, 'learning_rate': 9.008690406036829e-05, 'epoch': 0.2}
{'loss': 0.5086, 'learning_rate': 9.007245144514425e-05, 'epoch': 0.2}
{'loss': 0.4698, 'learning_rate': 9.005798946322832e-05, 'epoch': 0.2}
{'loss': 0.5222, 'learning_rate': 9.004351811800091e-05, 'epoch': 0.2}
|██        | 1326/6500 [8:46:02<31:38:18, 22.01s/it] 20%|██        | 1327/6500 [8:46:20<29:57:14, 20.85s/it]                                                         20%|██        | 1327/6500 [8:46:20<29:57:14, 20.85s/it] 20%|██        | 1328/6500 [8:46:38<28:46:48, 20.03s/it]                                                         20%|██        | 1328/6500 [8:46:38<28:46:48, 20.03s/it] 20%|██        | 1329/6500 [8:46:56<27:58:01, 19.47s/it]                                                         20%|██        | 1329/6500 [8:46:56<27:58:01, 19.47s/it] 20%|██        | 1330/6500 [8:47:14<27:23:48, 19.08s/it]                                                         20%|██        | 1330/6500 [8:47:14<27:23:48, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7749544978141785, 'eval_runtime': 5.343, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.2}
                                                         20%|██        | 1330/6500 [8:47:20<27:23:48, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1330
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1330
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1330/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1330/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5038, 'learning_rate': 9.002903741284463e-05, 'epoch': 0.2}
{'loss': 0.4825, 'learning_rate': 9.001454735114421e-05, 'epoch': 0.2}
{'loss': 0.4852, 'learning_rate': 9.000004793628665e-05, 'epoch': 0.21}
{'loss': 0.4843, 'learning_rate': 8.998553917166108e-05, 'epoch': 0.21}
{'loss': 0.4674, 'learning_rate': 8.997102106065884e-05, 'epoch': 0.21}
{'loss': 0.4851, 'learning_rate': 8.995649360667348e-05, 'epoch': 0.21}
 20%|██        | 1331/6500 [8:48:16<45:45:52, 31.87s/it]                                                         20%|██        | 1331/6500 [8:48:16<45:45:52, 31.87s/it] 20%|██        | 1332/6500 [8:48:34<39:51:52, 27.77s/it]                                                         20%|██        | 1332/6500 [8:48:34<39:51:52, 27.77s/it] 21%|██        | 1333/6500 [8:48:52<35:40:42, 24.86s/it]                                                         21%|██        | 1333/6500 [8:48:52<35:40:42, 24.86s/it] 21%|██        | 1334/6500 [8:49:10<32:45:06, 22.82s/it]                                                         21%|██        | 1334/6500 [8:49:10<32:45:06, 22.82s/it] 21%|██        | 1335/6500 [8:49:29<30:42:11, 21.40s/it]                                                         21%|██        | 1335/6500 [8:49:29<30:42:11, 21.40s/it] 21%|██        | 1336/6500 [8:49:47<29:16:06, 20.40s/it]                                                         21%{'loss': 0.5095, 'learning_rate': 8.994195681310067e-05, 'epoch': 0.21}
{'loss': 0.4538, 'learning_rate': 8.99274106833383e-05, 'epoch': 0.21}
{'loss': 0.4788, 'learning_rate': 8.991285522078644e-05, 'epoch': 0.21}
{'loss': 0.4636, 'learning_rate': 8.989829042884735e-05, 'epoch': 0.21}
|██        | 1336/6500 [8:49:47<29:16:06, 20.40s/it] 21%|██        | 1337/6500 [8:50:05<28:16:21, 19.71s/it]                                                         21%|██        | 1337/6500 [8:50:05<28:16:21, 19.71s/it] 21%|██        | 1338/6500 [8:50:23<27:35:23, 19.24s/it]                                                         21%|██        | 1338/6500 [8:50:23<27:35:23, 19.24s/it] 21%|██        | 1339/6500 [8:50:41<27:07:04, 18.92s/it]                                                         21%|██        | 1339/6500 [8:50:41<27:07:04, 18.92s/it] 21%|██        | 1340/6500 [8:50:59<26:47:25, 18.69s/it]                                                         21%|██        | 1340/6500 [8:50:59<26:47:25, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7659698724746704, 'eval_runtime': 5.3428, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.21}
                                                         21%|██        | 1340/6500 [8:51:04<26:47:25, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1340

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1340
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1340/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5098, 'learning_rate': 8.988371631092547e-05, 'epoch': 0.21}
{'loss': 0.5196, 'learning_rate': 8.986913287042739e-05, 'epoch': 0.21}
{'loss': 0.4638, 'learning_rate': 8.985454011076191e-05, 'epoch': 0.21}
{'loss': 0.4801, 'learning_rate': 8.983993803533999e-05, 'epoch': 0.21}
{'loss': 0.4702, 'learning_rate': 8.98253266475748e-05, 'epoch': 0.21}
{'loss': 1.0027, 'learning_rate': 8.981070595088164e-05, 'epoch': 0.21}
 21%|██        | 1341/6500 [8:51:56<43:22:38, 30.27s/it]                                                         21%|██        | 1341/6500 [8:51:56<43:22:38, 30.27s/it] 21%|██        | 1342/6500 [8:52:15<38:15:53, 26.71s/it]                                                         21%|██        | 1342/6500 [8:52:15<38:15:53, 26.71s/it] 21%|██        | 1343/6500 [8:52:33<34:32:34, 24.11s/it]                                                         21%|██        | 1343/6500 [8:52:33<34:32:34, 24.11s/it] 21%|██        | 1344/6500 [8:52:53<32:42:32, 22.84s/it]                                                         21%|██        | 1344/6500 [8:52:53<32:42:32, 22.84s/it] 21%|██        | 1345/6500 [8:53:11<30:46:52, 21.50s/it]                                                         21%|██        | 1345/6500 [8:53:11<30:46:52, 21.50s/it] 21%|██        | 1346/6500 [8:53:29<29:18:57, 20.48s/it]                                                         21%{'loss': 0.4738, 'learning_rate': 8.979607594867802e-05, 'epoch': 0.21}
{'loss': 0.4857, 'learning_rate': 8.978143664438361e-05, 'epoch': 0.21}
{'loss': 0.4413, 'learning_rate': 8.976678804142025e-05, 'epoch': 0.21}
{'loss': 0.4976, 'learning_rate': 8.975213014321198e-05, 'epoch': 0.21}
|██        | 1346/6500 [8:53:29<29:18:57, 20.48s/it] 21%|██        | 1347/6500 [8:53:47<28:18:25, 19.78s/it]                                                         21%|██        | 1347/6500 [8:53:47<28:18:25, 19.78s/it] 21%|██        | 1348/6500 [8:54:07<28:02:06, 19.59s/it]                                                         21%|██        | 1348/6500 [8:54:07<28:02:06, 19.59s/it] 21%|██        | 1349/6500 [8:54:25<27:24:46, 19.16s/it]                                                         21%|██        | 1349/6500 [8:54:25<27:24:46, 19.16s/it] 21%|██        | 1350/6500 [8:54:43<27:01:25, 18.89s/it]                                                         21%|██        | 1350/6500 [8:54:43<27:01:25, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7746984362602234, 'eval_runtime': 6.4111, 'eval_samples_per_second': 3.588, 'eval_steps_per_second': 0.936, 'epoch': 0.21}
                                                         21%|██        | 1350/6500 [8:54:49<27:01:25, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1350
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1350
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1350/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4728, 'learning_rate': 8.9737462953185e-05, 'epoch': 0.21}
{'loss': 0.4472, 'learning_rate': 8.972278647476764e-05, 'epoch': 0.21}
{'loss': 0.462, 'learning_rate': 8.970810071139047e-05, 'epoch': 0.21}
{'loss': 0.4541, 'learning_rate': 8.969340566648619e-05, 'epoch': 0.21}
{'loss': 0.4729, 'learning_rate': 8.967870134348966e-05, 'epoch': 0.21}
{'loss': 0.4642, 'learning_rate': 8.966398774583795e-05, 'epoch': 0.21}
 21%|██        | 1351/6500 [8:56:28<63:51:42, 44.65s/it]                                                         21%|██        | 1351/6500 [8:56:28<63:51:42, 44.65s/it] 21%|██        | 1352/6500 [8:56:46<52:23:26, 36.64s/it]                                                         21%|██        | 1352/6500 [8:56:46<52:23:26, 36.64s/it] 21%|██        | 1353/6500 [8:57:04<44:24:49, 31.06s/it]                                                         21%|██        | 1353/6500 [8:57:04<44:24:49, 31.06s/it] 21%|██        | 1354/6500 [8:57:22<38:48:09, 27.15s/it]                                                         21%|██        | 1354/6500 [8:57:22<38:48:09, 27.15s/it] 21%|██        | 1355/6500 [8:57:40<34:53:55, 24.42s/it]                                                         21%|██        | 1355/6500 [8:57:40<34:53:55, 24.42s/it] 21%|██        | 1356/6500 [8:57:58<32:10:43, 22.52s/it]                                                         21%{'loss': 0.4714, 'learning_rate': 8.964926487697027e-05, 'epoch': 0.21}
{'loss': 0.4741, 'learning_rate': 8.9634532740328e-05, 'epoch': 0.21}
{'loss': 0.4627, 'learning_rate': 8.961979133935468e-05, 'epoch': 0.21}
{'loss': 0.5198, 'learning_rate': 8.960504067749602e-05, 'epoch': 0.21}
|██        | 1356/6500 [8:57:58<32:10:43, 22.52s/it] 21%|██        | 1357/6500 [8:58:16<30:16:44, 21.19s/it]                                                         21%|██        | 1357/6500 [8:58:16<30:16:44, 21.19s/it] 21%|██        | 1358/6500 [8:58:35<29:09:36, 20.42s/it]                                                         21%|██        | 1358/6500 [8:58:35<29:09:36, 20.42s/it] 21%|██        | 1359/6500 [8:58:53<28:11:30, 19.74s/it]                                                         21%|██        | 1359/6500 [8:58:53<28:11:30, 19.74s/it] 21%|██        | 1360/6500 [8:59:11<27:30:17, 19.26s/it]                                                         21%|██        | 1360/6500 [8:59:11<27:30:17, 19.26s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7763218283653259, 'eval_runtime': 5.4775, 'eval_samples_per_second': 4.199, 'eval_steps_per_second': 1.095, 'epoch': 0.21}
                                                         21%|██        | 1360/6500 [8:59:16<27:30:17, 19.26s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1360

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1360
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1360

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1360/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4638, 'learning_rate': 8.959028075819992e-05, 'epoch': 0.21}
{'loss': 0.4779, 'learning_rate': 8.957551158491639e-05, 'epoch': 0.21}
{'loss': 0.4859, 'learning_rate': 8.956073316109766e-05, 'epoch': 0.21}
{'loss': 0.4472, 'learning_rate': 8.954594549019808e-05, 'epoch': 0.21}
{'loss': 0.4665, 'learning_rate': 8.953114857567419e-05, 'epoch': 0.21}
{'loss': 0.4895, 'learning_rate': 8.951634242098468e-05, 'epoch': 0.21}
 21%|██        | 1361/6500 [9:00:22<49:45:34, 34.86s/it]                                                         21%|██        | 1361/6500 [9:00:22<49:45:34, 34.86s/it] 21%|██        | 1362/6500 [9:00:40<42:32:27, 29.81s/it]                                                         21%|██        | 1362/6500 [9:00:40<42:32:27, 29.81s/it] 21%|██        | 1363/6500 [9:00:58<37:29:32, 26.27s/it]                                                         21%|██        | 1363/6500 [9:00:58<37:29:32, 26.27s/it] 21%|██        | 1364/6500 [9:01:16<34:00:34, 23.84s/it]                                                         21%|██        | 1364/6500 [9:01:16<34:00:34, 23.84s/it] 21%|██        | 1365/6500 [9:01:34<31:32:33, 22.11s/it]                                                         21%|██        | 1365/6500 [9:01:34<31:32:33, 22.11s/it] 21%|██        | 1366/6500 [9:01:52<29:49:22, 20.91s/it]                                                         21%{'loss': 0.4823, 'learning_rate': 8.950152702959038e-05, 'epoch': 0.21}
{'loss': 0.4683, 'learning_rate': 8.94867024049543e-05, 'epoch': 0.21}
{'loss': 0.4645, 'learning_rate': 8.947186855054164e-05, 'epoch': 0.21}
{'loss': 0.4567, 'learning_rate': 8.945702546981969e-05, 'epoch': 0.21}
|██        | 1366/6500 [9:01:53<29:49:22, 20.91s/it] 21%|██        | 1367/6500 [9:02:11<28:37:27, 20.08s/it]                                                         21%|██        | 1367/6500 [9:02:11<28:37:27, 20.08s/it] 21%|██        | 1368/6500 [9:02:29<27:47:07, 19.49s/it]                                                         21%|██        | 1368/6500 [9:02:29<27:47:07, 19.49s/it] 21%|██        | 1369/6500 [9:02:47<27:12:18, 19.09s/it]                                                         21%|██        | 1369/6500 [9:02:47<27:12:18, 19.09s/it] 21%|██        | 1370/6500 [9:03:05<26:48:20, 18.81s/it]                                                         21%|██        | 1370/6500 [9:03:05<26:48:20, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7646741271018982, 'eval_runtime': 5.3553, 'eval_samples_per_second': 4.295, 'eval_steps_per_second': 1.12, 'epoch': 0.21}
                                                         21%|██        | 1370/6500 [9:03:10<26:48:20, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1370
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1370
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1370
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1370
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1370/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1370/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5621, 'learning_rate': 8.944217316625793e-05, 'epoch': 0.21}
{'loss': 0.4535, 'learning_rate': 8.942731164332802e-05, 'epoch': 0.21}
{'loss': 0.452, 'learning_rate': 8.941244090450372e-05, 'epoch': 0.21}
{'loss': 0.4775, 'learning_rate': 8.9397560953261e-05, 'epoch': 0.21}
{'loss': 1.0041, 'learning_rate': 8.938267179307796e-05, 'epoch': 0.21}
{'loss': 0.4747, 'learning_rate': 8.936777342743481e-05, 'epoch': 0.21}
 21%|██        | 1371/6500 [9:04:24<52:40:21, 36.97s/it]                                                         21%|██        | 1371/6500 [9:04:24<52:40:21, 36.97s/it] 21%|██        | 1372/6500 [9:04:42<44:33:00, 31.28s/it]                                                         21%|██        | 1372/6500 [9:04:42<44:33:00, 31.28s/it] 21%|██        | 1373/6500 [9:05:00<38:51:33, 27.29s/it]                                                         21%|██        | 1373/6500 [9:05:00<38:51:33, 27.29s/it] 21%|██        | 1374/6500 [9:05:19<35:00:46, 24.59s/it]                                                         21%|██        | 1374/6500 [9:05:19<35:00:46, 24.59s/it] 21%|██        | 1375/6500 [9:05:37<32:11:29, 22.61s/it]                                                         21%|██        | 1375/6500 [9:05:37<32:11:29, 22.61s/it] 21%|██        | 1376/6500 [9:05:55<30:14:47, 21.25s/it]                                                         21%{'loss': 0.4716, 'learning_rate': 8.935286585981399e-05, 'epoch': 0.21}
{'loss': 0.4659, 'learning_rate': 8.933794909370006e-05, 'epoch': 0.21}
{'loss': 0.445, 'learning_rate': 8.93230231325797e-05, 'epoch': 0.21}
{'loss': 0.4913, 'learning_rate': 8.930808797994177e-05, 'epoch': 0.21}
|██        | 1376/6500 [9:05:55<30:14:47, 21.25s/it] 21%|██        | 1377/6500 [9:06:13<28:52:42, 20.29s/it]                                                         21%|██        | 1377/6500 [9:06:13<28:52:42, 20.29s/it] 21%|██        | 1378/6500 [9:06:31<27:56:02, 19.63s/it]                                                         21%|██        | 1378/6500 [9:06:31<27:56:02, 19.63s/it] 21%|██        | 1379/6500 [9:06:49<27:16:39, 19.18s/it]                                                         21%|██        | 1379/6500 [9:06:49<27:16:39, 19.18s/it] 21%|██        | 1380/6500 [9:07:07<26:49:50, 18.87s/it]                                                         21%|██        | 1380/6500 [9:07:07<26:49:50, 18.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7751954793930054, 'eval_runtime': 5.6288, 'eval_samples_per_second': 4.086, 'eval_steps_per_second': 1.066, 'epoch': 0.21}
                                                         21%|██        | 1380/6500 [9:07:13<26:49:50, 18.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1380
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1380
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1380

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1380/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4571, 'learning_rate': 8.929314363927727e-05, 'epoch': 0.21}
{'loss': 0.4482, 'learning_rate': 8.927819011407937e-05, 'epoch': 0.21}
{'loss': 0.4523, 'learning_rate': 8.926322740784332e-05, 'epoch': 0.21}
{'loss': 0.4537, 'learning_rate': 8.92482555240666e-05, 'epoch': 0.21}
{'loss': 0.4646, 'learning_rate': 8.923327446624878e-05, 'epoch': 0.21}
 21%|██        | 1381/6500 [9:08:18<48:48:58, 34.33s/it]                                                         21%|██        | 1381/6500 [9:08:18<48:48:58, 34.33s/it] 21%|██▏       | 1382/6500 [9:08:36<41:50:47, 29.43s/it]                                                         21%|██▏       | 1382/6500 [9:08:36<41:50:47, 29.43s/it] 21%|██▏       | 1383/6500 [9:08:54<36:57:30, 26.00s/it]                                                         21%|██▏       | 1383/6500 [9:08:54<36:57:30, 26.00s/it] 21%|██▏       | 1384/6500 [9:09:12<33:32:22, 23.60s/it]                                                         21%|██▏       | 1384/6500 [9:09:12<33:32:22, 23.60s/it] 21%|██▏       | 1385/6500 [9:09:30<31:09:23, 21.93s/it]                                                         21%|██▏       | 1385/6500 [9:09:30<31:09:23, 21.93s/it] 21%|██▏       | 1386/6500 [9:09:48<29:29:40, 20.76s/it]                                            {'loss': 0.4477, 'learning_rate': 8.921828423789158e-05, 'epoch': 0.21}
{'loss': 0.4856, 'learning_rate': 8.920328484249892e-05, 'epoch': 0.21}
{'loss': 0.4508, 'learning_rate': 8.918827628357677e-05, 'epoch': 0.21}
{'loss': 0.4851, 'learning_rate': 8.917325856463331e-05, 'epoch': 0.21}
{'loss': 0.4919, 'learning_rate': 8.915823168917884e-05, 'epoch': 0.21}
             21%|██▏       | 1386/6500 [9:09:48<29:29:40, 20.76s/it] 21%|██▏       | 1387/6500 [9:10:06<28:20:12, 19.95s/it]                                                         21%|██▏       | 1387/6500 [9:10:06<28:20:12, 19.95s/it] 21%|██▏       | 1388/6500 [9:10:24<27:33:22, 19.41s/it]                                                         21%|██▏       | 1388/6500 [9:10:24<27:33:22, 19.41s/it] 21%|██▏       | 1389/6500 [9:10:42<27:00:19, 19.02s/it]                                                         21%|██▏       | 1389/6500 [9:10:42<27:00:19, 19.02s/it] 21%|██▏       | 1390/6500 [9:11:01<26:48:19, 18.88s/it]                                                         21%|██▏       | 1390/6500 [9:11:01<26:48:19, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7786988019943237, 'eval_runtime': 5.4842, 'eval_samples_per_second': 4.194, 'eval_steps_per_second': 1.094, 'epoch': 0.21}
                                                         21%|██▏       | 1390/6500 [9:11:06<26:48:19, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1390the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1390
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1390/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4658, 'learning_rate': 8.91431956607258e-05, 'epoch': 0.21}
{'loss': 0.4769, 'learning_rate': 8.912815048278876e-05, 'epoch': 0.21}
{'loss': 0.4769, 'learning_rate': 8.911309615888446e-05, 'epoch': 0.21}
{'loss': 0.4679, 'learning_rate': 8.909803269253174e-05, 'epoch': 0.21}
{'loss': 0.4577, 'learning_rate': 8.908296008725161e-05, 'epoch': 0.21}
 21%|██▏       | 1391/6500 [9:11:56<42:31:10, 29.96s/it]                                                         21%|██▏       | 1391/6500 [9:11:56<42:31:10, 29.96s/it] 21%|██▏       | 1392/6500 [9:12:14<37:25:44, 26.38s/it]                                                         21%|██▏       | 1392/6500 [9:12:14<37:25:44, 26.38s/it] 21%|██▏       | 1393/6500 [9:12:32<33:52:12, 23.88s/it]                                                         21%|██▏       | 1393/6500 [9:12:32<33:52:12, 23.88s/it] 21%|██▏       | 1394/6500 [9:12:50<31:22:25, 22.12s/it]                                                         21%|██▏       | 1394/6500 [9:12:50<31:22:25, 22.12s/it] 21%|██▏       | 1395/6500 [9:13:08<29:38:14, 20.90s/it]                                                         21%|██▏       | 1395/6500 [9:13:08<29:38:14, 20.90s/it] 21%|██▏       | 1396/6500 [9:13:27<28:25:41, 20.05s/it]                                        {'loss': 0.4967, 'learning_rate': 8.906787834656717e-05, 'epoch': 0.21}
{'loss': 0.4631, 'learning_rate': 8.905278747400369e-05, 'epoch': 0.21}
{'loss': 0.4588, 'learning_rate': 8.903768747308861e-05, 'epoch': 0.22}
{'loss': 0.4399, 'learning_rate': 8.902257834735144e-05, 'epoch': 0.22}
{'loss': 0.4837, 'learning_rate': 8.900746010032383e-05, 'epoch': 0.22}
                 21%|██▏       | 1396/6500 [9:13:27<28:25:41, 20.05s/it] 21%|██▏       | 1397/6500 [9:13:45<27:35:28, 19.46s/it]                                                         21%|██▏       | 1397/6500 [9:13:45<27:35:28, 19.46s/it] 22%|██▏       | 1398/6500 [9:14:03<27:00:34, 19.06s/it]                                                         22%|██▏       | 1398/6500 [9:14:03<27:00:34, 19.06s/it] 22%|██▏       | 1399/6500 [9:14:21<26:36:55, 18.78s/it]                                                         22%|██▏       | 1399/6500 [9:14:21<26:36:55, 18.78s/it] 22%|██▏       | 1400/6500 [9:14:39<26:20:18, 18.59s/it]                                                         22%|██▏       | 1400/6500 [9:14:39<26:20:18, 18.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7731134295463562, 'eval_runtime': 5.3506, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.121, 'epoch': 0.22}
                                                         22%|██▏       | 1400/6500 [9:14:44<26:20:18, 18.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1400 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1400
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1400/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5259, 'learning_rate': 8.899233273553958e-05, 'epoch': 0.22}
{'loss': 0.465, 'learning_rate': 8.897719625653465e-05, 'epoch': 0.22}
{'loss': 0.449, 'learning_rate': 8.896205066684707e-05, 'epoch': 0.22}
{'loss': 0.4699, 'learning_rate': 8.894689597001704e-05, 'epoch': 0.22}
{'loss': 0.9967, 'learning_rate': 8.893173216958687e-05, 'epoch': 0.22}
 22%|██▏       | 1401/6500 [9:15:30<40:17:54, 28.45s/it]                                                         22%|██▏       | 1401/6500 [9:15:30<40:17:54, 28.45s/it] 22%|██▏       | 1402/6500 [9:15:49<35:51:55, 25.33s/it]                                                         22%|██▏       | 1402/6500 [9:15:49<35:51:55, 25.33s/it] 22%|██▏       | 1403/6500 [9:16:07<32:45:28, 23.14s/it]                                                         22%|██▏       | 1403/6500 [9:16:07<32:45:28, 23.14s/it] 22%|██▏       | 1404/6500 [9:16:25<30:35:29, 21.61s/it]                                                         22%|██▏       | 1404/6500 [9:16:25<30:35:29, 21.61s/it] 22%|██▏       | 1405/6500 [9:16:43<29:04:20, 20.54s/it]                                                         22%|██▏       | 1405/6500 [9:16:43<29:04:20, 20.54s/it] 22%|██▏       | 1406/6500 [9:17:01<28:14:59, 19.96s/it]                                        {'loss': 0.4791, 'learning_rate': 8.891655926910103e-05, 'epoch': 0.22}
{'loss': 0.4522, 'learning_rate': 8.890137727210607e-05, 'epoch': 0.22}
{'loss': 0.4389, 'learning_rate': 8.88861861821507e-05, 'epoch': 0.22}
{'loss': 0.4516, 'learning_rate': 8.887098600278573e-05, 'epoch': 0.22}
{'loss': 0.4926, 'learning_rate': 8.885577673756414e-05, 'epoch': 0.22}
                 22%|██▏       | 1406/6500 [9:17:01<28:14:59, 19.96s/it] 22%|██▏       | 1407/6500 [9:17:19<27:27:19, 19.41s/it]                                                         22%|██▏       | 1407/6500 [9:17:19<27:27:19, 19.41s/it] 22%|██▏       | 1408/6500 [9:17:37<26:54:09, 19.02s/it]                                                         22%|██▏       | 1408/6500 [9:17:37<26:54:09, 19.02s/it] 22%|██▏       | 1409/6500 [9:17:56<26:31:18, 18.75s/it]                                                         22%|██▏       | 1409/6500 [9:17:56<26:31:18, 18.75s/it] 22%|██▏       | 1410/6500 [9:18:14<26:15:25, 18.57s/it]                                                         22%|██▏       | 1410/6500 [9:18:14<26:15:25, 18.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7762330770492554, 'eval_runtime': 5.3285, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.22}
                                                         22%|██▏       | 1410/6500 [9:18:19<26:15:25, 18.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1410
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1410
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1410

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1410/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4462, 'learning_rate': 8.884055839004098e-05, 'epoch': 0.22}
{'loss': 0.4527, 'learning_rate': 8.882533096377344e-05, 'epoch': 0.22}
{'loss': 0.4381, 'learning_rate': 8.881009446232086e-05, 'epoch': 0.22}
{'loss': 0.4537, 'learning_rate': 8.879484888924467e-05, 'epoch': 0.22}
{'loss': 0.4579, 'learning_rate': 8.877959424810843e-05, 'epoch': 0.22}
 22%|██▏       | 1411/6500 [9:19:42<55:36:29, 39.34s/it]                                                         22%|██▏       | 1411/6500 [9:19:42<55:36:29, 39.34s/it] 22%|██▏       | 1412/6500 [9:20:00<46:33:05, 32.94s/it]                                                         22%|██▏       | 1412/6500 [9:20:00<46:33:05, 32.94s/it] 22%|██▏       | 1413/6500 [9:20:18<40:12:07, 28.45s/it]                                                         22%|██▏       | 1413/6500 [9:20:18<40:12:07, 28.45s/it] 22%|██▏       | 1414/6500 [9:20:36<35:47:03, 25.33s/it]                                                         22%|██▏       | 1414/6500 [9:20:36<35:47:03, 25.33s/it] 22%|██▏       | 1415/6500 [9:20:54<32:41:35, 23.15s/it]                                                         22%|██▏       | 1415/6500 [9:20:54<32:41:35, 23.15s/it] 22%|██▏       | 1416/6500 [9:21:12<30:32:31, 21.63s/it]                                        {'loss': 0.4471, 'learning_rate': 8.87643305424778e-05, 'epoch': 0.22}
{'loss': 0.4759, 'learning_rate': 8.87490577759206e-05, 'epoch': 0.22}
{'loss': 0.4457, 'learning_rate': 8.873377595200676e-05, 'epoch': 0.22}
{'loss': 0.4935, 'learning_rate': 8.871848507430829e-05, 'epoch': 0.22}
{'loss': 0.4691, 'learning_rate': 8.870318514639935e-05, 'epoch': 0.22}
                 22%|██▏       | 1416/6500 [9:21:12<30:32:31, 21.63s/it] 22%|██▏       | 1417/6500 [9:21:30<29:02:29, 20.57s/it]                                                         22%|██▏       | 1417/6500 [9:21:30<29:02:29, 20.57s/it] 22%|██▏       | 1418/6500 [9:21:48<27:59:49, 19.83s/it]                                                         22%|██▏       | 1418/6500 [9:21:48<27:59:49, 19.83s/it] 22%|██▏       | 1419/6500 [9:22:06<27:16:13, 19.32s/it]                                                         22%|██▏       | 1419/6500 [9:22:06<27:16:13, 19.32s/it] 22%|██▏       | 1420/6500 [9:22:24<26:46:26, 18.97s/it]                                                         22%|██▏       | 1420/6500 [9:22:24<26:46:26, 18.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7775291800498962, 'eval_runtime': 5.35, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.121, 'epoch': 0.22}
                                                         22%|██▏       | 1420/6500 [9:22:30<26:46:26, 18.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1420
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1420
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1420

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1420/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1420/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4537, 'learning_rate': 8.868787617185619e-05, 'epoch': 0.22}
{'loss': 0.4692, 'learning_rate': 8.86725581542572e-05, 'epoch': 0.22}
{'loss': 0.4591, 'learning_rate': 8.865723109718288e-05, 'epoch': 0.22}
{'loss': 0.4581, 'learning_rate': 8.864189500421582e-05, 'epoch': 0.22}
{'loss': 0.4569, 'learning_rate': 8.862654987894076e-05, 'epoch': 0.22}
 22%|██▏       | 1421/6500 [9:24:05<61:30:54, 43.60s/it]                                                         22%|██▏       | 1421/6500 [9:24:05<61:30:54, 43.60s/it] 22%|██▏       | 1422/6500 [9:24:23<50:39:36, 35.92s/it]                                                         22%|██▏       | 1422/6500 [9:24:23<50:39:36, 35.92s/it] 22%|██▏       | 1423/6500 [9:24:42<43:13:46, 30.65s/it]                                                         22%|██▏       | 1423/6500 [9:24:42<43:13:46, 30.65s/it] 22%|██▏       | 1424/6500 [9:25:00<37:52:44, 26.86s/it]                                                         22%|██▏       | 1424/6500 [9:25:00<37:52:44, 26.86s/it] 22%|██▏       | 1425/6500 [9:25:18<34:11:21, 24.25s/it]                                                         22%|██▏       | 1425/6500 [9:25:18<34:11:21, 24.25s/it] 22%|██▏       | 1426/6500 [9:25:36<31:34:29, 22.40s/it]                                        {'loss': 0.4838, 'learning_rate': 8.861119572494453e-05, 'epoch': 0.22}
{'loss': 0.4483, 'learning_rate': 8.859583254581605e-05, 'epoch': 0.22}
{'loss': 0.4616, 'learning_rate': 8.858046034514637e-05, 'epoch': 0.22}
{'loss': 0.4374, 'learning_rate': 8.856507912652867e-05, 'epoch': 0.22}
{'loss': 0.5006, 'learning_rate': 8.85496888935582e-05, 'epoch': 0.22}
                 22%|██▏       | 1426/6500 [9:25:36<31:34:29, 22.40s/it] 22%|██▏       | 1427/6500 [9:25:54<29:44:48, 21.11s/it]                                                         22%|██▏       | 1427/6500 [9:25:54<29:44:48, 21.11s/it] 22%|██▏       | 1428/6500 [9:26:12<28:28:37, 20.21s/it]                                                         22%|██▏       | 1428/6500 [9:26:12<28:28:37, 20.21s/it] 22%|██▏       | 1429/6500 [9:26:30<27:35:39, 19.59s/it]                                                         22%|██▏       | 1429/6500 [9:26:30<27:35:39, 19.59s/it] 22%|██▏       | 1430/6500 [9:26:49<27:04:54, 19.23s/it]                                                         22%|██▏       | 1430/6500 [9:26:49<27:04:54, 19.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7719859480857849, 'eval_runtime': 5.357, 'eval_samples_per_second': 4.293, 'eval_steps_per_second': 1.12, 'epoch': 0.22}
                                                         22%|██▏       | 1430/6500 [9:26:54<27:04:54, 19.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1430 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4954, 'learning_rate': 8.853428964983233e-05, 'epoch': 0.22}
{'loss': 0.4393, 'learning_rate': 8.851888139895057e-05, 'epoch': 0.22}
{'loss': 0.4562, 'learning_rate': 8.850346414451445e-05, 'epoch': 0.22}
{'loss': 0.5581, 'learning_rate': 8.84880378901277e-05, 'epoch': 0.22}
{'loss': 0.88, 'learning_rate': 8.847260263939612e-05, 'epoch': 0.22}
 22%|██▏       | 1431/6500 [9:27:40<40:51:23, 29.02s/it]                                                         22%|██▏       | 1431/6500 [9:27:40<40:51:23, 29.02s/it] 22%|██▏       | 1432/6500 [9:27:59<36:12:52, 25.72s/it]                                                         22%|██▏       | 1432/6500 [9:27:59<36:12:52, 25.72s/it] 22%|██▏       | 1433/6500 [9:28:17<32:58:01, 23.42s/it]                                                         22%|██▏       | 1433/6500 [9:28:17<32:58:01, 23.42s/it] 22%|██▏       | 1434/6500 [9:28:35<30:41:26, 21.81s/it]                                                         22%|██▏       | 1434/6500 [9:28:35<30:41:26, 21.81s/it] 22%|██▏       | 1435/6500 [9:28:53<29:05:58, 20.68s/it]                                                         22%|██▏       | 1435/6500 [9:28:53<29:05:58, 20.68s/it] 22%|██▏       | 1436/6500 [9:29:11<27:59:29, 19.90s/it]                                        {'loss': 0.4672, 'learning_rate': 8.845715839592758e-05, 'epoch': 0.22}
{'loss': 0.4564, 'learning_rate': 8.844170516333208e-05, 'epoch': 0.22}
{'loss': 0.4279, 'learning_rate': 8.842624294522174e-05, 'epoch': 0.22}
{'loss': 0.4709, 'learning_rate': 8.841077174521075e-05, 'epoch': 0.22}
{'loss': 0.458, 'learning_rate': 8.83952915669154e-05, 'epoch': 0.22}
                 22%|██▏       | 1436/6500 [9:29:11<27:59:29, 19.90s/it] 22%|██▏       | 1437/6500 [9:29:29<27:13:14, 19.36s/it]                                                         22%|██▏       | 1437/6500 [9:29:29<27:13:14, 19.36s/it] 22%|██▏       | 1438/6500 [9:29:47<26:41:14, 18.98s/it]                                                         22%|██▏       | 1438/6500 [9:29:47<26:41:14, 18.98s/it] 22%|██▏       | 1439/6500 [9:30:06<26:30:59, 18.86s/it]                                                         22%|██▏       | 1439/6500 [9:30:06<26:30:59, 18.86s/it] 22%|██▏       | 1440/6500 [9:30:24<26:12:42, 18.65s/it]                                                         22%|██▏       | 1440/6500 [9:30:24<26:12:42, 18.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7730333805084229, 'eval_runtime': 5.4823, 'eval_samples_per_second': 4.195, 'eval_steps_per_second': 1.094, 'epoch': 0.22}
                                                         22%|██▏       | 1440/6500 [9:30:29<26:12:42, 18.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1440/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1440/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4206, 'learning_rate': 8.837980241395408e-05, 'epoch': 0.22}
{'loss': 0.4505, 'learning_rate': 8.836430428994732e-05, 'epoch': 0.22}
{'loss': 0.4332, 'learning_rate': 8.834879719851768e-05, 'epoch': 0.22}
{'loss': 0.4457, 'learning_rate': 8.833328114328986e-05, 'epoch': 0.22}
{'loss': 0.4419, 'learning_rate': 8.831775612789063e-05, 'epoch': 0.22}
 22%|██▏       | 1441/6500 [9:31:17<40:55:01, 29.12s/it]                                                         22%|██▏       | 1441/6500 [9:31:17<40:55:01, 29.12s/it] 22%|██▏       | 1442/6500 [9:31:35<36:14:39, 25.80s/it]                                                         22%|██▏       | 1442/6500 [9:31:35<36:14:39, 25.80s/it] 22%|██▏       | 1443/6500 [9:31:53<32:58:10, 23.47s/it]                                                         22%|██▏       | 1443/6500 [9:31:53<32:58:10, 23.47s/it] 22%|██▏       | 1444/6500 [9:32:11<30:41:11, 21.85s/it]                                                         22%|██▏       | 1444/6500 [9:32:11<30:41:11, 21.85s/it] 22%|██▏       | 1445/6500 [9:32:29<29:05:22, 20.72s/it]                                                         22%|██▏       | 1445/6500 [9:32:29<29:05:22, 20.72s/it] 22%|██▏       | 1446/6500 [9:32:48<27:58:26, 19.93s/it]                                        {'loss': 0.4552, 'learning_rate': 8.83022221559489e-05, 'epoch': 0.22}
{'loss': 0.4493, 'learning_rate': 8.828667923109563e-05, 'epoch': 0.22}
{'loss': 0.444, 'learning_rate': 8.827112735696385e-05, 'epoch': 0.22}
{'loss': 0.4902, 'learning_rate': 8.825556653718876e-05, 'epoch': 0.22}
{'loss': 0.4548, 'learning_rate': 8.82399967754076e-05, 'epoch': 0.22}
                 22%|██▏       | 1446/6500 [9:32:48<27:58:26, 19.93s/it] 22%|██▏       | 1447/6500 [9:33:06<27:11:46, 19.38s/it]                                                         22%|██▏       | 1447/6500 [9:33:06<27:11:46, 19.38s/it] 22%|██▏       | 1448/6500 [9:33:24<26:39:53, 19.00s/it]                                                         22%|██▏       | 1448/6500 [9:33:24<26:39:53, 19.00s/it] 22%|██▏       | 1449/6500 [9:33:42<26:18:09, 18.75s/it]                                                         22%|██▏       | 1449/6500 [9:33:42<26:18:09, 18.75s/it] 22%|██▏       | 1450/6500 [9:34:00<26:03:07, 18.57s/it]                                                         22%|██▏       | 1450/6500 [9:34:00<26:03:07, 18.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7794604301452637, 'eval_runtime': 5.3424, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.22}
                                                         22%|██▏       | 1450/6500 [9:34:05<26:03:07, 18.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1450the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1450
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1450

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1450/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1450/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4626, 'learning_rate': 8.822441807525967e-05, 'epoch': 0.22}
{'loss': 0.4683, 'learning_rate': 8.820883044038644e-05, 'epoch': 0.22}
{'loss': 0.4391, 'learning_rate': 8.81932338744314e-05, 'epoch': 0.22}
{'loss': 0.4573, 'learning_rate': 8.817762838104016e-05, 'epoch': 0.22}
{'loss': 0.4679, 'learning_rate': 8.816201396386042e-05, 'epoch': 0.22}
 22%|██▏       | 1451/6500 [9:35:12<48:31:00, 34.59s/it]                                                         22%|██▏       | 1451/6500 [9:35:12<48:31:00, 34.59s/it] 22%|██▏       | 1452/6500 [9:35:30<41:31:19, 29.61s/it]                                                         22%|██▏       | 1452/6500 [9:35:30<41:31:19, 29.61s/it] 22%|██▏       | 1453/6500 [9:35:48<36:37:34, 26.13s/it]                                                         22%|██▏       | 1453/6500 [9:35:48<36:37:34, 26.13s/it] 22%|██▏       | 1454/6500 [9:36:06<33:12:11, 23.69s/it]                                                         22%|██▏       | 1454/6500 [9:36:06<33:12:11, 23.69s/it] 22%|██▏       | 1455/6500 [9:36:24<30:55:45, 22.07s/it]                                                         22%|██▏       | 1455/6500 [9:36:24<30:55:45, 22.07s/it] 22%|██▏       | 1456/6500 [9:36:42<29:14:16, 20.87s/it]                                        {'loss': 0.4606, 'learning_rate': 8.814639062654194e-05, 'epoch': 0.22}
{'loss': 0.4483, 'learning_rate': 8.813075837273658e-05, 'epoch': 0.22}
{'loss': 0.4387, 'learning_rate': 8.81151172060983e-05, 'epoch': 0.22}
{'loss': 0.4552, 'learning_rate': 8.80994671302831e-05, 'epoch': 0.22}
{'loss': 0.5261, 'learning_rate': 8.808380814894912e-05, 'epoch': 0.22}
                 22%|██▏       | 1456/6500 [9:36:42<29:14:16, 20.87s/it] 22%|██▏       | 1457/6500 [9:37:00<28:03:43, 20.03s/it]                                                         22%|██▏       | 1457/6500 [9:37:00<28:03:43, 20.03s/it] 22%|██▏       | 1458/6500 [9:37:19<27:15:12, 19.46s/it]                                                         22%|██▏       | 1458/6500 [9:37:19<27:15:12, 19.46s/it] 22%|██▏       | 1459/6500 [9:37:37<26:41:00, 19.06s/it]                                                         22%|██▏       | 1459/6500 [9:37:37<26:41:00, 19.06s/it] 22%|██▏       | 1460/6500 [9:37:55<26:17:31, 18.78s/it]                                                         22%|██▏       | 1460/6500 [9:37:55<26:17:31, 18.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.775945246219635, 'eval_runtime': 5.5094, 'eval_samples_per_second': 4.175, 'eval_steps_per_second': 1.089, 'epoch': 0.22}
                                                         22%|██▏       | 1460/6500 [9:38:00<26:17:31, 18.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1460
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1460
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1460

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1460/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1460/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1460/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4388, 'learning_rate': 8.806814026575654e-05, 'epoch': 0.22}
{'loss': 0.4299, 'learning_rate': 8.805246348436762e-05, 'epoch': 0.22}
{'loss': 0.4568, 'learning_rate': 8.803677780844674e-05, 'epoch': 0.23}
{'loss': 0.9846, 'learning_rate': 8.80210832416603e-05, 'epoch': 0.23}
{'loss': 0.4579, 'learning_rate': 8.800537978767682e-05, 'epoch': 0.23}
 22%|██▏       | 1461/6500 [9:39:21<54:35:37, 39.00s/it]                                                         22%|██▏       | 1461/6500 [9:39:21<54:35:37, 39.00s/it] 22%|██▏       | 1462/6500 [9:39:39<45:46:43, 32.71s/it]                                                         22%|██▏       | 1462/6500 [9:39:39<45:46:43, 32.71s/it] 23%|██▎       | 1463/6500 [9:39:57<39:35:31, 28.30s/it]                                                         23%|██▎       | 1463/6500 [9:39:57<39:35:31, 28.30s/it] 23%|██▎       | 1464/6500 [9:40:15<35:16:21, 25.21s/it]                                                         23%|██▎       | 1464/6500 [9:40:15<35:16:21, 25.21s/it] 23%|██▎       | 1465/6500 [9:40:33<32:15:14, 23.06s/it]                                                         23%|██▎       | 1465/6500 [9:40:33<32:15:14, 23.06s/it] 23%|██▎       | 1466/6500 [9:40:51<30:08:27, 21.55s/it]                                        {'loss': 0.4441, 'learning_rate': 8.79896674501669e-05, 'epoch': 0.23}
{'loss': 0.4626, 'learning_rate': 8.797394623280319e-05, 'epoch': 0.23}
{'loss': 0.4269, 'learning_rate': 8.795821613926045e-05, 'epoch': 0.23}
{'loss': 0.4761, 'learning_rate': 8.794247717321547e-05, 'epoch': 0.23}
{'loss': 0.4354, 'learning_rate': 8.792672933834713e-05, 'epoch': 0.23}
                 23%|██▎       | 1466/6500 [9:40:51<30:08:27, 21.55s/it] 23%|██▎       | 1467/6500 [9:41:09<28:40:35, 20.51s/it]                                                         23%|██▎       | 1467/6500 [9:41:09<28:40:35, 20.51s/it] 23%|██▎       | 1468/6500 [9:41:27<27:40:05, 19.79s/it]                                                         23%|██▎       | 1468/6500 [9:41:27<27:40:05, 19.79s/it] 23%|██▎       | 1469/6500 [9:41:45<26:57:41, 19.29s/it]                                                         23%|██▎       | 1469/6500 [9:41:45<26:57:41, 19.29s/it] 23%|██▎       | 1470/6500 [9:42:04<26:28:04, 18.94s/it]                                                         23%|██▎       | 1470/6500 [9:42:04<26:28:04, 18.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7727962136268616, 'eval_runtime': 6.5789, 'eval_samples_per_second': 3.496, 'eval_steps_per_second': 0.912, 'epoch': 0.23}
                                                         23%|██▎       | 1470/6500 [9:42:10<26:28:04, 18.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1470
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1470
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1470
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1470
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1470/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1470/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4336, 'learning_rate': 8.791097263833643e-05, 'epoch': 0.23}
{'loss': 0.4391, 'learning_rate': 8.789520707686635e-05, 'epoch': 0.23}
{'loss': 0.4412, 'learning_rate': 8.787943265762204e-05, 'epoch': 0.23}
{'loss': 0.4536, 'learning_rate': 8.786364938429065e-05, 'epoch': 0.23}
{'loss': 0.4402, 'learning_rate': 8.784785726056143e-05, 'epoch': 0.23}
 23%|██▎       | 1471/6500 [9:43:06<44:43:23, 32.01s/it]                                                         23%|██▎       | 1471/6500 [9:43:06<44:43:23, 32.01s/it] 23%|██▎       | 1472/6500 [9:43:24<38:50:41, 27.81s/it]                                                         23%|██▎       | 1472/6500 [9:43:24<38:50:41, 27.81s/it] 23%|██▎       | 1473/6500 [9:43:42<34:48:50, 24.93s/it]                                                         23%|██▎       | 1473/6500 [9:43:42<34:48:50, 24.93s/it] 23%|██▎       | 1474/6500 [9:44:00<31:55:58, 22.87s/it]                                                         23%|██▎       | 1474/6500 [9:44:00<31:55:58, 22.87s/it] 23%|██▎       | 1475/6500 [9:44:18<29:54:53, 21.43s/it]                                                         23%|██▎       | 1475/6500 [9:44:18<29:54:53, 21.43s/it] 23%|██▎       | 1476/6500 [9:44:37<28:30:35, 20.43s/it]                                        {'loss': 0.4717, 'learning_rate': 8.78320562901257e-05, 'epoch': 0.23}
{'loss': 0.4313, 'learning_rate': 8.781624647667684e-05, 'epoch': 0.23}
{'loss': 0.4874, 'learning_rate': 8.780042782391028e-05, 'epoch': 0.23}
{'loss': 0.4624, 'learning_rate': 8.778460033552356e-05, 'epoch': 0.23}
{'loss': 0.4369, 'learning_rate': 8.776876401521624e-05, 'epoch': 0.23}
                 23%|██▎       | 1476/6500 [9:44:37<28:30:35, 20.43s/it] 23%|██▎       | 1477/6500 [9:44:55<27:32:02, 19.73s/it]                                                         23%|██▎       | 1477/6500 [9:44:55<27:32:02, 19.73s/it] 23%|██▎       | 1478/6500 [9:45:13<26:51:42, 19.26s/it]                                                         23%|██▎       | 1478/6500 [9:45:13<26:51:42, 19.26s/it] 23%|██▎       | 1479/6500 [9:45:31<26:23:40, 18.92s/it]                                                         23%|██▎       | 1479/6500 [9:45:31<26:23:40, 18.92s/it] 23%|██▎       | 1480/6500 [9:45:49<26:03:56, 18.69s/it]                                                         23%|██▎       | 1480/6500 [9:45:49<26:03:56, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7764835357666016, 'eval_runtime': 5.359, 'eval_samples_per_second': 4.292, 'eval_steps_per_second': 1.12, 'epoch': 0.23}
                                                         23%|██▎       | 1480/6500 [9:45:54<26:03:56, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1480
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1480the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1480
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1480

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1480
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1480/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1480/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4563, 'learning_rate': 8.775291886668995e-05, 'epoch': 0.23}
{'loss': 0.4612, 'learning_rate': 8.773706489364843e-05, 'epoch': 0.23}
{'loss': 0.441, 'learning_rate': 8.772120209979745e-05, 'epoch': 0.23}
{'loss': 0.4528, 'learning_rate': 8.770533048884482e-05, 'epoch': 0.23}
{'loss': 0.4802, 'learning_rate': 8.768945006450043e-05, 'epoch': 0.23}
 23%|██▎       | 1481/6500 [9:47:24<57:43:10, 41.40s/it]                                                         23%|██▎       | 1481/6500 [9:47:24<57:43:10, 41.40s/it] 23%|██▎       | 1482/6500 [9:47:41<47:54:33, 34.37s/it]                                                         23%|██▎       | 1482/6500 [9:47:41<47:54:33, 34.37s/it] 23%|██▎       | 1483/6500 [9:47:59<41:02:27, 29.45s/it]                                                         23%|██▎       | 1483/6500 [9:47:59<41:02:27, 29.45s/it] 23%|██▎       | 1484/6500 [9:48:17<36:14:41, 26.01s/it]                                                         23%|██▎       | 1484/6500 [9:48:17<36:14:41, 26.01s/it] 23%|██▎       | 1485/6500 [9:48:35<32:53:51, 23.62s/it]                                                         23%|██▎       | 1485/6500 [9:48:35<32:53:51, 23.62s/it] 23%|██▎       | 1486/6500 [9:48:54<30:34:10, 21.95s/it]                                        {'loss': 0.4401, 'learning_rate': 8.767356083047626e-05, 'epoch': 0.23}
{'loss': 0.452, 'learning_rate': 8.765766279048629e-05, 'epoch': 0.23}
{'loss': 0.4281, 'learning_rate': 8.764175594824662e-05, 'epoch': 0.23}
{'loss': 0.4788, 'learning_rate': 8.762584030747534e-05, 'epoch': 0.23}
{'loss': 0.4942, 'learning_rate': 8.760991587189268e-05, 'epoch': 0.23}
                 23%|██▎       | 1486/6500 [9:48:54<30:34:10, 21.95s/it] 23%|██▎       | 1487/6500 [9:49:12<29:02:25, 20.85s/it]                                                         23%|██▎       | 1487/6500 [9:49:12<29:02:25, 20.85s/it] 23%|██▎       | 1488/6500 [9:49:30<27:52:57, 20.03s/it]                                                         23%|██▎       | 1488/6500 [9:49:30<27:52:57, 20.03s/it] 23%|██▎       | 1489/6500 [9:49:48<27:05:01, 19.46s/it]                                                         23%|██▎       | 1489/6500 [9:49:48<27:05:01, 19.46s/it] 23%|██▎       | 1490/6500 [9:50:06<26:31:42, 19.06s/it]                                                         23%|██▎       | 1490/6500 [9:50:06<26:31:42, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7736914157867432, 'eval_runtime': 5.3451, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.123, 'epoch': 0.23}
                                                         23%|██▎       | 1490/6500 [9:50:12<26:31:42, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1490
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1490 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1490


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1490/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1490/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4346, 'learning_rate': 8.759398264522085e-05, 'epoch': 0.23}
{'loss': 0.4567, 'learning_rate': 8.757804063118415e-05, 'epoch': 0.23}
{'loss': 0.4331, 'learning_rate': 8.756208983350893e-05, 'epoch': 0.23}
{'loss': 0.9719, 'learning_rate': 8.754613025592359e-05, 'epoch': 0.23}
{'loss': 0.4569, 'learning_rate': 8.753016190215859e-05, 'epoch': 0.23}
 23%|██▎       | 1491/6500 [9:52:08<69:26:38, 49.91s/it]                                                         23%|██▎       | 1491/6500 [9:52:08<69:26:38, 49.91s/it] 23%|██▎       | 1492/6500 [9:52:26<56:05:58, 40.33s/it]                                                         23%|██▎       | 1492/6500 [9:52:26<56:05:58, 40.33s/it] 23%|██▎       | 1493/6500 [9:52:44<46:46:28, 33.63s/it]                                                         23%|██▎       | 1493/6500 [9:52:44<46:46:28, 33.63s/it] 23%|██▎       | 1494/6500 [9:53:02<40:17:27, 28.97s/it]                                                         23%|██▎       | 1494/6500 [9:53:02<40:17:27, 28.97s/it] 23%|██▎       | 1495/6500 [9:53:20<35:46:44, 25.74s/it]                                                         23%|██▎       | 1495/6500 [9:53:20<35:46:44, 25.74s/it] 23%|██▎       | 1496/6500 [9:53:38<32:34:50, 23.44s/it]                                        {'loss': 0.4543, 'learning_rate': 8.751418477594645e-05, 'epoch': 0.23}
{'loss': 0.4146, 'learning_rate': 8.749819888102166e-05, 'epoch': 0.23}
{'loss': 0.4333, 'learning_rate': 8.748220422112092e-05, 'epoch': 0.23}
{'loss': 0.4689, 'learning_rate': 8.746620079998282e-05, 'epoch': 0.23}
{'loss': 0.4167, 'learning_rate': 8.745018862134808e-05, 'epoch': 0.23}
                 23%|██▎       | 1496/6500 [9:53:38<32:34:50, 23.44s/it] 23%|██▎       | 1497/6500 [9:53:57<30:22:29, 21.86s/it]                                                         23%|██▎       | 1497/6500 [9:53:57<30:22:29, 21.86s/it] 23%|██▎       | 1498/6500 [9:54:15<28:48:30, 20.73s/it]                                                         23%|██▎       | 1498/6500 [9:54:15<28:48:30, 20.73s/it] 23%|██▎       | 1499/6500 [9:54:33<27:43:05, 19.95s/it]                                                         23%|██▎       | 1499/6500 [9:54:33<27:43:05, 19.95s/it] 23%|██▎       | 1500/6500 [9:54:51<26:57:43, 19.41s/it]                                                         23%|██▎       | 1500/6500 [9:54:51<26:57:43, 19.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7726331949234009, 'eval_runtime': 5.3939, 'eval_samples_per_second': 4.264, 'eval_steps_per_second': 1.112, 'epoch': 0.23}
                                                         23%|██▎       | 1500/6500 [9:54:56<26:57:43, 19.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1500
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1500I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1500/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1500/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4358, 'learning_rate': 8.743416768895947e-05, 'epoch': 0.23}
{'loss': 0.4192, 'learning_rate': 8.741813800656174e-05, 'epoch': 0.23}
{'loss': 0.4429, 'learning_rate': 8.740209957790178e-05, 'epoch': 0.23}
{'loss': 0.4379, 'learning_rate': 8.738605240672843e-05, 'epoch': 0.23}
{'loss': 0.4338, 'learning_rate': 8.736999649679264e-05, 'epoch': 0.23}
 23%|██▎       | 1501/6500 [9:56:52<69:16:13, 49.88s/it]                                                         23%|██▎       | 1501/6500 [9:56:52<69:16:13, 49.88s/it] 23%|██▎       | 1502/6500 [9:57:10<55:56:47, 40.30s/it]                                                         23%|██▎       | 1502/6500 [9:57:10<55:56:47, 40.30s/it] 23%|██▎       | 1503/6500 [9:57:28<46:46:13, 33.69s/it]                                                         23%|██▎       | 1503/6500 [9:57:28<46:46:13, 33.69s/it] 23%|██▎       | 1504/6500 [9:57:46<40:13:41, 28.99s/it]                                                         23%|██▎       | 1504/6500 [9:57:46<40:13:41, 28.99s/it] 23%|██▎       | 1505/6500 [9:58:04<35:40:00, 25.71s/it]                                                         23%|██▎       | 1505/6500 [9:58:04<35:40:00, 25.71s/it] 23%|██▎       | 1506/6500 [9:58:22<32:27:34, 23.40s/it]                                        {'loss': 0.4575, 'learning_rate': 8.735393185184741e-05, 'epoch': 0.23}
{'loss': 0.433, 'learning_rate': 8.73378584756477e-05, 'epoch': 0.23}
{'loss': 0.5008, 'learning_rate': 8.73217763719506e-05, 'epoch': 0.23}
{'loss': 0.4553, 'learning_rate': 8.73056855445152e-05, 'epoch': 0.23}
{'loss': 0.4525, 'learning_rate': 8.728958599710262e-05, 'epoch': 0.23}
                 23%|██▎       | 1506/6500 [9:58:22<32:27:34, 23.40s/it] 23%|██▎       | 1507/6500 [9:58:41<30:22:00, 21.89s/it]                                                         23%|██▎       | 1507/6500 [9:58:41<30:22:00, 21.89s/it] 23%|██▎       | 1508/6500 [9:58:59<28:46:12, 20.75s/it]                                                         23%|██▎       | 1508/6500 [9:58:59<28:46:12, 20.75s/it] 23%|██▎       | 1509/6500 [9:59:17<27:43:40, 20.00s/it]                                                         23%|██▎       | 1509/6500 [9:59:17<27:43:40, 20.00s/it] 23%|██▎       | 1510/6500 [9:59:35<26:56:09, 19.43s/it]                                                         23%|██▎       | 1510/6500 [9:59:35<26:56:09, 19.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7778043150901794, 'eval_runtime': 5.552, 'eval_samples_per_second': 4.143, 'eval_steps_per_second': 1.081, 'epoch': 0.23}
                                                         23%|██▎       | 1510/6500 [9:59:41<26:56:09, 19.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1510
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1510
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1510
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1510

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1510
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1510/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4691, 'learning_rate': 8.727347773347603e-05, 'epoch': 0.23}
{'loss': 0.4352, 'learning_rate': 8.725736075740066e-05, 'epoch': 0.23}
{'loss': 0.4467, 'learning_rate': 8.724123507264372e-05, 'epoch': 0.23}
{'loss': 0.4528, 'learning_rate': 8.722510068297454e-05, 'epoch': 0.23}
{'loss': 0.4561, 'learning_rate': 8.720895759216439e-05, 'epoch': 0.23}
 23%|██▎       | 1511/6500 [10:00:52<50:39:51, 36.56s/it]                                                          23%|██▎       | 1511/6500 [10:00:52<50:39:51, 36.56s/it] 23%|██▎       | 1512/6500 [10:01:10<42:58:28, 31.02s/it]                                                          23%|██▎       | 1512/6500 [10:01:10<42:58:28, 31.02s/it] 23%|██▎       | 1513/6500 [10:01:28<37:34:08, 27.12s/it]                                                          23%|██▎       | 1513/6500 [10:01:28<37:34:08, 27.12s/it] 23%|██▎       | 1514/6500 [10:01:46<33:48:29, 24.41s/it]                                                          23%|██▎       | 1514/6500 [10:01:46<33:48:29, 24.41s/it] 23%|██▎       | 1515/6500 [10:02:04<31:11:01, 22.52s/it]                                                          23%|██▎       | 1515/6500 [10:02:04<31:11:01, 22.52s/it] 23%|██▎       | 1516/6500 [10:02:22<29:21:04, 21.20s/it]                        {'loss': 0.4293, 'learning_rate': 8.719280580398663e-05, 'epoch': 0.23}
{'loss': 0.4412, 'learning_rate': 8.717664532221668e-05, 'epoch': 0.23}
{'loss': 0.4397, 'learning_rate': 8.71604761506319e-05, 'epoch': 0.23}
{'loss': 0.5313, 'learning_rate': 8.714429829301176e-05, 'epoch': 0.23}
{'loss': 0.4368, 'learning_rate': 8.712811175313773e-05, 'epoch': 0.23}
                                  23%|██▎       | 1516/6500 [10:02:22<29:21:04, 21.20s/it] 23%|██▎       | 1517/6500 [10:02:40<28:04:34, 20.28s/it]                                                          23%|██▎       | 1517/6500 [10:02:40<28:04:34, 20.28s/it] 23%|██▎       | 1518/6500 [10:02:58<27:11:43, 19.65s/it]                                                          23%|██▎       | 1518/6500 [10:02:58<27:11:43, 19.65s/it] 23%|██▎       | 1519/6500 [10:03:17<26:48:44, 19.38s/it]                                                          23%|██▎       | 1519/6500 [10:03:17<26:48:44, 19.38s/it] 23%|██▎       | 1520/6500 [10:03:35<26:19:23, 19.03s/it]                                                          23%|██▎       | 1520/6500 [10:03:35<26:19:23, 19.03s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7714625597000122, 'eval_runtime': 6.0115, 'eval_samples_per_second': 3.826, 'eval_steps_per_second': 0.998, 'epoch': 0.23}
                                                          23%|██▎       | 1520/6500 [10:03:41<26:19:23, 19.03s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1520
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1520
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1520
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1520

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1520
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1520/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.422, 'learning_rate': 8.711191653479333e-05, 'epoch': 0.23}
{'loss': 0.4428, 'learning_rate': 8.709571264176409e-05, 'epoch': 0.23}
{'loss': 0.9658, 'learning_rate': 8.707950007783755e-05, 'epoch': 0.23}
{'loss': 0.4425, 'learning_rate': 8.706327884680332e-05, 'epoch': 0.23}
{'loss': 0.4506, 'learning_rate': 8.704704895245301e-05, 'epoch': 0.23}
 23%|██▎       | 1521/6500 [10:04:46<47:56:50, 34.67s/it]                                                          23%|██▎       | 1521/6500 [10:04:46<47:56:50, 34.67s/it] 23%|██▎       | 1522/6500 [10:05:04<41:02:19, 29.68s/it]                                                          23%|██▎       | 1522/6500 [10:05:04<41:02:19, 29.68s/it] 23%|██▎       | 1523/6500 [10:05:23<36:24:41, 26.34s/it]                                                          23%|██▎       | 1523/6500 [10:05:23<36:24:41, 26.34s/it] 23%|██▎       | 1524/6500 [10:05:41<32:57:52, 23.85s/it]                                                          23%|██▎       | 1524/6500 [10:05:41<32:57:52, 23.85s/it] 23%|██▎       | 1525/6500 [10:05:59<30:34:04, 22.12s/it]                                                          23%|██▎       | 1525/6500 [10:05:59<30:34:04, 22.12s/it] 23%|██▎       | 1526/6500 [10:06:17<28:54:09, 20.92s/it]                        {'loss': 0.4441, 'learning_rate': 8.703081039858026e-05, 'epoch': 0.23}
{'loss': 0.4164, 'learning_rate': 8.701456318898073e-05, 'epoch': 0.23}
{'loss': 0.466, 'learning_rate': 8.69983073274521e-05, 'epoch': 0.24}
{'loss': 0.4407, 'learning_rate': 8.69820428177941e-05, 'epoch': 0.24}
{'loss': 0.411, 'learning_rate': 8.696576966380843e-05, 'epoch': 0.24}
                                  23%|██▎       | 1526/6500 [10:06:17<28:54:09, 20.92s/it] 23%|██▎       | 1527/6500 [10:06:35<27:44:31, 20.08s/it]                                                          23%|██▎       | 1527/6500 [10:06:35<27:44:31, 20.08s/it] 24%|██▎       | 1528/6500 [10:06:54<26:55:57, 19.50s/it]                                                          24%|██▎       | 1528/6500 [10:06:54<26:55:57, 19.50s/it] 24%|██▎       | 1529/6500 [10:07:12<26:22:35, 19.10s/it]                                                          24%|██▎       | 1529/6500 [10:07:12<26:22:35, 19.10s/it] 24%|██▎       | 1530/6500 [10:07:30<25:59:26, 18.83s/it]                                                          24%|██▎       | 1530/6500 [10:07:30<25:59:26, 18.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7690179944038391, 'eval_runtime': 5.3537, 'eval_samples_per_second': 4.296, 'eval_steps_per_second': 1.121, 'epoch': 0.24}
                                                          24%|██▎       | 1530/6500 [10:07:35<25:59:26, 18.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1530
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1530
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1530
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1530

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1530
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1530/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1530/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4351, 'learning_rate': 8.694948786929886e-05, 'epoch': 0.24}
{'loss': 0.4165, 'learning_rate': 8.693319743807116e-05, 'epoch': 0.24}
{'loss': 0.448, 'learning_rate': 8.691689837393311e-05, 'epoch': 0.24}
{'loss': 0.4276, 'learning_rate': 8.690059068069454e-05, 'epoch': 0.24}
{'loss': 0.4478, 'learning_rate': 8.688427436216724e-05, 'epoch': 0.24}
 24%|██▎       | 1531/6500 [10:08:37<45:50:19, 33.21s/it]                                                          24%|██▎       | 1531/6500 [10:08:37<45:50:19, 33.21s/it] 24%|██▎       | 1532/6500 [10:08:55<39:33:33, 28.67s/it]                                                          24%|██▎       | 1532/6500 [10:08:55<39:33:33, 28.67s/it] 24%|██▎       | 1533/6500 [10:09:13<35:09:31, 25.48s/it]                                                          24%|██▎       | 1533/6500 [10:09:13<35:09:31, 25.48s/it] 24%|██▎       | 1534/6500 [10:09:31<32:05:26, 23.26s/it]                                                          24%|██▎       | 1534/6500 [10:09:31<32:05:26, 23.26s/it] 24%|██▎       | 1535/6500 [10:09:49<29:57:43, 21.72s/it]                                                          24%|██▎       | 1535/6500 [10:09:49<29:57:43, 21.72s/it] 24%|██▎       | 1536/6500 [10:10:08<28:38:17, 20.77s/it]                        {'loss': 0.4244, 'learning_rate': 8.686794942216508e-05, 'epoch': 0.24}
{'loss': 0.4446, 'learning_rate': 8.685161586450388e-05, 'epoch': 0.24}
{'loss': 0.4611, 'learning_rate': 8.683527369300156e-05, 'epoch': 0.24}
{'loss': 0.4371, 'learning_rate': 8.681892291147798e-05, 'epoch': 0.24}
{'loss': 0.4413, 'learning_rate': 8.6802563523755e-05, 'epoch': 0.24}
                                  24%|██▎       | 1536/6500 [10:10:08<28:38:17, 20.77s/it] 24%|██▎       | 1537/6500 [10:10:26<27:33:19, 19.99s/it]                                                          24%|██▎       | 1537/6500 [10:10:26<27:33:19, 19.99s/it] 24%|██▎       | 1538/6500 [10:10:44<26:51:03, 19.48s/it]                                                          24%|██▎       | 1538/6500 [10:10:44<26:51:03, 19.48s/it] 24%|██▎       | 1539/6500 [10:11:02<26:19:11, 19.10s/it]                                                          24%|██▎       | 1539/6500 [10:11:02<26:19:11, 19.10s/it] 24%|██▎       | 1540/6500 [10:11:20<25:56:45, 18.83s/it]                                                          24%|██▎       | 1540/6500 [10:11:20<25:56:45, 18.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7830345034599304, 'eval_runtime': 5.37, 'eval_samples_per_second': 4.283, 'eval_steps_per_second': 1.117, 'epoch': 0.24}
                                                          24%|██▎       | 1540/6500 [10:11:26<25:56:45, 18.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1540
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1540
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1540
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1540

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1540
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1540/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4374, 'learning_rate': 8.678619553365659e-05, 'epoch': 0.24}
{'loss': 0.4307, 'learning_rate': 8.676981894500862e-05, 'epoch': 0.24}
{'loss': 0.4279, 'learning_rate': 8.675343376163905e-05, 'epoch': 0.24}
{'loss': 0.4615, 'learning_rate': 8.673703998737778e-05, 'epoch': 0.24}
{'loss': 0.4441, 'learning_rate': 8.67206376260568e-05, 'epoch': 0.24}
 24%|██▎       | 1541/6500 [10:12:36<49:21:58, 35.84s/it]                                                          24%|██▎       | 1541/6500 [10:12:36<49:21:58, 35.84s/it] 24%|██▎       | 1542/6500 [10:12:54<42:00:33, 30.50s/it]                                                          24%|██▎       | 1542/6500 [10:12:54<42:00:33, 30.50s/it] 24%|██▎       | 1543/6500 [10:13:12<36:51:04, 26.76s/it]                                                          24%|██▎       | 1543/6500 [10:13:12<36:51:04, 26.76s/it] 24%|██▍       | 1544/6500 [10:13:30<33:14:40, 24.15s/it]                                                          24%|██▍       | 1544/6500 [10:13:30<33:14:40, 24.15s/it] 24%|██▍       | 1545/6500 [10:13:48<30:43:56, 22.33s/it]                                                          24%|██▍       | 1545/6500 [10:13:48<30:43:56, 22.33s/it] 24%|██▍       | 1546/6500 [10:14:06<28:59:08, 21.06s/it]                        {'loss': 0.442, 'learning_rate': 8.670422668151003e-05, 'epoch': 0.24}
{'loss': 0.4274, 'learning_rate': 8.668780715757345e-05, 'epoch': 0.24}
{'loss': 0.4519, 'learning_rate': 8.6671379058085e-05, 'epoch': 0.24}
{'loss': 0.502, 'learning_rate': 8.665494238688467e-05, 'epoch': 0.24}
{'loss': 0.4417, 'learning_rate': 8.663849714781442e-05, 'epoch': 0.24}
                                  24%|██▍       | 1546/6500 [10:14:06<28:59:08, 21.06s/it] 24%|██▍       | 1547/6500 [10:14:25<27:52:09, 20.26s/it]                                                          24%|██▍       | 1547/6500 [10:14:25<27:52:09, 20.26s/it] 24%|██▍       | 1548/6500 [10:14:43<26:59:30, 19.62s/it]                                                          24%|██▍       | 1548/6500 [10:14:43<26:59:30, 19.62s/it] 24%|██▍       | 1549/6500 [10:15:01<26:22:46, 19.18s/it]                                                          24%|██▍       | 1549/6500 [10:15:01<26:22:46, 19.18s/it] 24%|██▍       | 1550/6500 [10:15:19<25:57:58, 18.88s/it]                                                          24%|██▍       | 1550/6500 [10:15:19<25:57:58, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.770118236541748, 'eval_runtime': 5.3499, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.122, 'epoch': 0.24}
                                                          24%|██▍       | 1550/6500 [10:15:25<25:57:58, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1550
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1550
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1550
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1550

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1550
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1550/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4126, 'learning_rate': 8.662204334471822e-05, 'epoch': 0.24}
{'loss': 0.4385, 'learning_rate': 8.660558098144206e-05, 'epoch': 0.24}
{'loss': 0.9602, 'learning_rate': 8.658911006183391e-05, 'epoch': 0.24}
{'loss': 0.459, 'learning_rate': 8.657263058974375e-05, 'epoch': 0.24}
{'loss': 0.443, 'learning_rate': 8.655614256902355e-05, 'epoch': 0.24}
 24%|██▍       | 1551/6500 [10:16:45<53:22:58, 38.83s/it]                                                          24%|██▍       | 1551/6500 [10:16:45<53:22:58, 38.83s/it] 24%|██▍       | 1552/6500 [10:17:03<44:54:17, 32.67s/it]                                                          24%|██▍       | 1552/6500 [10:17:03<44:54:17, 32.67s/it] 24%|██▍       | 1553/6500 [10:17:21<38:51:46, 28.28s/it]                                                          24%|██▍       | 1553/6500 [10:17:21<38:51:46, 28.28s/it] 24%|██▍       | 1554/6500 [10:17:39<34:38:14, 25.21s/it]                                                          24%|██▍       | 1554/6500 [10:17:39<34:38:14, 25.21s/it] 24%|██▍       | 1555/6500 [10:17:57<31:46:51, 23.14s/it]                                                          24%|██▍       | 1555/6500 [10:17:57<31:46:51, 23.14s/it] 24%|██▍       | 1556/6500 [10:18:15<29:42:16, 21.63s/it]                        {'loss': 0.419, 'learning_rate': 8.65396460035273e-05, 'epoch': 0.24}
{'loss': 0.4236, 'learning_rate': 8.652314089711095e-05, 'epoch': 0.24}
{'loss': 0.4653, 'learning_rate': 8.650662725363249e-05, 'epoch': 0.24}
{'loss': 0.4238, 'learning_rate': 8.649010507695187e-05, 'epoch': 0.24}
{'loss': 0.4335, 'learning_rate': 8.647357437093105e-05, 'epoch': 0.24}
                                  24%|██▍       | 1556/6500 [10:18:15<29:42:16, 21.63s/it] 24%|██▍       | 1557/6500 [10:18:33<28:15:25, 20.58s/it]                                                          24%|██▍       | 1557/6500 [10:18:33<28:15:25, 20.58s/it] 24%|██▍       | 1558/6500 [10:18:52<27:14:36, 19.85s/it]                                                          24%|██▍       | 1558/6500 [10:18:52<27:14:36, 19.85s/it] 24%|██▍       | 1559/6500 [10:19:10<26:32:35, 19.34s/it]                                                          24%|██▍       | 1559/6500 [10:19:10<26:32:35, 19.34s/it] 24%|██▍       | 1560/6500 [10:19:28<26:03:51, 18.99s/it]                                                          24%|██▍       | 1560/6500 [10:19:28<26:03:51, 18.99s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7688918709754944, 'eval_runtime': 5.3588, 'eval_samples_per_second': 4.292, 'eval_steps_per_second': 1.12, 'epoch': 0.24}
                                                          24%|██▍       | 1560/6500 [10:19:33<26:03:51, 18.99s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1560
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1560 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1560/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1560


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1560
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1560/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1560/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4187, 'learning_rate': 8.645703513943397e-05, 'epoch': 0.24}
{'loss': 0.4326, 'learning_rate': 8.64404873863266e-05, 'epoch': 0.24}
{'loss': 0.4174, 'learning_rate': 8.642393111547687e-05, 'epoch': 0.24}
{'loss': 0.4295, 'learning_rate': 8.64073663307547e-05, 'epoch': 0.24}
{'loss': 0.4584, 'learning_rate': 8.639079303603202e-05, 'epoch': 0.24}
 24%|██▍       | 1561/6500 [10:20:41<48:15:13, 35.17s/it]                                                          24%|██▍       | 1561/6500 [10:20:41<48:15:13, 35.17s/it] 24%|██▍       | 1562/6500 [10:20:59<41:12:08, 30.04s/it]                                                          24%|██▍       | 1562/6500 [10:20:59<41:12:08, 30.04s/it] 24%|██▍       | 1563/6500 [10:21:17<36:15:34, 26.44s/it]                                                          24%|██▍       | 1563/6500 [10:21:17<36:15:34, 26.44s/it] 24%|██▍       | 1564/6500 [10:21:35<32:49:17, 23.94s/it]                                                          24%|██▍       | 1564/6500 [10:21:35<32:49:17, 23.94s/it] 24%|██▍       | 1565/6500 [10:21:53<30:24:20, 22.18s/it]                                                          24%|██▍       | 1565/6500 [10:21:53<30:24:20, 22.18s/it] 24%|██▍       | 1566/6500 [10:22:11<28:46:17, 20.99s/it]                        {'loss': 0.4167, 'learning_rate': 8.637421123518272e-05, 'epoch': 0.24}
{'loss': 0.4629, 'learning_rate': 8.635762093208269e-05, 'epoch': 0.24}
{'loss': 0.444, 'learning_rate': 8.634102213060984e-05, 'epoch': 0.24}
{'loss': 0.4355, 'learning_rate': 8.632441483464402e-05, 'epoch': 0.24}
{'loss': 0.4464, 'learning_rate': 8.630779904806709e-05, 'epoch': 0.24}
                                  24%|██▍       | 1566/6500 [10:22:11<28:46:17, 20.99s/it] 24%|██▍       | 1567/6500 [10:22:30<27:36:01, 20.14s/it]                                                          24%|██▍       | 1567/6500 [10:22:30<27:36:01, 20.14s/it] 24%|██▍       | 1568/6500 [10:22:48<26:56:26, 19.66s/it]                                                          24%|██▍       | 1568/6500 [10:22:48<26:56:26, 19.66s/it] 24%|██▍       | 1569/6500 [10:23:06<26:18:54, 19.21s/it]                                                          24%|██▍       | 1569/6500 [10:23:06<26:18:54, 19.21s/it] 24%|██▍       | 1570/6500 [10:23:24<25:53:39, 18.91s/it]                                                          24%|██▍       | 1570/6500 [10:23:24<25:53:39, 18.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7800244092941284, 'eval_runtime': 5.4887, 'eval_samples_per_second': 4.19, 'eval_steps_per_second': 1.093, 'epoch': 0.24}
                                                          24%|██▍       | 1570/6500 [10:23:30<25:53:39, 18.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1570
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1570/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1570

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1570

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1570
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1570/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1570/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4409, 'learning_rate': 8.629117477476289e-05, 'epoch': 0.24}
{'loss': 0.4286, 'learning_rate': 8.627454201861724e-05, 'epoch': 0.24}
{'loss': 0.4413, 'learning_rate': 8.625790078351794e-05, 'epoch': 0.24}
{'loss': 0.4595, 'learning_rate': 8.624125107335478e-05, 'epoch': 0.24}
{'loss': 0.422, 'learning_rate': 8.622459289201954e-05, 'epoch': 0.24}
 24%|██▍       | 1571/6500 [10:24:39<48:48:08, 35.64s/it]                                                          24%|██▍       | 1571/6500 [10:24:39<48:48:08, 35.64s/it] 24%|██▍       | 1572/6500 [10:24:57<41:34:04, 30.37s/it]                                                          24%|██▍       | 1572/6500 [10:24:57<41:34:04, 30.37s/it] 24%|██▍       | 1573/6500 [10:25:15<36:30:15, 26.67s/it]                                                          24%|██▍       | 1573/6500 [10:25:15<36:30:15, 26.67s/it] 24%|██▍       | 1574/6500 [10:25:33<32:58:51, 24.10s/it]                                                          24%|██▍       | 1574/6500 [10:25:33<32:58:51, 24.10s/it] 24%|██▍       | 1575/6500 [10:25:51<30:31:27, 22.31s/it]                                                          24%|██▍       | 1575/6500 [10:25:51<30:31:27, 22.31s/it] 24%|██▍       | 1576/6500 [10:26:10<28:48:14, 21.06s/it]                        {'loss': 0.4311, 'learning_rate': 8.620792624340596e-05, 'epoch': 0.24}
{'loss': 0.4115, 'learning_rate': 8.619125113140975e-05, 'epoch': 0.24}
{'loss': 0.4656, 'learning_rate': 8.617456755992867e-05, 'epoch': 0.24}
{'loss': 0.4837, 'learning_rate': 8.615787553286234e-05, 'epoch': 0.24}
{'loss': 0.4399, 'learning_rate': 8.614117505411246e-05, 'epoch': 0.24}
                                  24%|██▍       | 1576/6500 [10:26:10<28:48:14, 21.06s/it] 24%|██▍       | 1577/6500 [10:26:28<27:36:22, 20.19s/it]                                                          24%|██▍       | 1577/6500 [10:26:28<27:36:22, 20.19s/it] 24%|██▍       | 1578/6500 [10:26:46<26:46:17, 19.58s/it]                                                          24%|██▍       | 1578/6500 [10:26:46<26:46:17, 19.58s/it] 24%|██▍       | 1579/6500 [10:27:04<26:11:58, 19.17s/it]                                                          24%|██▍       | 1579/6500 [10:27:04<26:11:58, 19.17s/it] 24%|██▍       | 1580/6500 [10:27:22<25:47:51, 18.88s/it]                                                          24%|██▍       | 1580/6500 [10:27:22<25:47:51, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7668811678886414, 'eval_runtime': 5.3376, 'eval_samples_per_second': 4.309, 'eval_steps_per_second': 1.124, 'epoch': 0.24}
                                                          24%|██▍       | 1580/6500 [10:27:28<25:47:51, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1580
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1580/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1580/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1580



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1580
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1580/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4335, 'learning_rate': 8.612446612758265e-05, 'epoch': 0.24}
{'loss': 0.4273, 'learning_rate': 8.610774875717851e-05, 'epoch': 0.24}
{'loss': 0.971, 'learning_rate': 8.609102294680766e-05, 'epoch': 0.24}
{'loss': 0.4292, 'learning_rate': 8.607428870037962e-05, 'epoch': 0.24}
{'loss': 0.4394, 'learning_rate': 8.605754602180594e-05, 'epoch': 0.24}
 24%|██▍       | 1581/6500 [10:28:52<54:50:56, 40.14s/it]                                                          24%|██▍       | 1581/6500 [10:28:52<54:50:56, 40.14s/it] 24%|██▍       | 1582/6500 [10:29:10<45:46:19, 33.51s/it]                                                          24%|██▍       | 1582/6500 [10:29:10<45:46:19, 33.51s/it] 24%|██▍       | 1583/6500 [10:29:28<39:24:51, 28.86s/it]                                                          24%|██▍       | 1583/6500 [10:29:28<39:24:51, 28.86s/it] 24%|██▍       | 1584/6500 [10:29:47<35:25:14, 25.94s/it]                                                          24%|██▍       | 1584/6500 [10:29:47<35:25:14, 25.94s/it] 24%|██▍       | 1585/6500 [10:30:05<32:10:45, 23.57s/it]                                                          24%|██▍       | 1585/6500 [10:30:05<32:10:45, 23.57s/it] 24%|██▍       | 1586/6500 [10:30:23<29:55:56, 21.93s/it]                        {'loss': 0.4022, 'learning_rate': 8.60407949150001e-05, 'epoch': 0.24}
{'loss': 0.4484, 'learning_rate': 8.602403538387758e-05, 'epoch': 0.24}
{'loss': 0.4361, 'learning_rate': 8.600726743235583e-05, 'epoch': 0.24}
{'loss': 0.3996, 'learning_rate': 8.599049106435424e-05, 'epoch': 0.24}
{'loss': 0.4258, 'learning_rate': 8.59737062837942e-05, 'epoch': 0.24}
                                  24%|██▍       | 1586/6500 [10:30:23<29:55:56, 21.93s/it] 24%|██▍       | 1587/6500 [10:30:41<28:21:36, 20.78s/it]                                                          24%|██▍       | 1587/6500 [10:30:41<28:21:36, 20.78s/it] 24%|██▍       | 1588/6500 [10:31:00<27:16:06, 19.98s/it]                                                          24%|██▍       | 1588/6500 [10:31:00<27:16:06, 19.98s/it] 24%|██▍       | 1589/6500 [10:31:18<26:30:30, 19.43s/it]                                                          24%|██▍       | 1589/6500 [10:31:18<26:30:30, 19.43s/it] 24%|██▍       | 1590/6500 [10:31:36<25:59:32, 19.06s/it]                                                          24%|██▍       | 1590/6500 [10:31:36<25:59:32, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.770452082157135, 'eval_runtime': 5.3429, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.24}
                                                          24%|██▍       | 1590/6500 [10:31:41<25:59:32, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1590
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1590
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1590
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1590

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1590
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1590/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1590/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4072, 'learning_rate': 8.595691309459901e-05, 'epoch': 0.24}
{'loss': 0.4285, 'learning_rate': 8.594011150069403e-05, 'epoch': 0.24}
{'loss': 0.4241, 'learning_rate': 8.59233015060065e-05, 'epoch': 0.25}
{'loss': 0.4212, 'learning_rate': 8.590648311446567e-05, 'epoch': 0.25}
{'loss': 0.4341, 'learning_rate': 8.58896563300027e-05, 'epoch': 0.25}
 24%|██▍       | 1591/6500 [10:32:50<48:35:06, 35.63s/it]                                                          24%|██▍       | 1591/6500 [10:32:50<48:35:06, 35.63s/it] 24%|██▍       | 1592/6500 [10:33:08<41:22:35, 30.35s/it]                                                          24%|██▍       | 1592/6500 [10:33:08<41:22:35, 30.35s/it] 25%|██▍       | 1593/6500 [10:33:26<36:19:42, 26.65s/it]                                                          25%|██▍       | 1593/6500 [10:33:26<36:19:42, 26.65s/it] 25%|██▍       | 1594/6500 [10:33:44<32:47:41, 24.06s/it]                                                          25%|██▍       | 1594/6500 [10:33:44<32:47:41, 24.06s/it] 25%|██▍       | 1595/6500 [10:34:02<30:19:58, 22.26s/it]                                                          25%|██▍       | 1595/6500 [10:34:02<30:19:58, 22.26s/it] 25%|██▍       | 1596/6500 [10:34:20<28:37:34, 21.01s/it]                        {'loss': 0.4184, 'learning_rate': 8.58728211565508e-05, 'epoch': 0.25}
{'loss': 0.4681, 'learning_rate': 8.585597759804506e-05, 'epoch': 0.25}
{'loss': 0.4247, 'learning_rate': 8.583912565842257e-05, 'epoch': 0.25}
{'loss': 0.437, 'learning_rate': 8.582226534162235e-05, 'epoch': 0.25}
{'loss': 0.4377, 'learning_rate': 8.580539665158542e-05, 'epoch': 0.25}
                                  25%|██▍       | 1596/6500 [10:34:20<28:37:34, 21.01s/it] 25%|██▍       | 1597/6500 [10:34:39<27:26:17, 20.15s/it]                                                          25%|██▍       | 1597/6500 [10:34:39<27:26:17, 20.15s/it] 25%|██▍       | 1598/6500 [10:34:57<26:36:47, 19.54s/it]                                                          25%|██▍       | 1598/6500 [10:34:57<26:36:47, 19.54s/it] 25%|██▍       | 1599/6500 [10:35:15<26:02:43, 19.13s/it]                                                          25%|██▍       | 1599/6500 [10:35:15<26:02:43, 19.13s/it] 25%|██▍       | 1600/6500 [10:35:33<25:45:58, 18.93s/it]                                                          25%|██▍       | 1600/6500 [10:35:33<25:45:58, 18.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7728133797645569, 'eval_runtime': 5.3622, 'eval_samples_per_second': 4.289, 'eval_steps_per_second': 1.119, 'epoch': 0.25}
                                                          25%|██▍       | 1600/6500 [10:35:39<25:45:58, 18.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1600
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1600

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1600
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1600
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1600/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4114, 'learning_rate': 8.578851959225472e-05, 'epoch': 0.25}
{'loss': 0.4283, 'learning_rate': 8.577163416757517e-05, 'epoch': 0.25}
{'loss': 0.4429, 'learning_rate': 8.57547403814936e-05, 'epoch': 0.25}
{'loss': 0.436, 'learning_rate': 8.573783823795889e-05, 'epoch': 0.25}
{'loss': 0.4219, 'learning_rate': 8.572092774092176e-05, 'epoch': 0.25}
 25%|██▍       | 1601/6500 [10:36:40<45:14:55, 33.25s/it]                                                          25%|██▍       | 1601/6500 [10:36:40<45:14:55, 33.25s/it] 25%|██▍       | 1602/6500 [10:36:58<39:02:23, 28.69s/it]                                                          25%|██▍       | 1602/6500 [10:36:58<39:02:23, 28.69s/it] 25%|██▍       | 1603/6500 [10:37:16<34:41:37, 25.50s/it]                                                          25%|██▍       | 1603/6500 [10:37:16<34:41:37, 25.50s/it] 25%|██▍       | 1604/6500 [10:37:34<31:39:25, 23.28s/it]                                                          25%|██▍       | 1604/6500 [10:37:34<31:39:25, 23.28s/it] 25%|██▍       | 1605/6500 [10:37:52<29:32:35, 21.73s/it]                                                          25%|██▍       | 1605/6500 [10:37:52<29:32:35, 21.73s/it] 25%|██▍       | 1606/6500 [10:38:10<28:03:57, 20.65s/it]                        {'loss': 0.4242, 'learning_rate': 8.570400889433497e-05, 'epoch': 0.25}
{'loss': 0.4115, 'learning_rate': 8.568708170215319e-05, 'epoch': 0.25}
{'loss': 0.5173, 'learning_rate': 8.567014616833302e-05, 'epoch': 0.25}
{'loss': 0.4117, 'learning_rate': 8.56532022968331e-05, 'epoch': 0.25}
{'loss': 0.4151, 'learning_rate': 8.563625009161387e-05, 'epoch': 0.25}
                                  25%|██▍       | 1606/6500 [10:38:10<28:03:57, 20.65s/it] 25%|██▍       | 1607/6500 [10:38:29<27:02:31, 19.90s/it]                                                          25%|██▍       | 1607/6500 [10:38:29<27:02:31, 19.90s/it] 25%|██▍       | 1608/6500 [10:38:47<26:19:51, 19.38s/it]                                                          25%|██▍       | 1608/6500 [10:38:47<26:19:51, 19.38s/it] 25%|██▍       | 1609/6500 [10:39:05<25:50:35, 19.02s/it]                                                          25%|██▍       | 1609/6500 [10:39:05<25:50:35, 19.02s/it] 25%|██▍       | 1610/6500 [10:39:23<25:29:59, 18.77s/it]                                                          25%|██▍       | 1610/6500 [10:39:23<25:29:59, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7713947296142578, 'eval_runtime': 5.3482, 'eval_samples_per_second': 4.301, 'eval_steps_per_second': 1.122, 'epoch': 0.25}
                                                          25%|██▍       | 1610/6500 [10:39:29<25:29:59, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1610
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1610I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1610

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1610

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1610
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1610/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1610/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4307, 'learning_rate': 8.56192895566379e-05, 'epoch': 0.25}
{'loss': 0.9522, 'learning_rate': 8.560232069586953e-05, 'epoch': 0.25}
{'loss': 0.4279, 'learning_rate': 8.558534351327517e-05, 'epoch': 0.25}
{'loss': 0.4245, 'learning_rate': 8.556835801282314e-05, 'epoch': 0.25}
{'loss': 0.4265, 'learning_rate': 8.555136419848368e-05, 'epoch': 0.25}
 25%|██▍       | 1611/6500 [10:40:38<48:19:44, 35.59s/it]                                                          25%|██▍       | 1611/6500 [10:40:38<48:19:44, 35.59s/it] 25%|██▍       | 1612/6500 [10:40:56<41:09:34, 30.31s/it]                                                          25%|██▍       | 1612/6500 [10:40:56<41:09:34, 30.31s/it] 25%|██▍       | 1613/6500 [10:41:14<36:08:22, 26.62s/it]                                                          25%|██▍       | 1613/6500 [10:41:14<36:08:22, 26.62s/it] 25%|██▍       | 1614/6500 [10:41:32<32:37:39, 24.04s/it]                                                          25%|██▍       | 1614/6500 [10:41:32<32:37:39, 24.04s/it] 25%|██▍       | 1615/6500 [10:41:50<30:10:39, 22.24s/it]                                                          25%|██▍       | 1615/6500 [10:41:50<30:10:39, 22.24s/it] 25%|██▍       | 1616/6500 [10:42:09<28:41:10, 21.14s/it]                        {'loss': 0.4096, 'learning_rate': 8.553436207422898e-05, 'epoch': 0.25}
{'loss': 0.443, 'learning_rate': 8.55173516440332e-05, 'epoch': 0.25}
{'loss': 0.4076, 'learning_rate': 8.550033291187243e-05, 'epoch': 0.25}
{'loss': 0.4048, 'learning_rate': 8.548330588172469e-05, 'epoch': 0.25}
{'loss': 0.4105, 'learning_rate': 8.546627055756994e-05, 'epoch': 0.25}
                                  25%|██▍       | 1616/6500 [10:42:09<28:41:10, 21.14s/it] 25%|██▍       | 1617/6500 [10:42:27<27:26:13, 20.23s/it]                                                          25%|██▍       | 1617/6500 [10:42:27<27:26:13, 20.23s/it] 25%|██▍       | 1618/6500 [10:42:45<26:33:55, 19.59s/it]                                                          25%|██▍       | 1618/6500 [10:42:45<26:33:55, 19.59s/it] 25%|██▍       | 1619/6500 [10:43:03<25:57:59, 19.15s/it]                                                          25%|██▍       | 1619/6500 [10:43:03<25:57:59, 19.15s/it] 25%|██▍       | 1620/6500 [10:43:21<25:33:36, 18.86s/it]                                                          25%|██▍       | 1620/6500 [10:43:21<25:33:36, 18.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7766144871711731, 'eval_runtime': 5.3389, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.25}
                                                          25%|██▍       | 1620/6500 [10:43:26<25:33:36, 18.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1620
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1620 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1620
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1620


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1620
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1620/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1620/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4089, 'learning_rate': 8.544922694339008e-05, 'epoch': 0.25}
{'loss': 0.4177, 'learning_rate': 8.543217504316896e-05, 'epoch': 0.25}
{'loss': 0.4063, 'learning_rate': 8.541511486089237e-05, 'epoch': 0.25}
{'loss': 0.4378, 'learning_rate': 8.539804640054798e-05, 'epoch': 0.25}
{'loss': 0.4087, 'learning_rate': 8.538096966612548e-05, 'epoch': 0.25}
 25%|██▍       | 1621/6500 [10:45:17<65:03:52, 48.01s/it]                                                          25%|██▍       | 1621/6500 [10:45:17<65:03:52, 48.01s/it] 25%|██▍       | 1622/6500 [10:45:35<52:50:29, 39.00s/it]                                                          25%|██▍       | 1622/6500 [10:45:35<52:50:29, 39.00s/it] 25%|██▍       | 1623/6500 [10:45:53<44:16:51, 32.69s/it]                                                          25%|██▍       | 1623/6500 [10:45:53<44:16:51, 32.69s/it] 25%|██▍       | 1624/6500 [10:46:11<38:18:05, 28.28s/it]                                                          25%|██▍       | 1624/6500 [10:46:11<38:18:05, 28.28s/it] 25%|██▌       | 1625/6500 [10:46:29<34:06:34, 25.19s/it]                                                          25%|██▌       | 1625/6500 [10:46:29<34:06:34, 25.19s/it] 25%|██▌       | 1626/6500 [10:46:47<31:11:01, 23.03s/it]                        {'loss': 0.4371, 'learning_rate': 8.536388466161645e-05, 'epoch': 0.25}
{'loss': 0.4385, 'learning_rate': 8.534679139101439e-05, 'epoch': 0.25}
{'loss': 0.4179, 'learning_rate': 8.532968985831476e-05, 'epoch': 0.25}
{'loss': 0.4326, 'learning_rate': 8.531258006751492e-05, 'epoch': 0.25}
{'loss': 0.4288, 'learning_rate': 8.529546202261421e-05, 'epoch': 0.25}
                                  25%|██▌       | 1626/6500 [10:46:47<31:11:01, 23.03s/it] 25%|██▌       | 1627/6500 [10:47:05<29:08:47, 21.53s/it]                                                          25%|██▌       | 1627/6500 [10:47:05<29:08:47, 21.53s/it] 25%|██▌       | 1628/6500 [10:47:23<27:44:00, 20.49s/it]                                                          25%|██▌       | 1628/6500 [10:47:23<27:44:00, 20.49s/it] 25%|██▌       | 1629/6500 [10:47:41<26:45:24, 19.78s/it]                                                          25%|██▌       | 1629/6500 [10:47:41<26:45:24, 19.78s/it] 25%|██▌       | 1630/6500 [10:47:59<26:04:27, 19.27s/it]                                                          25%|██▌       | 1630/6500 [10:47:59<26:04:27, 19.27s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7752125859260559, 'eval_runtime': 5.9833, 'eval_samples_per_second': 3.844, 'eval_steps_per_second': 1.003, 'epoch': 0.25}
                                                          25%|██▌       | 1630/6500 [10:48:05<26:04:27, 19.27s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1630
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1630
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1630
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1630/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1630/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4246, 'learning_rate': 8.527833572761383e-05, 'epoch': 0.25}
{'loss': 0.4032, 'learning_rate': 8.526120118651699e-05, 'epoch': 0.25}
{'loss': 0.455, 'learning_rate': 8.524405840332875e-05, 'epoch': 0.25}
{'loss': 0.4182, 'learning_rate': 8.522690738205617e-05, 'epoch': 0.25}
{'loss': 0.4098, 'learning_rate': 8.520974812670814e-05, 'epoch': 0.25}
 25%|██▌       | 1631/6500 [10:49:19<50:21:45, 37.24s/it]                                                          25%|██▌       | 1631/6500 [10:49:19<50:21:45, 37.24s/it] 25%|██▌       | 1632/6500 [10:49:36<42:32:23, 31.46s/it]                                                          25%|██▌       | 1632/6500 [10:49:36<42:32:23, 31.46s/it] 25%|██▌       | 1633/6500 [10:49:55<37:10:20, 27.50s/it]                                                          25%|██▌       | 1633/6500 [10:49:55<37:10:20, 27.50s/it] 25%|██▌       | 1634/6500 [10:50:13<33:19:30, 24.65s/it]                                                          25%|██▌       | 1634/6500 [10:50:13<33:19:30, 24.65s/it] 25%|██▌       | 1635/6500 [10:50:31<30:38:38, 22.68s/it]                                                          25%|██▌       | 1635/6500 [10:50:31<30:38:38, 22.68s/it] 25%|██▌       | 1636/6500 [10:50:49<28:53:28, 21.38s/it]                        {'loss': 0.4009, 'learning_rate': 8.519258064129558e-05, 'epoch': 0.25}
{'loss': 0.4427, 'learning_rate': 8.517540492983127e-05, 'epoch': 0.25}
{'loss': 0.4866, 'learning_rate': 8.515822099632993e-05, 'epoch': 0.25}
{'loss': 0.4077, 'learning_rate': 8.514102884480819e-05, 'epoch': 0.25}
{'loss': 0.4095, 'learning_rate': 8.512382847928461e-05, 'epoch': 0.25}
                                  25%|██▌       | 1636/6500 [10:50:49<28:53:28, 21.38s/it] 25%|██▌       | 1637/6500 [10:51:07<27:35:49, 20.43s/it]                                                          25%|██▌       | 1637/6500 [10:51:07<27:35:49, 20.43s/it] 25%|██▌       | 1638/6500 [10:51:26<26:39:29, 19.74s/it]                                                          25%|██▌       | 1638/6500 [10:51:26<26:39:29, 19.74s/it] 25%|██▌       | 1639/6500 [10:51:44<26:02:17, 19.28s/it]                                                          25%|██▌       | 1639/6500 [10:51:44<26:02:17, 19.28s/it] 25%|██▌       | 1640/6500 [10:52:02<25:35:15, 18.95s/it]                                                          25%|██▌       | 1640/6500 [10:52:02<25:35:15, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7731268405914307, 'eval_runtime': 5.4851, 'eval_samples_per_second': 4.193, 'eval_steps_per_second': 1.094, 'epoch': 0.25}
                                                          25%|██▌       | 1640/6500 [10:52:07<25:35:15, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1640
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1640the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1640
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1640

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1640
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1640/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4232, 'learning_rate': 8.510661990377971e-05, 'epoch': 0.25}
{'loss': 0.9494, 'learning_rate': 8.508940312231584e-05, 'epoch': 0.25}
{'loss': 0.4293, 'learning_rate': 8.507217813891733e-05, 'epoch': 0.25}
{'loss': 0.4058, 'learning_rate': 8.505494495761043e-05, 'epoch': 0.25}
{'loss': 0.4018, 'learning_rate': 8.503770358242329e-05, 'epoch': 0.25}
 25%|██▌       | 1641/6500 [10:53:18<48:31:33, 35.95s/it]                                                          25%|██▌       | 1641/6500 [10:53:18<48:31:33, 35.95s/it] 25%|██▌       | 1642/6500 [10:53:36<41:13:56, 30.56s/it]                                                          25%|██▌       | 1642/6500 [10:53:36<41:13:56, 30.56s/it] 25%|██▌       | 1643/6500 [10:53:54<36:08:41, 26.79s/it]                                                          25%|██▌       | 1643/6500 [10:53:54<36:08:41, 26.79s/it] 25%|██▌       | 1644/6500 [10:54:12<32:35:09, 24.16s/it]                                                          25%|██▌       | 1644/6500 [10:54:12<32:35:09, 24.16s/it] 25%|██▌       | 1645/6500 [10:54:30<30:06:34, 22.33s/it]                                                          25%|██▌       | 1645/6500 [10:54:30<30:06:34, 22.33s/it] 25%|██▌       | 1646/6500 [10:54:48<28:22:42, 21.05s/it]                        {'loss': 0.4107, 'learning_rate': 8.502045401738595e-05, 'epoch': 0.25}
{'loss': 0.45, 'learning_rate': 8.500319626653042e-05, 'epoch': 0.25}
{'loss': 0.3984, 'learning_rate': 8.49859303338906e-05, 'epoch': 0.25}
{'loss': 0.4141, 'learning_rate': 8.496865622350227e-05, 'epoch': 0.25}
{'loss': 0.4065, 'learning_rate': 8.495137393940317e-05, 'epoch': 0.25}
                                  25%|██▌       | 1646/6500 [10:54:48<28:22:42, 21.05s/it] 25%|██▌       | 1647/6500 [10:55:06<27:10:11, 20.15s/it]                                                          25%|██▌       | 1647/6500 [10:55:06<27:10:11, 20.15s/it] 25%|██▌       | 1648/6500 [10:55:24<26:20:17, 19.54s/it]                                                          25%|██▌       | 1648/6500 [10:55:24<26:20:17, 19.54s/it] 25%|██▌       | 1649/6500 [10:55:42<25:55:12, 19.24s/it]                                                          25%|██▌       | 1649/6500 [10:55:42<25:55:12, 19.24s/it] 25%|██▌       | 1650/6500 [10:56:01<25:28:42, 18.91s/it]                                                          25%|██▌       | 1650/6500 [10:56:01<25:28:42, 18.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.773196280002594, 'eval_runtime': 5.4607, 'eval_samples_per_second': 4.212, 'eval_steps_per_second': 1.099, 'epoch': 0.25}
                                                          25%|██▌       | 1650/6500 [10:56:06<25:28:42, 18.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1650
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1650
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1650
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1650
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1650/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1650/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4128, 'learning_rate': 8.493408348563291e-05, 'epoch': 0.25}
{'loss': 0.4205, 'learning_rate': 8.491678486623304e-05, 'epoch': 0.25}
{'loss': 0.4179, 'learning_rate': 8.489947808524701e-05, 'epoch': 0.25}
{'loss': 0.4337, 'learning_rate': 8.488216314672018e-05, 'epoch': 0.25}
{'loss': 0.4056, 'learning_rate': 8.486484005469977e-05, 'epoch': 0.25}
 25%|██▌       | 1651/6500 [10:56:53<38:54:51, 28.89s/it]                                                          25%|██▌       | 1651/6500 [10:56:53<38:54:51, 28.89s/it] 25%|██▌       | 1652/6500 [10:57:11<34:31:31, 25.64s/it]                                                          25%|██▌       | 1652/6500 [10:57:11<34:31:31, 25.64s/it] 25%|██▌       | 1653/6500 [10:57:29<31:26:51, 23.36s/it]                                                          25%|██▌       | 1653/6500 [10:57:29<31:26:51, 23.36s/it] 25%|██▌       | 1654/6500 [10:57:47<29:18:19, 21.77s/it]                                                          25%|██▌       | 1654/6500 [10:57:47<29:18:19, 21.77s/it] 25%|██▌       | 1655/6500 [10:58:05<27:48:44, 20.67s/it]                                                          25%|██▌       | 1655/6500 [10:58:05<27:48:44, 20.67s/it] 25%|██▌       | 1656/6500 [10:58:23<26:46:14, 19.90s/it]                        {'loss': 0.4539, 'learning_rate': 8.4847508813235e-05, 'epoch': 0.25}
{'loss': 0.4244, 'learning_rate': 8.483016942637691e-05, 'epoch': 0.25}
{'loss': 0.4154, 'learning_rate': 8.48128218981785e-05, 'epoch': 0.26}
{'loss': 0.4278, 'learning_rate': 8.479546623269463e-05, 'epoch': 0.26}
{'loss': 0.4196, 'learning_rate': 8.47781024339821e-05, 'epoch': 0.26}
                                  25%|██▌       | 1656/6500 [10:58:23<26:46:14, 19.90s/it] 25%|██▌       | 1657/6500 [10:58:41<26:02:36, 19.36s/it]                                                          25%|██▌       | 1657/6500 [10:58:41<26:02:36, 19.36s/it] 26%|██▌       | 1658/6500 [10:58:59<25:32:47, 18.99s/it]                                                          26%|██▌       | 1658/6500 [10:58:59<25:32:47, 18.99s/it] 26%|██▌       | 1659/6500 [10:59:17<25:12:01, 18.74s/it]                                                          26%|██▌       | 1659/6500 [10:59:17<25:12:01, 18.74s/it] 26%|██▌       | 1660/6500 [10:59:36<24:57:34, 18.57s/it]                                                          26%|██▌       | 1660/6500 [10:59:36<24:57:34, 18.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7747694253921509, 'eval_runtime': 5.5401, 'eval_samples_per_second': 4.152, 'eval_steps_per_second': 1.083, 'epoch': 0.26}
                                                          26%|██▌       | 1660/6500 [10:59:41<24:57:34, 18.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1660
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1660
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1660/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1660


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1660
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1660/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4172, 'learning_rate': 8.476073050609958e-05, 'epoch': 0.26}
{'loss': 0.4224, 'learning_rate': 8.474335045310764e-05, 'epoch': 0.26}
{'loss': 0.4385, 'learning_rate': 8.472596227906876e-05, 'epoch': 0.26}
{'loss': 0.4083, 'learning_rate': 8.470856598804737e-05, 'epoch': 0.26}
{'loss': 0.4251, 'learning_rate': 8.469116158410969e-05, 'epoch': 0.26}
 26%|██▌       | 1661/6500 [11:00:14<32:47:31, 24.40s/it]                                                          26%|██▌       | 1661/6500 [11:00:14<32:47:31, 24.40s/it] 26%|██▌       | 1662/6500 [11:00:32<30:14:19, 22.50s/it]                                                          26%|██▌       | 1662/6500 [11:00:32<30:14:19, 22.50s/it] 26%|██▌       | 1663/6500 [11:00:50<28:26:11, 21.16s/it]                                                          26%|██▌       | 1663/6500 [11:00:50<28:26:11, 21.16s/it] 26%|██▌       | 1664/6500 [11:01:08<27:10:50, 20.23s/it]                                                          26%|██▌       | 1664/6500 [11:01:08<27:10:50, 20.23s/it] 26%|██▌       | 1665/6500 [11:01:26<26:24:33, 19.66s/it]                                                          26%|██▌       | 1665/6500 [11:01:26<26:24:33, 19.66s/it] 26%|██▌       | 1666/6500 [11:01:44<25:46:42, 19.20s/it]                        {'loss': 0.3921, 'learning_rate': 8.467374907132392e-05, 'epoch': 0.26}
{'loss': 0.4572, 'learning_rate': 8.465632845376013e-05, 'epoch': 0.26}
{'loss': 0.457, 'learning_rate': 8.463889973549027e-05, 'epoch': 0.26}
{'loss': 0.4023, 'learning_rate': 8.462146292058822e-05, 'epoch': 0.26}
{'loss': 0.4204, 'learning_rate': 8.460401801312969e-05, 'epoch': 0.26}
                                  26%|██▌       | 1666/6500 [11:01:44<25:46:42, 19.20s/it] 26%|██▌       | 1667/6500 [11:02:02<25:20:25, 18.88s/it]                                                          26%|██▌       | 1667/6500 [11:02:02<25:20:25, 18.88s/it] 26%|██▌       | 1668/6500 [11:02:20<25:02:35, 18.66s/it]                                                          26%|██▌       | 1668/6500 [11:02:20<25:02:35, 18.66s/it] 26%|██▌       | 1669/6500 [11:02:39<24:50:21, 18.51s/it]                                                          26%|██▌       | 1669/6500 [11:02:39<24:50:21, 18.51s/it] 26%|██▌       | 1670/6500 [11:02:57<24:42:41, 18.42s/it]                                                          26%|██▌       | 1670/6500 [11:02:57<24:42:41, 18.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7788051962852478, 'eval_runtime': 5.8337, 'eval_samples_per_second': 3.943, 'eval_steps_per_second': 1.029, 'epoch': 0.26}
                                                          26%|██▌       | 1670/6500 [11:03:03<24:42:41, 18.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1670
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1670the checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1670
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1670

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1670/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1670/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4083, 'learning_rate': 8.458656501719235e-05, 'epoch': 0.26}
{'loss': 0.9553, 'learning_rate': 8.456910393685573e-05, 'epoch': 0.26}
{'loss': 0.4215, 'learning_rate': 8.455163477620126e-05, 'epoch': 0.26}
{'loss': 0.4139, 'learning_rate': 8.453415753931222e-05, 'epoch': 0.26}
{'loss': 0.3822, 'learning_rate': 8.451667223027384e-05, 'epoch': 0.26}
 26%|██▌       | 1671/6500 [11:04:31<55:12:02, 41.15s/it]                                                          26%|██▌       | 1671/6500 [11:04:31<55:12:02, 41.15s/it] 26%|██▌       | 1672/6500 [11:04:50<46:25:12, 34.61s/it]                                                          26%|██▌       | 1672/6500 [11:04:51<46:25:12, 34.61s/it] 26%|██▌       | 1673/6500 [11:05:09<39:47:54, 29.68s/it]                                                          26%|██▌       | 1673/6500 [11:05:09<39:47:54, 29.68s/it] 26%|██▌       | 1674/6500 [11:05:27<35:05:46, 26.18s/it]                                                          26%|██▌       | 1674/6500 [11:05:27<35:05:46, 26.18s/it] 26%|██▌       | 1675/6500 [11:05:45<31:49:00, 23.74s/it]                                                          26%|██▌       | 1675/6500 [11:05:45<31:49:00, 23.74s/it] 26%|██▌       | 1676/6500 [11:06:03<29:31:31, 22.03s/it]                        {'loss': 0.422, 'learning_rate': 8.44991788531732e-05, 'epoch': 0.26}
{'loss': 0.4217, 'learning_rate': 8.448167741209925e-05, 'epoch': 0.26}
{'loss': 0.3715, 'learning_rate': 8.446416791114284e-05, 'epoch': 0.26}
{'loss': 0.4151, 'learning_rate': 8.444665035439674e-05, 'epoch': 0.26}
{'loss': 0.3925, 'learning_rate': 8.442912474595558e-05, 'epoch': 0.26}
                                  26%|██▌       | 1676/6500 [11:06:03<29:31:31, 22.03s/it] 26%|██▌       | 1677/6500 [11:06:21<27:56:02, 20.85s/it]                                                          26%|██▌       | 1677/6500 [11:06:21<27:56:02, 20.85s/it] 26%|██▌       | 1678/6500 [11:06:39<26:49:23, 20.03s/it]                                                          26%|██▌       | 1678/6500 [11:06:39<26:49:23, 20.03s/it] 26%|██▌       | 1679/6500 [11:06:57<26:03:07, 19.45s/it]                                                          26%|██▌       | 1679/6500 [11:06:57<26:03:07, 19.45s/it] 26%|██▌       | 1680/6500 [11:07:15<25:31:26, 19.06s/it]                                                          26%|██▌       | 1680/6500 [11:07:15<25:31:26, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7761590480804443, 'eval_runtime': 5.4178, 'eval_samples_per_second': 4.245, 'eval_steps_per_second': 1.107, 'epoch': 0.26}
                                                          26%|██▌       | 1680/6500 [11:07:21<25:31:26, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1680
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1680/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1680

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1680

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1680
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1680/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1680/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4091, 'learning_rate': 8.441159108991583e-05, 'epoch': 0.26}
{'loss': 0.4059, 'learning_rate': 8.439404939037587e-05, 'epoch': 0.26}
{'loss': 0.4186, 'learning_rate': 8.437649965143601e-05, 'epoch': 0.26}
{'loss': 0.4068, 'learning_rate': 8.435894187719834e-05, 'epoch': 0.26}
{'loss': 0.4166, 'learning_rate': 8.434137607176693e-05, 'epoch': 0.26}
 26%|██▌       | 1681/6500 [11:08:27<46:41:39, 34.88s/it]                                                          26%|██▌       | 1681/6500 [11:08:27<46:41:39, 34.88s/it] 26%|██▌       | 1682/6500 [11:08:45<39:54:43, 29.82s/it]                                                          26%|██▌       | 1682/6500 [11:08:45<39:54:43, 29.82s/it] 26%|██▌       | 1683/6500 [11:09:03<35:09:04, 26.27s/it]                                                          26%|██▌       | 1683/6500 [11:09:03<35:09:04, 26.27s/it] 26%|██▌       | 1684/6500 [11:09:21<31:59:52, 23.92s/it]                                                          26%|██▌       | 1684/6500 [11:09:21<31:59:52, 23.92s/it] 26%|██▌       | 1685/6500 [11:09:39<29:36:57, 22.14s/it]                                                          26%|██▌       | 1685/6500 [11:09:39<29:36:57, 22.14s/it] 26%|██▌       | 1686/6500 [11:09:57<27:58:07, 20.92s/it]                        {'loss': 0.4526, 'learning_rate': 8.432380223924766e-05, 'epoch': 0.26}
{'loss': 0.428, 'learning_rate': 8.430622038374831e-05, 'epoch': 0.26}
{'loss': 0.4292, 'learning_rate': 8.428863050937853e-05, 'epoch': 0.26}
{'loss': 0.4186, 'learning_rate': 8.427103262024985e-05, 'epoch': 0.26}
{'loss': 0.4068, 'learning_rate': 8.425342672047567e-05, 'epoch': 0.26}
                                  26%|██▌       | 1686/6500 [11:09:57<27:58:07, 20.92s/it] 26%|██▌       | 1687/6500 [11:10:16<26:49:34, 20.07s/it]                                                          26%|██▌       | 1687/6500 [11:10:16<26:49:34, 20.07s/it] 26%|██▌       | 1688/6500 [11:10:35<26:45:32, 20.02s/it]                                                          26%|██▌       | 1688/6500 [11:10:36<26:45:32, 20.02s/it] 26%|██▌       | 1689/6500 [11:10:54<26:02:48, 19.49s/it]                                                          26%|██▌       | 1689/6500 [11:10:54<26:02:48, 19.49s/it] 26%|██▌       | 1690/6500 [11:11:12<25:28:41, 19.07s/it]                                                          26%|██▌       | 1690/6500 [11:11:12<25:28:41, 19.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7743051052093506, 'eval_runtime': 6.5397, 'eval_samples_per_second': 3.517, 'eval_steps_per_second': 0.917, 'epoch': 0.26}
                                                          26%|██▌       | 1690/6500 [11:11:18<25:28:41, 19.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1690
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1690/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1690
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1690/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4095, 'learning_rate': 8.423581281417124e-05, 'epoch': 0.26}
{'loss': 0.434, 'learning_rate': 8.421819090545373e-05, 'epoch': 0.26}
{'loss': 0.4151, 'learning_rate': 8.420056099844216e-05, 'epoch': 0.26}
{'loss': 0.4036, 'learning_rate': 8.418292309725738e-05, 'epoch': 0.26}
{'loss': 0.4098, 'learning_rate': 8.416527720602218e-05, 'epoch': 0.26}
 26%|██▌       | 1691/6500 [11:12:27<47:47:49, 35.78s/it]                                                          26%|██▌       | 1691/6500 [11:12:27<47:47:49, 35.78s/it] 26%|██▌       | 1692/6500 [11:12:45<40:39:39, 30.44s/it]                                                          26%|██▌       | 1692/6500 [11:12:45<40:39:39, 30.44s/it] 26%|██▌       | 1693/6500 [11:13:03<35:40:28, 26.72s/it]                                                          26%|██▌       | 1693/6500 [11:13:03<35:40:28, 26.72s/it] 26%|██▌       | 1694/6500 [11:13:21<32:11:31, 24.11s/it]                                                          26%|██▌       | 1694/6500 [11:13:21<32:11:31, 24.11s/it] 26%|██▌       | 1695/6500 [11:13:39<29:45:32, 22.30s/it]                                                          26%|██▌       | 1695/6500 [11:13:39<29:45:32, 22.30s/it] 26%|██▌       | 1696/6500 [11:13:57<28:04:01, 21.03s/it]                        {'loss': 0.4272, 'learning_rate': 8.414762332886115e-05, 'epoch': 0.26}
{'loss': 0.4807, 'learning_rate': 8.412996146990079e-05, 'epoch': 0.26}
{'loss': 0.3972, 'learning_rate': 8.411229163326944e-05, 'epoch': 0.26}
{'loss': 0.3985, 'learning_rate': 8.409461382309733e-05, 'epoch': 0.26}
{'loss': 0.4201, 'learning_rate': 8.407692804351656e-05, 'epoch': 0.26}
                                  26%|██▌       | 1696/6500 [11:13:57<28:04:01, 21.03s/it] 26%|██▌       | 1697/6500 [11:14:15<26:59:08, 20.23s/it]                                                          26%|██▌       | 1697/6500 [11:14:15<26:59:08, 20.23s/it] 26%|██▌       | 1698/6500 [11:14:33<26:07:51, 19.59s/it]                                                          26%|██▌       | 1698/6500 [11:14:33<26:07:51, 19.59s/it] 26%|██▌       | 1699/6500 [11:14:51<25:32:27, 19.15s/it]                                                          26%|██▌       | 1699/6500 [11:14:51<25:32:27, 19.15s/it] 26%|██▌       | 1700/6500 [11:15:09<25:07:54, 18.85s/it]                                                          26%|██▌       | 1700/6500 [11:15:09<25:07:54, 18.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.778889536857605, 'eval_runtime': 5.3495, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.122, 'epoch': 0.26}
                                                          26%|██▌       | 1700/6500 [11:15:15<25:07:54, 18.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1700
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1700
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1700
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1700

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1700
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1700/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1700/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9309, 'learning_rate': 8.405923429866103e-05, 'epoch': 0.26}
{'loss': 0.4169, 'learning_rate': 8.404153259266657e-05, 'epoch': 0.26}
{'loss': 0.4068, 'learning_rate': 8.402382292967084e-05, 'epoch': 0.26}
{'loss': 0.4116, 'learning_rate': 8.400610531381338e-05, 'epoch': 0.26}
{'loss': 0.3874, 'learning_rate': 8.398837974923555e-05, 'epoch': 0.26}
 26%|██▌       | 1701/6500 [11:16:11<42:16:40, 31.72s/it]                                                          26%|██▌       | 1701/6500 [11:16:11<42:16:40, 31.72s/it] 26%|██▌       | 1702/6500 [11:16:29<36:47:24, 27.60s/it]                                                          26%|██▌       | 1702/6500 [11:16:29<36:47:24, 27.60s/it] 26%|██▌       | 1703/6500 [11:16:47<32:57:14, 24.73s/it]                                                          26%|██▌       | 1703/6500 [11:16:47<32:57:14, 24.73s/it] 26%|██▌       | 1704/6500 [11:17:05<30:16:45, 22.73s/it]                                                          26%|██▌       | 1704/6500 [11:17:05<30:16:45, 22.73s/it] 26%|██▌       | 1705/6500 [11:17:23<28:24:46, 21.33s/it]                                                          26%|██▌       | 1705/6500 [11:17:23<28:24:46, 21.33s/it] 26%|██▌       | 1706/6500 [11:17:41<27:06:24, 20.36s/it]                        {'loss': 0.4246, 'learning_rate': 8.397064624008062e-05, 'epoch': 0.26}
{'loss': 0.3996, 'learning_rate': 8.395290479049367e-05, 'epoch': 0.26}
{'loss': 0.3954, 'learning_rate': 8.393515540462164e-05, 'epoch': 0.26}
{'loss': 0.3983, 'learning_rate': 8.391739808661339e-05, 'epoch': 0.26}
{'loss': 0.3917, 'learning_rate': 8.389963284061955e-05, 'epoch': 0.26}
                                  26%|██▌       | 1706/6500 [11:17:41<27:06:24, 20.36s/it] 26%|██▋       | 1707/6500 [11:18:00<26:14:21, 19.71s/it]                                                          26%|██▋       | 1707/6500 [11:18:00<26:14:21, 19.71s/it] 26%|██▋       | 1708/6500 [11:18:18<25:36:15, 19.24s/it]                                                          26%|██▋       | 1708/6500 [11:18:18<25:36:15, 19.24s/it] 26%|██▋       | 1709/6500 [11:18:36<25:12:35, 18.94s/it]                                                          26%|██▋       | 1709/6500 [11:18:36<25:12:35, 18.94s/it] 26%|██▋       | 1710/6500 [11:18:54<24:53:31, 18.71s/it]                                                          26%|██▋       | 1710/6500 [11:18:54<24:53:31, 18.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7792060971260071, 'eval_runtime': 5.3457, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.122, 'epoch': 0.26}
                                                          26%|██▋       | 1710/6500 [11:19:00<24:53:31, 18.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1710
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1710
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1710
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1710

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1710
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1710/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1710/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.408, 'learning_rate': 8.388185967079265e-05, 'epoch': 0.26}
{'loss': 0.3932, 'learning_rate': 8.386407858128706e-05, 'epoch': 0.26}
{'loss': 0.4208, 'learning_rate': 8.384628957625899e-05, 'epoch': 0.26}
{'loss': 0.3882, 'learning_rate': 8.382849265986653e-05, 'epoch': 0.26}
{'loss': 0.4322, 'learning_rate': 8.381068783626959e-05, 'epoch': 0.26}
 26%|██▋       | 1711/6500 [11:20:42<60:37:21, 45.57s/it]                                                          26%|██▋       | 1711/6500 [11:20:42<60:37:21, 45.57s/it] 26%|██▋       | 1712/6500 [11:21:00<49:35:32, 37.29s/it]                                                          26%|██▋       | 1712/6500 [11:21:00<49:35:32, 37.29s/it] 26%|██▋       | 1713/6500 [11:21:19<42:01:44, 31.61s/it]                                                          26%|██▋       | 1713/6500 [11:21:19<42:01:44, 31.61s/it] 26%|██▋       | 1714/6500 [11:21:37<36:35:01, 27.52s/it]                                                          26%|██▋       | 1714/6500 [11:21:37<36:35:01, 27.52s/it] 26%|██▋       | 1715/6500 [11:21:55<32:47:10, 24.67s/it]                                                          26%|██▋       | 1715/6500 [11:21:55<32:47:10, 24.67s/it] 26%|██▋       | 1716/6500 [11:22:13<30:08:28, 22.68s/it]                        {'loss': 0.4153, 'learning_rate': 8.379287510962993e-05, 'epoch': 0.26}
{'loss': 0.3987, 'learning_rate': 8.377505448411118e-05, 'epoch': 0.26}
{'loss': 0.4127, 'learning_rate': 8.375722596387881e-05, 'epoch': 0.26}
{'loss': 0.4046, 'learning_rate': 8.373938955310011e-05, 'epoch': 0.26}
{'loss': 0.3925, 'learning_rate': 8.372154525594424e-05, 'epoch': 0.26}
                                  26%|██▋       | 1716/6500 [11:22:13<30:08:28, 22.68s/it] 26%|██▋       | 1717/6500 [11:22:31<28:17:16, 21.29s/it]                                                          26%|██▋       | 1717/6500 [11:22:31<28:17:16, 21.29s/it] 26%|██▋       | 1718/6500 [11:22:49<27:02:34, 20.36s/it]                                                          26%|██▋       | 1718/6500 [11:22:49<27:02:34, 20.36s/it] 26%|██▋       | 1719/6500 [11:23:07<26:10:58, 19.72s/it]                                                          26%|██▋       | 1719/6500 [11:23:07<26:10:58, 19.72s/it] 26%|██▋       | 1720/6500 [11:23:25<25:32:20, 19.23s/it]                                                          26%|██▋       | 1720/6500 [11:23:25<25:32:20, 19.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7785034775733948, 'eval_runtime': 5.5115, 'eval_samples_per_second': 4.173, 'eval_steps_per_second': 1.089, 'epoch': 0.26}
                                                          26%|██▋       | 1720/6500 [11:23:31<25:32:20, 19.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1720
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1720
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1720
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1720

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1720
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1720/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1720/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4127, 'learning_rate': 8.370369307658219e-05, 'epoch': 0.26}
{'loss': 0.43, 'learning_rate': 8.368583301918682e-05, 'epoch': 0.26}
{'loss': 0.3985, 'learning_rate': 8.36679650879328e-05, 'epoch': 0.27}
{'loss': 0.4041, 'learning_rate': 8.365008928699662e-05, 'epoch': 0.27}
{'loss': 0.3891, 'learning_rate': 8.363220562055669e-05, 'epoch': 0.27}
 26%|██▋       | 1721/6500 [11:24:22<40:17:44, 30.35s/it]                                                          26%|██▋       | 1721/6500 [11:24:22<40:17:44, 30.35s/it] 26%|██▋       | 1722/6500 [11:24:40<35:22:44, 26.66s/it]                                                          26%|██▋       | 1722/6500 [11:24:40<35:22:44, 26.66s/it] 27%|██▋       | 1723/6500 [11:24:58<31:55:47, 24.06s/it]                                                          27%|██▋       | 1723/6500 [11:24:58<31:55:47, 24.06s/it] 27%|██▋       | 1724/6500 [11:25:16<29:31:14, 22.25s/it]                                                          27%|██▋       | 1724/6500 [11:25:16<29:31:14, 22.25s/it] 27%|██▋       | 1725/6500 [11:25:34<27:49:56, 20.98s/it]                                                          27%|██▋       | 1725/6500 [11:25:34<27:49:56, 20.98s/it] 27%|██▋       | 1726/6500 [11:25:52<26:39:19, 20.10s/it]                        {'loss': 0.4345, 'learning_rate': 8.361431409279317e-05, 'epoch': 0.27}
{'loss': 0.4596, 'learning_rate': 8.359641470788811e-05, 'epoch': 0.27}
{'loss': 0.3978, 'learning_rate': 8.357850747002538e-05, 'epoch': 0.27}
{'loss': 0.3983, 'learning_rate': 8.35605923833907e-05, 'epoch': 0.27}
{'loss': 0.4025, 'learning_rate': 8.35426694521716e-05, 'epoch': 0.27}
                                  27%|██▋       | 1726/6500 [11:25:52<26:39:19, 20.10s/it] 27%|██▋       | 1727/6500 [11:26:10<25:50:45, 19.49s/it]                                                          27%|██▋       | 1727/6500 [11:26:10<25:50:45, 19.49s/it] 27%|██▋       | 1728/6500 [11:26:28<25:17:02, 19.07s/it]                                                          27%|██▋       | 1728/6500 [11:26:28<25:17:02, 19.07s/it] 27%|██▋       | 1729/6500 [11:26:46<24:53:30, 18.78s/it]                                                          27%|██▋       | 1729/6500 [11:26:46<24:53:30, 18.78s/it] 27%|██▋       | 1730/6500 [11:27:04<24:44:44, 18.68s/it]                                                          27%|██▋       | 1730/6500 [11:27:04<24:44:44, 18.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.783911406993866, 'eval_runtime': 5.3314, 'eval_samples_per_second': 4.314, 'eval_steps_per_second': 1.125, 'epoch': 0.27}
                                                          27%|██▋       | 1730/6500 [11:27:10<24:44:44, 18.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1730
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1730
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1730 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1730

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1730
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1730/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9267, 'learning_rate': 8.352473868055746e-05, 'epoch': 0.27}
{'loss': 0.4156, 'learning_rate': 8.350680007273947e-05, 'epoch': 0.27}
{'loss': 0.4184, 'learning_rate': 8.348885363291071e-05, 'epoch': 0.27}
{'loss': 0.3784, 'learning_rate': 8.3470899365266e-05, 'epoch': 0.27}
{'loss': 0.3938, 'learning_rate': 8.345293727400209e-05, 'epoch': 0.27}
 27%|██▋       | 1731/6500 [11:28:19<46:53:32, 35.40s/it]                                                          27%|██▋       | 1731/6500 [11:28:19<46:53:32, 35.40s/it] 27%|██▋       | 1732/6500 [11:28:37<39:57:51, 30.17s/it]                                                          27%|██▋       | 1732/6500 [11:28:37<39:57:51, 30.17s/it] 27%|██▋       | 1733/6500 [11:28:55<35:15:53, 26.63s/it]                                                          27%|██▋       | 1733/6500 [11:28:55<35:15:53, 26.63s/it] 27%|██▋       | 1734/6500 [11:29:13<31:50:01, 24.05s/it]                                                          27%|██▋       | 1734/6500 [11:29:13<31:50:01, 24.05s/it] 27%|██▋       | 1735/6500 [11:29:31<29:26:30, 22.24s/it]                                                          27%|██▋       | 1735/6500 [11:29:31<29:26:30, 22.24s/it] 27%|██▋       | 1736/6500 [11:29:49<27:46:43, 20.99s/it]                        {'loss': 0.4224, 'learning_rate': 8.343496736331749e-05, 'epoch': 0.27}
{'loss': 0.3862, 'learning_rate': 8.341698963741256e-05, 'epoch': 0.27}
{'loss': 0.3989, 'learning_rate': 8.339900410048947e-05, 'epoch': 0.27}
{'loss': 0.3933, 'learning_rate': 8.338101075675225e-05, 'epoch': 0.27}
{'loss': 0.396, 'learning_rate': 8.336300961040673e-05, 'epoch': 0.27}
                                  27%|██▋       | 1736/6500 [11:29:49<27:46:43, 20.99s/it] 27%|██▋       | 1737/6500 [11:30:07<26:37:27, 20.12s/it]                                                          27%|██▋       | 1737/6500 [11:30:07<26:37:27, 20.12s/it] 27%|██▋       | 1738/6500 [11:30:26<25:49:42, 19.53s/it]                                                          27%|██▋       | 1738/6500 [11:30:26<25:49:42, 19.53s/it] 27%|██▋       | 1739/6500 [11:30:44<25:16:41, 19.11s/it]                                                          27%|██▋       | 1739/6500 [11:30:44<25:16:41, 19.11s/it] 27%|██▋       | 1740/6500 [11:31:02<24:53:24, 18.82s/it]                                                          27%|██▋       | 1740/6500 [11:31:02<24:53:24, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7767433524131775, 'eval_runtime': 5.3427, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.27}
                                                          27%|██▋       | 1740/6500 [11:31:07<24:53:24, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1740
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1740
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1740

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1740
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1740/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1740/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3891, 'learning_rate': 8.334500066566054e-05, 'epoch': 0.27}
{'loss': 0.3986, 'learning_rate': 8.33269839267232e-05, 'epoch': 0.27}
{'loss': 0.4122, 'learning_rate': 8.330895939780601e-05, 'epoch': 0.27}
{'loss': 0.3875, 'learning_rate': 8.329092708312207e-05, 'epoch': 0.27}
{'loss': 0.4457, 'learning_rate': 8.327288698688634e-05, 'epoch': 0.27}
 27%|██▋       | 1741/6500 [11:32:23<49:26:33, 37.40s/it]                                                          27%|██▋       | 1741/6500 [11:32:23<49:26:33, 37.40s/it] 27%|██▋       | 1742/6500 [11:32:41<41:44:12, 31.58s/it]                                                          27%|██▋       | 1742/6500 [11:32:41<41:44:12, 31.58s/it] 27%|██▋       | 1743/6500 [11:32:59<36:20:43, 27.51s/it]                                                          27%|██▋       | 1743/6500 [11:32:59<36:20:43, 27.51s/it] 27%|██▋       | 1744/6500 [11:33:17<32:35:24, 24.67s/it]                                                          27%|██▋       | 1744/6500 [11:33:17<32:35:24, 24.67s/it] 27%|██▋       | 1745/6500 [11:33:35<29:58:11, 22.69s/it]                                                          27%|██▋       | 1745/6500 [11:33:35<29:58:11, 22.69s/it] 27%|██▋       | 1746/6500 [11:33:53<28:18:22, 21.44s/it]                        {'loss': 0.4048, 'learning_rate': 8.325483911331557e-05, 'epoch': 0.27}
{'loss': 0.4078, 'learning_rate': 8.323678346662835e-05, 'epoch': 0.27}
{'loss': 0.4197, 'learning_rate': 8.321872005104509e-05, 'epoch': 0.27}
{'loss': 0.3996, 'learning_rate': 8.3200648870788e-05, 'epoch': 0.27}
{'loss': 0.4143, 'learning_rate': 8.318256993008107e-05, 'epoch': 0.27}
                                  27%|██▋       | 1746/6500 [11:33:53<28:18:22, 21.44s/it] 27%|██▋       | 1747/6500 [11:34:11<26:58:36, 20.43s/it]                                                          27%|██▋       | 1747/6500 [11:34:11<26:58:36, 20.43s/it] 27%|██▋       | 1748/6500 [11:34:29<26:02:51, 19.73s/it]                                                          27%|██▋       | 1748/6500 [11:34:29<26:02:51, 19.73s/it] 27%|██▋       | 1749/6500 [11:34:48<25:25:51, 19.27s/it]                                                          27%|██▋       | 1749/6500 [11:34:48<25:25:51, 19.27s/it] 27%|██▋       | 1750/6500 [11:35:06<25:05:10, 19.01s/it]                                                          27%|██▋       | 1750/6500 [11:35:06<25:05:10, 19.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7768497467041016, 'eval_runtime': 5.3489, 'eval_samples_per_second': 4.3, 'eval_steps_per_second': 1.122, 'epoch': 0.27}
                                                          27%|██▋       | 1750/6500 [11:35:11<25:05:10, 19.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1750
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1750
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1750
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1750

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1750
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1750/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4023, 'learning_rate': 8.316448323315021e-05, 'epoch': 0.27}
{'loss': 0.4184, 'learning_rate': 8.3146388784223e-05, 'epoch': 0.27}
{'loss': 0.3947, 'learning_rate': 8.312828658752896e-05, 'epoch': 0.27}
{'loss': 0.395, 'learning_rate': 8.311017664729935e-05, 'epoch': 0.27}
{'loss': 0.3852, 'learning_rate': 8.309205896776727e-05, 'epoch': 0.27}
 27%|██▋       | 1751/6500 [11:36:26<49:11:22, 37.29s/it]                                                          27%|██▋       | 1751/6500 [11:36:26<49:11:22, 37.29s/it] 27%|██▋       | 1752/6500 [11:36:44<41:32:22, 31.50s/it]                                                          27%|██▋       | 1752/6500 [11:36:44<41:32:22, 31.50s/it] 27%|██▋       | 1753/6500 [11:37:02<36:11:23, 27.45s/it]                                                          27%|██▋       | 1753/6500 [11:37:02<36:11:23, 27.45s/it] 27%|██▋       | 1754/6500 [11:37:20<32:26:46, 24.61s/it]                                                          27%|██▋       | 1754/6500 [11:37:20<32:26:46, 24.61s/it] 27%|██▋       | 1755/6500 [11:37:38<29:50:27, 22.64s/it]                                                          27%|██▋       | 1755/6500 [11:37:38<29:50:27, 22.64s/it] 27%|██▋       | 1756/6500 [11:37:56<28:01:22, 21.27s/it]                        {'loss': 0.4806, 'learning_rate': 8.307393355316761e-05, 'epoch': 0.27}
{'loss': 0.4129, 'learning_rate': 8.305580040773706e-05, 'epoch': 0.27}
{'loss': 0.3926, 'learning_rate': 8.303765953571417e-05, 'epoch': 0.27}
{'loss': 0.4058, 'learning_rate': 8.30195109413392e-05, 'epoch': 0.27}
{'loss': 0.933, 'learning_rate': 8.300135462885435e-05, 'epoch': 0.27}
                                  27%|██▋       | 1756/6500 [11:37:56<28:01:22, 21.27s/it] 27%|██▋       | 1757/6500 [11:38:14<26:45:25, 20.31s/it]                                                          27%|██▋       | 1757/6500 [11:38:14<26:45:25, 20.31s/it] 27%|██▋       | 1758/6500 [11:38:32<25:52:52, 19.65s/it]                                                          27%|██▋       | 1758/6500 [11:38:32<25:52:52, 19.65s/it] 27%|██▋       | 1759/6500 [11:38:50<25:16:30, 19.19s/it]                                                          27%|██▋       | 1759/6500 [11:38:50<25:16:30, 19.19s/it] 27%|██▋       | 1760/6500 [11:39:08<24:50:38, 18.87s/it]                                                          27%|██▋       | 1760/6500 [11:39:08<24:50:38, 18.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.778144359588623, 'eval_runtime': 5.3327, 'eval_samples_per_second': 4.313, 'eval_steps_per_second': 1.125, 'epoch': 0.27}
                                                          27%|██▋       | 1760/6500 [11:39:14<24:50:38, 18.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1760
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1760/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1760

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1760

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1760
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1760/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4116, 'learning_rate': 8.298319060250348e-05, 'epoch': 0.27}
{'loss': 0.405, 'learning_rate': 8.296501886653236e-05, 'epoch': 0.27}
{'loss': 0.406, 'learning_rate': 8.29468394251885e-05, 'epoch': 0.27}
{'loss': 0.3727, 'learning_rate': 8.292865228272126e-05, 'epoch': 0.27}
{'loss': 0.4134, 'learning_rate': 8.291045744338175e-05, 'epoch': 0.27}
 27%|██▋       | 1761/6500 [11:40:18<44:48:42, 34.04s/it]                                                          27%|██▋       | 1761/6500 [11:40:18<44:48:42, 34.04s/it] 27%|██▋       | 1762/6500 [11:40:36<38:40:22, 29.38s/it]                                                          27%|██▋       | 1762/6500 [11:40:36<38:40:22, 29.38s/it] 27%|██▋       | 1763/6500 [11:40:54<34:09:24, 25.96s/it]                                                          27%|██▋       | 1763/6500 [11:40:54<34:09:24, 25.96s/it] 27%|██▋       | 1764/6500 [11:41:12<31:00:15, 23.57s/it]                                                          27%|██▋       | 1764/6500 [11:41:12<31:00:15, 23.57s/it] 27%|██▋       | 1765/6500 [11:41:30<28:48:08, 21.90s/it]                                                          27%|██▋       | 1765/6500 [11:41:30<28:48:08, 21.90s/it] 27%|██▋       | 1766/6500 [11:41:48<27:16:17, 20.74s/it]                        {'loss': 0.4017, 'learning_rate': 8.289225491142292e-05, 'epoch': 0.27}
{'loss': 0.3677, 'learning_rate': 8.287404469109947e-05, 'epoch': 0.27}
{'loss': 0.4004, 'learning_rate': 8.285582678666797e-05, 'epoch': 0.27}
{'loss': 0.3792, 'learning_rate': 8.283760120238672e-05, 'epoch': 0.27}
{'loss': 0.3955, 'learning_rate': 8.281936794251586e-05, 'epoch': 0.27}
                                  27%|██▋       | 1766/6500 [11:41:48<27:16:17, 20.74s/it] 27%|██▋       | 1767/6500 [11:42:07<26:12:42, 19.94s/it]                                                          27%|██▋       | 1767/6500 [11:42:07<26:12:42, 19.94s/it] 27%|██▋       | 1768/6500 [11:42:25<25:28:03, 19.38s/it]                                                          27%|██▋       | 1768/6500 [11:42:25<25:28:03, 19.38s/it] 27%|██▋       | 1769/6500 [11:42:43<24:57:42, 18.99s/it]                                                          27%|██▋       | 1769/6500 [11:42:43<24:57:42, 18.99s/it] 27%|██▋       | 1770/6500 [11:43:01<24:36:41, 18.73s/it]                                                          27%|██▋       | 1770/6500 [11:43:01<24:36:41, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7798622846603394, 'eval_runtime': 5.3424, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.27}
                                                          27%|██▋       | 1770/6500 [11:43:06<24:36:41, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1770
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1770
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1770
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1770

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1770
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1770/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1770/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3878, 'learning_rate': 8.280112701131726e-05, 'epoch': 0.27}
{'loss': 0.4011, 'learning_rate': 8.278287841305468e-05, 'epoch': 0.27}
{'loss': 0.3908, 'learning_rate': 8.276462215199357e-05, 'epoch': 0.27}
{'loss': 0.3976, 'learning_rate': 8.274635823240127e-05, 'epoch': 0.27}
{'loss': 0.4251, 'learning_rate': 8.27280866585468e-05, 'epoch': 0.27}
 27%|██▋       | 1771/6500 [11:44:44<57:46:49, 43.99s/it]                                                          27%|██▋       | 1771/6500 [11:44:44<57:46:49, 43.99s/it] 27%|██▋       | 1772/6500 [11:45:02<47:30:22, 36.17s/it]                                                          27%|██▋       | 1772/6500 [11:45:02<47:30:22, 36.17s/it] 27%|██▋       | 1773/6500 [11:45:20<40:19:10, 30.71s/it]                                                          27%|██▋       | 1773/6500 [11:45:20<40:19:10, 30.71s/it] 27%|██▋       | 1774/6500 [11:45:38<35:17:49, 26.89s/it]                                                          27%|██▋       | 1774/6500 [11:45:38<35:17:49, 26.89s/it] 27%|██▋       | 1775/6500 [11:45:56<31:47:44, 24.23s/it]                                                          27%|██▋       | 1775/6500 [11:45:56<31:47:44, 24.23s/it] 27%|██▋       | 1776/6500 [11:46:14<29:21:12, 22.37s/it]                        {'loss': 0.3988, 'learning_rate': 8.27098074347011e-05, 'epoch': 0.27}
{'loss': 0.4031, 'learning_rate': 8.269152056513678e-05, 'epoch': 0.27}
{'loss': 0.3949, 'learning_rate': 8.26732260541283e-05, 'epoch': 0.27}
{'loss': 0.3943, 'learning_rate': 8.265492390595186e-05, 'epoch': 0.27}
{'loss': 0.3821, 'learning_rate': 8.263661412488552e-05, 'epoch': 0.27}
                                  27%|██▋       | 1776/6500 [11:46:14<29:21:12, 22.37s/it] 27%|██▋       | 1777/6500 [11:46:32<27:39:13, 21.08s/it]                                                          27%|██▋       | 1777/6500 [11:46:32<27:39:13, 21.08s/it] 27%|██▋       | 1778/6500 [11:46:50<26:34:46, 20.26s/it]                                                          27%|██▋       | 1778/6500 [11:46:50<26:34:46, 20.26s/it] 27%|██▋       | 1779/6500 [11:47:08<25:46:10, 19.65s/it]                                                          27%|██▋       | 1779/6500 [11:47:08<25:46:10, 19.65s/it] 27%|██▋       | 1780/6500 [11:47:27<25:12:54, 19.23s/it]                                                          27%|██▋       | 1780/6500 [11:47:27<25:12:54, 19.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7773253321647644, 'eval_runtime': 5.3418, 'eval_samples_per_second': 4.306, 'eval_steps_per_second': 1.123, 'epoch': 0.27}
                                                          27%|██▋       | 1780/6500 [11:47:32<25:12:54, 19.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1780
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1780/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1780/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1780



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1780
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1780/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1780/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4293, 'learning_rate': 8.261829671520907e-05, 'epoch': 0.27}
{'loss': 0.3935, 'learning_rate': 8.259997168120409e-05, 'epoch': 0.27}
{'loss': 0.3988, 'learning_rate': 8.258163902715392e-05, 'epoch': 0.27}
{'loss': 0.3858, 'learning_rate': 8.256329875734375e-05, 'epoch': 0.27}
{'loss': 0.4142, 'learning_rate': 8.254495087606045e-05, 'epoch': 0.27}
 27%|██▋       | 1781/6500 [11:48:51<50:41:20, 38.67s/it]                                                          27%|██▋       | 1781/6500 [11:48:51<50:41:20, 38.67s/it] 27%|██▋       | 1782/6500 [11:49:09<42:39:26, 32.55s/it]                                                          27%|██▋       | 1782/6500 [11:49:09<42:39:26, 32.55s/it] 27%|██▋       | 1783/6500 [11:49:27<36:56:05, 28.19s/it]                                                          27%|██▋       | 1783/6500 [11:49:27<36:56:05, 28.19s/it] 27%|██▋       | 1784/6500 [11:49:45<32:54:42, 25.12s/it]                                                          27%|██▋       | 1784/6500 [11:49:45<32:54:42, 25.12s/it] 27%|██▋       | 1785/6500 [11:50:03<30:05:45, 22.98s/it]                                                          27%|██▋       | 1785/6500 [11:50:03<30:05:45, 22.98s/it] 27%|██▋       | 1786/6500 [11:50:21<28:08:32, 21.49s/it]                        {'loss': 0.4559, 'learning_rate': 8.252659538759279e-05, 'epoch': 0.27}
{'loss': 0.3965, 'learning_rate': 8.250823229623122e-05, 'epoch': 0.27}
{'loss': 0.3829, 'learning_rate': 8.2489861606268e-05, 'epoch': 0.28}
{'loss': 0.3947, 'learning_rate': 8.247148332199715e-05, 'epoch': 0.28}
{'loss': 0.9229, 'learning_rate': 8.245309744771452e-05, 'epoch': 0.28}
                                  27%|██▋       | 1786/6500 [11:50:21<28:08:32, 21.49s/it] 27%|██▋       | 1787/6500 [11:50:39<26:47:04, 20.46s/it]                                                          27%|██▋       | 1787/6500 [11:50:39<26:47:04, 20.46s/it] 28%|██▊       | 1788/6500 [11:50:57<25:50:16, 19.74s/it]                                                          28%|██▊       | 1788/6500 [11:50:57<25:50:16, 19.74s/it] 28%|██▊       | 1789/6500 [11:51:15<25:10:56, 19.24s/it]                                                          28%|██▊       | 1789/6500 [11:51:15<25:10:56, 19.24s/it] 28%|██▊       | 1790/6500 [11:51:33<24:43:39, 18.90s/it]                                                          28%|██▊       | 1790/6500 [11:51:33<24:43:39, 18.90s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7843990325927734, 'eval_runtime': 5.3563, 'eval_samples_per_second': 4.294, 'eval_steps_per_second': 1.12, 'epoch': 0.28}
                                                          28%|██▊       | 1790/6500 [11:51:38<24:43:39, 18.90s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1790
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1790
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1790
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1790

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1790
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1790/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4068, 'learning_rate': 8.24347039877177e-05, 'epoch': 0.28}
{'loss': 0.3904, 'learning_rate': 8.241630294630599e-05, 'epoch': 0.28}
{'loss': 0.3777, 'learning_rate': 8.239789432778058e-05, 'epoch': 0.28}
{'loss': 0.3913, 'learning_rate': 8.237947813644436e-05, 'epoch': 0.28}
{'loss': 0.4201, 'learning_rate': 8.236105437660198e-05, 'epoch': 0.28}
 28%|██▊       | 1791/6500 [11:52:47<46:13:56, 35.34s/it]                                                          28%|██▊       | 1791/6500 [11:52:47<46:13:56, 35.34s/it] 28%|██▊       | 1792/6500 [11:53:05<39:24:31, 30.13s/it]                                                          28%|██▊       | 1792/6500 [11:53:05<39:24:31, 30.13s/it] 28%|██▊       | 1793/6500 [11:53:23<34:38:14, 26.49s/it]                                                          28%|██▊       | 1793/6500 [11:53:23<34:38:14, 26.49s/it] 28%|██▊       | 1794/6500 [11:53:41<31:27:52, 24.07s/it]                                                          28%|██▊       | 1794/6500 [11:53:41<31:27:52, 24.07s/it] 28%|██▊       | 1795/6500 [11:53:59<29:08:47, 22.30s/it]                                                          28%|██▊       | 1795/6500 [11:53:59<29:08:47, 22.30s/it] 28%|██▊       | 1796/6500 [11:54:17<27:28:52, 21.03s/it]                        {'loss': 0.3754, 'learning_rate': 8.234262305255991e-05, 'epoch': 0.28}
{'loss': 0.3859, 'learning_rate': 8.232418416862633e-05, 'epoch': 0.28}
{'loss': 0.3765, 'learning_rate': 8.230573772911126e-05, 'epoch': 0.28}
{'loss': 0.3841, 'learning_rate': 8.228728373832642e-05, 'epoch': 0.28}
{'loss': 0.3838, 'learning_rate': 8.226882220058529e-05, 'epoch': 0.28}
                                  28%|██▊       | 1796/6500 [11:54:17<27:28:52, 21.03s/it] 28%|██▊       | 1797/6500 [11:54:36<26:19:32, 20.15s/it]                                                          28%|██▊       | 1797/6500 [11:54:36<26:19:32, 20.15s/it] 28%|██▊       | 1798/6500 [11:54:54<25:31:23, 19.54s/it]                                                          28%|██▊       | 1798/6500 [11:54:54<25:31:23, 19.54s/it] 28%|██▊       | 1799/6500 [11:55:12<24:57:56, 19.12s/it]                                                          28%|██▊       | 1799/6500 [11:55:12<24:57:56, 19.12s/it] 28%|██▊       | 1800/6500 [11:55:30<24:34:48, 18.83s/it]                                                          28%|██▊       | 1800/6500 [11:55:30<24:34:48, 18.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7800354957580566, 'eval_runtime': 5.3329, 'eval_samples_per_second': 4.313, 'eval_steps_per_second': 1.125, 'epoch': 0.28}
                                                          28%|██▊       | 1800/6500 [11:55:35<24:34:48, 18.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1800
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1800/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1800/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1800



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1800
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1800/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1800/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3776, 'learning_rate': 8.225035312020318e-05, 'epoch': 0.28}
{'loss': 0.4111, 'learning_rate': 8.223187650149712e-05, 'epoch': 0.28}
{'loss': 0.3819, 'learning_rate': 8.221339234878589e-05, 'epoch': 0.28}
{'loss': 0.4115, 'learning_rate': 8.219490066639007e-05, 'epoch': 0.28}
{'loss': 0.3973, 'learning_rate': 8.217640145863197e-05, 'epoch': 0.28}
 28%|██▊       | 1801/6500 [11:56:28<39:50:43, 30.53s/it]                                                          28%|██▊       | 1801/6500 [11:56:28<39:50:43, 30.53s/it] 28%|██▊       | 1802/6500 [11:56:46<34:57:00, 26.78s/it]                                                          28%|██▊       | 1802/6500 [11:56:46<34:57:00, 26.78s/it] 28%|██▊       | 1803/6500 [11:57:04<31:31:09, 24.16s/it]                                                          28%|██▊       | 1803/6500 [11:57:04<31:31:09, 24.16s/it] 28%|██▊       | 1804/6500 [11:57:22<29:07:05, 22.32s/it]                                                          28%|██▊       | 1804/6500 [11:57:22<29:07:05, 22.32s/it] 28%|██▊       | 1805/6500 [11:57:40<27:26:06, 21.04s/it]                                                          28%|██▊       | 1805/6500 [11:57:40<27:26:06, 21.04s/it] 28%|██▊       | 1806/6500 [11:57:58<26:15:59, 20.14s/it]                        {'loss': 0.389, 'learning_rate': 8.215789472983565e-05, 'epoch': 0.28}
{'loss': 0.4013, 'learning_rate': 8.213938048432697e-05, 'epoch': 0.28}
{'loss': 0.4003, 'learning_rate': 8.212085872643351e-05, 'epoch': 0.28}
{'loss': 0.3915, 'learning_rate': 8.210232946048462e-05, 'epoch': 0.28}
{'loss': 0.4, 'learning_rate': 8.208379269081141e-05, 'epoch': 0.28}
                                  28%|██▊       | 1806/6500 [11:57:58<26:15:59, 20.14s/it] 28%|██▊       | 1807/6500 [11:58:16<25:27:41, 19.53s/it]                                                          28%|██▊       | 1807/6500 [11:58:16<25:27:41, 19.53s/it] 28%|██▊       | 1808/6500 [11:58:34<24:54:14, 19.11s/it]                                                          28%|██▊       | 1808/6500 [11:58:34<24:54:14, 19.11s/it] 28%|██▊       | 1809/6500 [11:58:52<24:31:15, 18.82s/it]                                                          28%|██▊       | 1809/6500 [11:58:52<24:31:15, 18.82s/it] 28%|██▊       | 1810/6500 [11:59:11<24:22:03, 18.70s/it]                                                          28%|██▊       | 1810/6500 [11:59:11<24:22:03, 18.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.775834858417511, 'eval_runtime': 5.6365, 'eval_samples_per_second': 4.081, 'eval_steps_per_second': 1.064, 'epoch': 0.28}
                                                          28%|██▊       | 1810/6500 [11:59:16<24:22:03, 18.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1810
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1810
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1810
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1810

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1810
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1810/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.421, 'learning_rate': 8.206524842174672e-05, 'epoch': 0.28}
{'loss': 0.3795, 'learning_rate': 8.204669665762519e-05, 'epoch': 0.28}
{'loss': 0.3876, 'learning_rate': 8.202813740278314e-05, 'epoch': 0.28}
{'loss': 0.3665, 'learning_rate': 8.200957066155872e-05, 'epoch': 0.28}
{'loss': 0.4292, 'learning_rate': 8.199099643829177e-05, 'epoch': 0.28}
 28%|██▊       | 1811/6500 [12:00:25<46:04:56, 35.38s/it]                                                          28%|██▊       | 1811/6500 [12:00:25<46:04:56, 35.38s/it] 28%|██▊       | 1812/6500 [12:00:43<39:17:14, 30.17s/it]                                                          28%|██▊       | 1812/6500 [12:00:43<39:17:14, 30.17s/it] 28%|██▊       | 1813/6500 [12:01:01<34:32:20, 26.53s/it]                                                          28%|██▊       | 1813/6500 [12:01:01<34:32:20, 26.53s/it] 28%|██▊       | 1814/6500 [12:01:19<31:13:25, 23.99s/it]                                                          28%|██▊       | 1814/6500 [12:01:19<31:13:25, 23.99s/it] 28%|██▊       | 1815/6500 [12:01:37<28:54:23, 22.21s/it]                                                          28%|██▊       | 1815/6500 [12:01:37<28:54:23, 22.21s/it] 28%|██▊       | 1816/6500 [12:01:55<27:17:34, 20.98s/it]                        {'loss': 0.4366, 'learning_rate': 8.197241473732392e-05, 'epoch': 0.28}
{'loss': 0.3848, 'learning_rate': 8.195382556299852e-05, 'epoch': 0.28}
{'loss': 0.3984, 'learning_rate': 8.193522891966067e-05, 'epoch': 0.28}
{'loss': 0.3808, 'learning_rate': 8.191662481165724e-05, 'epoch': 0.28}
{'loss': 0.9213, 'learning_rate': 8.189801324333681e-05, 'epoch': 0.28}
                                  28%|██▊       | 1816/6500 [12:01:55<27:17:34, 20.98s/it] 28%|██▊       | 1817/6500 [12:02:13<26:10:23, 20.12s/it]                                                          28%|██▊       | 1817/6500 [12:02:13<26:10:23, 20.12s/it] 28%|██▊       | 1818/6500 [12:02:32<25:23:44, 19.53s/it]                                                          28%|██▊       | 1818/6500 [12:02:32<25:23:44, 19.53s/it] 28%|██▊       | 1819/6500 [12:02:50<24:51:09, 19.11s/it]                                                          28%|██▊       | 1819/6500 [12:02:50<24:51:09, 19.11s/it] 28%|██▊       | 1820/6500 [12:03:08<24:28:06, 18.82s/it]                                                          28%|██▊       | 1820/6500 [12:03:08<24:28:06, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7868393063545227, 'eval_runtime': 5.3425, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.28}
                                                          28%|██▊       | 1820/6500 [12:03:13<24:28:06, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1820
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1820
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1820
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1820

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1820
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1820/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4008, 'learning_rate': 8.187939421904973e-05, 'epoch': 0.28}
{'loss': 0.4017, 'learning_rate': 8.186076774314809e-05, 'epoch': 0.28}
{'loss': 0.3728, 'learning_rate': 8.184213381998568e-05, 'epoch': 0.28}
{'loss': 0.4026, 'learning_rate': 8.182349245391812e-05, 'epoch': 0.28}
{'loss': 0.3967, 'learning_rate': 8.180484364930267e-05, 'epoch': 0.28}
 28%|██▊       | 1821/6500 [12:04:36<51:31:02, 39.64s/it]                                                          28%|██▊       | 1821/6500 [12:04:36<51:31:02, 39.64s/it] 28%|██▊       | 1822/6500 [12:04:54<43:03:45, 33.14s/it]                                                          28%|██▊       | 1822/6500 [12:04:54<43:03:45, 33.14s/it] 28%|██▊       | 1823/6500 [12:05:12<37:08:15, 28.59s/it]                                                          28%|██▊       | 1823/6500 [12:05:12<37:08:15, 28.59s/it] 28%|██▊       | 1824/6500 [12:05:30<32:59:40, 25.40s/it]                                                          28%|██▊       | 1824/6500 [12:05:30<32:59:40, 25.40s/it] 28%|██▊       | 1825/6500 [12:05:48<30:06:06, 23.18s/it]                                                          28%|██▊       | 1825/6500 [12:05:48<30:06:06, 23.18s/it] 28%|██▊       | 1826/6500 [12:06:06<28:05:32, 21.64s/it]                        {'loss': 0.3535, 'learning_rate': 8.178618741049842e-05, 'epoch': 0.28}
{'loss': 0.3895, 'learning_rate': 8.17675237418661e-05, 'epoch': 0.28}
{'loss': 0.3792, 'learning_rate': 8.174885264776826e-05, 'epoch': 0.28}
{'loss': 0.3972, 'learning_rate': 8.173017413256915e-05, 'epoch': 0.28}
{'loss': 0.3921, 'learning_rate': 8.171148820063476e-05, 'epoch': 0.28}
                                  28%|██▊       | 1826/6500 [12:06:06<28:05:32, 21.64s/it] 28%|██▊       | 1827/6500 [12:06:25<26:54:16, 20.73s/it]                                                          28%|██▊       | 1827/6500 [12:06:25<26:54:16, 20.73s/it] 28%|██▊       | 1828/6500 [12:06:43<25:52:29, 19.94s/it]                                                          28%|██▊       | 1828/6500 [12:06:43<25:52:29, 19.94s/it] 28%|██▊       | 1829/6500 [12:07:01<25:09:38, 19.39s/it]                                                          28%|██▊       | 1829/6500 [12:07:01<25:09:38, 19.39s/it] 28%|██▊       | 1830/6500 [12:07:19<24:39:45, 19.01s/it]                                                          28%|██▊       | 1830/6500 [12:07:19<24:39:45, 19.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.782484233379364, 'eval_runtime': 5.3271, 'eval_samples_per_second': 4.318, 'eval_steps_per_second': 1.126, 'epoch': 0.28}
                                                          28%|██▊       | 1830/6500 [12:07:24<24:39:45, 19.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1830
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1830/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1830I AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1830

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1830
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1830/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1830/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.395, 'learning_rate': 8.169279485633282e-05, 'epoch': 0.28}
{'loss': 0.3883, 'learning_rate': 8.167409410403277e-05, 'epoch': 0.28}
{'loss': 0.3809, 'learning_rate': 8.16553859481058e-05, 'epoch': 0.28}
{'loss': 0.426, 'learning_rate': 8.163667039292484e-05, 'epoch': 0.28}
{'loss': 0.3848, 'learning_rate': 8.161794744286453e-05, 'epoch': 0.28}
 28%|██▊       | 1831/6500 [12:08:31<45:05:56, 34.77s/it]                                                          28%|██▊       | 1831/6500 [12:08:31<45:05:56, 34.77s/it] 28%|██▊       | 1832/6500 [12:08:49<38:33:28, 29.74s/it]                                                          28%|██▊       | 1832/6500 [12:08:49<38:33:28, 29.74s/it] 28%|██▊       | 1833/6500 [12:09:08<34:23:39, 26.53s/it]                                                          28%|██▊       | 1833/6500 [12:09:08<34:23:39, 26.53s/it] 28%|██▊       | 1834/6500 [12:09:26<31:06:55, 24.01s/it]                                                          28%|██▊       | 1834/6500 [12:09:26<31:06:55, 24.01s/it] 28%|██▊       | 1835/6500 [12:09:44<28:47:36, 22.22s/it]                                                          28%|██▊       | 1835/6500 [12:09:44<28:47:36, 22.22s/it] 28%|██▊       | 1836/6500 [12:10:02<27:10:26, 20.97s/it]                        {'loss': 0.3988, 'learning_rate': 8.159921710230125e-05, 'epoch': 0.28}
{'loss': 0.4096, 'learning_rate': 8.158047937561309e-05, 'epoch': 0.28}
{'loss': 0.3836, 'learning_rate': 8.156173426717988e-05, 'epoch': 0.28}
{'loss': 0.3959, 'learning_rate': 8.15429817813832e-05, 'epoch': 0.28}
{'loss': 0.4063, 'learning_rate': 8.152422192260631e-05, 'epoch': 0.28}
                                  28%|██▊       | 1836/6500 [12:10:02<27:10:26, 20.97s/it] 28%|██▊       | 1837/6500 [12:10:20<26:02:33, 20.11s/it]                                                          28%|██▊       | 1837/6500 [12:10:20<26:02:33, 20.11s/it] 28%|██▊       | 1838/6500 [12:10:38<25:15:46, 19.51s/it]                                                          28%|██▊       | 1838/6500 [12:10:38<25:15:46, 19.51s/it] 28%|██▊       | 1839/6500 [12:10:56<24:43:28, 19.10s/it]                                                          28%|██▊       | 1839/6500 [12:10:56<24:43:28, 19.10s/it] 28%|██▊       | 1840/6500 [12:11:14<24:22:01, 18.82s/it]                                                          28%|██▊       | 1840/6500 [12:11:14<24:22:01, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7795488834381104, 'eval_runtime': 6.7801, 'eval_samples_per_second': 3.392, 'eval_steps_per_second': 0.885, 'epoch': 0.28}
                                                          28%|██▊       | 1840/6500 [12:11:21<24:22:01, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1840
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1840the checkpoint model will be saved in 
I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1840
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1840
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1840/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1840/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4083, 'learning_rate': 8.150545469523421e-05, 'epoch': 0.28}
{'loss': 0.3804, 'learning_rate': 8.148668010365364e-05, 'epoch': 0.28}
{'loss': 0.391, 'learning_rate': 8.146789815225303e-05, 'epoch': 0.28}
{'loss': 0.3693, 'learning_rate': 8.144910884542256e-05, 'epoch': 0.28}
{'loss': 0.4816, 'learning_rate': 8.14303121875541e-05, 'epoch': 0.28}
 28%|██▊       | 1841/6500 [12:12:56<56:23:54, 43.58s/it]                                                          28%|██▊       | 1841/6500 [12:12:56<56:23:54, 43.58s/it] 28%|██▊       | 1842/6500 [12:13:14<46:27:27, 35.91s/it]                                                          28%|██▊       | 1842/6500 [12:13:14<46:27:27, 35.91s/it] 28%|██▊       | 1843/6500 [12:13:33<39:50:04, 30.79s/it]                                                          28%|██▊       | 1843/6500 [12:13:33<39:50:04, 30.79s/it] 28%|██▊       | 1844/6500 [12:13:50<34:50:49, 26.94s/it]                                                          28%|██▊       | 1844/6500 [12:13:50<34:50:49, 26.94s/it] 28%|██▊       | 1845/6500 [12:14:08<31:22:03, 24.26s/it]                                                          28%|██▊       | 1845/6500 [12:14:09<31:22:03, 24.26s/it] 28%|██▊       | 1846/6500 [12:14:28<29:24:42, 22.75s/it]                        {'loss': 0.3716, 'learning_rate': 8.141150818304129e-05, 'epoch': 0.28}
{'loss': 0.3807, 'learning_rate': 8.139269683627942e-05, 'epoch': 0.28}
{'loss': 0.3962, 'learning_rate': 8.137387815166551e-05, 'epoch': 0.28}
{'loss': 0.9204, 'learning_rate': 8.135505213359835e-05, 'epoch': 0.28}
{'loss': 0.3921, 'learning_rate': 8.133621878647842e-05, 'epoch': 0.28}
                                  28%|██▊       | 1846/6500 [12:14:28<29:24:42, 22.75s/it] 28%|██▊       | 1847/6500 [12:14:46<27:35:59, 21.35s/it]                                                          28%|██▊       | 1847/6500 [12:14:46<27:35:59, 21.35s/it] 28%|██▊       | 1848/6500 [12:15:04<26:20:17, 20.38s/it]                                                          28%|██▊       | 1848/6500 [12:15:04<26:20:17, 20.38s/it] 28%|██▊       | 1849/6500 [12:15:22<25:27:12, 19.70s/it]                                                          28%|██▊       | 1849/6500 [12:15:22<25:27:12, 19.70s/it] 28%|██▊       | 1850/6500 [12:15:41<25:03:25, 19.40s/it]                                                          28%|██▊       | 1850/6500 [12:15:41<25:03:25, 19.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7885565757751465, 'eval_runtime': 5.5342, 'eval_samples_per_second': 4.156, 'eval_steps_per_second': 1.084, 'epoch': 0.28}
                                                          28%|██▊       | 1850/6500 [12:15:46<25:03:25, 19.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1850
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1850
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1850the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1850

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1850
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1850/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1850/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3855, 'learning_rate': 8.131737811470786e-05, 'epoch': 0.28}
{'loss': 0.3904, 'learning_rate': 8.129853012269056e-05, 'epoch': 0.28}
{'loss': 0.3611, 'learning_rate': 8.127967481483217e-05, 'epoch': 0.29}
{'loss': 0.4085, 'learning_rate': 8.126081219553995e-05, 'epoch': 0.29}
{'loss': 0.3701, 'learning_rate': 8.124194226922296e-05, 'epoch': 0.29}
 28%|██▊       | 1851/6500 [12:16:56<46:35:01, 36.07s/it]                                                          28%|██▊       | 1851/6500 [12:16:56<46:35:01, 36.07s/it] 28%|██▊       | 1852/6500 [12:17:14<39:37:19, 30.69s/it]                                                          28%|██▊       | 1852/6500 [12:17:14<39:37:19, 30.69s/it] 29%|██▊       | 1853/6500 [12:17:32<34:42:40, 26.89s/it]                                                          29%|██▊       | 1853/6500 [12:17:32<34:42:40, 26.89s/it] 29%|██▊       | 1854/6500 [12:17:50<31:19:21, 24.27s/it]                                                          29%|██▊       | 1854/6500 [12:17:50<31:19:21, 24.27s/it] 29%|██▊       | 1855/6500 [12:18:08<28:54:58, 22.41s/it]                                                          29%|██▊       | 1855/6500 [12:18:08<28:54:58, 22.41s/it] 29%|██▊       | 1856/6500 [12:18:26<27:14:26, 21.12s/it]                        {'loss': 0.362, 'learning_rate': 8.122306504029194e-05, 'epoch': 0.29}
{'loss': 0.3735, 'learning_rate': 8.120418051315927e-05, 'epoch': 0.29}
{'loss': 0.3764, 'learning_rate': 8.118528869223914e-05, 'epoch': 0.29}
{'loss': 0.3944, 'learning_rate': 8.11663895819474e-05, 'epoch': 0.29}
{'loss': 0.3689, 'learning_rate': 8.114748318670159e-05, 'epoch': 0.29}
                                  29%|██▊       | 1856/6500 [12:18:26<27:14:26, 21.12s/it] 29%|██▊       | 1857/6500 [12:18:44<26:04:25, 20.22s/it]                                                          29%|██▊       | 1857/6500 [12:18:44<26:04:25, 20.22s/it] 29%|██▊       | 1858/6500 [12:19:02<25:16:17, 19.60s/it]                                                          29%|██▊       | 1858/6500 [12:19:02<25:16:17, 19.60s/it] 29%|██▊       | 1859/6500 [12:19:21<24:55:11, 19.33s/it]                                                          29%|██▊       | 1859/6500 [12:19:21<24:55:11, 19.33s/it] 29%|██▊       | 1860/6500 [12:19:39<24:30:23, 19.01s/it]                                                          29%|██▊       | 1860/6500 [12:19:39<24:30:23, 19.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7840438485145569, 'eval_runtime': 5.3562, 'eval_samples_per_second': 4.294, 'eval_steps_per_second': 1.12, 'epoch': 0.29}
                                                          29%|██▊       | 1860/6500 [12:19:45<24:30:23, 19.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1860
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1860
 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1860
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1860
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1860/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1860/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4117, 'learning_rate': 8.112856951092097e-05, 'epoch': 0.29}
{'loss': 0.3682, 'learning_rate': 8.110964855902647e-05, 'epoch': 0.29}
{'loss': 0.416, 'learning_rate': 8.109072033544079e-05, 'epoch': 0.29}
{'loss': 0.4002, 'learning_rate': 8.107178484458824e-05, 'epoch': 0.29}
{'loss': 0.3967, 'learning_rate': 8.105284209089492e-05, 'epoch': 0.29}
 29%|██▊       | 1861/6500 [12:21:29<59:34:03, 46.23s/it]                                                          29%|██▊       | 1861/6500 [12:21:29<59:34:03, 46.23s/it] 29%|██▊       | 1862/6500 [12:21:47<48:40:51, 37.79s/it]                                                          29%|██▊       | 1862/6500 [12:21:47<48:40:51, 37.79s/it] 29%|██▊       | 1863/6500 [12:22:05<41:01:20, 31.85s/it]                                                          29%|██▊       | 1863/6500 [12:22:05<41:01:20, 31.85s/it] 29%|██▊       | 1864/6500 [12:22:23<35:45:02, 27.76s/it]                                                          29%|██▊       | 1864/6500 [12:22:23<35:45:02, 27.76s/it] 29%|██▊       | 1865/6500 [12:22:42<31:58:55, 24.84s/it]                                                          29%|██▊       | 1865/6500 [12:22:42<31:58:55, 24.84s/it] 29%|██▊       | 1866/6500 [12:23:00<29:21:54, 22.81s/it]                        {'loss': 0.3987, 'learning_rate': 8.103389207878856e-05, 'epoch': 0.29}
{'loss': 0.3938, 'learning_rate': 8.101493481269862e-05, 'epoch': 0.29}
{'loss': 0.3903, 'learning_rate': 8.099597029705625e-05, 'epoch': 0.29}
{'loss': 0.3723, 'learning_rate': 8.097699853629426e-05, 'epoch': 0.29}
{'loss': 0.4221, 'learning_rate': 8.095801953484723e-05, 'epoch': 0.29}
                                  29%|██▊       | 1866/6500 [12:23:00<29:21:54, 22.81s/it] 29%|██▊       | 1867/6500 [12:23:18<27:32:14, 21.40s/it]                                                          29%|██▊       | 1867/6500 [12:23:18<27:32:14, 21.40s/it] 29%|██▊       | 1868/6500 [12:23:36<26:16:11, 20.42s/it]                                                          29%|██▊       | 1868/6500 [12:23:36<26:16:11, 20.42s/it] 29%|██▉       | 1869/6500 [12:23:54<25:23:00, 19.73s/it]                                                          29%|██▉       | 1869/6500 [12:23:54<25:23:00, 19.73s/it] 29%|██▉       | 1870/6500 [12:24:12<24:46:51, 19.27s/it]                                                          29%|██▉       | 1870/6500 [12:24:12<24:46:51, 19.27s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7766607403755188, 'eval_runtime': 6.8192, 'eval_samples_per_second': 3.373, 'eval_steps_per_second': 0.88, 'epoch': 0.29}
                                                          29%|██▉       | 1870/6500 [12:24:19<24:46:51, 19.27s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1870
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1870
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1870/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1870


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1870
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1870/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1870/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3783, 'learning_rate': 8.093903329715135e-05, 'epoch': 0.29}
{'loss': 0.3845, 'learning_rate': 8.092003982764455e-05, 'epoch': 0.29}
{'loss': 0.3708, 'learning_rate': 8.090103913076642e-05, 'epoch': 0.29}
{'loss': 0.4119, 'learning_rate': 8.08820312109583e-05, 'epoch': 0.29}
{'loss': 0.4483, 'learning_rate': 8.086301607266314e-05, 'epoch': 0.29}
 29%|██▉       | 1871/6500 [12:25:41<51:42:23, 40.21s/it]                                                          29%|██▉       | 1871/6500 [12:25:41<51:42:23, 40.21s/it] 29%|██▉       | 1872/6500 [12:25:59<43:07:43, 33.55s/it]                                                          29%|██▉       | 1872/6500 [12:25:59<43:07:43, 33.55s/it] 29%|██▉       | 1873/6500 [12:26:17<37:08:34, 28.90s/it]                                                          29%|██▉       | 1873/6500 [12:26:17<37:08:34, 28.90s/it] 29%|██▉       | 1874/6500 [12:26:35<32:56:59, 25.64s/it]                                                          29%|██▉       | 1874/6500 [12:26:35<32:56:59, 25.64s/it] 29%|██▉       | 1875/6500 [12:26:54<30:17:55, 23.58s/it]                                                          29%|██▉       | 1875/6500 [12:26:54<30:17:55, 23.58s/it] 29%|██▉       | 1876/6500 [12:27:12<28:10:45, 21.94s/it]                        {'loss': 0.3815, 'learning_rate': 8.084399372032562e-05, 'epoch': 0.29}
{'loss': 0.3771, 'learning_rate': 8.082496415839212e-05, 'epoch': 0.29}
{'loss': 0.3896, 'learning_rate': 8.080592739131063e-05, 'epoch': 0.29}
{'loss': 0.9071, 'learning_rate': 8.078688342353095e-05, 'epoch': 0.29}
{'loss': 0.3901, 'learning_rate': 8.076783225950444e-05, 'epoch': 0.29}
                                  29%|██▉       | 1876/6500 [12:27:12<28:10:45, 21.94s/it] 29%|██▉       | 1877/6500 [12:27:30<26:42:02, 20.79s/it]                                                          29%|██▉       | 1877/6500 [12:27:30<26:42:02, 20.79s/it] 29%|██▉       | 1878/6500 [12:27:48<25:40:02, 19.99s/it]                                                          29%|██▉       | 1878/6500 [12:27:48<25:40:02, 19.99s/it] 29%|██▉       | 1879/6500 [12:28:07<24:56:54, 19.44s/it]                                                          29%|██▉       | 1879/6500 [12:28:07<24:56:54, 19.44s/it] 29%|██▉       | 1880/6500 [12:28:25<24:27:17, 19.06s/it]                                                          29%|██▉       | 1880/6500 [12:28:25<24:27:17, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7926811575889587, 'eval_runtime': 5.342, 'eval_samples_per_second': 4.306, 'eval_steps_per_second': 1.123, 'epoch': 0.29}
                                                          29%|██▉       | 1880/6500 [12:28:30<24:27:17, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1880
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1880
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1880/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1880


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1880
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1880/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1880/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3811, 'learning_rate': 8.074877390368423e-05, 'epoch': 0.29}
{'loss': 0.3692, 'learning_rate': 8.072970836052504e-05, 'epoch': 0.29}
{'loss': 0.3758, 'learning_rate': 8.07106356344834e-05, 'epoch': 0.29}
{'loss': 0.4029, 'learning_rate': 8.069155573001739e-05, 'epoch': 0.29}
{'loss': 0.3733, 'learning_rate': 8.067246865158682e-05, 'epoch': 0.29}
 29%|██▉       | 1881/6500 [12:29:30<42:21:59, 33.02s/it]                                                          29%|██▉       | 1881/6500 [12:29:30<42:21:59, 33.02s/it] 29%|██▉       | 1882/6500 [12:29:48<36:35:50, 28.53s/it]                                                          29%|██▉       | 1882/6500 [12:29:48<36:35:50, 28.53s/it] 29%|██▉       | 1883/6500 [12:30:06<32:33:16, 25.38s/it]                                                          29%|██▉       | 1883/6500 [12:30:06<32:33:16, 25.38s/it] 29%|██▉       | 1884/6500 [12:30:24<29:43:10, 23.18s/it]                                                          29%|██▉       | 1884/6500 [12:30:24<29:43:10, 23.18s/it] 29%|██▉       | 1885/6500 [12:30:43<27:44:38, 21.64s/it]                                                          29%|██▉       | 1885/6500 [12:30:43<27:44:38, 21.64s/it] 29%|██▉       | 1886/6500 [12:31:01<26:21:47, 20.57s/it]                        {'loss': 0.3769, 'learning_rate': 8.065337440365321e-05, 'epoch': 0.29}
{'loss': 0.3681, 'learning_rate': 8.06342729906797e-05, 'epoch': 0.29}
{'loss': 0.3774, 'learning_rate': 8.061516441713115e-05, 'epoch': 0.29}
{'loss': 0.3838, 'learning_rate': 8.059604868747405e-05, 'epoch': 0.29}
{'loss': 0.3739, 'learning_rate': 8.057692580617659e-05, 'epoch': 0.29}
                                  29%|██▉       | 1886/6500 [12:31:01<26:21:47, 20.57s/it] 29%|██▉       | 1887/6500 [12:31:19<25:24:28, 19.83s/it]                                                          29%|██▉       | 1887/6500 [12:31:19<25:24:28, 19.83s/it] 29%|██▉       | 1888/6500 [12:31:37<24:44:37, 19.31s/it]                                                          29%|██▉       | 1888/6500 [12:31:37<24:44:37, 19.31s/it] 29%|██▉       | 1889/6500 [12:31:55<24:17:04, 18.96s/it]                                                          29%|██▉       | 1889/6500 [12:31:55<24:17:04, 18.96s/it] 29%|██▉       | 1890/6500 [12:32:13<23:58:24, 18.72s/it]                                                          29%|██▉       | 1890/6500 [12:32:13<23:58:24, 18.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7826951146125793, 'eval_runtime': 5.3834, 'eval_samples_per_second': 4.272, 'eval_steps_per_second': 1.115, 'epoch': 0.29}
                                                          29%|██▉       | 1890/6500 [12:32:18<23:58:24, 18.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1890
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1890
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1890/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1890


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1890
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1890/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1890/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1890/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.39, 'learning_rate': 8.055779577770866e-05, 'epoch': 0.29}
{'loss': 0.3746, 'learning_rate': 8.053865860654175e-05, 'epoch': 0.29}
{'loss': 0.4007, 'learning_rate': 8.051951429714906e-05, 'epoch': 0.29}
{'loss': 0.3821, 'learning_rate': 8.050036285400546e-05, 'epoch': 0.29}
{'loss': 0.3697, 'learning_rate': 8.04812042815875e-05, 'epoch': 0.29}
 29%|██▉       | 1891/6500 [12:33:15<40:37:05, 31.73s/it]                                                          29%|██▉       | 1891/6500 [12:33:15<40:37:05, 31.73s/it] 29%|██▉       | 1892/6500 [12:33:33<35:21:12, 27.62s/it]                                                          29%|██▉       | 1892/6500 [12:33:33<35:21:12, 27.62s/it] 29%|██▉       | 1893/6500 [12:33:51<31:39:58, 24.74s/it]                                                          29%|██▉       | 1893/6500 [12:33:51<31:39:58, 24.74s/it] 29%|██▉       | 1894/6500 [12:34:09<29:05:52, 22.74s/it]                                                          29%|██▉       | 1894/6500 [12:34:09<29:05:52, 22.74s/it] 29%|██▉       | 1895/6500 [12:34:27<27:17:57, 21.34s/it]                                                          29%|██▉       | 1895/6500 [12:34:27<27:17:57, 21.34s/it] 29%|██▉       | 1896/6500 [12:34:46<26:03:17, 20.37s/it]                        {'loss': 0.3859, 'learning_rate': 8.046203858437337e-05, 'epoch': 0.29}
{'loss': 0.3757, 'learning_rate': 8.044286576684293e-05, 'epoch': 0.29}
{'loss': 0.3838, 'learning_rate': 8.04236858334777e-05, 'epoch': 0.29}
{'loss': 0.3803, 'learning_rate': 8.04044987887609e-05, 'epoch': 0.29}
{'loss': 0.4054, 'learning_rate': 8.038530463717738e-05, 'epoch': 0.29}
                                  29%|██▉       | 1896/6500 [12:34:46<26:03:17, 20.37s/it] 29%|██▉       | 1897/6500 [12:35:04<25:10:29, 19.69s/it]                                                          29%|██▉       | 1897/6500 [12:35:04<25:10:29, 19.69s/it] 29%|██▉       | 1898/6500 [12:35:22<24:33:36, 19.21s/it]                                                          29%|██▉       | 1898/6500 [12:35:22<24:33:36, 19.21s/it] 29%|██▉       | 1899/6500 [12:35:40<24:08:34, 18.89s/it]                                                          29%|██▉       | 1899/6500 [12:35:40<24:08:34, 18.89s/it] 29%|██▉       | 1900/6500 [12:35:58<23:50:52, 18.66s/it]                                                          29%|██▉       | 1900/6500 [12:35:58<23:50:52, 18.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7770766615867615, 'eval_runtime': 5.3405, 'eval_samples_per_second': 4.307, 'eval_steps_per_second': 1.123, 'epoch': 0.29}
                                                          29%|██▉       | 1900/6500 [12:36:03<23:50:52, 18.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1900
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1900/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1900
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1900/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.377, 'learning_rate': 8.036610338321362e-05, 'epoch': 0.29}
{'loss': 0.3873, 'learning_rate': 8.034689503135783e-05, 'epoch': 0.29}
{'loss': 0.3644, 'learning_rate': 8.032767958609986e-05, 'epoch': 0.29}
{'loss': 0.421, 'learning_rate': 8.030845705193116e-05, 'epoch': 0.29}
{'loss': 0.4213, 'learning_rate': 8.028922743334492e-05, 'epoch': 0.29}
 29%|██▉       | 1901/6500 [12:37:40<55:55:54, 43.78s/it]                                                          29%|██▉       | 1901/6500 [12:37:40<55:55:54, 43.78s/it] 29%|██▉       | 1902/6500 [12:37:58<46:02:08, 36.04s/it]                                                          29%|██▉       | 1902/6500 [12:37:58<46:02:08, 36.04s/it] 29%|██▉       | 1903/6500 [12:38:16<39:06:27, 30.63s/it]                                                          29%|██▉       | 1903/6500 [12:38:16<39:06:27, 30.63s/it] 29%|██▉       | 1904/6500 [12:38:34<34:15:48, 26.84s/it]                                                          29%|██▉       | 1904/6500 [12:38:34<34:15:48, 26.84s/it] 29%|██▉       | 1905/6500 [12:38:52<30:52:29, 24.19s/it]                                                          29%|██▉       | 1905/6500 [12:38:52<30:52:29, 24.19s/it] 29%|██▉       | 1906/6500 [12:39:10<28:30:57, 22.35s/it]                        {'loss': 0.3649, 'learning_rate': 8.026999073483593e-05, 'epoch': 0.29}
{'loss': 0.3821, 'learning_rate': 8.025074696090063e-05, 'epoch': 0.29}
{'loss': 0.3864, 'learning_rate': 8.023149611603717e-05, 'epoch': 0.29}
{'loss': 0.912, 'learning_rate': 8.021223820474529e-05, 'epoch': 0.29}
{'loss': 0.3941, 'learning_rate': 8.019297323152642e-05, 'epoch': 0.29}
                                  29%|██▉       | 1906/6500 [12:39:10<28:30:57, 22.35s/it] 29%|██▉       | 1907/6500 [12:39:29<26:58:39, 21.15s/it]                                                          29%|██▉       | 1907/6500 [12:39:29<26:58:39, 21.15s/it] 29%|██▉       | 1908/6500 [12:39:47<25:48:10, 20.23s/it]                                                          29%|██▉       | 1908/6500 [12:39:47<25:48:10, 20.23s/it] 29%|██▉       | 1909/6500 [12:40:05<24:58:46, 19.59s/it]                                                          29%|██▉       | 1909/6500 [12:40:05<24:58:46, 19.59s/it] 29%|██▉       | 1910/6500 [12:40:23<24:25:01, 19.15s/it]                                                          29%|██▉       | 1910/6500 [12:40:23<24:25:01, 19.15s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7884806990623474, 'eval_runtime': 5.337, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.29}
                                                          29%|██▉       | 1910/6500 [12:40:28<24:25:01, 19.15s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1910
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1910
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1910/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1910


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1910
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1910/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1910/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3852, 'learning_rate': 8.017370120088365e-05, 'epoch': 0.29}
{'loss': 0.3525, 'learning_rate': 8.015442211732167e-05, 'epoch': 0.29}
{'loss': 0.3945, 'learning_rate': 8.013513598534688e-05, 'epoch': 0.29}
{'loss': 0.3849, 'learning_rate': 8.011584280946726e-05, 'epoch': 0.29}
{'loss': 0.3473, 'learning_rate': 8.00965425941925e-05, 'epoch': 0.29}
 29%|██▉       | 1911/6500 [12:41:49<49:53:18, 39.14s/it]                                                          29%|██▉       | 1911/6500 [12:41:49<49:53:18, 39.14s/it] 29%|██▉       | 1912/6500 [12:42:07<41:47:35, 32.79s/it]                                                          29%|██▉       | 1912/6500 [12:42:07<41:47:35, 32.79s/it] 29%|██▉       | 1913/6500 [12:42:25<36:07:35, 28.35s/it]                                                          29%|██▉       | 1913/6500 [12:42:25<36:07:35, 28.35s/it] 29%|██▉       | 1914/6500 [12:42:43<32:09:34, 25.25s/it]                                                          29%|██▉       | 1914/6500 [12:42:43<32:09:34, 25.25s/it] 29%|██▉       | 1915/6500 [12:43:01<29:30:47, 23.17s/it]                                                          29%|██▉       | 1915/6500 [12:43:01<29:30:47, 23.17s/it] 29%|██▉       | 1916/6500 [12:43:19<27:33:25, 21.64s/it]                        {'loss': 0.3833, 'learning_rate': 8.007723534403389e-05, 'epoch': 0.29}
{'loss': 0.3657, 'learning_rate': 8.005792106350441e-05, 'epoch': 0.29}
{'loss': 0.3757, 'learning_rate': 8.003859975711862e-05, 'epoch': 0.3}
{'loss': 0.375, 'learning_rate': 8.001927142939278e-05, 'epoch': 0.3}
{'loss': 0.3788, 'learning_rate': 7.999993608484477e-05, 'epoch': 0.3}
                                  29%|██▉       | 1916/6500 [12:43:19<27:33:25, 21.64s/it] 29%|██▉       | 1917/6500 [12:43:37<26:12:07, 20.58s/it]                                                          29%|██▉       | 1917/6500 [12:43:37<26:12:07, 20.58s/it] 30%|██▉       | 1918/6500 [12:43:55<25:15:10, 19.84s/it]                                                          30%|██▉       | 1918/6500 [12:43:55<25:15:10, 19.84s/it] 30%|██▉       | 1919/6500 [12:44:14<24:36:15, 19.34s/it]                                                          30%|██▉       | 1919/6500 [12:44:14<24:36:15, 19.34s/it] 30%|██▉       | 1920/6500 [12:44:32<24:09:13, 18.99s/it]                                                          30%|██▉       | 1920/6500 [12:44:32<24:09:13, 18.99s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7852291464805603, 'eval_runtime': 5.3444, 'eval_samples_per_second': 4.304, 'eval_steps_per_second': 1.123, 'epoch': 0.3}
                                                          30%|██▉       | 1920/6500 [12:44:37<24:09:13, 18.99s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1920
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1920
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1920
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1920
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1920/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1920/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3676, 'learning_rate': 7.998059372799409e-05, 'epoch': 0.3}
{'loss': 0.3728, 'learning_rate': 7.996124436336192e-05, 'epoch': 0.3}
{'loss': 0.4193, 'learning_rate': 7.994188799547105e-05, 'epoch': 0.3}
{'loss': 0.3758, 'learning_rate': 7.99225246288459e-05, 'epoch': 0.3}
{'loss': 0.3952, 'learning_rate': 7.990315426801255e-05, 'epoch': 0.3}
 30%|██▉       | 1921/6500 [12:45:58<49:53:58, 39.23s/it]                                                          30%|██▉       | 1921/6500 [12:45:58<49:53:58, 39.23s/it] 30%|██▉       | 1922/6500 [12:46:16<41:48:58, 32.88s/it]                                                          30%|██▉       | 1922/6500 [12:46:16<41:48:58, 32.88s/it] 30%|██▉       | 1923/6500 [12:46:34<36:06:42, 28.40s/it]                                                          30%|██▉       | 1923/6500 [12:46:34<36:06:42, 28.40s/it] 30%|██▉       | 1924/6500 [12:46:53<32:17:30, 25.40s/it]                                                          30%|██▉       | 1924/6500 [12:46:53<32:17:30, 25.40s/it] 30%|██▉       | 1925/6500 [12:47:11<29:29:54, 23.21s/it]                                                          30%|██▉       | 1925/6500 [12:47:11<29:29:54, 23.21s/it] 30%|██▉       | 1926/6500 [12:47:29<27:30:59, 21.66s/it]                        {'loss': 0.3894, 'learning_rate': 7.988377691749871e-05, 'epoch': 0.3}
{'loss': 0.3751, 'learning_rate': 7.986439258183372e-05, 'epoch': 0.3}
{'loss': 0.3859, 'learning_rate': 7.984500126554853e-05, 'epoch': 0.3}
{'loss': 0.3899, 'learning_rate': 7.982560297317575e-05, 'epoch': 0.3}
{'loss': 0.389, 'learning_rate': 7.980619770924962e-05, 'epoch': 0.3}
                                  30%|██▉       | 1926/6500 [12:47:29<27:30:59, 21.66s/it] 30%|██▉       | 1927/6500 [12:47:47<26:08:13, 20.58s/it]                                                          30%|██▉       | 1927/6500 [12:47:47<26:08:13, 20.58s/it] 30%|██▉       | 1928/6500 [12:48:05<25:10:43, 19.83s/it]                                                          30%|██▉       | 1928/6500 [12:48:05<25:10:43, 19.83s/it] 30%|██▉       | 1929/6500 [12:48:23<24:31:20, 19.31s/it]                                                          30%|██▉       | 1929/6500 [12:48:23<24:31:20, 19.31s/it] 30%|██▉       | 1930/6500 [12:48:41<24:03:59, 18.96s/it]                                                          30%|██▉       | 1930/6500 [12:48:41<24:03:59, 18.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7825676202774048, 'eval_runtime': 5.4667, 'eval_samples_per_second': 4.207, 'eval_steps_per_second': 1.098, 'epoch': 0.3}
                                                          30%|██▉       | 1930/6500 [12:48:47<24:03:59, 18.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1930
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1930
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1930
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1930

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1930
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1930/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1930/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3747, 'learning_rate': 7.9786785478306e-05, 'epoch': 0.3}
{'loss': 0.3655, 'learning_rate': 7.97673662848824e-05, 'epoch': 0.3}
{'loss': 0.3905, 'learning_rate': 7.974794013351789e-05, 'epoch': 0.3}
{'loss': 0.4473, 'learning_rate': 7.972850702875327e-05, 'epoch': 0.3}
{'loss': 0.3779, 'learning_rate': 7.970906697513088e-05, 'epoch': 0.3}
 30%|██▉       | 1931/6500 [12:50:23<55:35:01, 43.80s/it]                                                          30%|██▉       | 1931/6500 [12:50:23<55:35:01, 43.80s/it] 30%|██▉       | 1932/6500 [12:50:41<45:43:42, 36.04s/it]                                                          30%|██▉       | 1932/6500 [12:50:41<45:43:42, 36.04s/it] 30%|██▉       | 1933/6500 [12:50:59<38:50:01, 30.61s/it]                                                          30%|██▉       | 1933/6500 [12:50:59<38:50:01, 30.61s/it] 30%|██▉       | 1934/6500 [12:51:17<34:01:48, 26.83s/it]                                                          30%|██▉       | 1934/6500 [12:51:17<34:01:48, 26.83s/it] 30%|██▉       | 1935/6500 [12:51:35<30:40:34, 24.19s/it]                                                          30%|██▉       | 1935/6500 [12:51:35<30:40:34, 24.19s/it] 30%|██▉       | 1936/6500 [12:51:53<28:20:27, 22.35s/it]                        {'loss': 0.3685, 'learning_rate': 7.96896199771947e-05, 'epoch': 0.3}
{'loss': 0.3937, 'learning_rate': 7.96701660394904e-05, 'epoch': 0.3}
{'loss': 0.913, 'learning_rate': 7.965070516656517e-05, 'epoch': 0.3}
{'loss': 0.387, 'learning_rate': 7.963123736296787e-05, 'epoch': 0.3}
{'loss': 0.3669, 'learning_rate': 7.961176263324901e-05, 'epoch': 0.3}
                                  30%|██▉       | 1936/6500 [12:51:53<28:20:27, 22.35s/it] 30%|██▉       | 1937/6500 [12:52:11<26:42:52, 21.08s/it]                                                          30%|██▉       | 1937/6500 [12:52:11<26:42:52, 21.08s/it] 30%|██▉       | 1938/6500 [12:52:29<25:34:13, 20.18s/it]                                                          30%|██▉       | 1938/6500 [12:52:29<25:34:13, 20.18s/it] 30%|██▉       | 1939/6500 [12:52:47<24:47:17, 19.57s/it]                                                          30%|██▉       | 1939/6500 [12:52:47<24:47:17, 19.57s/it] 30%|██▉       | 1940/6500 [12:53:06<24:25:48, 19.29s/it]                                                          30%|██▉       | 1940/6500 [12:53:06<24:25:48, 19.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7904180288314819, 'eval_runtime': 5.3306, 'eval_samples_per_second': 4.315, 'eval_steps_per_second': 1.126, 'epoch': 0.3}
                                                          30%|██▉       | 1940/6500 [12:53:11<24:25:48, 19.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1940
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1940the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1940
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1940
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1940/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3802, 'learning_rate': 7.959228098196067e-05, 'epoch': 0.3}
{'loss': 0.3482, 'learning_rate': 7.957279241365659e-05, 'epoch': 0.3}
{'loss': 0.398, 'learning_rate': 7.955329693289207e-05, 'epoch': 0.3}
{'loss': 0.3627, 'learning_rate': 7.953379454422409e-05, 'epoch': 0.3}
{'loss': 0.3556, 'learning_rate': 7.951428525221121e-05, 'epoch': 0.3}
 30%|██▉       | 1941/6500 [12:54:17<44:07:48, 34.85s/it]                                                          30%|██▉       | 1941/6500 [12:54:17<44:07:48, 34.85s/it] 30%|██▉       | 1942/6500 [12:54:35<37:44:16, 29.81s/it]                                                          30%|██▉       | 1942/6500 [12:54:35<37:44:16, 29.81s/it] 30%|██▉       | 1943/6500 [12:54:53<33:15:16, 26.27s/it]                                                          30%|██▉       | 1943/6500 [12:54:53<33:15:16, 26.27s/it] 30%|██▉       | 1944/6500 [12:55:11<30:06:56, 23.80s/it]                                                          30%|██▉       | 1944/6500 [12:55:11<30:06:56, 23.80s/it] 30%|██▉       | 1945/6500 [12:55:30<28:07:43, 22.23s/it]                                                          30%|██▉       | 1945/6500 [12:55:30<28:07:43, 22.23s/it] 30%|██▉       | 1946/6500 [12:55:48<26:32:44, 20.98s/it]                        {'loss': 0.3702, 'learning_rate': 7.94947690614136e-05, 'epoch': 0.3}
{'loss': 0.367, 'learning_rate': 7.947524597639304e-05, 'epoch': 0.3}
{'loss': 0.3737, 'learning_rate': 7.945571600171295e-05, 'epoch': 0.3}
{'loss': 0.3624, 'learning_rate': 7.943617914193833e-05, 'epoch': 0.3}
{'loss': 0.3933, 'learning_rate': 7.941663540163582e-05, 'epoch': 0.3}
                                  30%|██▉       | 1946/6500 [12:55:48<26:32:44, 20.98s/it] 30%|██▉       | 1947/6500 [12:56:06<25:26:36, 20.12s/it]                                                          30%|██▉       | 1947/6500 [12:56:06<25:26:36, 20.12s/it] 30%|██▉       | 1948/6500 [12:56:24<24:40:45, 19.52s/it]                                                          30%|██▉       | 1948/6500 [12:56:24<24:40:45, 19.52s/it] 30%|██▉       | 1949/6500 [12:56:42<24:09:00, 19.10s/it]                                                          30%|██▉       | 1949/6500 [12:56:42<24:09:00, 19.10s/it] 30%|███       | 1950/6500 [12:57:00<23:47:11, 18.82s/it]                                                          30%|███       | 1950/6500 [12:57:00<23:47:11, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7890638113021851, 'eval_runtime': 5.4201, 'eval_samples_per_second': 4.243, 'eval_steps_per_second': 1.107, 'epoch': 0.3}
                                                          30%|███       | 1950/6500 [12:57:06<23:47:11, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1950
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1950 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1950the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1950

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1950
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1950/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.362, 'learning_rate': 7.939708478537364e-05, 'epoch': 0.3}
{'loss': 0.3991, 'learning_rate': 7.937752729772164e-05, 'epoch': 0.3}
{'loss': 0.3751, 'learning_rate': 7.935796294325124e-05, 'epoch': 0.3}
{'loss': 0.3724, 'learning_rate': 7.93383917265355e-05, 'epoch': 0.3}
{'loss': 0.3764, 'learning_rate': 7.931881365214908e-05, 'epoch': 0.3}
 30%|███       | 1951/6500 [12:58:05<41:08:01, 32.55s/it]                                                          30%|███       | 1951/6500 [12:58:05<41:08:01, 32.55s/it] 30%|███       | 1952/6500 [12:58:23<35:37:11, 28.20s/it]                                                          30%|███       | 1952/6500 [12:58:23<35:37:11, 28.20s/it] 30%|███       | 1953/6500 [12:58:41<31:45:15, 25.14s/it]                                                          30%|███       | 1953/6500 [12:58:41<31:45:15, 25.14s/it] 30%|███       | 1954/6500 [12:58:59<29:03:36, 23.01s/it]                                                          30%|███       | 1954/6500 [12:58:59<29:03:36, 23.01s/it] 30%|███       | 1955/6500 [12:59:17<27:10:55, 21.53s/it]                                                          30%|███       | 1955/6500 [12:59:17<27:10:55, 21.53s/it] 30%|███       | 1956/6500 [12:59:35<25:58:03, 20.57s/it]                        {'loss': 0.3684, 'learning_rate': 7.929922872466823e-05, 'epoch': 0.3}
{'loss': 0.3659, 'learning_rate': 7.92796369486708e-05, 'epoch': 0.3}
{'loss': 0.3796, 'learning_rate': 7.926003832873627e-05, 'epoch': 0.3}
{'loss': 0.4017, 'learning_rate': 7.924043286944569e-05, 'epoch': 0.3}
{'loss': 0.3593, 'learning_rate': 7.92208205753817e-05, 'epoch': 0.3}
                                  30%|███       | 1956/6500 [12:59:35<25:58:03, 20.57s/it] 30%|███       | 1957/6500 [12:59:53<25:01:37, 19.83s/it]                                                          30%|███       | 1957/6500 [12:59:53<25:01:37, 19.83s/it] 30%|███       | 1958/6500 [13:00:12<24:22:22, 19.32s/it]                                                          30%|███       | 1958/6500 [13:00:12<24:22:22, 19.32s/it] 30%|███       | 1959/6500 [13:00:30<23:55:33, 18.97s/it]                                                          30%|███       | 1959/6500 [13:00:30<23:55:33, 18.97s/it] 30%|███       | 1960/6500 [13:00:48<23:37:20, 18.73s/it]                                                          30%|███       | 1960/6500 [13:00:48<23:37:20, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7776860594749451, 'eval_runtime': 5.4889, 'eval_samples_per_second': 4.19, 'eval_steps_per_second': 1.093, 'epoch': 0.3}
                                                          30%|███       | 1960/6500 [13:00:53<23:37:20, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1960
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1960
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1960
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1960

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1960
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1960/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1960/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3777, 'learning_rate': 7.920120145112855e-05, 'epoch': 0.3}
{'loss': 0.3577, 'learning_rate': 7.918157550127213e-05, 'epoch': 0.3}
{'loss': 0.4035, 'learning_rate': 7.916194273039986e-05, 'epoch': 0.3}
{'loss': 0.4335, 'learning_rate': 7.914230314310079e-05, 'epoch': 0.3}
{'loss': 0.3661, 'learning_rate': 7.912265674396553e-05, 'epoch': 0.3}
 30%|███       | 1961/6500 [13:01:45<37:57:32, 30.11s/it]                                                          30%|███       | 1961/6500 [13:01:45<37:57:32, 30.11s/it] 30%|███       | 1962/6500 [13:02:03<33:23:07, 26.48s/it]                                                          30%|███       | 1962/6500 [13:02:03<33:23:07, 26.48s/it] 30%|███       | 1963/6500 [13:02:21<30:11:42, 23.96s/it]                                                          30%|███       | 1963/6500 [13:02:21<30:11:42, 23.96s/it] 30%|███       | 1964/6500 [13:02:39<27:58:13, 22.20s/it]                                                          30%|███       | 1964/6500 [13:02:39<27:58:13, 22.20s/it] 30%|███       | 1965/6500 [13:02:57<26:25:05, 20.97s/it]                                                          30%|███       | 1965/6500 [13:02:57<26:25:05, 20.97s/it] 30%|███       | 1966/6500 [13:03:15<25:20:19, 20.12s/it]                        {'loss': 0.3753, 'learning_rate': 7.910300353758633e-05, 'epoch': 0.3}
{'loss': 0.3585, 'learning_rate': 7.9083343528557e-05, 'epoch': 0.3}
{'loss': 0.8967, 'learning_rate': 7.906367672147297e-05, 'epoch': 0.3}
{'loss': 0.3825, 'learning_rate': 7.904400312093119e-05, 'epoch': 0.3}
{'loss': 0.3721, 'learning_rate': 7.902432273153028e-05, 'epoch': 0.3}
                                  30%|███       | 1966/6500 [13:03:15<25:20:19, 20.12s/it] 30%|███       | 1967/6500 [13:03:33<24:35:13, 19.53s/it]                                                          30%|███       | 1967/6500 [13:03:33<24:35:13, 19.53s/it] 30%|███       | 1968/6500 [13:03:51<24:03:48, 19.11s/it]                                                          30%|███       | 1968/6500 [13:03:51<24:03:48, 19.11s/it] 30%|███       | 1969/6500 [13:04:09<23:42:18, 18.83s/it]                                                          30%|███       | 1969/6500 [13:04:09<23:42:18, 18.83s/it] 30%|███       | 1970/6500 [13:04:28<23:29:32, 18.67s/it]                                                          30%|███       | 1970/6500 [13:04:28<23:29:32, 18.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7954548597335815, 'eval_runtime': 5.3362, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.3}
                                                          30%|███       | 1970/6500 [13:04:33<23:29:32, 18.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1970
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1970 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1970
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1970

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1970/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1970/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1970/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3441, 'learning_rate': 7.900463555787042e-05, 'epoch': 0.3}
{'loss': 0.3684, 'learning_rate': 7.898494160455334e-05, 'epoch': 0.3}
{'loss': 0.3894, 'learning_rate': 7.896524087618238e-05, 'epoch': 0.3}
{'loss': 0.3515, 'learning_rate': 7.894553337736248e-05, 'epoch': 0.3}
{'loss': 0.3666, 'learning_rate': 7.892581911270013e-05, 'epoch': 0.3}
 30%|███       | 1971/6500 [13:06:00<51:07:12, 40.63s/it]                                                          30%|███       | 1971/6500 [13:06:00<51:07:12, 40.63s/it] 30%|███       | 1972/6500 [13:06:18<42:42:48, 33.96s/it]                                                          30%|███       | 1972/6500 [13:06:18<42:42:48, 33.96s/it] 30%|███       | 1973/6500 [13:06:36<36:41:00, 29.17s/it]                                                          30%|███       | 1973/6500 [13:06:36<36:41:00, 29.17s/it] 30%|███       | 1974/6500 [13:06:54<32:27:49, 25.82s/it]                                                          30%|███       | 1974/6500 [13:06:54<32:27:49, 25.82s/it] 30%|███       | 1975/6500 [13:07:12<29:31:36, 23.49s/it]                                                          30%|███       | 1975/6500 [13:07:12<29:31:36, 23.49s/it] 30%|███       | 1976/6500 [13:07:30<27:34:21, 21.94s/it]                        {'loss': 0.3473, 'learning_rate': 7.890609808680347e-05, 'epoch': 0.3}
{'loss': 0.3682, 'learning_rate': 7.888637030428211e-05, 'epoch': 0.3}
{'loss': 0.3648, 'learning_rate': 7.886663576974733e-05, 'epoch': 0.3}
{'loss': 0.3658, 'learning_rate': 7.884689448781196e-05, 'epoch': 0.3}
{'loss': 0.3808, 'learning_rate': 7.882714646309038e-05, 'epoch': 0.3}
                                  30%|███       | 1976/6500 [13:07:30<27:34:21, 21.94s/it] 30%|███       | 1977/6500 [13:07:48<26:07:06, 20.79s/it]                                                          30%|███       | 1977/6500 [13:07:48<26:07:06, 20.79s/it] 30%|███       | 1978/6500 [13:08:07<25:06:27, 19.99s/it]                                                          30%|███       | 1978/6500 [13:08:07<25:06:27, 19.99s/it] 30%|███       | 1979/6500 [13:08:25<24:24:25, 19.43s/it]                                                          30%|███       | 1979/6500 [13:08:25<24:24:25, 19.43s/it] 30%|███       | 1980/6500 [13:08:43<23:55:20, 19.05s/it]                                                          30%|███       | 1980/6500 [13:08:43<23:55:20, 19.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7853179574012756, 'eval_runtime': 5.3499, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.122, 'epoch': 0.3}
                                                          30%|███       | 1980/6500 [13:08:48<23:55:20, 19.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1980
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1980the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1980
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1980

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1980
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1980/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1980/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.353, 'learning_rate': 7.88073917001986e-05, 'epoch': 0.3}
{'loss': 0.4035, 'learning_rate': 7.878763020375415e-05, 'epoch': 0.3}
{'loss': 0.3682, 'learning_rate': 7.876786197837617e-05, 'epoch': 0.31}
{'loss': 0.3675, 'learning_rate': 7.874808702868536e-05, 'epoch': 0.31}
{'loss': 0.3843, 'learning_rate': 7.872830535930401e-05, 'epoch': 0.31}
 30%|███       | 1981/6500 [13:09:25<32:32:53, 25.93s/it]                                                          30%|███       | 1981/6500 [13:09:25<32:32:53, 25.93s/it] 30%|███       | 1982/6500 [13:09:43<29:34:57, 23.57s/it]                                                          30%|███       | 1982/6500 [13:09:43<29:34:57, 23.57s/it] 31%|███       | 1983/6500 [13:10:01<27:30:37, 21.93s/it]                                                          31%|███       | 1983/6500 [13:10:01<27:30:37, 21.93s/it] 31%|███       | 1984/6500 [13:10:19<26:04:05, 20.78s/it]                                                          31%|███       | 1984/6500 [13:10:19<26:04:05, 20.78s/it] 31%|███       | 1985/6500 [13:10:37<25:03:52, 19.99s/it]                                                          31%|███       | 1985/6500 [13:10:37<25:03:52, 19.99s/it] 31%|███       | 1986/6500 [13:10:55<24:22:08, 19.43s/it]                        {'loss': 0.3634, 'learning_rate': 7.870851697485596e-05, 'epoch': 0.31}
{'loss': 0.3803, 'learning_rate': 7.868872187996659e-05, 'epoch': 0.31}
{'loss': 0.3654, 'learning_rate': 7.866892007926294e-05, 'epoch': 0.31}
{'loss': 0.3891, 'learning_rate': 7.864911157737352e-05, 'epoch': 0.31}
{'loss': 0.3618, 'learning_rate': 7.862929637892845e-05, 'epoch': 0.31}
                                  31%|███       | 1986/6500 [13:10:55<24:22:08, 19.43s/it] 31%|███       | 1987/6500 [13:11:14<23:53:09, 19.05s/it]                                                          31%|███       | 1987/6500 [13:11:14<23:53:09, 19.05s/it] 31%|███       | 1988/6500 [13:11:32<23:37:41, 18.85s/it]                                                          31%|███       | 1988/6500 [13:11:32<23:37:41, 18.85s/it] 31%|███       | 1989/6500 [13:11:50<23:22:05, 18.65s/it]                                                          31%|███       | 1989/6500 [13:11:50<23:22:05, 18.65s/it] 31%|███       | 1990/6500 [13:12:08<23:11:18, 18.51s/it]                                                          31%|███       | 1990/6500 [13:12:08<23:11:18, 18.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7785055637359619, 'eval_runtime': 5.4949, 'eval_samples_per_second': 4.186, 'eval_steps_per_second': 1.092, 'epoch': 0.31}
                                                          31%|███       | 1990/6500 [13:12:14<23:11:18, 18.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-1990
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1990/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1990 

/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1990

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1990
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-1990/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3661, 'learning_rate': 7.860947448855944e-05, 'epoch': 0.31}
{'loss': 0.3531, 'learning_rate': 7.85896459108997e-05, 'epoch': 0.31}
{'loss': 0.4511, 'learning_rate': 7.856981065058407e-05, 'epoch': 0.31}
{'loss': 0.3727, 'learning_rate': 7.85499687122489e-05, 'epoch': 0.31}
{'loss': 0.3537, 'learning_rate': 7.853012010053212e-05, 'epoch': 0.31}
 31%|███       | 1991/6500 [13:13:23<44:23:09, 35.44s/it]                                                          31%|███       | 1991/6500 [13:13:23<44:23:09, 35.44s/it] 31%|███       | 1992/6500 [13:13:41<37:49:54, 30.21s/it]                                                          31%|███       | 1992/6500 [13:13:41<37:49:54, 30.21s/it] 31%|███       | 1993/6500 [13:13:59<33:17:00, 26.59s/it]                                                          31%|███       | 1993/6500 [13:13:59<33:17:00, 26.59s/it] 31%|███       | 1994/6500 [13:14:19<30:28:10, 24.34s/it]                                                          31%|███       | 1994/6500 [13:14:19<30:28:10, 24.34s/it] 31%|███       | 1995/6500 [13:14:37<28:06:45, 22.47s/it]                                                          31%|███       | 1995/6500 [13:14:37<28:06:45, 22.47s/it] 31%|███       | 1996/6500 [13:14:55<26:27:21, 21.15s/it]                        {'loss': 0.3707, 'learning_rate': 7.851026482007324e-05, 'epoch': 0.31}
{'loss': 0.8154, 'learning_rate': 7.849040287551331e-05, 'epoch': 0.31}
{'loss': 0.4594, 'learning_rate': 7.847053427149494e-05, 'epoch': 0.31}
{'loss': 0.3628, 'learning_rate': 7.845065901266227e-05, 'epoch': 0.31}
{'loss': 0.3789, 'learning_rate': 7.843077710366105e-05, 'epoch': 0.31}
                                  31%|███       | 1996/6500 [13:14:55<26:27:21, 21.15s/it] 31%|███       | 1997/6500 [13:15:13<25:23:28, 20.30s/it]                                                          31%|███       | 1997/6500 [13:15:13<25:23:28, 20.30s/it] 31%|███       | 1998/6500 [13:15:31<24:33:22, 19.64s/it]                                                          31%|███       | 1998/6500 [13:15:31<24:33:22, 19.64s/it] 31%|███       | 1999/6500 [13:15:49<23:58:29, 19.18s/it]                                                          31%|███       | 1999/6500 [13:15:49<23:58:29, 19.18s/it] 31%|███       | 2000/6500 [13:16:07<23:34:35, 18.86s/it]                                                          31%|███       | 2000/6500 [13:16:07<23:34:35, 18.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7965870499610901, 'eval_runtime': 6.7731, 'eval_samples_per_second': 3.396, 'eval_steps_per_second': 0.886, 'epoch': 0.31}
                                                          31%|███       | 2000/6500 [13:16:14<23:34:35, 18.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2000
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2000
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2000

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2000
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2000/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3466, 'learning_rate': 7.841088854913853e-05, 'epoch': 0.31}
{'loss': 0.383, 'learning_rate': 7.839099335374355e-05, 'epoch': 0.31}
{'loss': 0.3713, 'learning_rate': 7.837109152212652e-05, 'epoch': 0.31}
{'loss': 0.3359, 'learning_rate': 7.835118305893933e-05, 'epoch': 0.31}
{'loss': 0.3753, 'learning_rate': 7.833126796883548e-05, 'epoch': 0.31}
 31%|███       | 2001/6500 [13:17:24<45:20:25, 36.28s/it]                                                          31%|███       | 2001/6500 [13:17:24<45:20:25, 36.28s/it] 31%|███       | 2002/6500 [13:17:42<38:28:51, 30.80s/it]                                                          31%|███       | 2002/6500 [13:17:42<38:28:51, 30.80s/it] 31%|███       | 2003/6500 [13:18:00<33:41:20, 26.97s/it]                                                          31%|███       | 2003/6500 [13:18:00<33:41:20, 26.97s/it] 31%|███       | 2004/6500 [13:18:19<30:37:34, 24.52s/it]                                                          31%|███       | 2004/6500 [13:18:19<30:37:34, 24.52s/it] 31%|███       | 2005/6500 [13:18:37<28:13:20, 22.60s/it]                                                          31%|███       | 2005/6500 [13:18:37<28:13:20, 22.60s/it] 31%|███       | 2006/6500 [13:18:55<26:31:32, 21.25s/it]                        {'loss': 0.3465, 'learning_rate': 7.831134625646999e-05, 'epoch': 0.31}
{'loss': 0.3768, 'learning_rate': 7.829141792649946e-05, 'epoch': 0.31}
{'loss': 0.3714, 'learning_rate': 7.8271482983582e-05, 'epoch': 0.31}
{'loss': 0.3785, 'learning_rate': 7.825154143237729e-05, 'epoch': 0.31}
{'loss': 0.3554, 'learning_rate': 7.823159327754655e-05, 'epoch': 0.31}
                                  31%|███       | 2006/6500 [13:18:55<26:31:32, 21.25s/it] 31%|███       | 2007/6500 [13:19:13<25:20:59, 20.31s/it]                                                          31%|███       | 2007/6500 [13:19:13<25:20:59, 20.31s/it] 31%|███       | 2008/6500 [13:19:32<24:32:00, 19.66s/it]                                                          31%|███       | 2008/6500 [13:19:32<24:32:00, 19.66s/it] 31%|███       | 2009/6500 [13:19:50<23:57:30, 19.21s/it]                                                          31%|███       | 2009/6500 [13:19:50<23:57:30, 19.21s/it] 31%|███       | 2010/6500 [13:20:08<23:33:53, 18.89s/it]                                                          31%|███       | 2010/6500 [13:20:08<23:33:53, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.789792001247406, 'eval_runtime': 5.3552, 'eval_samples_per_second': 4.295, 'eval_steps_per_second': 1.12, 'epoch': 0.31}
                                                          31%|███       | 2010/6500 [13:20:13<23:33:53, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2010
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2010
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2010the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2010

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2010
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2010/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3668, 'learning_rate': 7.821163852375251e-05, 'epoch': 0.31}
{'loss': 0.3993, 'learning_rate': 7.819167717565951e-05, 'epoch': 0.31}
{'loss': 0.3727, 'learning_rate': 7.817170923793338e-05, 'epoch': 0.31}
{'loss': 0.3803, 'learning_rate': 7.815173471524149e-05, 'epoch': 0.31}
{'loss': 0.3718, 'learning_rate': 7.813175361225278e-05, 'epoch': 0.31}
 31%|███       | 2011/6500 [13:21:29<46:40:27, 37.43s/it]                                                          31%|███       | 2011/6500 [13:21:29<46:40:27, 37.43s/it] 31%|███       | 2012/6500 [13:21:47<39:24:10, 31.61s/it]                                                          31%|███       | 2012/6500 [13:21:47<39:24:10, 31.61s/it] 31%|███       | 2013/6500 [13:22:05<34:18:56, 27.53s/it]                                                          31%|███       | 2013/6500 [13:22:05<34:18:56, 27.53s/it] 31%|███       | 2014/6500 [13:22:23<30:44:47, 24.67s/it]                                                          31%|███       | 2014/6500 [13:22:23<30:44:47, 24.67s/it] 31%|███       | 2015/6500 [13:22:41<28:15:38, 22.68s/it]                                                          31%|███       | 2015/6500 [13:22:41<28:15:38, 22.68s/it] 31%|███       | 2016/6500 [13:22:59<26:31:34, 21.30s/it]                        {'loss': 0.3677, 'learning_rate': 7.811176593363772e-05, 'epoch': 0.31}
{'loss': 0.3638, 'learning_rate': 7.809177168406827e-05, 'epoch': 0.31}
{'loss': 0.3867, 'learning_rate': 7.807177086821802e-05, 'epoch': 0.31}
{'loss': 0.3749, 'learning_rate': 7.805176349076199e-05, 'epoch': 0.31}
{'loss': 0.363, 'learning_rate': 7.80317495563768e-05, 'epoch': 0.31}
                                  31%|███       | 2016/6500 [13:22:59<26:31:34, 21.30s/it] 31%|███       | 2017/6500 [13:23:17<25:19:08, 20.33s/it]                                                          31%|███       | 2017/6500 [13:23:17<25:19:08, 20.33s/it] 31%|███       | 2018/6500 [13:23:35<24:28:50, 19.66s/it]                                                          31%|███       | 2018/6500 [13:23:35<24:28:50, 19.66s/it] 31%|███       | 2019/6500 [13:23:53<23:54:29, 19.21s/it]                                                          31%|███       | 2019/6500 [13:23:53<23:54:29, 19.21s/it] 31%|███       | 2020/6500 [13:24:12<23:44:26, 19.08s/it]                                                          31%|███       | 2020/6500 [13:24:12<23:44:26, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7798037528991699, 'eval_runtime': 5.529, 'eval_samples_per_second': 4.16, 'eval_steps_per_second': 1.085, 'epoch': 0.31}
                                                          31%|███       | 2020/6500 [13:24:17<23:44:26, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2020
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2020/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2020

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2020

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2020
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2020/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2020/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.356, 'learning_rate': 7.80117290697406e-05, 'epoch': 0.31}
{'loss': 0.3811, 'learning_rate': 7.799170203553304e-05, 'epoch': 0.31}
{'loss': 0.4278, 'learning_rate': 7.797166845843531e-05, 'epoch': 0.31}
{'loss': 0.3596, 'learning_rate': 7.795162834313014e-05, 'epoch': 0.31}
{'loss': 0.351, 'learning_rate': 7.793158169430181e-05, 'epoch': 0.31}
 31%|███       | 2021/6500 [13:25:22<42:49:15, 34.42s/it]                                                          31%|███       | 2021/6500 [13:25:22<42:49:15, 34.42s/it] 31%|███       | 2022/6500 [13:25:40<36:40:51, 29.49s/it]                                                          31%|███       | 2022/6500 [13:25:40<36:40:51, 29.49s/it] 31%|███       | 2023/6500 [13:25:58<32:23:21, 26.04s/it]                                                          31%|███       | 2023/6500 [13:25:58<32:23:21, 26.04s/it] 31%|███       | 2024/6500 [13:26:16<29:23:26, 23.64s/it]                                                          31%|███       | 2024/6500 [13:26:16<29:23:26, 23.64s/it] 31%|███       | 2025/6500 [13:26:34<27:18:20, 21.97s/it]                                                          31%|███       | 2025/6500 [13:26:34<27:18:20, 21.97s/it] 31%|███       | 2026/6500 [13:26:53<26:09:11, 21.04s/it]                        {'loss': 0.3787, 'learning_rate': 7.79115285166361e-05, 'epoch': 0.31}
{'loss': 0.8915, 'learning_rate': 7.789146881482027e-05, 'epoch': 0.31}
{'loss': 0.3745, 'learning_rate': 7.787140259354322e-05, 'epoch': 0.31}
{'loss': 0.3602, 'learning_rate': 7.785132985749526e-05, 'epoch': 0.31}
{'loss': 0.3585, 'learning_rate': 7.78312506113683e-05, 'epoch': 0.31}
                                  31%|███       | 2026/6500 [13:26:53<26:09:11, 21.04s/it] 31%|███       | 2027/6500 [13:27:11<25:02:12, 20.15s/it]                                                          31%|███       | 2027/6500 [13:27:11<25:02:12, 20.15s/it] 31%|███       | 2028/6500 [13:27:29<24:16:31, 19.54s/it]                                                          31%|███       | 2028/6500 [13:27:29<24:16:31, 19.54s/it] 31%|███       | 2029/6500 [13:27:47<23:45:07, 19.12s/it]                                                          31%|███       | 2029/6500 [13:27:47<23:45:07, 19.12s/it] 31%|███       | 2030/6500 [13:28:06<23:23:23, 18.84s/it]                                                          31%|███       | 2030/6500 [13:28:06<23:23:23, 18.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7959132194519043, 'eval_runtime': 5.3412, 'eval_samples_per_second': 4.306, 'eval_steps_per_second': 1.123, 'epoch': 0.31}
                                                          31%|███       | 2030/6500 [13:28:11<23:23:23, 18.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2030
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2030/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2030the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2030

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2030
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2030/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3437, 'learning_rate': 7.78111648598557e-05, 'epoch': 0.31}
{'loss': 0.3895, 'learning_rate': 7.779107260765242e-05, 'epoch': 0.31}
{'loss': 0.347, 'learning_rate': 7.777097385945489e-05, 'epoch': 0.31}
{'loss': 0.3521, 'learning_rate': 7.775086861996108e-05, 'epoch': 0.31}
{'loss': 0.3442, 'learning_rate': 7.773075689387043e-05, 'epoch': 0.31}
 31%|███       | 2031/6500 [13:28:58<35:50:03, 28.87s/it]                                                          31%|███       | 2031/6500 [13:28:58<35:50:03, 28.87s/it] 31%|███▏      | 2032/6500 [13:29:16<31:48:25, 25.63s/it]                                                          31%|███▏      | 2032/6500 [13:29:16<31:48:25, 25.63s/it] 31%|███▏      | 2033/6500 [13:29:34<28:58:16, 23.35s/it]                                                          31%|███▏      | 2033/6500 [13:29:34<28:58:16, 23.35s/it] 31%|███▏      | 2034/6500 [13:29:52<26:59:18, 21.76s/it]                                                          31%|███▏      | 2034/6500 [13:29:52<26:59:18, 21.76s/it] 31%|███▏      | 2035/6500 [13:30:10<25:36:06, 20.64s/it]                                                          31%|███▏      | 2035/6500 [13:30:10<25:36:06, 20.64s/it] 31%|███▏      | 2036/6500 [13:30:28<24:38:14, 19.87s/it]      {'loss': 0.3544, 'learning_rate': 7.771063868588399e-05, 'epoch': 0.31}
{'loss': 0.3638, 'learning_rate': 7.769051400070425e-05, 'epoch': 0.31}
{'loss': 0.3548, 'learning_rate': 7.767038284303521e-05, 'epoch': 0.31}
{'loss': 0.3977, 'learning_rate': 7.76502452175824e-05, 'epoch': 0.31}
{'loss': 0.3584, 'learning_rate': 7.763010112905291e-05, 'epoch': 0.31}
                                                    31%|███▏      | 2036/6500 [13:30:28<24:38:14, 19.87s/it] 31%|███▏      | 2037/6500 [13:30:47<24:08:28, 19.47s/it]                                                          31%|███▏      | 2037/6500 [13:30:47<24:08:28, 19.47s/it] 31%|███▏      | 2038/6500 [13:31:05<23:38:43, 19.08s/it]                                                          31%|███▏      | 2038/6500 [13:31:05<23:38:43, 19.08s/it] 31%|███▏      | 2039/6500 [13:31:23<23:17:00, 18.79s/it]                                                          31%|███▏      | 2039/6500 [13:31:23<23:17:00, 18.79s/it] 31%|███▏      | 2040/6500 [13:31:41<23:05:13, 18.64s/it]                                                          31%|███▏      | 2040/6500 [13:31:41<23:05:13, 18.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7895632982254028, 'eval_runtime': 5.3277, 'eval_samples_per_second': 4.317, 'eval_steps_per_second': 1.126, 'epoch': 0.31}
                                                          31%|███▏      | 2040/6500 [13:31:46<23:05:13, 18.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2040
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2040/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2040
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2040
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2040/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3907, 'learning_rate': 7.760995058215525e-05, 'epoch': 0.31}
{'loss': 0.3819, 'learning_rate': 7.758979358159954e-05, 'epoch': 0.31}
{'loss': 0.3703, 'learning_rate': 7.756963013209733e-05, 'epoch': 0.31}
{'loss': 0.3719, 'learning_rate': 7.754946023836168e-05, 'epoch': 0.31}
{'loss': 0.3669, 'learning_rate': 7.752928390510722e-05, 'epoch': 0.31}
 31%|███▏      | 2041/6500 [13:33:04<46:48:32, 37.79s/it]                                                          31%|███▏      | 2041/6500 [13:33:04<46:48:32, 37.79s/it] 31%|███▏      | 2042/6500 [13:33:22<39:28:26, 31.88s/it]                                                          31%|███▏      | 2042/6500 [13:33:22<39:28:26, 31.88s/it] 31%|███▏      | 2043/6500 [13:33:40<34:17:57, 27.70s/it]                                                          31%|███▏      | 2043/6500 [13:33:40<34:17:57, 27.70s/it] 31%|███▏      | 2044/6500 [13:33:58<30:40:11, 24.78s/it]                                                          31%|███▏      | 2044/6500 [13:33:58<30:40:11, 24.78s/it] 31%|███▏      | 2045/6500 [13:34:16<28:09:01, 22.75s/it]                                                          31%|███▏      | 2045/6500 [13:34:16<28:09:01, 22.75s/it] 31%|███▏      | 2046/6500 [13:34:34<26:23:43, 21.33s/it]  {'loss': 0.3628, 'learning_rate': 7.750910113705001e-05, 'epoch': 0.31}
{'loss': 0.377, 'learning_rate': 7.748891193890768e-05, 'epoch': 0.31}
{'loss': 0.3989, 'learning_rate': 7.746871631539934e-05, 'epoch': 0.32}
{'loss': 0.3396, 'learning_rate': 7.744851427124554e-05, 'epoch': 0.32}
{'loss': 0.3706, 'learning_rate': 7.742830581116843e-05, 'epoch': 0.32}
                                                        31%|███▏      | 2046/6500 [13:34:34<26:23:43, 21.33s/it] 31%|███▏      | 2047/6500 [13:34:52<25:10:32, 20.35s/it]                                                          31%|███▏      | 2047/6500 [13:34:52<25:10:32, 20.35s/it] 32%|███▏      | 2048/6500 [13:35:10<24:19:38, 19.67s/it]                                                          32%|███▏      | 2048/6500 [13:35:10<24:19:38, 19.67s/it] 32%|███▏      | 2049/6500 [13:35:28<23:44:34, 19.20s/it]                                                          32%|███▏      | 2049/6500 [13:35:28<23:44:34, 19.20s/it] 32%|███▏      | 2050/6500 [13:35:46<23:20:07, 18.88s/it]                                                          32%|███▏      | 2050/6500 [13:35:46<23:20:07, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7807930111885071, 'eval_runtime': 6.6041, 'eval_samples_per_second': 3.483, 'eval_steps_per_second': 0.909, 'epoch': 0.32}
                                                          32%|███▏      | 2050/6500 [13:35:53<23:20:07, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2050
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2050/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2050


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2050
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2050/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2050/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3514, 'learning_rate': 7.74080909398916e-05, 'epoch': 0.32}
{'loss': 0.3989, 'learning_rate': 7.738786966214016e-05, 'epoch': 0.32}
{'loss': 0.4206, 'learning_rate': 7.736764198264072e-05, 'epoch': 0.32}
{'loss': 0.3544, 'learning_rate': 7.734740790612136e-05, 'epoch': 0.32}
{'loss': 0.3758, 'learning_rate': 7.732716743731166e-05, 'epoch': 0.32}
 32%|███▏      | 2051/6500 [13:36:59<43:15:55, 35.01s/it]                                                          32%|███▏      | 2051/6500 [13:36:59<43:15:55, 35.01s/it] 32%|███▏      | 2052/6500 [13:37:17<36:56:23, 29.90s/it]                                                          32%|███▏      | 2052/6500 [13:37:17<36:56:23, 29.90s/it] 32%|███▏      | 2053/6500 [13:37:35<32:36:46, 26.40s/it]                                                          32%|███▏      | 2053/6500 [13:37:35<32:36:46, 26.40s/it] 32%|███▏      | 2054/6500 [13:37:53<29:29:43, 23.88s/it]                                                          32%|███▏      | 2054/6500 [13:37:53<29:29:43, 23.88s/it] 32%|███▏      | 2055/6500 [13:38:11<27:19:10, 22.13s/it]                                                          32%|███▏      | 2055/6500 [13:38:11<27:19:10, 22.13s/it] 32%|███▏      | 2056/6500 [13:38:29<25:48:22, 20.91s/it]  {'loss': 0.3478, 'learning_rate': 7.730692058094273e-05, 'epoch': 0.32}
{'loss': 0.8915, 'learning_rate': 7.728666734174715e-05, 'epoch': 0.32}
{'loss': 0.3627, 'learning_rate': 7.726640772445899e-05, 'epoch': 0.32}
{'loss': 0.3714, 'learning_rate': 7.72461417338138e-05, 'epoch': 0.32}
{'loss': 0.3405, 'learning_rate': 7.722586937454864e-05, 'epoch': 0.32}
                                                        32%|███▏      | 2056/6500 [13:38:29<25:48:22, 20.91s/it] 32%|███▏      | 2057/6500 [13:38:47<24:49:30, 20.11s/it]                                                          32%|███▏      | 2057/6500 [13:38:47<24:49:30, 20.11s/it] 32%|███▏      | 2058/6500 [13:39:05<24:04:45, 19.51s/it]                                                          32%|███▏      | 2058/6500 [13:39:05<24:04:45, 19.51s/it] 32%|███▏      | 2059/6500 [13:39:23<23:33:22, 19.10s/it]                                                          32%|███▏      | 2059/6500 [13:39:23<23:33:22, 19.10s/it] 32%|███▏      | 2060/6500 [13:39:42<23:11:11, 18.80s/it]                                                          32%|███▏      | 2060/6500 [13:39:42<23:11:11, 18.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8059830665588379, 'eval_runtime': 5.3342, 'eval_samples_per_second': 4.312, 'eval_steps_per_second': 1.125, 'epoch': 0.32}
                                                          32%|███▏      | 2060/6500 [13:39:47<23:11:11, 18.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2060
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2060/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2060


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2060

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2060
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2060/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2060/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3832, 'learning_rate': 7.720559065140203e-05, 'epoch': 0.32}
{'loss': 0.3638, 'learning_rate': 7.718530556911403e-05, 'epoch': 0.32}
{'loss': 0.3411, 'learning_rate': 7.716501413242616e-05, 'epoch': 0.32}
{'loss': 0.3578, 'learning_rate': 7.714471634608138e-05, 'epoch': 0.32}
{'loss': 0.3481, 'learning_rate': 7.712441221482421e-05, 'epoch': 0.32}
 32%|███▏      | 2061/6500 [13:41:37<58:50:26, 47.72s/it]                                                          32%|███▏      | 2061/6500 [13:41:37<58:50:26, 47.72s/it] 32%|███▏      | 2062/6500 [13:41:55<47:48:06, 38.78s/it]                                                          32%|███▏      | 2062/6500 [13:41:55<47:48:06, 38.78s/it] 32%|███▏      | 2063/6500 [13:42:13<40:15:23, 32.66s/it]                                                          32%|███▏      | 2063/6500 [13:42:13<40:15:23, 32.66s/it] 32%|███▏      | 2064/6500 [13:42:31<34:48:37, 28.25s/it]                                                          32%|███▏      | 2064/6500 [13:42:31<34:48:37, 28.25s/it] 32%|███▏      | 2065/6500 [13:42:49<31:00:48, 25.17s/it]                                                          32%|███▏      | 2065/6500 [13:42:49<31:00:48, 25.17s/it] 32%|███▏      | 2066/6500 [13:43:07<28:24:20, 23.06s/it]  {'loss': 0.3591, 'learning_rate': 7.710410174340061e-05, 'epoch': 0.32}
{'loss': 0.3592, 'learning_rate': 7.7083784936558e-05, 'epoch': 0.32}
{'loss': 0.3591, 'learning_rate': 7.706346179904535e-05, 'epoch': 0.32}
{'loss': 0.3647, 'learning_rate': 7.704313233561305e-05, 'epoch': 0.32}
{'loss': 0.3588, 'learning_rate': 7.7022796551013e-05, 'epoch': 0.32}
                                                        32%|███▏      | 2066/6500 [13:43:07<28:24:20, 23.06s/it] 32%|███▏      | 2067/6500 [13:43:25<26:32:42, 21.56s/it]                                                          32%|███▏      | 2067/6500 [13:43:25<26:32:42, 21.56s/it] 32%|███▏      | 2068/6500 [13:43:43<25:17:00, 20.54s/it]                                                          32%|███▏      | 2068/6500 [13:43:43<25:17:00, 20.54s/it] 32%|███▏      | 2069/6500 [13:44:02<24:35:31, 19.98s/it]                                                          32%|███▏      | 2069/6500 [13:44:02<24:35:31, 19.98s/it] 32%|███▏      | 2070/6500 [13:44:20<23:53:30, 19.42s/it]                                                          32%|███▏      | 2070/6500 [13:44:20<23:53:30, 19.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7975353002548218, 'eval_runtime': 5.3306, 'eval_samples_per_second': 4.315, 'eval_steps_per_second': 1.126, 'epoch': 0.32}
                                                          32%|███▏      | 2070/6500 [13:44:26<23:53:30, 19.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2070
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2070
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2070
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2070

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2070
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2070/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3883, 'learning_rate': 7.700245444999854e-05, 'epoch': 0.32}
{'loss': 0.3461, 'learning_rate': 7.698210603732454e-05, 'epoch': 0.32}
{'loss': 0.3692, 'learning_rate': 7.69617513177473e-05, 'epoch': 0.32}
{'loss': 0.3739, 'learning_rate': 7.694139029602465e-05, 'epoch': 0.32}
{'loss': 0.3455, 'learning_rate': 7.692102297691578e-05, 'epoch': 0.32}
 32%|███▏      | 2071/6500 [13:45:43<47:12:57, 38.38s/it]                                                          32%|███▏      | 2071/6500 [13:45:43<47:12:57, 38.38s/it] 32%|███▏      | 2072/6500 [13:46:01<39:39:39, 32.24s/it]                                                          32%|███▏      | 2072/6500 [13:46:01<39:39:39, 32.24s/it] 32%|███▏      | 2073/6500 [13:46:19<34:22:20, 27.95s/it]                                                          32%|███▏      | 2073/6500 [13:46:19<34:22:20, 27.95s/it] 32%|███▏      | 2074/6500 [13:46:37<30:40:33, 24.95s/it]                                                          32%|███▏      | 2074/6500 [13:46:37<30:40:33, 24.95s/it] 32%|███▏      | 2075/6500 [13:46:55<28:05:34, 22.86s/it]                                                          32%|███▏      | 2075/6500 [13:46:55<28:05:34, 22.86s/it] 32%|███▏      | 2076/6500 [13:47:13<26:17:59, 21.40s/it]  {'loss': 0.3611, 'learning_rate': 7.690064936518151e-05, 'epoch': 0.32}
{'loss': 0.3738, 'learning_rate': 7.688026946558397e-05, 'epoch': 0.32}
{'loss': 0.3761, 'learning_rate': 7.685988328288691e-05, 'epoch': 0.32}
{'loss': 0.3587, 'learning_rate': 7.683949082185544e-05, 'epoch': 0.32}
{'loss': 0.3588, 'learning_rate': 7.681909208725617e-05, 'epoch': 0.32}
                                                        32%|███▏      | 2076/6500 [13:47:13<26:17:59, 21.40s/it] 32%|███▏      | 2077/6500 [13:47:31<25:03:16, 20.39s/it]                                                          32%|███▏      | 2077/6500 [13:47:31<25:03:16, 20.39s/it] 32%|███▏      | 2078/6500 [13:47:49<24:11:04, 19.69s/it]                                                          32%|███▏      | 2078/6500 [13:47:49<24:11:04, 19.69s/it] 32%|███▏      | 2079/6500 [13:48:07<23:35:14, 19.21s/it]                                                          32%|███▏      | 2079/6500 [13:48:07<23:35:14, 19.21s/it] 32%|███▏      | 2080/6500 [13:48:25<23:10:21, 18.87s/it]                                                          32%|███▏      | 2080/6500 [13:48:25<23:10:21, 18.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7834509015083313, 'eval_runtime': 5.3246, 'eval_samples_per_second': 4.32, 'eval_steps_per_second': 1.127, 'epoch': 0.32}
                                                          32%|███▏      | 2080/6500 [13:48:30<23:10:21, 18.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2080
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2080/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2080
 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2080

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2080
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2080/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3424, 'learning_rate': 7.679868708385718e-05, 'epoch': 0.32}
{'loss': 0.4541, 'learning_rate': 7.677827581642804e-05, 'epoch': 0.32}
{'loss': 0.3567, 'learning_rate': 7.675785828973972e-05, 'epoch': 0.32}
{'loss': 0.339, 'learning_rate': 7.673743450856473e-05, 'epoch': 0.32}
{'loss': 0.373, 'learning_rate': 7.6717004477677e-05, 'epoch': 0.32}
 32%|███▏      | 2081/6500 [13:49:47<46:19:47, 37.74s/it]                                                          32%|███▏      | 2081/6500 [13:49:47<46:19:47, 37.74s/it] 32%|███▏      | 2082/6500 [13:50:05<39:01:58, 31.81s/it]                                                          32%|███▏      | 2082/6500 [13:50:05<39:01:58, 31.81s/it] 32%|███▏      | 2083/6500 [13:50:23<33:55:36, 27.65s/it]                                                          32%|███▏      | 2083/6500 [13:50:23<33:55:36, 27.65s/it] 32%|███▏      | 2084/6500 [13:50:41<30:21:14, 24.75s/it]                                                          32%|███▏      | 2084/6500 [13:50:41<30:21:14, 24.75s/it] 32%|███▏      | 2085/6500 [13:50:59<27:57:32, 22.80s/it]                                                          32%|███▏      | 2085/6500 [13:50:59<27:57:32, 22.80s/it] 32%|███▏      | 2086/6500 [13:51:17<26:14:57, 21.41s/it]  {'loss': 0.8906, 'learning_rate': 7.669656820185189e-05, 'epoch': 0.32}
{'loss': 0.3698, 'learning_rate': 7.66761256858663e-05, 'epoch': 0.32}
{'loss': 0.365, 'learning_rate': 7.66556769344985e-05, 'epoch': 0.32}
{'loss': 0.3611, 'learning_rate': 7.663522195252832e-05, 'epoch': 0.32}
{'loss': 0.3334, 'learning_rate': 7.661476074473695e-05, 'epoch': 0.32}
                                                        32%|███▏      | 2086/6500 [13:51:17<26:14:57, 21.41s/it] 32%|███▏      | 2087/6500 [13:51:35<25:01:59, 20.42s/it]                                                          32%|███▏      | 2087/6500 [13:51:35<25:01:59, 20.42s/it] 32%|███▏      | 2088/6500 [13:51:53<24:09:46, 19.72s/it]                                                          32%|███▏      | 2088/6500 [13:51:53<24:09:46, 19.72s/it] 32%|███▏      | 2089/6500 [13:52:11<23:34:02, 19.23s/it]                                                          32%|███▏      | 2089/6500 [13:52:11<23:34:02, 19.23s/it] 32%|███▏      | 2090/6500 [13:52:29<23:08:56, 18.90s/it]                                                          32%|███▏      | 2090/6500 [13:52:29<23:08:56, 18.90s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8054400086402893, 'eval_runtime': 5.3192, 'eval_samples_per_second': 4.324, 'eval_steps_per_second': 1.128, 'epoch': 0.32}
                                                          32%|███▏      | 2090/6500 [13:52:35<23:08:56, 18.90s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2090
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2090 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2090
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2090

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2090
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2090/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2090/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3822, 'learning_rate': 7.659429331590706e-05, 'epoch': 0.32}
{'loss': 0.3559, 'learning_rate': 7.65738196708228e-05, 'epoch': 0.32}
{'loss': 0.3441, 'learning_rate': 7.655333981426978e-05, 'epoch': 0.32}
{'loss': 0.3576, 'learning_rate': 7.6532853751035e-05, 'epoch': 0.32}
{'loss': 0.3504, 'learning_rate': 7.651236148590699e-05, 'epoch': 0.32}
 32%|███▏      | 2091/6500 [13:53:48<45:07:09, 36.84s/it]                                                          32%|███▏      | 2091/6500 [13:53:48<45:07:09, 36.84s/it] 32%|███▏      | 2092/6500 [13:54:06<38:10:26, 31.18s/it]                                                          32%|███▏      | 2092/6500 [13:54:06<38:10:26, 31.18s/it] 32%|███▏      | 2093/6500 [13:54:24<33:18:49, 27.21s/it]                                                          32%|███▏      | 2093/6500 [13:54:24<33:18:49, 27.21s/it] 32%|███▏      | 2094/6500 [13:54:42<29:54:59, 24.44s/it]                                                          32%|███▏      | 2094/6500 [13:54:42<29:54:59, 24.44s/it] 32%|███▏      | 2095/6500 [13:55:00<27:33:24, 22.52s/it]                                                          32%|███▏      | 2095/6500 [13:55:00<27:33:24, 22.52s/it] 32%|███▏      | 2096/6500 [13:55:18<25:54:45, 21.18s/it]  {'loss': 0.3604, 'learning_rate': 7.64918630236757e-05, 'epoch': 0.32}
{'loss': 0.3452, 'learning_rate': 7.647135836913249e-05, 'epoch': 0.32}
{'loss': 0.3642, 'learning_rate': 7.645084752707019e-05, 'epoch': 0.32}
{'loss': 0.3434, 'learning_rate': 7.643033050228312e-05, 'epoch': 0.32}
{'loss': 0.3808, 'learning_rate': 7.640980729956699e-05, 'epoch': 0.32}
                                                        32%|███▏      | 2096/6500 [13:55:18<25:54:45, 21.18s/it] 32%|███▏      | 2097/6500 [13:55:36<24:45:33, 20.24s/it]                                                          32%|███▏      | 2097/6500 [13:55:36<24:45:33, 20.24s/it] 32%|███▏      | 2098/6500 [13:55:54<23:57:22, 19.59s/it]                                                          32%|███▏      | 2098/6500 [13:55:54<23:57:22, 19.59s/it] 32%|███▏      | 2099/6500 [13:56:12<23:24:00, 19.14s/it]                                                          32%|███▏      | 2099/6500 [13:56:12<23:24:00, 19.14s/it] 32%|███▏      | 2100/6500 [13:56:30<23:01:22, 18.84s/it]                                                          32%|███▏      | 2100/6500 [13:56:30<23:01:22, 18.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7986575961112976, 'eval_runtime': 5.3256, 'eval_samples_per_second': 4.319, 'eval_steps_per_second': 1.127, 'epoch': 0.32}
                                                          32%|███▏      | 2100/6500 [13:56:36<23:01:22, 18.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2100the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2100/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2100


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2100/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2100/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3727, 'learning_rate': 7.6389277923719e-05, 'epoch': 0.32}
{'loss': 0.3697, 'learning_rate': 7.636874237953771e-05, 'epoch': 0.32}
{'loss': 0.366, 'learning_rate': 7.634820067182323e-05, 'epoch': 0.32}
{'loss': 0.3627, 'learning_rate': 7.632765280537703e-05, 'epoch': 0.32}
{'loss': 0.3686, 'learning_rate': 7.630709878500208e-05, 'epoch': 0.32}
 32%|███▏      | 2101/6500 [13:57:57<47:48:48, 39.13s/it]                                                          32%|███▏      | 2101/6500 [13:57:57<47:48:48, 39.13s/it] 32%|███▏      | 2102/6500 [13:58:15<40:02:58, 32.78s/it]                                                          32%|███▏      | 2102/6500 [13:58:15<40:02:58, 32.78s/it] 32%|███▏      | 2103/6500 [13:58:33<34:36:42, 28.34s/it]                                                          32%|███▏      | 2103/6500 [13:58:33<34:36:42, 28.34s/it] 32%|███▏      | 2104/6500 [13:58:51<30:48:38, 25.23s/it]                                                          32%|███▏      | 2104/6500 [13:58:51<30:48:38, 25.23s/it] 32%|███▏      | 2105/6500 [13:59:09<28:09:45, 23.07s/it]                                                          32%|███▏      | 2105/6500 [13:59:09<28:09:45, 23.07s/it] 32%|███▏      | 2106/6500 [13:59:27<26:18:31, 21.55s/it]  {'loss': 0.3453, 'learning_rate': 7.628653861550275e-05, 'epoch': 0.32}
{'loss': 0.388, 'learning_rate': 7.626597230168482e-05, 'epoch': 0.32}
{'loss': 0.3539, 'learning_rate': 7.624539984835557e-05, 'epoch': 0.32}
{'loss': 0.3537, 'learning_rate': 7.622482126032368e-05, 'epoch': 0.32}
{'loss': 0.3478, 'learning_rate': 7.620423654239928e-05, 'epoch': 0.32}
                                                        32%|███▏      | 2106/6500 [13:59:27<26:18:31, 21.55s/it] 32%|███▏      | 2107/6500 [13:59:45<25:01:14, 20.50s/it]                                                          32%|███▏      | 2107/6500 [13:59:45<25:01:14, 20.50s/it] 32%|███▏      | 2108/6500 [14:00:03<24:07:30, 19.77s/it]                                                          32%|███▏      | 2108/6500 [14:00:03<24:07:30, 19.77s/it] 32%|███▏      | 2109/6500 [14:00:21<23:30:19, 19.27s/it]                                                          32%|███▏      | 2109/6500 [14:00:21<23:30:19, 19.27s/it] 32%|███▏      | 2110/6500 [14:00:39<23:08:33, 18.98s/it]                                                          32%|███▏      | 2110/6500 [14:00:39<23:08:33, 18.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7887817025184631, 'eval_runtime': 5.3908, 'eval_samples_per_second': 4.267, 'eval_steps_per_second': 1.113, 'epoch': 0.32}
                                                          32%|███▏      | 2110/6500 [14:00:45<23:08:33, 18.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2110

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2110
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2110/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3674, 'learning_rate': 7.618364569939391e-05, 'epoch': 0.32}
{'loss': 0.4218, 'learning_rate': 7.616304873612055e-05, 'epoch': 0.32}
{'loss': 0.3684, 'learning_rate': 7.614244565739361e-05, 'epoch': 0.33}
{'loss': 0.3538, 'learning_rate': 7.612183646802895e-05, 'epoch': 0.33}
{'loss': 0.3694, 'learning_rate': 7.610122117284386e-05, 'epoch': 0.33}
 32%|███▏      | 2111/6500 [14:02:09<49:07:49, 40.30s/it]                                                          32%|███▏      | 2111/6500 [14:02:09<49:07:49, 40.30s/it] 32%|███▏      | 2112/6500 [14:02:27<40:56:48, 33.59s/it]                                                          32%|███▏      | 2112/6500 [14:02:27<40:56:48, 33.59s/it] 33%|███▎      | 2113/6500 [14:02:45<35:13:07, 28.90s/it]                                                          33%|███▎      | 2113/6500 [14:02:45<35:13:07, 28.90s/it] 33%|███▎      | 2114/6500 [14:03:03<31:13:00, 25.62s/it]                                                          33%|███▎      | 2114/6500 [14:03:03<31:13:00, 25.62s/it] 33%|███▎      | 2115/6500 [14:03:21<28:25:29, 23.34s/it]                                                          33%|███▎      | 2115/6500 [14:03:21<28:25:29, 23.34s/it] 33%|███▎      | 2116/6500 [14:03:39<26:28:33, 21.74s/it]  {'loss': 0.8942, 'learning_rate': 7.6080599776657e-05, 'epoch': 0.33}
{'loss': 0.3691, 'learning_rate': 7.605997228428853e-05, 'epoch': 0.33}
{'loss': 0.351, 'learning_rate': 7.603933870055997e-05, 'epoch': 0.33}
{'loss': 0.3428, 'learning_rate': 7.601869903029432e-05, 'epoch': 0.33}
{'loss': 0.3413, 'learning_rate': 7.599805327831596e-05, 'epoch': 0.33}
                                                        33%|███▎      | 2116/6500 [14:03:39<26:28:33, 21.74s/it] 33%|███▎      | 2117/6500 [14:03:58<25:23:20, 20.85s/it]                                                          33%|███▎      | 2117/6500 [14:03:58<25:23:20, 20.85s/it] 33%|███▎      | 2118/6500 [14:04:16<24:21:49, 20.02s/it]                                                          33%|███▎      | 2118/6500 [14:04:16<24:21:49, 20.02s/it] 33%|███▎      | 2119/6500 [14:04:34<23:39:24, 19.44s/it]                                                          33%|███▎      | 2119/6500 [14:04:34<23:39:24, 19.44s/it] 33%|███▎      | 2120/6500 [14:04:52<23:09:55, 19.04s/it]                                                          33%|███▎      | 2120/6500 [14:04:52<23:09:55, 19.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8074427843093872, 'eval_runtime': 5.3171, 'eval_samples_per_second': 4.326, 'eval_steps_per_second': 1.128, 'epoch': 0.33}
                                                          33%|███▎      | 2120/6500 [14:04:58<23:09:55, 19.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2120
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2120 
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2120
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2120
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2120/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.374, 'learning_rate': 7.597740144945073e-05, 'epoch': 0.33}
{'loss': 0.3482, 'learning_rate': 7.595674354852584e-05, 'epoch': 0.33}
{'loss': 0.351, 'learning_rate': 7.593607958036998e-05, 'epoch': 0.33}
{'loss': 0.3469, 'learning_rate': 7.59154095498132e-05, 'epoch': 0.33}
{'loss': 0.3501, 'learning_rate': 7.589473346168702e-05, 'epoch': 0.33}
 33%|███▎      | 2121/6500 [14:05:42<34:26:27, 28.31s/it]                                                          33%|███▎      | 2121/6500 [14:05:42<34:26:27, 28.31s/it] 33%|███▎      | 2122/6500 [14:06:00<30:40:26, 25.22s/it]                                                          33%|███▎      | 2122/6500 [14:06:00<30:40:26, 25.22s/it] 33%|███▎      | 2123/6500 [14:06:18<28:01:54, 23.06s/it]                                                          33%|███▎      | 2123/6500 [14:06:18<28:01:54, 23.06s/it] 33%|███▎      | 2124/6500 [14:06:36<26:11:22, 21.55s/it]                                                          33%|███▎      | 2124/6500 [14:06:36<26:11:22, 21.55s/it] 33%|███▎      | 2125/6500 [14:06:54<24:54:08, 20.49s/it]                                                          33%|███▎      | 2125/6500 [14:06:54<24:54:08, 20.49s/it] 33%|███▎      | 2126/6500 [14:07:12<24:00:23, 19.76s/it]  {'loss': 0.3513, 'learning_rate': 7.587405132082433e-05, 'epoch': 0.33}
{'loss': 0.3458, 'learning_rate': 7.585336313205944e-05, 'epoch': 0.33}
{'loss': 0.3695, 'learning_rate': 7.583266890022814e-05, 'epoch': 0.33}
{'loss': 0.3417, 'learning_rate': 7.581196863016754e-05, 'epoch': 0.33}
{'loss': 0.3739, 'learning_rate': 7.579126232671621e-05, 'epoch': 0.33}
                                                        33%|███▎      | 2126/6500 [14:07:12<24:00:23, 19.76s/it] 33%|███▎      | 2127/6500 [14:07:30<23:22:48, 19.25s/it]                                                          33%|███▎      | 2127/6500 [14:07:30<23:22:48, 19.25s/it] 33%|███▎      | 2128/6500 [14:07:49<22:57:04, 18.90s/it]                                                          33%|███▎      | 2128/6500 [14:07:49<22:57:04, 18.90s/it] 33%|███▎      | 2129/6500 [14:08:07<22:39:33, 18.66s/it]                                                          33%|███▎      | 2129/6500 [14:08:07<22:39:33, 18.66s/it] 33%|███▎      | 2130/6500 [14:08:25<22:27:33, 18.50s/it]                                                          33%|███▎      | 2130/6500 [14:08:25<22:27:33, 18.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8051117062568665, 'eval_runtime': 5.322, 'eval_samples_per_second': 4.322, 'eval_steps_per_second': 1.127, 'epoch': 0.33}
                                                          33%|███▎      | 2130/6500 [14:08:30<22:27:33, 18.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2130
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2130 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2130
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2130
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2130/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2130/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3543, 'learning_rate': 7.577054999471411e-05, 'epoch': 0.33}
{'loss': 0.3521, 'learning_rate': 7.574983163900265e-05, 'epoch': 0.33}
{'loss': 0.3575, 'learning_rate': 7.572910726442462e-05, 'epoch': 0.33}
{'loss': 0.3515, 'learning_rate': 7.57083768758242e-05, 'epoch': 0.33}
{'loss': 0.3577, 'learning_rate': 7.568764047804698e-05, 'epoch': 0.33}
 33%|███▎      | 2131/6500 [14:09:32<40:12:25, 33.13s/it]                                                          33%|███▎      | 2131/6500 [14:09:32<40:12:25, 33.13s/it] 33%|███▎      | 2132/6500 [14:09:50<34:40:43, 28.58s/it]                                                          33%|███▎      | 2132/6500 [14:09:50<34:40:43, 28.58s/it] 33%|███▎      | 2133/6500 [14:10:08<30:48:37, 25.40s/it]                                                          33%|███▎      | 2133/6500 [14:10:08<30:48:37, 25.40s/it] 33%|███▎      | 2134/6500 [14:10:26<28:12:10, 23.25s/it]                                                          33%|███▎      | 2134/6500 [14:10:26<28:12:10, 23.25s/it] 33%|███▎      | 2135/6500 [14:10:44<26:20:17, 21.72s/it]                                                          33%|███▎      | 2135/6500 [14:10:44<26:20:17, 21.72s/it] 33%|███▎      | 2136/6500 [14:11:02<25:00:01, 20.62s/it]  {'loss': 0.3526, 'learning_rate': 7.566689807593998e-05, 'epoch': 0.33}
{'loss': 0.3799, 'learning_rate': 7.56461496743516e-05, 'epoch': 0.33}
{'loss': 0.3411, 'learning_rate': 7.562539527813169e-05, 'epoch': 0.33}
{'loss': 0.3623, 'learning_rate': 7.560463489213143e-05, 'epoch': 0.33}
{'loss': 0.3371, 'learning_rate': 7.558386852120344e-05, 'epoch': 0.33}
                                                        33%|███▎      | 2136/6500 [14:11:02<25:00:01, 20.62s/it] 33%|███▎      | 2137/6500 [14:11:21<24:06:35, 19.89s/it]                                                          33%|███▎      | 2137/6500 [14:11:21<24:06:35, 19.89s/it] 33%|███▎      | 2138/6500 [14:11:39<23:28:21, 19.37s/it]                                                          33%|███▎      | 2138/6500 [14:11:39<23:28:21, 19.37s/it] 33%|███▎      | 2139/6500 [14:11:57<23:00:20, 18.99s/it]                                                          33%|███▎      | 2139/6500 [14:11:57<23:00:20, 18.99s/it] 33%|███▎      | 2140/6500 [14:12:15<22:40:56, 18.73s/it]                                                          33%|███▎      | 2140/6500 [14:12:15<22:40:56, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7914566397666931, 'eval_runtime': 5.4763, 'eval_samples_per_second': 4.2, 'eval_steps_per_second': 1.096, 'epoch': 0.33}
                                                          33%|███▎      | 2140/6500 [14:12:20<22:40:56, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2140 

/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2140
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2140/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2140/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3914, 'learning_rate': 7.556309617020175e-05, 'epoch': 0.33}
{'loss': 0.4008, 'learning_rate': 7.554231784398173e-05, 'epoch': 0.33}
{'loss': 0.3459, 'learning_rate': 7.552153354740023e-05, 'epoch': 0.33}
{'loss': 0.3613, 'learning_rate': 7.550074328531545e-05, 'epoch': 0.33}
{'loss': 0.351, 'learning_rate': 7.547994706258694e-05, 'epoch': 0.33}
 33%|███▎      | 2141/6500 [14:13:58<53:12:22, 43.94s/it]                                                          33%|███▎      | 2141/6500 [14:13:58<53:12:22, 43.94s/it] 33%|███▎      | 2142/6500 [14:14:16<43:45:20, 36.15s/it]                                                          33%|███▎      | 2142/6500 [14:14:16<43:45:20, 36.15s/it] 33%|███▎      | 2143/6500 [14:14:34<37:07:48, 30.68s/it]                                                          33%|███▎      | 2143/6500 [14:14:34<37:07:48, 30.68s/it] 33%|███▎      | 2144/6500 [14:14:52<32:29:54, 26.86s/it]                                                          33%|███▎      | 2144/6500 [14:14:52<32:29:54, 26.86s/it] 33%|███▎      | 2145/6500 [14:15:10<29:15:35, 24.19s/it]                                                          33%|███▎      | 2145/6500 [14:15:10<29:15:35, 24.19s/it] 33%|███▎      | 2146/6500 [14:15:27<26:59:28, 22.32s/it]  {'loss': 0.8821, 'learning_rate': 7.545914488407575e-05, 'epoch': 0.33}
{'loss': 0.3591, 'learning_rate': 7.543833675464422e-05, 'epoch': 0.33}
{'loss': 0.3591, 'learning_rate': 7.541752267915615e-05, 'epoch': 0.33}
{'loss': 0.3319, 'learning_rate': 7.539670266247671e-05, 'epoch': 0.33}
{'loss': 0.3602, 'learning_rate': 7.537587670947244e-05, 'epoch': 0.33}
                                                        33%|███▎      | 2146/6500 [14:15:28<26:59:28, 22.32s/it] 33%|███▎      | 2147/6500 [14:15:46<25:25:26, 21.03s/it]                                                          33%|███▎      | 2147/6500 [14:15:46<25:25:26, 21.03s/it] 33%|███▎      | 2148/6500 [14:16:04<24:20:07, 20.13s/it]                                                          33%|███▎      | 2148/6500 [14:16:04<24:20:07, 20.13s/it] 33%|███▎      | 2149/6500 [14:16:22<23:34:53, 19.51s/it]                                                          33%|███▎      | 2149/6500 [14:16:22<23:34:53, 19.51s/it] 33%|███▎      | 2150/6500 [14:16:40<23:12:51, 19.21s/it]                                                          33%|███▎      | 2150/6500 [14:16:40<23:12:51, 19.21s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8095588684082031, 'eval_runtime': 5.6011, 'eval_samples_per_second': 4.106, 'eval_steps_per_second': 1.071, 'epoch': 0.33}
                                                          33%|███▎      | 2150/6500 [14:16:46<23:12:51, 19.21s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2150
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2150 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2150/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3582, 'learning_rate': 7.535504482501126e-05, 'epoch': 0.33}
{'loss': 0.3219, 'learning_rate': 7.53342070139625e-05, 'epoch': 0.33}
{'loss': 0.3549, 'learning_rate': 7.53133632811969e-05, 'epoch': 0.33}
{'loss': 0.3343, 'learning_rate': 7.529251363158653e-05, 'epoch': 0.33}
{'loss': 0.3544, 'learning_rate': 7.527165807000492e-05, 'epoch': 0.33}
 33%|███▎      | 2151/6500 [14:17:46<40:06:28, 33.20s/it]                                                          33%|███▎      | 2151/6500 [14:17:46<40:06:28, 33.20s/it] 33%|███▎      | 2152/6500 [14:18:04<34:34:59, 28.63s/it]                                                          33%|███▎      | 2152/6500 [14:18:04<34:34:59, 28.63s/it] 33%|███▎      | 2153/6500 [14:18:22<30:42:48, 25.44s/it]                                                          33%|███▎      | 2153/6500 [14:18:22<30:42:48, 25.44s/it] 33%|███▎      | 2154/6500 [14:18:40<28:00:01, 23.19s/it]                                                          33%|███▎      | 2154/6500 [14:18:40<28:00:01, 23.19s/it] 33%|███▎      | 2155/6500 [14:18:58<26:06:40, 21.63s/it]                                                          33%|███▎      | 2155/6500 [14:18:58<26:06:40, 21.63s/it] 33%|███▎      | 2156/6500 [14:19:16<24:47:36, 20.55s/it]  {'loss': 0.3526, 'learning_rate': 7.525079660132685e-05, 'epoch': 0.33}
{'loss': 0.3593, 'learning_rate': 7.522992923042861e-05, 'epoch': 0.33}
{'loss': 0.3467, 'learning_rate': 7.520905596218781e-05, 'epoch': 0.33}
{'loss': 0.338, 'learning_rate': 7.518817680148347e-05, 'epoch': 0.33}
{'loss': 0.3866, 'learning_rate': 7.516729175319592e-05, 'epoch': 0.33}
                                                        33%|███▎      | 2156/6500 [14:19:16<24:47:36, 20.55s/it] 33%|███▎      | 2157/6500 [14:19:34<23:52:18, 19.79s/it]                                                          33%|███▎      | 2157/6500 [14:19:34<23:52:18, 19.79s/it] 33%|███▎      | 2158/6500 [14:19:52<23:13:51, 19.26s/it]                                                          33%|███▎      | 2158/6500 [14:19:52<23:13:51, 19.26s/it] 33%|███▎      | 2159/6500 [14:20:11<23:12:08, 19.24s/it]                                                          33%|███▎      | 2159/6500 [14:20:11<23:12:08, 19.24s/it] 33%|███▎      | 2160/6500 [14:20:29<22:47:55, 18.91s/it]                                                          33%|███▎      | 2160/6500 [14:20:29<22:47:55, 18.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8086296319961548, 'eval_runtime': 6.2662, 'eval_samples_per_second': 3.67, 'eval_steps_per_second': 0.958, 'epoch': 0.33}
                                                          33%|███▎      | 2160/6500 [14:20:36<22:47:55, 18.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2160
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2160
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2160/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2160/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2160/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3449, 'learning_rate': 7.514640082220697e-05, 'epoch': 0.33}
{'loss': 0.3577, 'learning_rate': 7.512550401339971e-05, 'epoch': 0.33}
{'loss': 0.3758, 'learning_rate': 7.510460133165867e-05, 'epoch': 0.33}
{'loss': 0.3418, 'learning_rate': 7.508369278186967e-05, 'epoch': 0.33}
{'loss': 0.3513, 'learning_rate': 7.506277836892001e-05, 'epoch': 0.33}
 33%|███▎      | 2161/6500 [14:21:51<45:23:43, 37.66s/it]                                                          33%|███▎      | 2161/6500 [14:21:51<45:23:43, 37.66s/it] 33%|███▎      | 2162/6500 [14:22:09<38:15:35, 31.75s/it]                                                          33%|███▎      | 2162/6500 [14:22:09<38:15:35, 31.75s/it] 33%|███▎      | 2163/6500 [14:22:27<33:15:43, 27.61s/it]                                                          33%|███▎      | 2163/6500 [14:22:27<33:15:43, 27.61s/it] 33%|███▎      | 2164/6500 [14:22:45<29:45:40, 24.71s/it]                                                          33%|███▎      | 2164/6500 [14:22:45<29:45:40, 24.71s/it] 33%|███▎      | 2165/6500 [14:23:03<27:18:57, 22.68s/it]                                                          33%|███▎      | 2165/6500 [14:23:03<27:18:57, 22.68s/it] 33%|███▎      | 2166/6500 [14:23:21<25:44:46, 21.39s/it]  {'loss': 0.3667, 'learning_rate': 7.50418580976983e-05, 'epoch': 0.33}
{'loss': 0.3549, 'learning_rate': 7.502093197309452e-05, 'epoch': 0.33}
{'loss': 0.3404, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.33}
{'loss': 0.3391, 'learning_rate': 7.49790621833075e-05, 'epoch': 0.33}
{'loss': 0.3689, 'learning_rate': 7.495811852791107e-05, 'epoch': 0.33}
                                                        33%|███▎      | 2166/6500 [14:23:21<25:44:46, 21.39s/it] 33%|███▎      | 2167/6500 [14:23:39<24:34:09, 20.41s/it]                                                          33%|███▎      | 2167/6500 [14:23:39<24:34:09, 20.41s/it] 33%|███▎      | 2168/6500 [14:23:57<23:42:31, 19.70s/it]                                                          33%|███▎      | 2168/6500 [14:23:57<23:42:31, 19.70s/it] 33%|███▎      | 2169/6500 [14:24:15<23:07:18, 19.22s/it]                                                          33%|███▎      | 2169/6500 [14:24:15<23:07:18, 19.22s/it] 33%|███▎      | 2170/6500 [14:24:33<22:42:21, 18.88s/it]                                                          33%|███▎      | 2170/6500 [14:24:33<22:42:21, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7959308624267578, 'eval_runtime': 5.7154, 'eval_samples_per_second': 4.024, 'eval_steps_per_second': 1.05, 'epoch': 0.33}
                                                          33%|███▎      | 2170/6500 [14:24:39<22:42:21, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2170I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2170
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2170/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2170/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4191, 'learning_rate': 7.493716903870621e-05, 'epoch': 0.33}
{'loss': 0.3379, 'learning_rate': 7.49162137205897e-05, 'epoch': 0.33}
{'loss': 0.3361, 'learning_rate': 7.489525257845971e-05, 'epoch': 0.33}
{'loss': 0.3658, 'learning_rate': 7.48742856172158e-05, 'epoch': 0.33}
{'loss': 0.8782, 'learning_rate': 7.485331284175887e-05, 'epoch': 0.33}
 33%|███▎      | 2171/6500 [14:26:05<48:56:48, 40.70s/it]                                                          33%|███▎      | 2171/6500 [14:26:05<48:56:48, 40.70s/it] 33%|███▎      | 2172/6500 [14:26:23<40:44:16, 33.89s/it]                                                          33%|███▎      | 2172/6500 [14:26:23<40:44:16, 33.89s/it] 33%|███▎      | 2173/6500 [14:26:41<34:58:50, 29.10s/it]                                                          33%|███▎      | 2173/6500 [14:26:41<34:58:50, 29.10s/it] 33%|███▎      | 2174/6500 [14:26:59<30:57:17, 25.76s/it]                                                          33%|███▎      | 2174/6500 [14:26:59<30:57:17, 25.76s/it] 33%|███▎      | 2175/6500 [14:27:17<28:08:19, 23.42s/it]                                                          33%|███▎      | 2175/6500 [14:27:17<28:08:19, 23.42s/it] 33%|███▎      | 2176/6500 [14:27:35<26:10:52, 21.80s/it]  {'loss': 0.3552, 'learning_rate': 7.483233425699119e-05, 'epoch': 0.33}
{'loss': 0.3476, 'learning_rate': 7.481134986781634e-05, 'epoch': 0.33}
{'loss': 0.3579, 'learning_rate': 7.47903596791393e-05, 'epoch': 0.34}
{'loss': 0.3325, 'learning_rate': 7.476936369586645e-05, 'epoch': 0.34}
{'loss': 0.3683, 'learning_rate': 7.47483619229054e-05, 'epoch': 0.34}
                                                        33%|███▎      | 2176/6500 [14:27:35<26:10:52, 21.80s/it] 33%|███▎      | 2177/6500 [14:27:53<24:49:07, 20.67s/it]                                                          33%|███▎      | 2177/6500 [14:27:53<24:49:07, 20.67s/it] 34%|███▎      | 2178/6500 [14:28:11<23:52:21, 19.88s/it]                                                          34%|███▎      | 2178/6500 [14:28:11<23:52:21, 19.88s/it] 34%|███▎      | 2179/6500 [14:28:29<23:13:01, 19.34s/it]                                                          34%|███▎      | 2179/6500 [14:28:29<23:13:01, 19.34s/it] 34%|███▎      | 2180/6500 [14:28:47<22:45:55, 18.97s/it]                                                          34%|███▎      | 2180/6500 [14:28:47<22:45:55, 18.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8076443672180176, 'eval_runtime': 5.8261, 'eval_samples_per_second': 3.948, 'eval_steps_per_second': 1.03, 'epoch': 0.34}
                                                          34%|███▎      | 2180/6500 [14:28:53<22:45:55, 18.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2180/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2180

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2180

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2180/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3335, 'learning_rate': 7.472735436516524e-05, 'epoch': 0.34}
{'loss': 0.3356, 'learning_rate': 7.470634102755636e-05, 'epoch': 0.34}
{'loss': 0.3496, 'learning_rate': 7.468532191499045e-05, 'epoch': 0.34}
{'loss': 0.3424, 'learning_rate': 7.466429703238065e-05, 'epoch': 0.34}
{'loss': 0.3581, 'learning_rate': 7.464326638464138e-05, 'epoch': 0.34}
 34%|███▎      | 2181/6500 [14:29:52<39:24:35, 32.85s/it]                                                          34%|███▎      | 2181/6500 [14:29:52<39:24:35, 32.85s/it] 34%|███▎      | 2182/6500 [14:30:11<34:25:13, 28.70s/it]                                                          34%|███▎      | 2182/6500 [14:30:11<34:25:13, 28.70s/it] 34%|███▎      | 2183/6500 [14:30:29<30:34:07, 25.49s/it]                                                          34%|███▎      | 2183/6500 [14:30:29<30:34:07, 25.49s/it] 34%|███▎      | 2184/6500 [14:30:47<27:53:09, 23.26s/it]                                                          34%|███▎      | 2184/6500 [14:30:47<27:53:09, 23.26s/it] 34%|███▎      | 2185/6500 [14:31:05<26:00:49, 21.70s/it]                                                          34%|███▎      | 2185/6500 [14:31:05<26:00:49, 21.70s/it] 34%|███▎      | 2186/6500 [14:31:23<24:42:05, 20.61s/it]  {'loss': 0.3518, 'learning_rate': 7.462222997668841e-05, 'epoch': 0.34}
{'loss': 0.3678, 'learning_rate': 7.460118781343893e-05, 'epoch': 0.34}
{'loss': 0.3412, 'learning_rate': 7.458013989981133e-05, 'epoch': 0.34}
{'loss': 0.3692, 'learning_rate': 7.45590862407255e-05, 'epoch': 0.34}
{'loss': 0.3591, 'learning_rate': 7.453802684110257e-05, 'epoch': 0.34}
                                                        34%|███▎      | 2186/6500 [14:31:23<24:42:05, 20.61s/it] 34%|███▎      | 2187/6500 [14:31:42<23:47:15, 19.86s/it]                                                          34%|███▎      | 2187/6500 [14:31:42<23:47:15, 19.86s/it] 34%|███▎      | 2188/6500 [14:32:00<23:09:37, 19.34s/it]                                                          34%|███▎      | 2188/6500 [14:32:00<23:09:37, 19.34s/it] 34%|███▎      | 2189/6500 [14:32:18<22:44:02, 18.98s/it]                                                          34%|███▎      | 2189/6500 [14:32:18<22:44:02, 18.98s/it] 34%|███▎      | 2190/6500 [14:32:36<22:25:46, 18.73s/it]                                                          34%|███▎      | 2190/6500 [14:32:36<22:25:46, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8027814626693726, 'eval_runtime': 5.3169, 'eval_samples_per_second': 4.326, 'eval_steps_per_second': 1.128, 'epoch': 0.34}
                                                          34%|███▎      | 2190/6500 [14:32:41<22:25:46, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2190
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2190
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2190
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2190
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2190/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3521, 'learning_rate': 7.451696170586507e-05, 'epoch': 0.34}
{'loss': 0.3538, 'learning_rate': 7.449589083993684e-05, 'epoch': 0.34}
{'loss': 0.3574, 'learning_rate': 7.447481424824305e-05, 'epoch': 0.34}
{'loss': 0.3494, 'learning_rate': 7.445373193571026e-05, 'epoch': 0.34}
{'loss': 0.3602, 'learning_rate': 7.443264390726629e-05, 'epoch': 0.34}
 34%|███▎      | 2191/6500 [14:33:27<34:08:12, 28.52s/it]                                                          34%|███▎      | 2191/6500 [14:33:27<34:08:12, 28.52s/it] 34%|███▎      | 2192/6500 [14:33:45<30:22:04, 25.38s/it]                                                          34%|███▎      | 2192/6500 [14:33:45<30:22:04, 25.38s/it] 34%|███▎      | 2193/6500 [14:34:03<27:43:27, 23.17s/it]                                                          34%|███▎      | 2193/6500 [14:34:03<27:43:27, 23.17s/it] 34%|███▍      | 2194/6500 [14:34:21<25:52:18, 21.63s/it]                                                          34%|███▍      | 2194/6500 [14:34:21<25:52:18, 21.63s/it] 34%|███▍      | 2195/6500 [14:34:39<24:34:51, 20.56s/it]                                                          34%|███▍      | 2195/6500 [14:34:39<24:34:51, 20.56s/it] 34%|███▍      | 2196/6500 [14:34:58<23:41:01, 19.81s/it]  {'loss': 0.3705, 'learning_rate': 7.441155016784037e-05, 'epoch': 0.34}
{'loss': 0.3375, 'learning_rate': 7.439045072236301e-05, 'epoch': 0.34}
{'loss': 0.3496, 'learning_rate': 7.436934557576612e-05, 'epoch': 0.34}
{'loss': 0.331, 'learning_rate': 7.434823473298283e-05, 'epoch': 0.34}
{'loss': 0.3723, 'learning_rate': 7.432711819894775e-05, 'epoch': 0.34}
                                                        34%|███▍      | 2196/6500 [14:34:58<23:41:01, 19.81s/it] 34%|███▍      | 2197/6500 [14:35:16<23:03:33, 19.29s/it]                                                          34%|███▍      | 2197/6500 [14:35:16<23:03:33, 19.29s/it] 34%|███▍      | 2198/6500 [14:35:34<22:42:39, 19.01s/it]                                                          34%|███▍      | 2198/6500 [14:35:34<22:42:39, 19.01s/it] 34%|███▍      | 2199/6500 [14:35:52<22:22:59, 18.74s/it]                                                          34%|███▍      | 2199/6500 [14:35:52<22:22:59, 18.74s/it] 34%|███▍      | 2200/6500 [14:36:10<22:09:44, 18.55s/it]                                                          34%|███▍      | 2200/6500 [14:36:10<22:09:44, 18.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7964072227478027, 'eval_runtime': 5.9651, 'eval_samples_per_second': 3.856, 'eval_steps_per_second': 1.006, 'epoch': 0.34}
                                                          34%|███▍      | 2200/6500 [14:36:16<22:09:44, 18.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2200/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2200/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4126, 'learning_rate': 7.430599597859666e-05, 'epoch': 0.34}
{'loss': 0.3378, 'learning_rate': 7.428486807686683e-05, 'epoch': 0.34}
{'loss': 0.3417, 'learning_rate': 7.426373449869673e-05, 'epoch': 0.34}
{'loss': 0.356, 'learning_rate': 7.424259524902622e-05, 'epoch': 0.34}
{'loss': 0.8788, 'learning_rate': 7.422145033279646e-05, 'epoch': 0.34}
 34%|███▍      | 2201/6500 [14:37:12<37:39:59, 31.54s/it]                                                          34%|███▍      | 2201/6500 [14:37:12<37:39:59, 31.54s/it] 34%|███▍      | 2202/6500 [14:37:30<32:48:33, 27.48s/it]                                                          34%|███▍      | 2202/6500 [14:37:30<32:48:33, 27.48s/it] 34%|███▍      | 2203/6500 [14:37:48<29:24:33, 24.64s/it]                                                          34%|███▍      | 2203/6500 [14:37:48<29:24:33, 24.64s/it] 34%|███▍      | 2204/6500 [14:38:06<27:01:44, 22.65s/it]                                                          34%|███▍      | 2204/6500 [14:38:06<27:01:44, 22.65s/it] 34%|███▍      | 2205/6500 [14:38:24<25:21:45, 21.26s/it]                                                          34%|███▍      | 2205/6500 [14:38:24<25:21:45, 21.26s/it] 34%|███▍      | 2206/6500 [14:38:42<24:12:25, 20.29s/it]  {'loss': 0.36, 'learning_rate': 7.420029975494995e-05, 'epoch': 0.34}
{'loss': 0.3531, 'learning_rate': 7.417914352043052e-05, 'epoch': 0.34}
{'loss': 0.315, 'learning_rate': 7.41579816341833e-05, 'epoch': 0.34}
{'loss': 0.334, 'learning_rate': 7.413681410115474e-05, 'epoch': 0.34}
{'loss': 0.373, 'learning_rate': 7.411564092629267e-05, 'epoch': 0.34}
                                                        34%|███▍      | 2206/6500 [14:38:42<24:12:25, 20.29s/it] 34%|███▍      | 2207/6500 [14:39:00<23:26:24, 19.66s/it]                                                          34%|███▍      | 2207/6500 [14:39:00<23:26:24, 19.66s/it] 34%|███▍      | 2208/6500 [14:39:18<22:54:58, 19.22s/it]                                                          34%|███▍      | 2208/6500 [14:39:18<22:54:58, 19.22s/it] 34%|███▍      | 2209/6500 [14:39:37<22:30:43, 18.89s/it]                                                          34%|███▍      | 2209/6500 [14:39:37<22:30:43, 18.89s/it] 34%|███▍      | 2210/6500 [14:39:55<22:15:39, 18.68s/it]                                                          34%|███▍      | 2210/6500 [14:39:55<22:15:39, 18.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8088605999946594, 'eval_runtime': 5.3274, 'eval_samples_per_second': 4.317, 'eval_steps_per_second': 1.126, 'epoch': 0.34}
                                                          34%|███▍      | 2210/6500 [14:40:00<22:15:39, 18.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2210
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2210
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2210/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2210/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2210/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3268, 'learning_rate': 7.409446211454615e-05, 'epoch': 0.34}
{'loss': 0.3364, 'learning_rate': 7.407327767086561e-05, 'epoch': 0.34}
{'loss': 0.3296, 'learning_rate': 7.405208760020276e-05, 'epoch': 0.34}
{'loss': 0.3411, 'learning_rate': 7.403089190751069e-05, 'epoch': 0.34}
{'loss': 0.3401, 'learning_rate': 7.400969059774375e-05, 'epoch': 0.34}
 34%|███▍      | 2211/6500 [14:40:38<31:10:49, 26.17s/it]                                                          34%|███▍      | 2211/6500 [14:40:38<31:10:49, 26.17s/it] 34%|███▍      | 2212/6500 [14:40:56<28:15:57, 23.73s/it]                                                          34%|███▍      | 2212/6500 [14:40:56<28:15:57, 23.73s/it] 34%|███▍      | 2213/6500 [14:41:15<26:13:27, 22.02s/it]                                                          34%|███▍      | 2213/6500 [14:41:15<26:13:27, 22.02s/it] 34%|███▍      | 2214/6500 [14:41:33<25:00:06, 21.00s/it]                                                          34%|███▍      | 2214/6500 [14:41:33<25:00:06, 21.00s/it] 34%|███▍      | 2215/6500 [14:41:51<23:56:44, 20.12s/it]                                                          34%|███▍      | 2215/6500 [14:41:51<23:56:44, 20.12s/it] 34%|███▍      | 2216/6500 [14:42:09<23:12:46, 19.51s/it]  {'loss': 0.3462, 'learning_rate': 7.39884836758576e-05, 'epoch': 0.34}
{'loss': 0.3649, 'learning_rate': 7.396727114680925e-05, 'epoch': 0.34}
{'loss': 0.3421, 'learning_rate': 7.394605301555699e-05, 'epoch': 0.34}
{'loss': 0.3857, 'learning_rate': 7.392482928706044e-05, 'epoch': 0.34}
{'loss': 0.3562, 'learning_rate': 7.390359996628051e-05, 'epoch': 0.34}
                                                        34%|███▍      | 2216/6500 [14:42:09<23:12:46, 19.51s/it] 34%|███▍      | 2217/6500 [14:42:28<22:58:43, 19.31s/it]                                                          34%|███▍      | 2217/6500 [14:42:28<22:58:43, 19.31s/it] 34%|███▍      | 2218/6500 [14:42:46<22:32:41, 18.95s/it]                                                          34%|███▍      | 2218/6500 [14:42:46<22:32:41, 18.95s/it] 34%|███▍      | 2219/6500 [14:43:04<22:14:13, 18.70s/it]                                                          34%|███▍      | 2219/6500 [14:43:04<22:14:13, 18.70s/it] 34%|███▍      | 2220/6500 [14:43:22<22:01:22, 18.52s/it]                                                          34%|███▍      | 2220/6500 [14:43:22<22:01:22, 18.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8020369410514832, 'eval_runtime': 5.3218, 'eval_samples_per_second': 4.322, 'eval_steps_per_second': 1.127, 'epoch': 0.34}
                                                          34%|███▍      | 2220/6500 [14:43:28<22:01:22, 18.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2220
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2220/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3503, 'learning_rate': 7.388236505817943e-05, 'epoch': 0.34}
{'loss': 0.3586, 'learning_rate': 7.386112456772071e-05, 'epoch': 0.34}
{'loss': 0.3426, 'learning_rate': 7.38398784998692e-05, 'epoch': 0.34}
{'loss': 0.3583, 'learning_rate': 7.381862685959104e-05, 'epoch': 0.34}
{'loss': 0.3461, 'learning_rate': 7.379736965185368e-05, 'epoch': 0.34}
 34%|███▍      | 2221/6500 [14:44:18<35:04:07, 29.50s/it]                                                          34%|███▍      | 2221/6500 [14:44:18<35:04:07, 29.50s/it] 34%|███▍      | 2222/6500 [14:44:36<30:58:34, 26.07s/it]                                                          34%|███▍      | 2222/6500 [14:44:36<30:58:34, 26.07s/it] 34%|███▍      | 2223/6500 [14:44:54<28:06:21, 23.66s/it]                                                          34%|███▍      | 2223/6500 [14:44:54<28:06:21, 23.66s/it] 34%|███▍      | 2224/6500 [14:45:12<26:05:32, 21.97s/it]                                                          34%|███▍      | 2224/6500 [14:45:12<26:05:32, 21.97s/it] 34%|███▍      | 2225/6500 [14:45:30<24:41:16, 20.79s/it]                                                          34%|███▍      | 2225/6500 [14:45:30<24:41:16, 20.79s/it] 34%|███▍      | 2226/6500 [14:45:48<23:42:59, 19.98s/it]  {'loss': 0.3686, 'learning_rate': 7.377610688162586e-05, 'epoch': 0.34}
{'loss': 0.3277, 'learning_rate': 7.375483855387761e-05, 'epoch': 0.34}
{'loss': 0.3412, 'learning_rate': 7.373356467358027e-05, 'epoch': 0.34}
{'loss': 0.3467, 'learning_rate': 7.371228524570649e-05, 'epoch': 0.34}
{'loss': 0.4205, 'learning_rate': 7.369100027523022e-05, 'epoch': 0.34}
                                                        34%|███▍      | 2226/6500 [14:45:48<23:42:59, 19.98s/it] 34%|███▍      | 2227/6500 [14:46:06<23:02:21, 19.41s/it]                                                          34%|███▍      | 2227/6500 [14:46:06<23:02:21, 19.41s/it] 34%|███▍      | 2228/6500 [14:46:24<22:34:10, 19.02s/it]                                                          34%|███▍      | 2228/6500 [14:46:24<22:34:10, 19.02s/it] 34%|███▍      | 2229/6500 [14:46:42<22:14:58, 18.75s/it]                                                          34%|███▍      | 2229/6500 [14:46:42<22:14:58, 18.75s/it] 34%|███▍      | 2230/6500 [14:47:00<22:01:36, 18.57s/it]                                                          34%|███▍      | 2230/6500 [14:47:00<22:01:36, 18.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7960413098335266, 'eval_runtime': 6.3118, 'eval_samples_per_second': 3.644, 'eval_steps_per_second': 0.951, 'epoch': 0.34}
                                                          34%|███▍      | 2230/6500 [14:47:07<22:01:36, 18.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2230
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2230
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2230/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3497, 'learning_rate': 7.366970976712668e-05, 'epoch': 0.34}
{'loss': 0.3402, 'learning_rate': 7.364841372637239e-05, 'epoch': 0.34}
{'loss': 0.3494, 'learning_rate': 7.362711215794517e-05, 'epoch': 0.34}
{'loss': 0.6894, 'learning_rate': 7.360580506682414e-05, 'epoch': 0.34}
{'loss': 0.5242, 'learning_rate': 7.35844924579897e-05, 'epoch': 0.34}
 34%|███▍      | 2231/6500 [14:48:30<47:29:18, 40.05s/it]                                                          34%|███▍      | 2231/6500 [14:48:30<47:29:18, 40.05s/it] 34%|███▍      | 2232/6500 [14:48:48<39:37:24, 33.42s/it]                                                          34%|███▍      | 2232/6500 [14:48:48<39:37:24, 33.42s/it] 34%|███▍      | 2233/6500 [14:49:06<34:07:15, 28.79s/it]                                                          34%|███▍      | 2233/6500 [14:49:06<34:07:15, 28.79s/it] 34%|███▍      | 2234/6500 [14:49:24<30:15:49, 25.54s/it]                                                          34%|███▍      | 2234/6500 [14:49:24<30:15:49, 25.54s/it] 34%|███▍      | 2235/6500 [14:49:42<27:34:58, 23.28s/it]                                                          34%|███▍      | 2235/6500 [14:49:42<27:34:58, 23.28s/it] 34%|███▍      | 2236/6500 [14:50:00<25:43:15, 21.72s/it]  {'loss': 0.3489, 'learning_rate': 7.356317433642357e-05, 'epoch': 0.34}
{'loss': 0.3477, 'learning_rate': 7.354185070710869e-05, 'epoch': 0.34}
{'loss': 0.3256, 'learning_rate': 7.352052157502932e-05, 'epoch': 0.34}
{'loss': 0.3573, 'learning_rate': 7.349918694517106e-05, 'epoch': 0.34}
{'loss': 0.347, 'learning_rate': 7.347784682252072e-05, 'epoch': 0.34}
                                                        34%|███▍      | 2236/6500 [14:50:00<25:43:15, 21.72s/it] 34%|███▍      | 2237/6500 [14:50:19<24:35:52, 20.77s/it]                                                          34%|███▍      | 2237/6500 [14:50:19<24:35:52, 20.77s/it] 34%|███▍      | 2238/6500 [14:50:37<23:37:46, 19.96s/it]                                                          34%|███▍      | 2238/6500 [14:50:37<23:37:46, 19.96s/it] 34%|███▍      | 2239/6500 [14:50:55<22:57:55, 19.40s/it]                                                          34%|███▍      | 2239/6500 [14:50:55<22:57:55, 19.40s/it] 34%|███▍      | 2240/6500 [14:51:13<22:30:11, 19.02s/it]                                                          34%|███▍      | 2240/6500 [14:51:13<22:30:11, 19.02s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8092632293701172, 'eval_runtime': 5.3589, 'eval_samples_per_second': 4.292, 'eval_steps_per_second': 1.12, 'epoch': 0.34}
                                                          34%|███▍      | 2240/6500 [14:51:19<22:30:11, 19.02s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2240
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2240/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2240
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2240/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3127, 'learning_rate': 7.345650121206645e-05, 'epoch': 0.34}
{'loss': 0.3499, 'learning_rate': 7.343515011879763e-05, 'epoch': 0.34}
{'loss': 0.3212, 'learning_rate': 7.341379354770496e-05, 'epoch': 0.35}
{'loss': 0.3538, 'learning_rate': 7.33924315037804e-05, 'epoch': 0.35}
{'loss': 0.3384, 'learning_rate': 7.337106399201721e-05, 'epoch': 0.35}
 34%|███▍      | 2241/6500 [14:52:32<43:46:17, 37.00s/it]                                                          34%|███▍      | 2241/6500 [14:52:32<43:46:17, 37.00s/it] 34%|███▍      | 2242/6500 [14:52:50<37:00:25, 31.29s/it]                                                          34%|███▍      | 2242/6500 [14:52:50<37:00:25, 31.29s/it] 35%|███▍      | 2243/6500 [14:53:08<32:16:56, 27.30s/it]                                                          35%|███▍      | 2243/6500 [14:53:08<32:16:56, 27.30s/it] 35%|███▍      | 2244/6500 [14:53:26<28:58:57, 24.52s/it]                                                          35%|███▍      | 2244/6500 [14:53:26<28:58:57, 24.52s/it] 35%|███▍      | 2245/6500 [14:53:44<26:40:59, 22.58s/it]                                                          35%|███▍      | 2245/6500 [14:53:44<26:40:59, 22.58s/it] 35%|███▍      | 2246/6500 [14:54:02<25:04:31, 21.22s/it]  {'loss': 0.3456, 'learning_rate': 7.334969101740991e-05, 'epoch': 0.35}
{'loss': 0.3345, 'learning_rate': 7.33283125849543e-05, 'epoch': 0.35}
{'loss': 0.3427, 'learning_rate': 7.330692869964746e-05, 'epoch': 0.35}
{'loss': 0.3624, 'learning_rate': 7.328553936648774e-05, 'epoch': 0.35}
{'loss': 0.3331, 'learning_rate': 7.326414459047477e-05, 'epoch': 0.35}
                                                        35%|███▍      | 2246/6500 [14:54:02<25:04:31, 21.22s/it] 35%|███▍      | 2247/6500 [14:54:21<24:06:27, 20.41s/it]                                                          35%|███▍      | 2247/6500 [14:54:21<24:06:27, 20.41s/it] 35%|███▍      | 2248/6500 [14:54:39<23:17:50, 19.72s/it]                                                          35%|███▍      | 2248/6500 [14:54:39<23:17:50, 19.72s/it] 35%|███▍      | 2249/6500 [14:54:57<22:43:38, 19.25s/it]                                                          35%|███▍      | 2249/6500 [14:54:57<22:43:38, 19.25s/it] 35%|███▍      | 2250/6500 [14:55:15<22:20:00, 18.92s/it]                                                          35%|███▍      | 2250/6500 [14:55:15<22:20:00, 18.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8108540177345276, 'eval_runtime': 5.4719, 'eval_samples_per_second': 4.203, 'eval_steps_per_second': 1.097, 'epoch': 0.35}
                                                          35%|███▍      | 2250/6500 [14:55:21<22:20:00, 18.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2250the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2250
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2250/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2250/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3525, 'learning_rate': 7.324274437660947e-05, 'epoch': 0.35}
{'loss': 0.3374, 'learning_rate': 7.322133872989398e-05, 'epoch': 0.35}
{'loss': 0.3352, 'learning_rate': 7.319992765533174e-05, 'epoch': 0.35}
{'loss': 0.3393, 'learning_rate': 7.317851115792749e-05, 'epoch': 0.35}
{'loss': 0.3615, 'learning_rate': 7.315708924268716e-05, 'epoch': 0.35}
 35%|███▍      | 2251/6500 [14:56:12<35:49:35, 30.35s/it]                                                          35%|███▍      | 2251/6500 [14:56:12<35:49:35, 30.35s/it] 35%|███▍      | 2252/6500 [14:56:30<31:26:52, 26.65s/it]                                                          35%|███▍      | 2252/6500 [14:56:30<31:26:52, 26.65s/it] 35%|███▍      | 2253/6500 [14:56:48<28:22:57, 24.06s/it]                                                          35%|███▍      | 2253/6500 [14:56:48<28:22:57, 24.06s/it] 35%|███▍      | 2254/6500 [14:57:06<26:14:31, 22.25s/it]                                                          35%|███▍      | 2254/6500 [14:57:06<26:14:31, 22.25s/it] 35%|███▍      | 2255/6500 [14:57:24<24:45:03, 20.99s/it]                                                          35%|███▍      | 2255/6500 [14:57:24<24:45:03, 20.99s/it] 35%|███▍      | 2256/6500 [14:57:42<23:43:07, 20.12s/it]  {'loss': 0.3439, 'learning_rate': 7.313566191461804e-05, 'epoch': 0.35}
{'loss': 0.3434, 'learning_rate': 7.311422917872861e-05, 'epoch': 0.35}
{'loss': 0.3338, 'learning_rate': 7.309279104002864e-05, 'epoch': 0.35}
{'loss': 0.354, 'learning_rate': 7.307134750352916e-05, 'epoch': 0.35}
{'loss': 0.4059, 'learning_rate': 7.304989857424249e-05, 'epoch': 0.35}
                                                        35%|███▍      | 2256/6500 [14:57:42<23:43:07, 20.12s/it] 35%|███▍      | 2257/6500 [14:58:01<23:00:00, 19.51s/it]                                                          35%|███▍      | 2257/6500 [14:58:01<23:00:00, 19.51s/it] 35%|███▍      | 2258/6500 [14:58:19<22:29:57, 19.09s/it]                                                          35%|███▍      | 2258/6500 [14:58:19<22:29:57, 19.09s/it] 35%|███▍      | 2259/6500 [14:58:37<22:09:20, 18.81s/it]                                                          35%|███▍      | 2259/6500 [14:58:37<22:09:20, 18.81s/it] 35%|███▍      | 2260/6500 [14:58:55<21:55:01, 18.61s/it]                                                          35%|███▍      | 2260/6500 [14:58:55<21:55:01, 18.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7940714359283447, 'eval_runtime': 5.3364, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.35}
                                                          35%|███▍      | 2260/6500 [14:59:00<21:55:01, 18.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2260

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2260
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2260

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2260/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2260/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3393, 'learning_rate': 7.302844425718218e-05, 'epoch': 0.35}
{'loss': 0.3162, 'learning_rate': 7.300698455736302e-05, 'epoch': 0.35}
{'loss': 0.3632, 'learning_rate': 7.298551947980113e-05, 'epoch': 0.35}
{'loss': 0.8649, 'learning_rate': 7.296404902951381e-05, 'epoch': 0.35}
{'loss': 0.3567, 'learning_rate': 7.294257321151964e-05, 'epoch': 0.35}
 35%|███▍      | 2261/6500 [14:59:56<36:58:33, 31.40s/it]                                                          35%|███▍      | 2261/6500 [14:59:56<36:58:33, 31.40s/it] 35%|███▍      | 2262/6500 [15:00:14<32:15:30, 27.40s/it]                                                          35%|███▍      | 2262/6500 [15:00:14<32:15:30, 27.40s/it] 35%|███▍      | 2263/6500 [15:00:33<29:00:39, 24.65s/it]                                                          35%|███▍      | 2263/6500 [15:00:33<29:00:39, 24.65s/it] 35%|███▍      | 2264/6500 [15:00:51<26:39:42, 22.66s/it]                                                          35%|███▍      | 2264/6500 [15:00:51<26:39:42, 22.66s/it] 35%|███▍      | 2265/6500 [15:01:09<25:01:56, 21.28s/it]                                                          35%|███▍      | 2265/6500 [15:01:09<25:01:56, 21.28s/it] 35%|███▍      | 2266/6500 [15:01:27<23:53:49, 20.32s/it]  {'loss': 0.343, 'learning_rate': 7.292109203083848e-05, 'epoch': 0.35}
{'loss': 0.3418, 'learning_rate': 7.289960549249141e-05, 'epoch': 0.35}
{'loss': 0.3214, 'learning_rate': 7.287811360150081e-05, 'epoch': 0.35}
{'loss': 0.3684, 'learning_rate': 7.285661636289025e-05, 'epoch': 0.35}
{'loss': 0.3304, 'learning_rate': 7.283511378168458e-05, 'epoch': 0.35}
                                                        35%|███▍      | 2266/6500 [15:01:27<23:53:49, 20.32s/it] 35%|███▍      | 2267/6500 [15:01:45<23:05:55, 19.64s/it]                                                          35%|███▍      | 2267/6500 [15:01:45<23:05:55, 19.64s/it] 35%|███▍      | 2268/6500 [15:02:03<22:35:15, 19.21s/it]                                                          35%|███▍      | 2268/6500 [15:02:03<22:35:15, 19.21s/it] 35%|███▍      | 2269/6500 [15:02:21<22:12:01, 18.89s/it]                                                          35%|███▍      | 2269/6500 [15:02:21<22:12:01, 18.89s/it] 35%|███▍      | 2270/6500 [15:02:40<22:13:52, 18.92s/it]                                                          35%|███▍      | 2270/6500 [15:02:40<22:13:52, 18.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8088679909706116, 'eval_runtime': 5.6122, 'eval_samples_per_second': 4.098, 'eval_steps_per_second': 1.069, 'epoch': 0.35}
                                                          35%|███▍      | 2270/6500 [15:02:46<22:13:52, 18.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2270/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2270/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2270/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3274, 'learning_rate': 7.28136058629099e-05, 'epoch': 0.35}
{'loss': 0.3394, 'learning_rate': 7.279209261159357e-05, 'epoch': 0.35}
{'loss': 0.3334, 'learning_rate': 7.277057403276416e-05, 'epoch': 0.35}
{'loss': 0.3405, 'learning_rate': 7.274905013145153e-05, 'epoch': 0.35}
{'loss': 0.3288, 'learning_rate': 7.272752091268673e-05, 'epoch': 0.35}
 35%|███▍      | 2271/6500 [15:03:50<40:13:51, 34.25s/it]                                                          35%|███▍      | 2271/6500 [15:03:50<40:13:51, 34.25s/it] 35%|███▍      | 2272/6500 [15:04:08<34:29:38, 29.37s/it]                                                          35%|███▍      | 2272/6500 [15:04:08<34:29:38, 29.37s/it] 35%|███▍      | 2273/6500 [15:04:26<30:28:43, 25.96s/it]                                                          35%|███▍      | 2273/6500 [15:04:26<30:28:43, 25.96s/it] 35%|███▍      | 2274/6500 [15:04:44<27:40:16, 23.57s/it]                                                          35%|███▍      | 2274/6500 [15:04:44<27:40:16, 23.57s/it] 35%|███▌      | 2275/6500 [15:05:02<25:43:27, 21.92s/it]                                                          35%|███▌      | 2275/6500 [15:05:02<25:43:27, 21.92s/it] 35%|███▌      | 2276/6500 [15:05:20<24:21:18, 20.76s/it]  {'loss': 0.3555, 'learning_rate': 7.270598638150211e-05, 'epoch': 0.35}
{'loss': 0.3303, 'learning_rate': 7.268444654293122e-05, 'epoch': 0.35}
{'loss': 0.3607, 'learning_rate': 7.266290140200889e-05, 'epoch': 0.35}
{'loss': 0.3483, 'learning_rate': 7.264135096377115e-05, 'epoch': 0.35}
{'loss': 0.3443, 'learning_rate': 7.261979523325528e-05, 'epoch': 0.35}
                                                        35%|███▌      | 2276/6500 [15:05:20<24:21:18, 20.76s/it] 35%|███▌      | 2277/6500 [15:05:38<23:24:11, 19.95s/it]                                                          35%|███▌      | 2277/6500 [15:05:38<23:24:11, 19.95s/it] 35%|███▌      | 2278/6500 [15:05:57<22:49:05, 19.46s/it]                                                          35%|███▌      | 2278/6500 [15:05:57<22:49:05, 19.46s/it] 35%|███▌      | 2279/6500 [15:06:15<22:31:17, 19.21s/it]                                                          35%|███▌      | 2279/6500 [15:06:15<22:31:17, 19.21s/it] 35%|███▌      | 2280/6500 [15:06:33<22:07:46, 18.88s/it]                                                          35%|███▌      | 2280/6500 [15:06:33<22:07:46, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8056790828704834, 'eval_runtime': 5.5943, 'eval_samples_per_second': 4.111, 'eval_steps_per_second': 1.073, 'epoch': 0.35}
                                                          35%|███▌      | 2280/6500 [15:06:39<22:07:46, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2280
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2280
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2280the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2280
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2280/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2280/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3504, 'learning_rate': 7.25982342154998e-05, 'epoch': 0.35}
{'loss': 0.3414, 'learning_rate': 7.257666791554448e-05, 'epoch': 0.35}
{'loss': 0.3414, 'learning_rate': 7.25550963384303e-05, 'epoch': 0.35}
{'loss': 0.3439, 'learning_rate': 7.253351948919948e-05, 'epoch': 0.35}
{'loss': 0.3615, 'learning_rate': 7.25119373728955e-05, 'epoch': 0.35}
 35%|███▌      | 2281/6500 [15:07:48<41:47:14, 35.66s/it]                                                          35%|███▌      | 2281/6500 [15:07:48<41:47:14, 35.66s/it] 35%|███▌      | 2282/6500 [15:08:06<35:35:42, 30.38s/it]                                                          35%|███▌      | 2282/6500 [15:08:06<35:35:42, 30.38s/it] 35%|███▌      | 2283/6500 [15:08:24<31:14:04, 26.66s/it]                                                          35%|███▌      | 2283/6500 [15:08:24<31:14:04, 26.66s/it] 35%|███▌      | 2284/6500 [15:08:42<28:11:33, 24.07s/it]                                                          35%|███▌      | 2284/6500 [15:08:42<28:11:33, 24.07s/it] 35%|███▌      | 2285/6500 [15:09:00<26:04:16, 22.27s/it]                                                          35%|███▌      | 2285/6500 [15:09:00<26:04:16, 22.27s/it] 35%|███▌      | 2286/6500 [15:09:18<24:37:15, 21.03s/it]  {'loss': 0.3261, 'learning_rate': 7.249034999456301e-05, 'epoch': 0.35}
{'loss': 0.3383, 'learning_rate': 7.246875735924797e-05, 'epoch': 0.35}
{'loss': 0.3239, 'learning_rate': 7.244715947199749e-05, 'epoch': 0.35}
{'loss': 0.3668, 'learning_rate': 7.242555633785999e-05, 'epoch': 0.35}
{'loss': 0.391, 'learning_rate': 7.240394796188505e-05, 'epoch': 0.35}
                                                        35%|███▌      | 2286/6500 [15:09:18<24:37:15, 21.03s/it] 35%|███▌      | 2287/6500 [15:09:37<23:35:47, 20.16s/it]                                                          35%|███▌      | 2287/6500 [15:09:37<23:35:47, 20.16s/it] 35%|███▌      | 2288/6500 [15:09:55<22:52:24, 19.55s/it]                                                          35%|███▌      | 2288/6500 [15:09:55<22:52:24, 19.55s/it] 35%|███▌      | 2289/6500 [15:10:13<22:22:08, 19.12s/it]                                                          35%|███▌      | 2289/6500 [15:10:13<22:22:08, 19.12s/it] 35%|███▌      | 2290/6500 [15:10:31<22:01:22, 18.83s/it]                                                          35%|███▌      | 2290/6500 [15:10:31<22:01:22, 18.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7976159453392029, 'eval_runtime': 5.3193, 'eval_samples_per_second': 4.324, 'eval_steps_per_second': 1.128, 'epoch': 0.35}
                                                          35%|███▌      | 2290/6500 [15:10:36<22:01:22, 18.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2290/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2290

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2290

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2290/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3451, 'learning_rate': 7.238233434912347e-05, 'epoch': 0.35}
{'loss': 0.3493, 'learning_rate': 7.236071550462733e-05, 'epoch': 0.35}
{'loss': 0.3401, 'learning_rate': 7.23390914334499e-05, 'epoch': 0.35}
{'loss': 0.88, 'learning_rate': 7.231746214064566e-05, 'epoch': 0.35}
{'loss': 0.3448, 'learning_rate': 7.229582763127036e-05, 'epoch': 0.35}
 35%|███▌      | 2291/6500 [15:11:24<34:06:00, 29.17s/it]                                                          35%|███▌      | 2291/6500 [15:11:24<34:06:00, 29.17s/it] 35%|███▌      | 2292/6500 [15:11:42<30:11:32, 25.83s/it]                                                          35%|███▌      | 2292/6500 [15:11:42<30:11:32, 25.83s/it] 35%|███▌      | 2293/6500 [15:12:00<27:26:49, 23.49s/it]                                                          35%|███▌      | 2293/6500 [15:12:00<27:26:49, 23.49s/it] 35%|███▌      | 2294/6500 [15:12:18<25:30:58, 21.84s/it]                                                          35%|███▌      | 2294/6500 [15:12:18<25:30:58, 21.84s/it] 35%|███▌      | 2295/6500 [15:12:37<24:24:33, 20.90s/it]                                                          35%|███▌      | 2295/6500 [15:12:37<24:24:33, 20.90s/it] 35%|███▌      | 2296/6500 [15:12:55<23:24:59, 20.05s/it]  {'loss': 0.3435, 'learning_rate': 7.227418791038089e-05, 'epoch': 0.35}
{'loss': 0.3129, 'learning_rate': 7.225254298303543e-05, 'epoch': 0.35}
{'loss': 0.3527, 'learning_rate': 7.223089285429335e-05, 'epoch': 0.35}
{'loss': 0.3318, 'learning_rate': 7.220923752921524e-05, 'epoch': 0.35}
{'loss': 0.3125, 'learning_rate': 7.218757701286287e-05, 'epoch': 0.35}
                                                        35%|███▌      | 2296/6500 [15:12:55<23:24:59, 20.05s/it] 35%|███▌      | 2297/6500 [15:13:13<22:43:19, 19.46s/it]                                                          35%|███▌      | 2297/6500 [15:13:13<22:43:19, 19.46s/it] 35%|███▌      | 2298/6500 [15:13:31<22:15:05, 19.06s/it]                                                          35%|███▌      | 2298/6500 [15:13:31<22:15:05, 19.06s/it] 35%|███▌      | 2299/6500 [15:13:49<21:55:50, 18.79s/it]                                                          35%|███▌      | 2299/6500 [15:13:49<21:55:50, 18.79s/it] 35%|███▌      | 2300/6500 [15:14:08<21:41:47, 18.60s/it]                                                          35%|███▌      | 2300/6500 [15:14:08<21:41:47, 18.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8108226656913757, 'eval_runtime': 5.3222, 'eval_samples_per_second': 4.322, 'eval_steps_per_second': 1.127, 'epoch': 0.35}
                                                          35%|███▌      | 2300/6500 [15:14:13<21:41:47, 18.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2300 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2300
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2300

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2300/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2300/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3316, 'learning_rate': 7.21659113102993e-05, 'epoch': 0.35}
{'loss': 0.325, 'learning_rate': 7.214424042658872e-05, 'epoch': 0.35}
{'loss': 0.3329, 'learning_rate': 7.212256436679658e-05, 'epoch': 0.35}
{'loss': 0.337, 'learning_rate': 7.210088313598953e-05, 'epoch': 0.35}
{'loss': 0.3284, 'learning_rate': 7.207919673923542e-05, 'epoch': 0.35}
 35%|███▌      | 2301/6500 [15:15:17<39:31:04, 33.88s/it]                                                          35%|███▌      | 2301/6500 [15:15:17<39:31:04, 33.88s/it] 35%|███▌      | 2302/6500 [15:15:35<33:56:51, 29.11s/it]                                                          35%|███▌      | 2302/6500 [15:15:35<33:56:51, 29.11s/it] 35%|███▌      | 2303/6500 [15:15:53<30:03:14, 25.78s/it]                                                          35%|███▌      | 2303/6500 [15:15:53<30:03:14, 25.78s/it] 35%|███▌      | 2304/6500 [15:16:11<27:19:39, 23.45s/it]                                                          35%|███▌      | 2304/6500 [15:16:11<27:19:39, 23.45s/it] 35%|███▌      | 2305/6500 [15:16:29<25:25:49, 21.82s/it]                                                          35%|███▌      | 2305/6500 [15:16:29<25:25:49, 21.82s/it] 35%|███▌      | 2306/6500 [15:16:47<24:06:34, 20.69s/it]  {'loss': 0.3352, 'learning_rate': 7.20575051816033e-05, 'epoch': 0.35}
{'loss': 0.3359, 'learning_rate': 7.203580846816348e-05, 'epoch': 0.35}
{'loss': 0.3542, 'learning_rate': 7.20141066039874e-05, 'epoch': 0.36}
{'loss': 0.3343, 'learning_rate': 7.199239959414775e-05, 'epoch': 0.36}
{'loss': 0.3457, 'learning_rate': 7.197068744371841e-05, 'epoch': 0.36}
                                                        35%|███▌      | 2306/6500 [15:16:47<24:06:34, 20.69s/it] 35%|███▌      | 2307/6500 [15:17:05<23:11:14, 19.91s/it]                                                          35%|███▌      | 2307/6500 [15:17:05<23:11:14, 19.91s/it] 36%|███▌      | 2308/6500 [15:17:23<22:32:56, 19.36s/it]                                                          36%|███▌      | 2308/6500 [15:17:23<22:32:56, 19.36s/it] 36%|███▌      | 2309/6500 [15:17:41<22:06:24, 18.99s/it]                                                          36%|███▌      | 2309/6500 [15:17:41<22:06:24, 18.99s/it] 36%|███▌      | 2310/6500 [15:18:00<21:48:17, 18.73s/it]                                                          36%|███▌      | 2310/6500 [15:18:00<21:48:17, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8139998316764832, 'eval_runtime': 5.3142, 'eval_samples_per_second': 4.328, 'eval_steps_per_second': 1.129, 'epoch': 0.36}
                                                          36%|███▌      | 2310/6500 [15:18:05<21:48:17, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2310
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2310
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2310
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2310
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2310/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3412, 'learning_rate': 7.194897015777447e-05, 'epoch': 0.36}
{'loss': 0.3257, 'learning_rate': 7.19272477413922e-05, 'epoch': 0.36}
{'loss': 0.3424, 'learning_rate': 7.190552019964909e-05, 'epoch': 0.36}
{'loss': 0.3473, 'learning_rate': 7.188378753762382e-05, 'epoch': 0.36}
{'loss': 0.3445, 'learning_rate': 7.186204976039628e-05, 'epoch': 0.36}
 36%|███▌      | 2311/6500 [15:19:30<46:41:49, 40.13s/it]                                                          36%|███▌      | 2311/6500 [15:19:30<46:41:49, 40.13s/it] 36%|███▌      | 2312/6500 [15:19:48<38:57:01, 33.48s/it]                                                          36%|███▌      | 2312/6500 [15:19:48<38:57:01, 33.48s/it] 36%|███▌      | 2313/6500 [15:20:06<33:31:49, 28.83s/it]                                                          36%|███▌      | 2313/6500 [15:20:06<33:31:49, 28.83s/it] 36%|███▌      | 2314/6500 [15:20:24<29:45:02, 25.59s/it]                                                          36%|███▌      | 2314/6500 [15:20:24<29:45:02, 25.59s/it] 36%|███▌      | 2315/6500 [15:20:42<27:06:58, 23.33s/it]                                                          36%|███▌      | 2315/6500 [15:20:42<27:06:58, 23.33s/it] 36%|███▌      | 2316/6500 [15:21:00<25:16:32, 21.75s/it]  {'loss': 0.3269, 'learning_rate': 7.184030687304752e-05, 'epoch': 0.36}
{'loss': 0.334, 'learning_rate': 7.181855888065982e-05, 'epoch': 0.36}
{'loss': 0.319, 'learning_rate': 7.179680578831666e-05, 'epoch': 0.36}
{'loss': 0.4272, 'learning_rate': 7.177504760110265e-05, 'epoch': 0.36}
{'loss': 0.3275, 'learning_rate': 7.175328432410366e-05, 'epoch': 0.36}
                                                        36%|███▌      | 2316/6500 [15:21:00<25:16:32, 21.75s/it] 36%|███▌      | 2317/6500 [15:21:18<23:59:50, 20.65s/it]                                                          36%|███▌      | 2317/6500 [15:21:18<23:59:50, 20.65s/it] 36%|███▌      | 2318/6500 [15:21:36<23:09:45, 19.94s/it]                                                          36%|███▌      | 2318/6500 [15:21:36<23:09:45, 19.94s/it] 36%|███▌      | 2319/6500 [15:21:54<22:33:21, 19.42s/it]                                                          36%|███▌      | 2319/6500 [15:21:54<22:33:21, 19.42s/it] 36%|███▌      | 2320/6500 [15:22:12<22:06:16, 19.04s/it]                                                          36%|███▌      | 2320/6500 [15:22:12<22:06:16, 19.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7971424460411072, 'eval_runtime': 5.3258, 'eval_samples_per_second': 4.319, 'eval_steps_per_second': 1.127, 'epoch': 0.36}
                                                          36%|███▌      | 2320/6500 [15:22:18<22:06:16, 19.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2320
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2320/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2320


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2320/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2320/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3172, 'learning_rate': 7.173151596240673e-05, 'epoch': 0.36}
{'loss': 0.3446, 'learning_rate': 7.170974252110008e-05, 'epoch': 0.36}
{'loss': 0.8567, 'learning_rate': 7.168796400527312e-05, 'epoch': 0.36}
{'loss': 0.3481, 'learning_rate': 7.16661804200164e-05, 'epoch': 0.36}
{'loss': 0.3373, 'learning_rate': 7.164439177042178e-05, 'epoch': 0.36}
 36%|███▌      | 2321/6500 [15:23:24<40:23:38, 34.80s/it]                                                          36%|███▌      | 2321/6500 [15:23:24<40:23:38, 34.80s/it] 36%|███▌      | 2322/6500 [15:23:42<34:31:39, 29.75s/it]                                                          36%|███▌      | 2322/6500 [15:23:42<34:31:39, 29.75s/it] 36%|███▌      | 2323/6500 [15:24:00<30:24:50, 26.21s/it]                                                          36%|███▌      | 2323/6500 [15:24:00<30:24:50, 26.21s/it] 36%|███▌      | 2324/6500 [15:24:18<27:32:50, 23.75s/it]                                                          36%|███▌      | 2324/6500 [15:24:18<27:32:50, 23.75s/it] 36%|███▌      | 2325/6500 [15:24:36<25:32:54, 22.03s/it]                                                          36%|███▌      | 2325/6500 [15:24:36<25:32:54, 22.03s/it] 36%|███▌      | 2326/6500 [15:24:54<24:09:36, 20.84s/it]  {'loss': 0.3359, 'learning_rate': 7.162259806158215e-05, 'epoch': 0.36}
{'loss': 0.3204, 'learning_rate': 7.16007992985917e-05, 'epoch': 0.36}
{'loss': 0.3474, 'learning_rate': 7.157899548654576e-05, 'epoch': 0.36}
{'loss': 0.3259, 'learning_rate': 7.155718663054083e-05, 'epoch': 0.36}
{'loss': 0.3094, 'learning_rate': 7.153537273567459e-05, 'epoch': 0.36}
                                                        36%|███▌      | 2326/6500 [15:24:54<24:09:36, 20.84s/it] 36%|███▌      | 2327/6500 [15:25:12<23:11:04, 20.00s/it]                                                          36%|███▌      | 2327/6500 [15:25:12<23:11:04, 20.00s/it] 36%|███▌      | 2328/6500 [15:25:31<22:38:53, 19.54s/it]                                                          36%|███▌      | 2328/6500 [15:25:31<22:38:53, 19.54s/it] 36%|███▌      | 2329/6500 [15:25:49<22:13:50, 19.19s/it]                                                          36%|███▌      | 2329/6500 [15:25:49<22:13:50, 19.19s/it] 36%|███▌      | 2330/6500 [15:26:07<21:50:59, 18.86s/it]                                                          36%|███▌      | 2330/6500 [15:26:07<21:50:59, 18.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.81068354845047, 'eval_runtime': 7.1058, 'eval_samples_per_second': 3.237, 'eval_steps_per_second': 0.844, 'epoch': 0.36}
                                                          36%|███▌      | 2330/6500 [15:26:14<21:50:59, 18.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2330
 I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2330/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2330/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3328, 'learning_rate': 7.15135538070459e-05, 'epoch': 0.36}
{'loss': 0.3188, 'learning_rate': 7.149172984975482e-05, 'epoch': 0.36}
{'loss': 0.3366, 'learning_rate': 7.146990086890258e-05, 'epoch': 0.36}
{'loss': 0.324, 'learning_rate': 7.144806686959151e-05, 'epoch': 0.36}
{'loss': 0.341, 'learning_rate': 7.142622785692524e-05, 'epoch': 0.36}
 36%|███▌      | 2331/6500 [15:27:20<40:32:31, 35.01s/it]                                                          36%|███▌      | 2331/6500 [15:27:20<40:32:31, 35.01s/it] 36%|███▌      | 2332/6500 [15:27:38<34:38:26, 29.92s/it]                                                          36%|███▌      | 2332/6500 [15:27:38<34:38:26, 29.92s/it] 36%|███▌      | 2333/6500 [15:27:56<30:30:37, 26.36s/it]                                                          36%|███▌      | 2333/6500 [15:27:56<30:30:37, 26.36s/it] 36%|███▌      | 2334/6500 [15:28:14<27:37:13, 23.87s/it]                                                          36%|███▌      | 2334/6500 [15:28:14<27:37:13, 23.87s/it] 36%|███▌      | 2335/6500 [15:28:32<25:36:36, 22.14s/it]                                                          36%|███▌      | 2335/6500 [15:28:32<25:36:36, 22.14s/it] 36%|███▌      | 2336/6500 [15:28:50<24:12:14, 20.93s/it]  {'loss': 0.322, 'learning_rate': 7.140438383600848e-05, 'epoch': 0.36}
{'loss': 0.345, 'learning_rate': 7.138253481194714e-05, 'epoch': 0.36}
{'loss': 0.337, 'learning_rate': 7.136068078984829e-05, 'epoch': 0.36}
{'loss': 0.3305, 'learning_rate': 7.133882177482019e-05, 'epoch': 0.36}
{'loss': 0.3456, 'learning_rate': 7.131695777197224e-05, 'epoch': 0.36}
                                                        36%|███▌      | 2336/6500 [15:28:50<24:12:14, 20.93s/it] 36%|███▌      | 2337/6500 [15:29:08<23:13:29, 20.08s/it]                                                          36%|███▌      | 2337/6500 [15:29:08<23:13:29, 20.08s/it] 36%|███▌      | 2338/6500 [15:29:26<22:32:34, 19.50s/it]                                                          36%|███▌      | 2338/6500 [15:29:26<22:32:34, 19.50s/it] 36%|███▌      | 2339/6500 [15:29:44<22:04:34, 19.10s/it]                                                          36%|███▌      | 2339/6500 [15:29:44<22:04:34, 19.10s/it] 36%|███▌      | 2340/6500 [15:30:03<21:45:11, 18.82s/it]                                                          36%|███▌      | 2340/6500 [15:30:03<21:45:11, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8176093101501465, 'eval_runtime': 5.4447, 'eval_samples_per_second': 4.224, 'eval_steps_per_second': 1.102, 'epoch': 0.36}
                                                          36%|███▌      | 2340/6500 [15:30:08<21:45:11, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2340
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2340
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2340/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3367, 'learning_rate': 7.129508878641502e-05, 'epoch': 0.36}
{'loss': 0.3379, 'learning_rate': 7.127321482326026e-05, 'epoch': 0.36}
{'loss': 0.3242, 'learning_rate': 7.125133588762088e-05, 'epoch': 0.36}
{'loss': 0.36, 'learning_rate': 7.122945198461097e-05, 'epoch': 0.36}
{'loss': 0.3241, 'learning_rate': 7.120756311934571e-05, 'epoch': 0.36}
 36%|███▌      | 2341/6500 [15:31:13<39:29:40, 34.19s/it]                                                          36%|███▌      | 2341/6500 [15:31:13<39:29:40, 34.19s/it] 36%|███▌      | 2342/6500 [15:31:31<33:53:10, 29.34s/it]                                                          36%|███▌      | 2342/6500 [15:31:31<33:53:10, 29.34s/it] 36%|███▌      | 2343/6500 [15:31:49<29:58:03, 25.95s/it]                                                          36%|███▌      | 2343/6500 [15:31:49<29:58:03, 25.95s/it] 36%|███▌      | 2344/6500 [15:32:07<27:20:40, 23.69s/it]                                                          36%|███▌      | 2344/6500 [15:32:07<27:20:40, 23.69s/it] 36%|███▌      | 2345/6500 [15:32:25<25:24:26, 22.01s/it]                                                          36%|███▌      | 2345/6500 [15:32:25<25:24:26, 22.01s/it] 36%|███▌      | 2346/6500 [15:32:43<24:03:35, 20.85s/it]  {'loss': 0.3192, 'learning_rate': 7.118566929694152e-05, 'epoch': 0.36}
{'loss': 0.322, 'learning_rate': 7.116377052251595e-05, 'epoch': 0.36}
{'loss': 0.345, 'learning_rate': 7.11418668011877e-05, 'epoch': 0.36}
{'loss': 0.3983, 'learning_rate': 7.111995813807662e-05, 'epoch': 0.36}
{'loss': 0.3261, 'learning_rate': 7.109804453830375e-05, 'epoch': 0.36}
                                                        36%|███▌      | 2346/6500 [15:32:43<24:03:35, 20.85s/it] 36%|███▌      | 2347/6500 [15:33:02<23:07:14, 20.04s/it]                                                          36%|███▌      | 2347/6500 [15:33:02<23:07:14, 20.04s/it] 36%|███▌      | 2348/6500 [15:33:20<22:28:00, 19.48s/it]                                                          36%|███▌      | 2348/6500 [15:33:20<22:28:00, 19.48s/it] 36%|███▌      | 2349/6500 [15:33:38<22:01:07, 19.10s/it]                                                          36%|███▌      | 2349/6500 [15:33:38<22:01:07, 19.10s/it] 36%|███▌      | 2350/6500 [15:33:56<21:46:32, 18.89s/it]                                                          36%|███▌      | 2350/6500 [15:33:56<21:46:32, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8000202178955078, 'eval_runtime': 5.4827, 'eval_samples_per_second': 4.195, 'eval_steps_per_second': 1.094, 'epoch': 0.36}
                                                          36%|███▌      | 2350/6500 [15:34:02<21:46:32, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2350/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2350/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3191, 'learning_rate': 7.107612600699124e-05, 'epoch': 0.36}
{'loss': 0.3376, 'learning_rate': 7.105420254926241e-05, 'epoch': 0.36}
{'loss': 0.8651, 'learning_rate': 7.103227417024176e-05, 'epoch': 0.36}
{'loss': 0.3439, 'learning_rate': 7.10103408750549e-05, 'epoch': 0.36}
{'loss': 0.3295, 'learning_rate': 7.09884026688286e-05, 'epoch': 0.36}
 36%|███▌      | 2351/6500 [15:35:00<37:11:27, 32.27s/it]                                                          36%|███▌      | 2351/6500 [15:35:00<37:11:27, 32.27s/it] 36%|███▌      | 2352/6500 [15:35:18<32:16:36, 28.01s/it]                                                          36%|███▌      | 2352/6500 [15:35:18<32:16:36, 28.01s/it] 36%|███▌      | 2353/6500 [15:35:36<28:56:12, 25.12s/it]                                                          36%|███▌      | 2353/6500 [15:35:36<28:56:12, 25.12s/it] 36%|███▌      | 2354/6500 [15:35:54<26:28:49, 22.99s/it]                                                          36%|███▌      | 2354/6500 [15:35:54<26:28:49, 22.99s/it] 36%|███▌      | 2355/6500 [15:36:12<24:46:03, 21.51s/it]                                                          36%|███▌      | 2355/6500 [15:36:12<24:46:03, 21.51s/it] 36%|███▌      | 2356/6500 [15:36:30<23:34:10, 20.48s/it]  {'loss': 0.3247, 'learning_rate': 7.09664595566908e-05, 'epoch': 0.36}
{'loss': 0.3173, 'learning_rate': 7.094451154377054e-05, 'epoch': 0.36}
{'loss': 0.3507, 'learning_rate': 7.092255863519806e-05, 'epoch': 0.36}
{'loss': 0.314, 'learning_rate': 7.090060083610471e-05, 'epoch': 0.36}
{'loss': 0.3296, 'learning_rate': 7.087863815162298e-05, 'epoch': 0.36}
                                                        36%|███▌      | 2356/6500 [15:36:30<23:34:10, 20.48s/it] 36%|███▋      | 2357/6500 [15:36:49<22:44:26, 19.76s/it]                                                          36%|███▋      | 2357/6500 [15:36:49<22:44:26, 19.76s/it] 36%|███▋      | 2358/6500 [15:37:07<22:09:57, 19.27s/it]                                                          36%|███▋      | 2358/6500 [15:37:07<22:09:57, 19.27s/it] 36%|███▋      | 2359/6500 [15:37:25<21:45:59, 18.92s/it]                                                          36%|███▋      | 2359/6500 [15:37:25<21:45:59, 18.92s/it] 36%|███▋      | 2360/6500 [15:37:43<21:39:55, 18.84s/it]                                                          36%|███▋      | 2360/6500 [15:37:43<21:39:55, 18.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8141468167304993, 'eval_runtime': 5.3292, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.36}
                                                          36%|███▋      | 2360/6500 [15:37:49<21:39:55, 18.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2360
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2360
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2360

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2360/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2360/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3272, 'learning_rate': 7.085667058688656e-05, 'epoch': 0.36}
{'loss': 0.3357, 'learning_rate': 7.083469814703017e-05, 'epoch': 0.36}
{'loss': 0.3316, 'learning_rate': 7.081272083718977e-05, 'epoch': 0.36}
{'loss': 0.3344, 'learning_rate': 7.079073866250241e-05, 'epoch': 0.36}
{'loss': 0.3463, 'learning_rate': 7.07687516281063e-05, 'epoch': 0.36}
 36%|███▋      | 2361/6500 [15:38:40<34:41:19, 30.17s/it]                                                          36%|███▋      | 2361/6500 [15:38:40<34:41:19, 30.17s/it] 36%|███▋      | 2362/6500 [15:38:58<30:29:14, 26.52s/it]                                                          36%|███▋      | 2362/6500 [15:38:58<30:29:14, 26.52s/it] 36%|███▋      | 2363/6500 [15:39:16<27:32:16, 23.96s/it]                                                          36%|███▋      | 2363/6500 [15:39:16<27:32:16, 23.96s/it] 36%|███▋      | 2364/6500 [15:39:34<25:28:52, 22.18s/it]                                                          36%|███▋      | 2364/6500 [15:39:34<25:28:52, 22.18s/it] 36%|███▋      | 2365/6500 [15:39:52<24:02:46, 20.94s/it]                                                          36%|███▋      | 2365/6500 [15:39:52<24:02:46, 20.94s/it] 36%|███▋      | 2366/6500 [15:40:10<23:02:45, 20.07s/it]  {'loss': 0.3263, 'learning_rate': 7.07467597391408e-05, 'epoch': 0.36}
{'loss': 0.3534, 'learning_rate': 7.072476300074633e-05, 'epoch': 0.36}
{'loss': 0.3339, 'learning_rate': 7.070276141806452e-05, 'epoch': 0.36}
{'loss': 0.3314, 'learning_rate': 7.06807549962381e-05, 'epoch': 0.36}
{'loss': 0.3343, 'learning_rate': 7.065874374041095e-05, 'epoch': 0.36}
                                                        36%|███▋      | 2366/6500 [15:40:10<23:02:45, 20.07s/it] 36%|███▋      | 2367/6500 [15:40:28<22:20:51, 19.47s/it]                                                          36%|███▋      | 2367/6500 [15:40:28<22:20:51, 19.47s/it] 36%|███▋      | 2368/6500 [15:40:46<21:51:47, 19.05s/it]                                                          36%|███▋      | 2368/6500 [15:40:46<21:51:47, 19.05s/it] 36%|███▋      | 2369/6500 [15:41:04<21:32:00, 18.77s/it]                                                          36%|███▋      | 2369/6500 [15:41:04<21:32:00, 18.77s/it] 36%|███▋      | 2370/6500 [15:41:23<21:22:20, 18.63s/it]                                                          36%|███▋      | 2370/6500 [15:41:23<21:22:20, 18.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8155825138092041, 'eval_runtime': 5.4451, 'eval_samples_per_second': 4.224, 'eval_steps_per_second': 1.102, 'epoch': 0.36}
                                                          36%|███▋      | 2370/6500 [15:41:28<21:22:20, 18.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2370
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2370
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2370
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2370
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2370/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2370/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2370/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3433, 'learning_rate': 7.063672765572806e-05, 'epoch': 0.36}
{'loss': 0.3352, 'learning_rate': 7.061470674733554e-05, 'epoch': 0.36}
{'loss': 0.3328, 'learning_rate': 7.059268102038066e-05, 'epoch': 0.37}
{'loss': 0.3543, 'learning_rate': 7.057065048001181e-05, 'epoch': 0.37}
{'loss': 0.3177, 'learning_rate': 7.054861513137847e-05, 'epoch': 0.37}
 36%|███▋      | 2371/6500 [15:42:15<32:58:19, 28.75s/it]                                                          36%|███▋      | 2371/6500 [15:42:15<32:58:19, 28.75s/it] 36%|███▋      | 2372/6500 [15:42:33<29:16:19, 25.53s/it]                                                          36%|███▋      | 2372/6500 [15:42:33<29:16:19, 25.53s/it] 37%|███▋      | 2373/6500 [15:42:51<26:40:39, 23.27s/it]                                                          37%|███▋      | 2373/6500 [15:42:51<26:40:39, 23.27s/it] 37%|███▋      | 2374/6500 [15:43:09<24:51:51, 21.69s/it]                                                          37%|███▋      | 2374/6500 [15:43:09<24:51:51, 21.69s/it] 37%|███▋      | 2375/6500 [15:43:27<23:36:18, 20.60s/it]                                                          37%|███▋      | 2375/6500 [15:43:27<23:36:18, 20.60s/it] 37%|███▋      | 2376/6500 [15:43:45<22:49:09, 19.92s/it]  {'loss': 0.3312, 'learning_rate': 7.052657497963129e-05, 'epoch': 0.37}
{'loss': 0.3105, 'learning_rate': 7.050453002992201e-05, 'epoch': 0.37}
{'loss': 0.3649, 'learning_rate': 7.04824802874035e-05, 'epoch': 0.37}
{'loss': 0.376, 'learning_rate': 7.046042575722976e-05, 'epoch': 0.37}
{'loss': 0.3213, 'learning_rate': 7.04383664445559e-05, 'epoch': 0.37}
                                                        37%|███▋      | 2376/6500 [15:43:45<22:49:09, 19.92s/it] 37%|███▋      | 2377/6500 [15:44:04<22:11:07, 19.37s/it]                                                          37%|███▋      | 2377/6500 [15:44:04<22:11:07, 19.37s/it] 37%|███▋      | 2378/6500 [15:44:22<21:44:22, 18.99s/it]                                                          37%|███▋      | 2378/6500 [15:44:22<21:44:22, 18.99s/it] 37%|███▋      | 2379/6500 [15:44:40<21:26:10, 18.73s/it]                                                          37%|███▋      | 2379/6500 [15:44:40<21:26:10, 18.73s/it] 37%|███▋      | 2380/6500 [15:44:58<21:13:41, 18.55s/it]                                                          37%|███▋      | 2380/6500 [15:44:58<21:13:41, 18.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8041245937347412, 'eval_runtime': 5.3217, 'eval_samples_per_second': 4.322, 'eval_steps_per_second': 1.127, 'epoch': 0.37}
                                                          37%|███▋      | 2380/6500 [15:45:03<21:13:41, 18.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2380
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2380
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2380

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2380/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3386, 'learning_rate': 7.041630235453816e-05, 'epoch': 0.37}
{'loss': 0.3293, 'learning_rate': 7.039423349233387e-05, 'epoch': 0.37}
{'loss': 0.8643, 'learning_rate': 7.03721598631015e-05, 'epoch': 0.37}
{'loss': 0.3263, 'learning_rate': 7.035008147200062e-05, 'epoch': 0.37}
{'loss': 0.3302, 'learning_rate': 7.032799832419193e-05, 'epoch': 0.37}
 37%|███▋      | 2381/6500 [15:45:50<32:49:04, 28.68s/it]                                                          37%|███▋      | 2381/6500 [15:45:50<32:49:04, 28.68s/it] 37%|███▋      | 2382/6500 [15:46:08<29:08:47, 25.48s/it]                                                          37%|███▋      | 2382/6500 [15:46:08<29:08:47, 25.48s/it] 37%|███▋      | 2383/6500 [15:46:26<26:34:19, 23.24s/it]                                                          37%|███▋      | 2383/6500 [15:46:26<26:34:19, 23.24s/it] 37%|███▋      | 2384/6500 [15:46:44<24:47:15, 21.68s/it]                                                          37%|███▋      | 2384/6500 [15:46:44<24:47:15, 21.68s/it] 37%|███▋      | 2385/6500 [15:47:02<23:32:10, 20.59s/it]                                                          37%|███▋      | 2385/6500 [15:47:02<23:32:10, 20.59s/it] 37%|███▋      | 2386/6500 [15:47:20<22:40:15, 19.84s/it]  {'loss': 0.3043, 'learning_rate': 7.030591042483723e-05, 'epoch': 0.37}
{'loss': 0.3366, 'learning_rate': 7.028381777909943e-05, 'epoch': 0.37}
{'loss': 0.3283, 'learning_rate': 7.026172039214256e-05, 'epoch': 0.37}
{'loss': 0.2913, 'learning_rate': 7.023961826913174e-05, 'epoch': 0.37}
{'loss': 0.3271, 'learning_rate': 7.02175114152332e-05, 'epoch': 0.37}
                                                        37%|███▋      | 2386/6500 [15:47:20<22:40:15, 19.84s/it] 37%|███▋      | 2387/6500 [15:47:38<22:03:44, 19.31s/it]                                                          37%|███▋      | 2387/6500 [15:47:38<22:03:44, 19.31s/it] 37%|███▋      | 2388/6500 [15:47:57<21:38:56, 18.95s/it]                                                          37%|███▋      | 2388/6500 [15:47:57<21:38:56, 18.95s/it] 37%|███▋      | 2389/6500 [15:48:15<21:21:29, 18.70s/it]                                                          37%|███▋      | 2389/6500 [15:48:15<21:21:29, 18.70s/it] 37%|███▋      | 2390/6500 [15:48:33<21:09:32, 18.53s/it]                                                          37%|███▋      | 2390/6500 [15:48:33<21:09:32, 18.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8142805695533752, 'eval_runtime': 5.325, 'eval_samples_per_second': 4.319, 'eval_steps_per_second': 1.127, 'epoch': 0.37}
                                                          37%|███▋      | 2390/6500 [15:48:38<21:09:32, 18.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2390I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2390


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2390/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3095, 'learning_rate': 7.01953998356143e-05, 'epoch': 0.37}
{'loss': 0.3272, 'learning_rate': 7.017328353544346e-05, 'epoch': 0.37}
{'loss': 0.3293, 'learning_rate': 7.015116251989027e-05, 'epoch': 0.37}
{'loss': 0.3373, 'learning_rate': 7.012903679412536e-05, 'epoch': 0.37}
{'loss': 0.3255, 'learning_rate': 7.010690636332047e-05, 'epoch': 0.37}
 37%|███▋      | 2391/6500 [15:49:49<40:53:33, 35.83s/it]                                                          37%|███▋      | 2391/6500 [15:49:49<40:53:33, 35.83s/it] 37%|███▋      | 2392/6500 [15:50:07<34:55:02, 30.60s/it]                                                          37%|███▋      | 2392/6500 [15:50:07<34:55:02, 30.60s/it] 37%|███▋      | 2393/6500 [15:50:25<30:35:09, 26.81s/it]                                                          37%|███▋      | 2393/6500 [15:50:25<30:35:09, 26.81s/it] 37%|███▋      | 2394/6500 [15:50:43<27:33:34, 24.16s/it]                                                          37%|███▋      | 2394/6500 [15:50:43<27:33:34, 24.16s/it] 37%|███▋      | 2395/6500 [15:51:01<25:26:53, 22.32s/it]                                                          37%|███▋      | 2395/6500 [15:51:01<25:26:53, 22.32s/it] 37%|███▋      | 2396/6500 [15:51:19<23:58:26, 21.03s/it]  {'loss': 0.3293, 'learning_rate': 7.008477123264848e-05, 'epoch': 0.37}
{'loss': 0.3614, 'learning_rate': 7.006263140728333e-05, 'epoch': 0.37}
{'loss': 0.3349, 'learning_rate': 7.004048689240007e-05, 'epoch': 0.37}
{'loss': 0.3353, 'learning_rate': 7.001833769317486e-05, 'epoch': 0.37}
{'loss': 0.3519, 'learning_rate': 6.999618381478492e-05, 'epoch': 0.37}
                                                        37%|███▋      | 2396/6500 [15:51:19<23:58:26, 21.03s/it] 37%|███▋      | 2397/6500 [15:51:38<23:00:02, 20.18s/it]                                                          37%|███▋      | 2397/6500 [15:51:38<23:00:02, 20.18s/it] 37%|███▋      | 2398/6500 [15:51:56<22:16:27, 19.55s/it]                                                          37%|███▋      | 2398/6500 [15:51:56<22:16:27, 19.55s/it] 37%|███▋      | 2399/6500 [15:52:14<21:46:05, 19.11s/it]                                                          37%|███▋      | 2399/6500 [15:52:14<21:46:05, 19.11s/it] 37%|███▋      | 2400/6500 [15:52:32<21:25:18, 18.81s/it]                                                          37%|███▋      | 2400/6500 [15:52:32<21:25:18, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8149675130844116, 'eval_runtime': 5.3194, 'eval_samples_per_second': 4.324, 'eval_steps_per_second': 1.128, 'epoch': 0.37}
                                                          37%|███▋      | 2400/6500 [15:52:37<21:25:18, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2400
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2400
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2400/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3242, 'learning_rate': 6.997402526240857e-05, 'epoch': 0.37}
{'loss': 0.3266, 'learning_rate': 6.995186204122528e-05, 'epoch': 0.37}
{'loss': 0.3495, 'learning_rate': 6.992969415641555e-05, 'epoch': 0.37}
{'loss': 0.3272, 'learning_rate': 6.990752161316098e-05, 'epoch': 0.37}
{'loss': 0.3108, 'learning_rate': 6.988534441664427e-05, 'epoch': 0.37}
 37%|███▋      | 2401/6500 [15:53:41<38:43:30, 34.01s/it]                                                          37%|███▋      | 2401/6500 [15:53:41<38:43:30, 34.01s/it] 37%|███▋      | 2402/6500 [15:53:59<33:13:51, 29.19s/it]                                                          37%|███▋      | 2402/6500 [15:53:59<33:13:51, 29.19s/it] 37%|███▋      | 2403/6500 [15:54:17<29:23:22, 25.82s/it]                                                          37%|███▋      | 2403/6500 [15:54:17<29:23:22, 25.82s/it] 37%|███▋      | 2404/6500 [15:54:35<26:42:19, 23.47s/it]                                                          37%|███▋      | 2404/6500 [15:54:35<26:42:19, 23.47s/it] 37%|███▋      | 2405/6500 [15:54:53<24:50:31, 21.84s/it]                                                          37%|███▋      | 2405/6500 [15:54:53<24:50:31, 21.84s/it] 37%|███▋      | 2406/6500 [15:55:11<23:32:51, 20.71s/it]  {'loss': 0.3241, 'learning_rate': 6.986316257204921e-05, 'epoch': 0.37}
{'loss': 0.3395, 'learning_rate': 6.984097608456067e-05, 'epoch': 0.37}
{'loss': 0.4058, 'learning_rate': 6.98187849593646e-05, 'epoch': 0.37}
{'loss': 0.3132, 'learning_rate': 6.979658920164806e-05, 'epoch': 0.37}
{'loss': 0.3255, 'learning_rate': 6.977438881659916e-05, 'epoch': 0.37}
                                                        37%|███▋      | 2406/6500 [15:55:11<23:32:51, 20.71s/it] 37%|███▋      | 2407/6500 [15:55:29<22:38:16, 19.91s/it]                                                          37%|███▋      | 2407/6500 [15:55:29<22:38:16, 19.91s/it] 37%|███▋      | 2408/6500 [15:55:48<22:17:28, 19.61s/it]                                                          37%|███▋      | 2408/6500 [15:55:48<22:17:28, 19.61s/it] 37%|███▋      | 2409/6500 [15:56:06<21:47:10, 19.17s/it]                                                          37%|███▋      | 2409/6500 [15:56:06<21:47:10, 19.17s/it] 37%|███▋      | 2410/6500 [15:56:25<21:25:14, 18.85s/it]                                                          37%|███▋      | 2410/6500 [15:56:25<21:25:14, 18.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8061496615409851, 'eval_runtime': 5.8278, 'eval_samples_per_second': 3.947, 'eval_steps_per_second': 1.03, 'epoch': 0.37}
                                                          37%|███▋      | 2410/6500 [15:56:30<21:25:14, 18.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2410
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2410

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2410/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2410/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3438, 'learning_rate': 6.975218380940709e-05, 'epoch': 0.37}
{'loss': 0.8469, 'learning_rate': 6.972997418526215e-05, 'epoch': 0.37}
{'loss': 0.3331, 'learning_rate': 6.970775994935571e-05, 'epoch': 0.37}
{'loss': 0.3246, 'learning_rate': 6.96855411068802e-05, 'epoch': 0.37}
{'loss': 0.3315, 'learning_rate': 6.966331766302916e-05, 'epoch': 0.37}
 37%|███▋      | 2411/6500 [15:58:07<50:01:03, 44.04s/it]                                                          37%|███▋      | 2411/6500 [15:58:07<50:01:03, 44.04s/it] 37%|███▋      | 2412/6500 [15:58:25<41:05:55, 36.19s/it]                                                          37%|███▋      | 2412/6500 [15:58:25<41:05:55, 36.19s/it] 37%|███▋      | 2413/6500 [15:58:43<34:52:23, 30.72s/it]                                                          37%|███▋      | 2413/6500 [15:58:43<34:52:23, 30.72s/it] 37%|███▋      | 2414/6500 [15:59:01<30:31:29, 26.89s/it]                                                          37%|███▋      | 2414/6500 [15:59:01<30:31:29, 26.89s/it] 37%|███▋      | 2415/6500 [15:59:21<28:03:29, 24.73s/it]                                                          37%|███▋      | 2415/6500 [15:59:21<28:03:29, 24.73s/it] 37%|███▋      | 2416/6500 [15:59:39<25:46:51, 22.73s/it]  {'loss': 0.3119, 'learning_rate': 6.964108962299717e-05, 'epoch': 0.37}
{'loss': 0.3442, 'learning_rate': 6.961885699197989e-05, 'epoch': 0.37}
{'loss': 0.3109, 'learning_rate': 6.959661977517408e-05, 'epoch': 0.37}
{'loss': 0.3116, 'learning_rate': 6.957437797777754e-05, 'epoch': 0.37}
{'loss': 0.3226, 'learning_rate': 6.955213160498917e-05, 'epoch': 0.37}
                                                        37%|███▋      | 2416/6500 [15:59:39<25:46:51, 22.73s/it] 37%|███▋      | 2417/6500 [15:59:57<24:11:19, 21.33s/it]                                                          37%|███▋      | 2417/6500 [15:59:57<24:11:19, 21.33s/it] 37%|███▋      | 2418/6500 [16:00:15<23:05:22, 20.36s/it]                                                          37%|███▋      | 2418/6500 [16:00:15<23:05:22, 20.36s/it] 37%|███▋      | 2419/6500 [16:00:33<22:18:58, 19.69s/it]                                                          37%|███▋      | 2419/6500 [16:00:33<22:18:58, 19.69s/it] 37%|███▋      | 2420/6500 [16:00:51<21:46:55, 19.22s/it]                                                          37%|███▋      | 2420/6500 [16:00:51<21:46:55, 19.22s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8165848851203918, 'eval_runtime': 5.3323, 'eval_samples_per_second': 4.313, 'eval_steps_per_second': 1.125, 'epoch': 0.37}
                                                          37%|███▋      | 2420/6500 [16:00:57<21:46:55, 19.22s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2420

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2420/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2420


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2420/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2420/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3108, 'learning_rate': 6.952988066200891e-05, 'epoch': 0.37}
{'loss': 0.3359, 'learning_rate': 6.950762515403779e-05, 'epoch': 0.37}
{'loss': 0.3198, 'learning_rate': 6.94853650862779e-05, 'epoch': 0.37}
{'loss': 0.3379, 'learning_rate': 6.946310046393239e-05, 'epoch': 0.37}
{'loss': 0.3135, 'learning_rate': 6.944083129220548e-05, 'epoch': 0.37}
 37%|███▋      | 2421/6500 [16:01:56<37:12:28, 32.84s/it]                                                          37%|███▋      | 2421/6500 [16:01:56<37:12:28, 32.84s/it] 37%|███▋      | 2422/6500 [16:02:14<32:10:39, 28.41s/it]                                                          37%|███▋      | 2422/6500 [16:02:14<32:10:39, 28.41s/it] 37%|███▋      | 2423/6500 [16:02:32<28:38:12, 25.29s/it]                                                          37%|███▋      | 2423/6500 [16:02:32<28:38:12, 25.29s/it] 37%|███▋      | 2424/6500 [16:02:50<26:09:43, 23.11s/it]                                                          37%|███▋      | 2424/6500 [16:02:50<26:09:43, 23.11s/it] 37%|███▋      | 2425/6500 [16:03:09<24:37:11, 21.75s/it]                                                          37%|███▋      | 2425/6500 [16:03:09<24:37:11, 21.75s/it] 37%|███▋      | 2426/6500 [16:03:27<23:21:34, 20.64s/it]  {'loss': 0.3409, 'learning_rate': 6.941855757630248e-05, 'epoch': 0.37}
{'loss': 0.3153, 'learning_rate': 6.939627932142969e-05, 'epoch': 0.37}
{'loss': 0.3228, 'learning_rate': 6.937399653279454e-05, 'epoch': 0.37}
{'loss': 0.3279, 'learning_rate': 6.935170921560552e-05, 'epoch': 0.37}
{'loss': 0.3171, 'learning_rate': 6.932941737507211e-05, 'epoch': 0.37}
                                                        37%|███▋      | 2426/6500 [16:03:27<23:21:34, 20.64s/it] 37%|███▋      | 2427/6500 [16:03:45<22:33:36, 19.94s/it]                                                          37%|███▋      | 2427/6500 [16:03:45<22:33:36, 19.94s/it] 37%|███▋      | 2428/6500 [16:04:03<21:55:16, 19.38s/it]                                                          37%|███▋      | 2428/6500 [16:04:03<21:55:16, 19.38s/it] 37%|███▋      | 2429/6500 [16:04:21<21:29:18, 19.00s/it]                                                          37%|███▋      | 2429/6500 [16:04:21<21:29:18, 19.00s/it] 37%|███▋      | 2430/6500 [16:04:39<21:11:06, 18.74s/it]                                                          37%|███▋      | 2430/6500 [16:04:39<21:11:06, 18.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8239739537239075, 'eval_runtime': 5.5265, 'eval_samples_per_second': 4.162, 'eval_steps_per_second': 1.086, 'epoch': 0.37}
                                                          37%|███▋      | 2430/6500 [16:04:45<21:11:06, 18.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2430 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2430
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3267, 'learning_rate': 6.930712101640492e-05, 'epoch': 0.37}
{'loss': 0.3261, 'learning_rate': 6.928482014481558e-05, 'epoch': 0.37}
{'loss': 0.3429, 'learning_rate': 6.92625147655168e-05, 'epoch': 0.37}
{'loss': 0.3214, 'learning_rate': 6.924020488372229e-05, 'epoch': 0.37}
{'loss': 0.3163, 'learning_rate': 6.921789050464688e-05, 'epoch': 0.37}
 37%|███▋      | 2431/6500 [16:06:00<42:14:05, 37.37s/it]                                                          37%|███▋      | 2431/6500 [16:06:00<42:14:05, 37.37s/it] 37%|███▋      | 2432/6500 [16:06:18<35:39:32, 31.56s/it]                                                          37%|███▋      | 2432/6500 [16:06:18<35:39:32, 31.56s/it] 37%|███▋      | 2433/6500 [16:06:36<31:03:14, 27.49s/it]                                                          37%|███▋      | 2433/6500 [16:06:36<31:03:14, 27.49s/it] 37%|███▋      | 2434/6500 [16:06:54<27:50:07, 24.65s/it]                                                          37%|███▋      | 2434/6500 [16:06:54<27:50:07, 24.65s/it] 37%|███▋      | 2435/6500 [16:07:12<25:35:11, 22.66s/it]                                                          37%|███▋      | 2435/6500 [16:07:12<25:35:11, 22.66s/it] 37%|███▋      | 2436/6500 [16:07:30<24:01:07, 21.28s/it]  {'loss': 0.3095, 'learning_rate': 6.91955716335064e-05, 'epoch': 0.37}
{'loss': 0.3474, 'learning_rate': 6.917324827551778e-05, 'epoch': 0.37}
{'loss': 0.3819, 'learning_rate': 6.915092043589895e-05, 'epoch': 0.38}
{'loss': 0.3181, 'learning_rate': 6.912858811986888e-05, 'epoch': 0.38}
{'loss': 0.3125, 'learning_rate': 6.910625133264766e-05, 'epoch': 0.38}
                                                        37%|███▋      | 2436/6500 [16:07:30<24:01:07, 21.28s/it] 37%|███▋      | 2437/6500 [16:07:48<22:55:44, 20.32s/it]                                                          37%|███▋      | 2437/6500 [16:07:48<22:55:44, 20.32s/it] 38%|███▊      | 2438/6500 [16:08:06<22:10:33, 19.65s/it]                                                          38%|███▊      | 2438/6500 [16:08:06<22:10:33, 19.65s/it] 38%|███▊      | 2439/6500 [16:08:25<21:39:06, 19.19s/it]                                                          38%|███▊      | 2439/6500 [16:08:25<21:39:06, 19.19s/it] 38%|███▊      | 2440/6500 [16:08:43<21:17:32, 18.88s/it]                                                          38%|███▊      | 2440/6500 [16:08:43<21:17:32, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.808806300163269, 'eval_runtime': 5.3287, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.38}
                                                          38%|███▊      | 2440/6500 [16:08:48<21:17:32, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2440
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2440/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2440/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.341, 'learning_rate': 6.908391007945636e-05, 'epoch': 0.38}
{'loss': 0.8436, 'learning_rate': 6.906156436551712e-05, 'epoch': 0.38}
{'loss': 0.3379, 'learning_rate': 6.903921419605308e-05, 'epoch': 0.38}
{'loss': 0.333, 'learning_rate': 6.90168595762885e-05, 'epoch': 0.38}
{'loss': 0.3052, 'learning_rate': 6.899450051144862e-05, 'epoch': 0.38}
 38%|███▊      | 2441/6500 [16:09:49<37:29:34, 33.25s/it]                                                          38%|███▊      | 2441/6500 [16:09:49<37:29:34, 33.25s/it] 38%|███▊      | 2442/6500 [16:10:07<32:19:21, 28.67s/it]                                                          38%|███▊      | 2442/6500 [16:10:07<32:19:21, 28.67s/it] 38%|███▊      | 2443/6500 [16:10:25<28:42:41, 25.48s/it]                                                          38%|███▊      | 2443/6500 [16:10:25<28:42:41, 25.48s/it] 38%|███▊      | 2444/6500 [16:10:43<26:10:59, 23.24s/it]                                                          38%|███▊      | 2444/6500 [16:10:44<26:10:59, 23.24s/it] 38%|███▊      | 2445/6500 [16:11:03<24:48:37, 22.03s/it]                                                          38%|███▊      | 2445/6500 [16:11:03<24:48:37, 22.03s/it] 38%|███▊      | 2446/6500 [16:11:21<23:29:33, 20.86s/it]  {'loss': 0.3173, 'learning_rate': 6.897213700675973e-05, 'epoch': 0.38}
{'loss': 0.3425, 'learning_rate': 6.894976906744916e-05, 'epoch': 0.38}
{'loss': 0.3141, 'learning_rate': 6.89273966987453e-05, 'epoch': 0.38}
{'loss': 0.3178, 'learning_rate': 6.890501990587754e-05, 'epoch': 0.38}
{'loss': 0.3177, 'learning_rate': 6.888263869407631e-05, 'epoch': 0.38}
                                                        38%|███▊      | 2446/6500 [16:11:21<23:29:33, 20.86s/it] 38%|███▊      | 2447/6500 [16:11:39<22:33:52, 20.04s/it]                                                          38%|███▊      | 2447/6500 [16:11:39<22:33:52, 20.04s/it] 38%|███▊      | 2448/6500 [16:11:57<21:55:08, 19.47s/it]                                                          38%|███▊      | 2448/6500 [16:11:57<21:55:08, 19.47s/it] 38%|███▊      | 2449/6500 [16:12:15<21:28:08, 19.08s/it]                                                          38%|███▊      | 2449/6500 [16:12:15<21:28:08, 19.08s/it] 38%|███▊      | 2450/6500 [16:12:33<21:09:38, 18.81s/it]                                                          38%|███▊      | 2450/6500 [16:12:33<21:09:38, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8156434893608093, 'eval_runtime': 5.6843, 'eval_samples_per_second': 4.046, 'eval_steps_per_second': 1.056, 'epoch': 0.38}
                                                          38%|███▊      | 2450/6500 [16:12:39<21:09:38, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2450

the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2450
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2450

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2450/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3223, 'learning_rate': 6.886025306857311e-05, 'epoch': 0.38}
{'loss': 0.3143, 'learning_rate': 6.883786303460041e-05, 'epoch': 0.38}
{'loss': 0.3198, 'learning_rate': 6.881546859739179e-05, 'epoch': 0.38}
{'loss': 0.3267, 'learning_rate': 6.879306976218177e-05, 'epoch': 0.38}
{'loss': 0.3184, 'learning_rate': 6.877066653420594e-05, 'epoch': 0.38}
 38%|███▊      | 2451/6500 [16:13:30<33:59:16, 30.22s/it]                                                          38%|███▊      | 2451/6500 [16:13:30<33:59:16, 30.22s/it] 38%|███▊      | 2452/6500 [16:13:48<29:53:24, 26.58s/it]                                                          38%|███▊      | 2452/6500 [16:13:48<29:53:24, 26.58s/it] 38%|███▊      | 2453/6500 [16:14:06<27:01:29, 24.04s/it]                                                          38%|███▊      | 2453/6500 [16:14:06<27:01:29, 24.04s/it] 38%|███▊      | 2454/6500 [16:14:25<25:01:25, 22.27s/it]                                                          38%|███▊      | 2454/6500 [16:14:25<25:01:25, 22.27s/it] 38%|███▊      | 2455/6500 [16:14:43<23:37:41, 21.03s/it]                                                          38%|███▊      | 2455/6500 [16:14:43<23:37:41, 21.03s/it] 38%|███▊      | 2456/6500 [16:15:01<22:38:51, 20.16s/it]  {'loss': 0.3384, 'learning_rate': 6.874825891870093e-05, 'epoch': 0.38}
{'loss': 0.3272, 'learning_rate': 6.872584692090442e-05, 'epoch': 0.38}
{'loss': 0.3281, 'learning_rate': 6.870343054605503e-05, 'epoch': 0.38}
{'loss': 0.3364, 'learning_rate': 6.868100979939249e-05, 'epoch': 0.38}
{'loss': 0.3217, 'learning_rate': 6.865858468615747e-05, 'epoch': 0.38}
                                                        38%|███▊      | 2456/6500 [16:15:01<22:38:51, 20.16s/it] 38%|███▊      | 2457/6500 [16:15:19<22:06:10, 19.68s/it]                                                          38%|███▊      | 2457/6500 [16:15:19<22:06:10, 19.68s/it] 38%|███▊      | 2458/6500 [16:15:38<21:35:23, 19.23s/it]                                                          38%|███▊      | 2458/6500 [16:15:38<21:35:23, 19.23s/it] 38%|███▊      | 2459/6500 [16:15:56<21:14:14, 18.92s/it]                                                          38%|███▊      | 2459/6500 [16:15:56<21:14:14, 18.92s/it] 38%|███▊      | 2460/6500 [16:16:14<20:59:40, 18.71s/it]                                                          38%|███▊      | 2460/6500 [16:16:14<20:59:40, 18.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8222836256027222, 'eval_runtime': 5.342, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.38}
                                                          38%|███▊      | 2460/6500 [16:16:19<20:59:40, 18.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2460
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2460
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2460

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2460/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2460/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3324, 'learning_rate': 6.863615521159172e-05, 'epoch': 0.38}
{'loss': 0.3223, 'learning_rate': 6.8613721380938e-05, 'epoch': 0.38}
{'loss': 0.3321, 'learning_rate': 6.85912831994401e-05, 'epoch': 0.38}
{'loss': 0.3126, 'learning_rate': 6.856884067234279e-05, 'epoch': 0.38}
{'loss': 0.3134, 'learning_rate': 6.854639380489184e-05, 'epoch': 0.38}
 38%|███▊      | 2461/6500 [16:17:59<49:58:51, 44.55s/it]                                                          38%|███▊      | 2461/6500 [16:17:59<49:58:51, 44.55s/it] 38%|███▊      | 2462/6500 [16:18:17<41:02:46, 36.59s/it]                                                          38%|███▊      | 2462/6500 [16:18:17<41:02:46, 36.59s/it] 38%|███▊      | 2463/6500 [16:18:35<34:47:21, 31.02s/it]                                                          38%|███▊      | 2463/6500 [16:18:35<34:47:21, 31.02s/it] 38%|███▊      | 2464/6500 [16:18:53<30:30:35, 27.21s/it]                                                          38%|███▊      | 2464/6500 [16:18:53<30:30:35, 27.21s/it] 38%|███▊      | 2465/6500 [16:19:11<27:24:36, 24.46s/it]                                                          38%|███▊      | 2465/6500 [16:19:11<27:24:36, 24.46s/it] 38%|███▊      | 2466/6500 [16:19:29<25:15:12, 22.54s/it]  {'loss': 0.3143, 'learning_rate': 6.852394260233414e-05, 'epoch': 0.38}
{'loss': 0.3539, 'learning_rate': 6.850148706991745e-05, 'epoch': 0.38}
{'loss': 0.3755, 'learning_rate': 6.847902721289068e-05, 'epoch': 0.38}
{'loss': 0.3185, 'learning_rate': 6.845656303650365e-05, 'epoch': 0.38}
{'loss': 0.3343, 'learning_rate': 6.843409454600722e-05, 'epoch': 0.38}
                                                        38%|███▊      | 2466/6500 [16:19:29<25:15:12, 22.54s/it] 38%|███▊      | 2467/6500 [16:19:47<23:44:59, 21.20s/it]                                                          38%|███▊      | 2467/6500 [16:19:47<23:44:59, 21.20s/it] 38%|███▊      | 2468/6500 [16:20:06<22:42:32, 20.28s/it]                                                          38%|███▊      | 2468/6500 [16:20:06<22:42:32, 20.28s/it] 38%|███▊      | 2469/6500 [16:20:24<21:59:21, 19.64s/it]                                                          38%|███▊      | 2469/6500 [16:20:24<21:59:21, 19.64s/it] 38%|███▊      | 2470/6500 [16:20:42<21:28:50, 19.19s/it]                                                          38%|███▊      | 2470/6500 [16:20:42<21:28:50, 19.19s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8078299760818481, 'eval_runtime': 5.3386, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.38}
                                                          38%|███▊      | 2470/6500 [16:20:47<21:28:50, 19.19s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2470
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2470/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2470
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2470/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2470/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.54, 'learning_rate': 6.841162174665326e-05, 'epoch': 0.38}
{'loss': 0.6538, 'learning_rate': 6.838914464369467e-05, 'epoch': 0.38}
{'loss': 0.3197, 'learning_rate': 6.836666324238532e-05, 'epoch': 0.38}
{'loss': 0.323, 'learning_rate': 6.834417754798011e-05, 'epoch': 0.38}
{'loss': 0.2984, 'learning_rate': 6.832168756573496e-05, 'epoch': 0.38}
 38%|███▊      | 2471/6500 [16:22:02<41:55:18, 37.46s/it]                                                          38%|███▊      | 2471/6500 [16:22:02<41:55:18, 37.46s/it] 38%|███▊      | 2472/6500 [16:22:20<35:21:56, 31.61s/it]                                                          38%|███▊      | 2472/6500 [16:22:20<35:21:56, 31.61s/it] 38%|███▊      | 2473/6500 [16:22:38<30:55:54, 27.65s/it]                                                          38%|███▊      | 2473/6500 [16:22:38<30:55:54, 27.65s/it] 38%|███▊      | 2474/6500 [16:22:56<27:41:49, 24.77s/it]                                                          38%|███▊      | 2474/6500 [16:22:56<27:41:49, 24.77s/it] 38%|███▊      | 2475/6500 [16:23:14<25:26:14, 22.75s/it]                                                          38%|███▊      | 2475/6500 [16:23:14<25:26:14, 22.75s/it] 38%|███▊      | 2476/6500 [16:23:32<23:51:58, 21.35s/it]  {'loss': 0.3312, 'learning_rate': 6.82991933009067e-05, 'epoch': 0.38}
{'loss': 0.3186, 'learning_rate': 6.827669475875328e-05, 'epoch': 0.38}
{'loss': 0.2874, 'learning_rate': 6.825419194453359e-05, 'epoch': 0.38}
{'loss': 0.3307, 'learning_rate': 6.823168486350753e-05, 'epoch': 0.38}
{'loss': 0.3034, 'learning_rate': 6.820917352093597e-05, 'epoch': 0.38}
                                                        38%|███▊      | 2476/6500 [16:23:32<23:51:58, 21.35s/it] 38%|███▊      | 2477/6500 [16:23:51<22:46:35, 20.38s/it]                                                          38%|███▊      | 2477/6500 [16:23:51<22:46:35, 20.38s/it] 38%|███▊      | 2478/6500 [16:24:09<22:00:55, 19.71s/it]                                                          38%|███▊      | 2478/6500 [16:24:09<22:00:55, 19.71s/it] 38%|███▊      | 2479/6500 [16:24:27<21:29:17, 19.24s/it]                                                          38%|███▊      | 2479/6500 [16:24:27<21:29:17, 19.24s/it] 38%|███▊      | 2480/6500 [16:24:45<21:07:06, 18.91s/it]                                                          38%|███▊      | 2480/6500 [16:24:45<21:07:06, 18.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8186439871788025, 'eval_runtime': 5.463, 'eval_samples_per_second': 4.21, 'eval_steps_per_second': 1.098, 'epoch': 0.38}
                                                          38%|███▊      | 2480/6500 [16:24:50<21:07:06, 18.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2480
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2480 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2480/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2480


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2480
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2480/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3163, 'learning_rate': 6.818665792208082e-05, 'epoch': 0.38}
{'loss': 0.3199, 'learning_rate': 6.816413807220495e-05, 'epoch': 0.38}
{'loss': 0.3201, 'learning_rate': 6.814161397657224e-05, 'epoch': 0.38}
{'loss': 0.3109, 'learning_rate': 6.811908564044756e-05, 'epoch': 0.38}
{'loss': 0.3222, 'learning_rate': 6.809655306909681e-05, 'epoch': 0.38}
 38%|███▊      | 2481/6500 [16:25:42<33:57:30, 30.42s/it]                                                          38%|███▊      | 2481/6500 [16:25:42<33:57:30, 30.42s/it] 38%|███▊      | 2482/6500 [16:26:00<29:48:09, 26.70s/it]                                                          38%|███▊      | 2482/6500 [16:26:00<29:48:09, 26.70s/it] 38%|███▊      | 2483/6500 [16:26:18<26:53:45, 24.10s/it]                                                          38%|███▊      | 2483/6500 [16:26:18<26:53:45, 24.10s/it] 38%|███▊      | 2484/6500 [16:26:36<24:51:38, 22.29s/it]                                                          38%|███▊      | 2484/6500 [16:26:36<24:51:38, 22.29s/it] 38%|███▊      | 2485/6500 [16:26:54<23:26:26, 21.02s/it]                                                          38%|███▊      | 2485/6500 [16:26:54<23:26:26, 21.02s/it] 38%|███▊      | 2486/6500 [16:27:13<22:27:16, 20.14s/it]  {'loss': 0.3322, 'learning_rate': 6.807401626778679e-05, 'epoch': 0.38}
{'loss': 0.3169, 'learning_rate': 6.805147524178535e-05, 'epoch': 0.38}
{'loss': 0.3256, 'learning_rate': 6.802892999636134e-05, 'epoch': 0.38}
{'loss': 0.3206, 'learning_rate': 6.800638053678455e-05, 'epoch': 0.38}
{'loss': 0.3172, 'learning_rate': 6.79838268683258e-05, 'epoch': 0.38}
                                                        38%|███▊      | 2486/6500 [16:27:13<22:27:16, 20.14s/it] 38%|███▊      | 2487/6500 [16:27:31<21:46:08, 19.53s/it]                                                          38%|███▊      | 2487/6500 [16:27:31<21:46:08, 19.53s/it] 38%|███▊      | 2488/6500 [16:27:49<21:17:31, 19.11s/it]                                                          38%|███▊      | 2488/6500 [16:27:49<21:17:31, 19.11s/it] 38%|███▊      | 2489/6500 [16:28:07<21:03:09, 18.90s/it]                                                          38%|███▊      | 2489/6500 [16:28:07<21:03:09, 18.90s/it] 38%|███▊      | 2490/6500 [16:28:25<20:47:48, 18.67s/it]                                                          38%|███▊      | 2490/6500 [16:28:25<20:47:48, 18.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8309453129768372, 'eval_runtime': 5.3204, 'eval_samples_per_second': 4.323, 'eval_steps_per_second': 1.128, 'epoch': 0.38}
                                                          38%|███▊      | 2490/6500 [16:28:31<20:47:48, 18.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2490
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2490
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2490

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2490/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2490/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.315, 'learning_rate': 6.796126899625688e-05, 'epoch': 0.38}
{'loss': 0.3402, 'learning_rate': 6.793870692585051e-05, 'epoch': 0.38}
{'loss': 0.3187, 'learning_rate': 6.791614066238047e-05, 'epoch': 0.38}
{'loss': 0.3192, 'learning_rate': 6.789357021112148e-05, 'epoch': 0.38}
{'loss': 0.307, 'learning_rate': 6.787099557734927e-05, 'epoch': 0.38}
 38%|███▊      | 2491/6500 [16:29:58<45:21:51, 40.74s/it]                                                          38%|███▊      | 2491/6500 [16:29:58<45:21:51, 40.74s/it] 38%|███▊      | 2492/6500 [16:30:16<37:47:33, 33.95s/it]                                                          38%|███▊      | 2492/6500 [16:30:16<37:47:33, 33.95s/it] 38%|███▊      | 2493/6500 [16:30:34<32:33:33, 29.25s/it]                                                          38%|███▊      | 2493/6500 [16:30:34<32:33:33, 29.25s/it] 38%|███▊      | 2494/6500 [16:30:52<28:49:19, 25.90s/it]                                                          38%|███▊      | 2494/6500 [16:30:52<28:49:19, 25.90s/it] 38%|███▊      | 2495/6500 [16:31:10<26:13:56, 23.58s/it]                                                          38%|███▊      | 2495/6500 [16:31:10<26:13:56, 23.58s/it] 38%|███▊      | 2496/6500 [16:31:29<24:29:42, 22.02s/it]  {'loss': 0.3339, 'learning_rate': 6.784841676634048e-05, 'epoch': 0.38}
{'loss': 0.3844, 'learning_rate': 6.78258337833728e-05, 'epoch': 0.38}
{'loss': 0.3206, 'learning_rate': 6.780324663372485e-05, 'epoch': 0.38}
{'loss': 0.3023, 'learning_rate': 6.778065532267624e-05, 'epoch': 0.38}
{'loss': 0.3317, 'learning_rate': 6.775805985550756e-05, 'epoch': 0.38}
                                                        38%|███▊      | 2496/6500 [16:31:29<24:29:42, 22.02s/it] 38%|███▊      | 2497/6500 [16:31:47<23:10:56, 20.85s/it]                                                          38%|███▊      | 2497/6500 [16:31:47<23:10:56, 20.85s/it] 38%|███▊      | 2498/6500 [16:32:05<22:18:02, 20.06s/it]                                                          38%|███▊      | 2498/6500 [16:32:05<22:18:02, 20.06s/it] 38%|███▊      | 2499/6500 [16:32:23<21:39:08, 19.48s/it]                                                          38%|███▊      | 2499/6500 [16:32:23<21:39:08, 19.48s/it] 38%|███▊      | 2500/6500 [16:32:41<21:12:11, 19.08s/it]                                                          38%|███▊      | 2500/6500 [16:32:41<21:12:11, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8146904110908508, 'eval_runtime': 6.5154, 'eval_samples_per_second': 3.53, 'eval_steps_per_second': 0.921, 'epoch': 0.38}
                                                          38%|███▊      | 2500/6500 [16:32:48<21:12:11, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2500/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2500

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2500/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8361, 'learning_rate': 6.773546023750037e-05, 'epoch': 0.38}
{'loss': 0.3305, 'learning_rate': 6.771285647393719e-05, 'epoch': 0.38}
{'loss': 0.3089, 'learning_rate': 6.769024857010148e-05, 'epoch': 0.39}
{'loss': 0.32, 'learning_rate': 6.766763653127773e-05, 'epoch': 0.39}
{'loss': 0.3021, 'learning_rate': 6.764502036275138e-05, 'epoch': 0.39}
 38%|███▊      | 2501/6500 [16:33:39<34:04:44, 30.68s/it]                                                          38%|███▊      | 2501/6500 [16:33:39<34:04:44, 30.68s/it] 38%|███▊      | 2502/6500 [16:33:57<29:51:18, 26.88s/it]                                                          38%|███▊      | 2502/6500 [16:33:57<29:51:18, 26.88s/it] 39%|███▊      | 2503/6500 [16:34:15<26:54:08, 24.23s/it]                                                          39%|███▊      | 2503/6500 [16:34:15<26:54:08, 24.23s/it] 39%|███▊      | 2504/6500 [16:34:33<24:50:13, 22.38s/it]                                                          39%|███▊      | 2504/6500 [16:34:33<24:50:13, 22.38s/it] 39%|███▊      | 2505/6500 [16:34:52<23:35:16, 21.26s/it]                                                          39%|███▊      | 2505/6500 [16:34:52<23:35:16, 21.26s/it] 39%|███▊      | 2506/6500 [16:35:10<22:31:28, 20.30s/it]  {'loss': 0.3396, 'learning_rate': 6.762240006980878e-05, 'epoch': 0.39}
{'loss': 0.299, 'learning_rate': 6.759977565773734e-05, 'epoch': 0.39}
{'loss': 0.3021, 'learning_rate': 6.757714713182533e-05, 'epoch': 0.39}
{'loss': 0.3098, 'learning_rate': 6.755451449736204e-05, 'epoch': 0.39}
{'loss': 0.3122, 'learning_rate': 6.753187775963773e-05, 'epoch': 0.39}
                                                        39%|███▊      | 2506/6500 [16:35:10<22:31:28, 20.30s/it] 39%|███▊      | 2507/6500 [16:35:28<21:47:17, 19.64s/it]                                                          39%|███▊      | 2507/6500 [16:35:28<21:47:17, 19.64s/it] 39%|███▊      | 2508/6500 [16:35:46<21:16:25, 19.18s/it]                                                          39%|███▊      | 2508/6500 [16:35:46<21:16:25, 19.18s/it] 39%|███▊      | 2509/6500 [16:36:04<20:55:08, 18.87s/it]                                                          39%|███▊      | 2509/6500 [16:36:04<20:55:08, 18.87s/it] 39%|███▊      | 2510/6500 [16:36:22<20:40:26, 18.65s/it]                                                          39%|███▊      | 2510/6500 [16:36:22<20:40:26, 18.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8174249529838562, 'eval_runtime': 5.4673, 'eval_samples_per_second': 4.207, 'eval_steps_per_second': 1.097, 'epoch': 0.39}
                                                          39%|███▊      | 2510/6500 [16:36:28<20:40:26, 18.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2510
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2510

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2510
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2510

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2510
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2510/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2510/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3176, 'learning_rate': 6.750923692394359e-05, 'epoch': 0.39}
{'loss': 0.3097, 'learning_rate': 6.748659199557177e-05, 'epoch': 0.39}
{'loss': 0.327, 'learning_rate': 6.74639429798154e-05, 'epoch': 0.39}
{'loss': 0.3068, 'learning_rate': 6.744128988196853e-05, 'epoch': 0.39}
{'loss': 0.3294, 'learning_rate': 6.741863270732619e-05, 'epoch': 0.39}
 39%|███▊      | 2511/6500 [16:38:08<49:38:50, 44.81s/it]                                                          39%|███▊      | 2511/6500 [16:38:08<49:38:50, 44.81s/it] 39%|███▊      | 2512/6500 [16:38:26<40:43:27, 36.76s/it]                                                          39%|███▊      | 2512/6500 [16:38:26<40:43:27, 36.76s/it] 39%|███▊      | 2513/6500 [16:38:44<34:28:50, 31.13s/it]                                                          39%|███▊      | 2513/6500 [16:38:44<34:28:50, 31.13s/it] 39%|███▊      | 2514/6500 [16:39:02<30:07:32, 27.21s/it]                                                          39%|███▊      | 2514/6500 [16:39:02<30:07:32, 27.21s/it] 39%|███▊      | 2515/6500 [16:39:20<27:04:23, 24.46s/it]                                                          39%|███▊      | 2515/6500 [16:39:20<27:04:23, 24.46s/it] 39%|███▊      | 2516/6500 [16:39:38<24:56:18, 22.53s/it]  {'loss': 0.3206, 'learning_rate': 6.739597146118436e-05, 'epoch': 0.39}
{'loss': 0.3128, 'learning_rate': 6.737330614884001e-05, 'epoch': 0.39}
{'loss': 0.3234, 'learning_rate': 6.735063677559095e-05, 'epoch': 0.39}
{'loss': 0.3193, 'learning_rate': 6.732796334673603e-05, 'epoch': 0.39}
{'loss': 0.3178, 'learning_rate': 6.730528586757505e-05, 'epoch': 0.39}
                                                        39%|███▊      | 2516/6500 [16:39:38<24:56:18, 22.53s/it] 39%|███▊      | 2517/6500 [16:39:56<23:27:13, 21.20s/it]                                                          39%|███▊      | 2517/6500 [16:39:56<23:27:13, 21.20s/it] 39%|███▊      | 2518/6500 [16:40:14<22:25:33, 20.27s/it]                                                          39%|███▊      | 2518/6500 [16:40:14<22:25:33, 20.27s/it] 39%|███▉      | 2519/6500 [16:40:33<21:42:39, 19.63s/it]                                                          39%|███▉      | 2519/6500 [16:40:33<21:42:39, 19.63s/it] 39%|███▉      | 2520/6500 [16:40:51<21:13:22, 19.20s/it]                                                          39%|███▉      | 2520/6500 [16:40:51<21:13:22, 19.20s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8292225003242493, 'eval_runtime': 5.7963, 'eval_samples_per_second': 3.968, 'eval_steps_per_second': 1.035, 'epoch': 0.39}
                                                          39%|███▉      | 2520/6500 [16:40:57<21:13:22, 19.20s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2520
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2520

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2520
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2520

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2520
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2520/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2520/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.319, 'learning_rate': 6.72826043434087e-05, 'epoch': 0.39}
{'loss': 0.3373, 'learning_rate': 6.725991877953868e-05, 'epoch': 0.39}
{'loss': 0.2994, 'learning_rate': 6.723722918126758e-05, 'epoch': 0.39}
{'loss': 0.31, 'learning_rate': 6.721453555389897e-05, 'epoch': 0.39}
{'loss': 0.2971, 'learning_rate': 6.719183790273733e-05, 'epoch': 0.39}
 39%|███▉      | 2521/6500 [16:42:31<48:04:51, 43.50s/it]                                                          39%|███▉      | 2521/6500 [16:42:31<48:04:51, 43.50s/it] 39%|███▉      | 2522/6500 [16:42:49<39:37:13, 35.86s/it]                                                          39%|███▉      | 2522/6500 [16:42:49<39:37:13, 35.86s/it] 39%|███▉      | 2523/6500 [16:43:07<33:42:32, 30.51s/it]                                                          39%|███▉      | 2523/6500 [16:43:07<33:42:32, 30.51s/it] 39%|███▉      | 2524/6500 [16:43:25<29:33:45, 26.77s/it]                                                          39%|███▉      | 2524/6500 [16:43:25<29:33:45, 26.77s/it] 39%|███▉      | 2525/6500 [16:43:43<26:40:18, 24.16s/it]                                                          39%|███▉      | 2525/6500 [16:43:43<26:40:18, 24.16s/it] 39%|███▉      | 2526/6500 [16:44:01<24:39:52, 22.34s/it]  {'loss': 0.3417, 'learning_rate': 6.716913623308812e-05, 'epoch': 0.39}
{'loss': 0.3705, 'learning_rate': 6.714643055025769e-05, 'epoch': 0.39}
{'loss': 0.3085, 'learning_rate': 6.712372085955339e-05, 'epoch': 0.39}
{'loss': 0.3273, 'learning_rate': 6.710100716628344e-05, 'epoch': 0.39}
{'loss': 0.3083, 'learning_rate': 6.707828947575706e-05, 'epoch': 0.39}
                                                        39%|███▉      | 2526/6500 [16:44:01<24:39:52, 22.34s/it] 39%|███▉      | 2527/6500 [16:44:19<23:15:34, 21.08s/it]                                                          39%|███▉      | 2527/6500 [16:44:19<23:15:34, 21.08s/it] 39%|███▉      | 2528/6500 [16:44:37<22:16:45, 20.19s/it]                                                          39%|███▉      | 2528/6500 [16:44:37<22:16:45, 20.19s/it] 39%|███▉      | 2529/6500 [16:44:56<21:36:04, 19.58s/it]                                                          39%|███▉      | 2529/6500 [16:44:56<21:36:04, 19.58s/it] 39%|███▉      | 2530/6500 [16:45:14<21:07:48, 19.16s/it]                                                          39%|███▉      | 2530/6500 [16:45:14<21:07:48, 19.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8123849630355835, 'eval_runtime': 5.9519, 'eval_samples_per_second': 3.864, 'eval_steps_per_second': 1.008, 'epoch': 0.39}
                                                          39%|███▉      | 2530/6500 [16:45:20<21:07:48, 19.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2530
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2530
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2530

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2530

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2530
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2530/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2530/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8468, 'learning_rate': 6.705556779328433e-05, 'epoch': 0.39}
{'loss': 0.325, 'learning_rate': 6.703284212417632e-05, 'epoch': 0.39}
{'loss': 0.321, 'learning_rate': 6.701011247374505e-05, 'epoch': 0.39}
{'loss': 0.2979, 'learning_rate': 6.69873788473034e-05, 'epoch': 0.39}
{'loss': 0.3076, 'learning_rate': 6.696464125016522e-05, 'epoch': 0.39}
 39%|███▉      | 2531/6500 [16:46:16<35:18:27, 32.03s/it]                                                          39%|███▉      | 2531/6500 [16:46:16<35:18:27, 32.03s/it] 39%|███▉      | 2532/6500 [16:46:34<30:40:16, 27.83s/it]                                                          39%|███▉      | 2532/6500 [16:46:34<30:40:16, 27.83s/it] 39%|███▉      | 2533/6500 [16:46:52<27:25:35, 24.89s/it]                                                          39%|███▉      | 2533/6500 [16:46:52<27:25:35, 24.89s/it] 39%|███▉      | 2534/6500 [16:47:10<25:10:11, 22.85s/it]                                                          39%|███▉      | 2534/6500 [16:47:10<25:10:11, 22.85s/it] 39%|███▉      | 2535/6500 [16:47:28<23:35:31, 21.42s/it]                                                          39%|███▉      | 2535/6500 [16:47:28<23:35:31, 21.42s/it] 39%|███▉      | 2536/6500 [16:47:46<22:29:18, 20.42s/it]  {'loss': 0.3341, 'learning_rate': 6.694189968764532e-05, 'epoch': 0.39}
{'loss': 0.2904, 'learning_rate': 6.691915416505935e-05, 'epoch': 0.39}
{'loss': 0.3204, 'learning_rate': 6.689640468772398e-05, 'epoch': 0.39}
{'loss': 0.3094, 'learning_rate': 6.687365126095674e-05, 'epoch': 0.39}
{'loss': 0.323, 'learning_rate': 6.685089389007612e-05, 'epoch': 0.39}
                                                        39%|███▉      | 2536/6500 [16:47:46<22:29:18, 20.42s/it] 39%|███▉      | 2537/6500 [16:48:04<21:43:21, 19.73s/it]                                                          39%|███▉      | 2537/6500 [16:48:04<21:43:21, 19.73s/it] 39%|███▉      | 2538/6500 [16:48:23<21:19:19, 19.37s/it]                                                          39%|███▉      | 2538/6500 [16:48:23<21:19:19, 19.37s/it] 39%|███▉      | 2539/6500 [16:48:41<20:55:04, 19.01s/it]                                                          39%|███▉      | 2539/6500 [16:48:41<20:55:04, 19.01s/it] 39%|███▉      | 2540/6500 [16:48:59<20:38:16, 18.76s/it]                                                          39%|███▉      | 2540/6500 [16:48:59<20:38:16, 18.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8191754817962646, 'eval_runtime': 5.3373, 'eval_samples_per_second': 4.309, 'eval_steps_per_second': 1.124, 'epoch': 0.39}
                                                          39%|███▉      | 2540/6500 [16:49:05<20:38:16, 18.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2540
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2540 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2540
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2540

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2540
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2540/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2540/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3271, 'learning_rate': 6.682813258040151e-05, 'epoch': 0.39}
{'loss': 0.3193, 'learning_rate': 6.68053673372532e-05, 'epoch': 0.39}
{'loss': 0.3227, 'learning_rate': 6.678259816595246e-05, 'epoch': 0.39}
{'loss': 0.3148, 'learning_rate': 6.675982507182145e-05, 'epoch': 0.39}
{'loss': 0.342, 'learning_rate': 6.673704806018326e-05, 'epoch': 0.39}
 39%|███▉      | 2541/6500 [16:50:19<40:51:50, 37.16s/it]                                                          39%|███▉      | 2541/6500 [16:50:19<40:51:50, 37.16s/it] 39%|███▉      | 2542/6500 [16:50:37<34:32:41, 31.42s/it]                                                          39%|███▉      | 2542/6500 [16:50:37<34:32:41, 31.42s/it] 39%|███▉      | 2543/6500 [16:50:56<30:19:09, 27.58s/it]                                                          39%|███▉      | 2543/6500 [16:50:56<30:19:09, 27.58s/it] 39%|███▉      | 2544/6500 [16:51:14<27:08:33, 24.70s/it]                                                          39%|███▉      | 2544/6500 [16:51:14<27:08:33, 24.70s/it] 39%|███▉      | 2545/6500 [16:51:32<24:56:18, 22.70s/it]                                                          39%|███▉      | 2545/6500 [16:51:32<24:56:18, 22.70s/it] 39%|███▉      | 2546/6500 [16:51:50<23:23:58, 21.30s/it]  {'loss': 0.3075, 'learning_rate': 6.67142671363618e-05, 'epoch': 0.39}
{'loss': 0.3244, 'learning_rate': 6.669148230568205e-05, 'epoch': 0.39}
{'loss': 0.3345, 'learning_rate': 6.666869357346978e-05, 'epoch': 0.39}
{'loss': 0.3077, 'learning_rate': 6.664590094505174e-05, 'epoch': 0.39}
{'loss': 0.3294, 'learning_rate': 6.662310442575556e-05, 'epoch': 0.39}
                                                        39%|███▉      | 2546/6500 [16:51:50<23:23:58, 21.30s/it] 39%|███▉      | 2547/6500 [16:52:08<22:19:31, 20.33s/it]                                                          39%|███▉      | 2547/6500 [16:52:08<22:19:31, 20.33s/it] 39%|███▉      | 2548/6500 [16:52:26<21:34:59, 19.66s/it]                                                          39%|███▉      | 2548/6500 [16:52:26<21:34:59, 19.66s/it] 39%|███▉      | 2549/6500 [16:52:44<21:03:58, 19.19s/it]                                                          39%|███▉      | 2549/6500 [16:52:44<21:03:58, 19.19s/it] 39%|███▉      | 2550/6500 [16:53:02<20:42:37, 18.88s/it]                                                          39%|███▉      | 2550/6500 [16:53:02<20:42:37, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8289843201637268, 'eval_runtime': 5.3376, 'eval_samples_per_second': 4.309, 'eval_steps_per_second': 1.124, 'epoch': 0.39}
                                                          39%|███▉      | 2550/6500 [16:53:08<20:42:37, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2550
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2550
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2550
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2550

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2550
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2550/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2550/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3208, 'learning_rate': 6.660030402090981e-05, 'epoch': 0.39}
{'loss': 0.3277, 'learning_rate': 6.65774997358439e-05, 'epoch': 0.39}
{'loss': 0.3055, 'learning_rate': 6.655469157588823e-05, 'epoch': 0.39}
{'loss': 0.3164, 'learning_rate': 6.653187954637405e-05, 'epoch': 0.39}
{'loss': 0.3009, 'learning_rate': 6.650906365263356e-05, 'epoch': 0.39}
 39%|███▉      | 2551/6500 [16:54:43<47:31:29, 43.32s/it]                                                          39%|███▉      | 2551/6500 [16:54:43<47:31:29, 43.32s/it] 39%|███▉      | 2552/6500 [16:55:01<39:09:47, 35.71s/it]                                                          39%|███▉      | 2552/6500 [16:55:01<39:09:47, 35.71s/it] 39%|███▉      | 2553/6500 [16:55:19<33:19:18, 30.39s/it]                                                          39%|███▉      | 2553/6500 [16:55:19<33:19:18, 30.39s/it] 39%|███▉      | 2554/6500 [16:55:37<29:19:10, 26.75s/it]                                                          39%|███▉      | 2554/6500 [16:55:37<29:19:10, 26.75s/it] 39%|███▉      | 2555/6500 [16:55:55<26:26:31, 24.13s/it]                                                          39%|███▉      | 2555/6500 [16:55:55<26:26:31, 24.13s/it] 39%|███▉      | 2556/6500 [16:56:13<24:26:10, 22.31s/it]  {'loss': 0.39, 'learning_rate': 6.64862438999998e-05, 'epoch': 0.39}
{'loss': 0.3112, 'learning_rate': 6.646342029380679e-05, 'epoch': 0.39}
{'loss': 0.3076, 'learning_rate': 6.644059283938938e-05, 'epoch': 0.39}
{'loss': 0.3259, 'learning_rate': 6.641776154208334e-05, 'epoch': 0.39}
{'loss': 0.841, 'learning_rate': 6.639492640722536e-05, 'epoch': 0.39}
                                                        39%|███▉      | 2556/6500 [16:56:13<24:26:10, 22.31s/it] 39%|███▉      | 2557/6500 [16:56:31<23:02:26, 21.04s/it]                                                          39%|███▉      | 2557/6500 [16:56:31<23:02:26, 21.04s/it] 39%|███▉      | 2558/6500 [16:56:49<22:04:12, 20.16s/it]                                                          39%|███▉      | 2558/6500 [16:56:49<22:04:12, 20.16s/it] 39%|███▉      | 2559/6500 [16:57:07<21:24:09, 19.55s/it]                                                          39%|███▉      | 2559/6500 [16:57:07<21:24:09, 19.55s/it] 39%|███▉      | 2560/6500 [16:57:25<20:55:27, 19.12s/it]                                                          39%|███▉      | 2560/6500 [16:57:25<20:55:27, 19.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8121513724327087, 'eval_runtime': 5.7643, 'eval_samples_per_second': 3.99, 'eval_steps_per_second': 1.041, 'epoch': 0.39}
                                                          39%|███▉      | 2560/6500 [16:57:31<20:55:27, 19.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2560
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2560
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2560
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2560

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2560
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2560/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2560/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3294, 'learning_rate': 6.637208744015303e-05, 'epoch': 0.39}
{'loss': 0.312, 'learning_rate': 6.63492446462048e-05, 'epoch': 0.39}
{'loss': 0.3156, 'learning_rate': 6.632639803072003e-05, 'epoch': 0.39}
{'loss': 0.2918, 'learning_rate': 6.630354759903898e-05, 'epoch': 0.39}
{'loss': 0.3274, 'learning_rate': 6.628069335650282e-05, 'epoch': 0.39}
 39%|███▉      | 2561/6500 [16:58:07<28:26:47, 26.00s/it]                                                          39%|███▉      | 2561/6500 [16:58:07<28:26:47, 26.00s/it] 39%|███▉      | 2562/6500 [16:58:26<25:51:02, 23.63s/it]                                                          39%|███▉      | 2562/6500 [16:58:26<25:51:02, 23.63s/it] 39%|███▉      | 2563/6500 [16:58:44<24:04:14, 22.01s/it]                                                          39%|███▉      | 2563/6500 [16:58:44<24:04:14, 22.01s/it] 39%|███▉      | 2564/6500 [16:59:02<22:49:22, 20.87s/it]                                                          39%|███▉      | 2564/6500 [16:59:02<22:49:22, 20.87s/it] 39%|███▉      | 2565/6500 [16:59:20<21:54:29, 20.04s/it]                                                          39%|███▉      | 2565/6500 [16:59:20<21:54:29, 20.04s/it] 39%|███▉      | 2566/6500 [16:59:38<21:16:00, 19.46s/it]  {'loss': 0.3067, 'learning_rate': 6.625783530845359e-05, 'epoch': 0.39}
{'loss': 0.2838, 'learning_rate': 6.623497346023418e-05, 'epoch': 0.39}
{'loss': 0.3211, 'learning_rate': 6.621210781718844e-05, 'epoch': 0.4}
{'loss': 0.2928, 'learning_rate': 6.618923838466108e-05, 'epoch': 0.4}
{'loss': 0.3268, 'learning_rate': 6.616636516799766e-05, 'epoch': 0.4}
                                                        39%|███▉      | 2566/6500 [16:59:38<21:16:00, 19.46s/it] 39%|███▉      | 2567/6500 [16:59:56<20:49:24, 19.06s/it]                                                          39%|███▉      | 2567/6500 [16:59:56<20:49:24, 19.06s/it] 40%|███▉      | 2568/6500 [17:00:15<20:31:28, 18.79s/it]                                                          40%|███▉      | 2568/6500 [17:00:15<20:31:28, 18.79s/it] 40%|███▉      | 2569/6500 [17:00:33<20:21:06, 18.64s/it]                                                          40%|███▉      | 2569/6500 [17:00:33<20:21:06, 18.64s/it] 40%|███▉      | 2570/6500 [17:00:51<20:19:59, 18.63s/it]                                                          40%|███▉      | 2570/6500 [17:00:51<20:19:59, 18.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8189619183540344, 'eval_runtime': 5.3438, 'eval_samples_per_second': 4.304, 'eval_steps_per_second': 1.123, 'epoch': 0.4}
                                                          40%|███▉      | 2570/6500 [17:00:57<20:19:59, 18.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2570
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2570
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2570the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2570

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2570
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2570/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3004, 'learning_rate': 6.61434881725447e-05, 'epoch': 0.4}
{'loss': 0.3301, 'learning_rate': 6.612060740364952e-05, 'epoch': 0.4}
{'loss': 0.3012, 'learning_rate': 6.609772286666037e-05, 'epoch': 0.4}
{'loss': 0.3396, 'learning_rate': 6.607483456692638e-05, 'epoch': 0.4}
{'loss': 0.3269, 'learning_rate': 6.605194250979755e-05, 'epoch': 0.4}
 40%|███▉      | 2571/6500 [17:01:48<32:55:02, 30.16s/it]                                                          40%|███▉      | 2571/6500 [17:01:48<32:55:02, 30.16s/it] 40%|███▉      | 2572/6500 [17:02:07<28:56:38, 26.53s/it]                                                          40%|███▉      | 2572/6500 [17:02:07<28:56:38, 26.53s/it] 40%|███▉      | 2573/6500 [17:02:25<26:09:24, 23.98s/it]                                                          40%|███▉      | 2573/6500 [17:02:25<26:09:24, 23.98s/it] 40%|███▉      | 2574/6500 [17:02:43<24:14:31, 22.23s/it]                                                          40%|███▉      | 2574/6500 [17:02:43<24:14:31, 22.23s/it] 40%|███▉      | 2575/6500 [17:03:01<22:51:52, 20.97s/it]                                                          40%|███▉      | 2575/6500 [17:03:01<22:51:52, 20.97s/it] 40%|███▉      | 2576/6500 [17:03:19<21:54:02, 20.09s/it]  {'loss': 0.3277, 'learning_rate': 6.602904670062476e-05, 'epoch': 0.4}
{'loss': 0.3289, 'learning_rate': 6.600614714475975e-05, 'epoch': 0.4}
{'loss': 0.3166, 'learning_rate': 6.598324384755518e-05, 'epoch': 0.4}
{'loss': 0.3167, 'learning_rate': 6.596033681436452e-05, 'epoch': 0.4}
{'loss': 0.3072, 'learning_rate': 6.593742605054218e-05, 'epoch': 0.4}
                                                        40%|███▉      | 2576/6500 [17:03:19<21:54:02, 20.09s/it] 40%|███▉      | 2577/6500 [17:03:37<21:14:07, 19.49s/it]                                                          40%|███▉      | 2577/6500 [17:03:37<21:14:07, 19.49s/it] 40%|███▉      | 2578/6500 [17:03:55<20:46:36, 19.07s/it]                                                          40%|███▉      | 2578/6500 [17:03:55<20:46:36, 19.07s/it] 40%|███▉      | 2579/6500 [17:04:16<21:33:44, 19.80s/it]                                                          40%|███▉      | 2579/6500 [17:04:17<21:33:44, 19.80s/it] 40%|███▉      | 2580/6500 [17:04:35<21:03:24, 19.34s/it]                                                          40%|███▉      | 2580/6500 [17:04:35<21:03:24, 19.34s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8287482261657715, 'eval_runtime': 6.2218, 'eval_samples_per_second': 3.697, 'eval_steps_per_second': 0.964, 'epoch': 0.4}
                                                          40%|███▉      | 2580/6500 [17:04:41<21:03:24, 19.34s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2580
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2580/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2580

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2580

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2580
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2580/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3414, 'learning_rate': 6.59145115614434e-05, 'epoch': 0.4}
{'loss': 0.3089, 'learning_rate': 6.58915933524243e-05, 'epoch': 0.4}
{'loss': 0.3087, 'learning_rate': 6.58686714288419e-05, 'epoch': 0.4}
{'loss': 0.3107, 'learning_rate': 6.584574579605401e-05, 'epoch': 0.4}
{'loss': 0.3359, 'learning_rate': 6.58228164594194e-05, 'epoch': 0.4}
 40%|███▉      | 2581/6500 [17:05:14<27:42:31, 25.45s/it]                                                          40%|███▉      | 2581/6500 [17:05:14<27:42:31, 25.45s/it] 40%|███▉      | 2582/6500 [17:05:32<25:16:51, 23.23s/it]                                                          40%|███▉      | 2582/6500 [17:05:32<25:16:51, 23.23s/it] 40%|███▉      | 2583/6500 [17:05:51<23:34:59, 21.67s/it]                                                          40%|███▉      | 2583/6500 [17:05:51<23:34:59, 21.67s/it] 40%|███▉      | 2584/6500 [17:06:09<22:24:03, 20.59s/it]                                                          40%|███▉      | 2584/6500 [17:06:09<22:24:03, 20.59s/it] 40%|███▉      | 2585/6500 [17:06:27<21:34:11, 19.83s/it]                                                          40%|███▉      | 2585/6500 [17:06:27<21:34:11, 19.83s/it] 40%|███▉      | 2586/6500 [17:06:45<21:04:10, 19.38s/it]  {'loss': 0.3763, 'learning_rate': 6.579988342429763e-05, 'epoch': 0.4}
{'loss': 0.3074, 'learning_rate': 6.577694669604919e-05, 'epoch': 0.4}
{'loss': 0.312, 'learning_rate': 6.575400628003538e-05, 'epoch': 0.4}
{'loss': 0.3191, 'learning_rate': 6.57310621816184e-05, 'epoch': 0.4}
{'loss': 0.8375, 'learning_rate': 6.570811440616125e-05, 'epoch': 0.4}
                                                        40%|███▉      | 2586/6500 [17:06:45<21:04:10, 19.38s/it] 40%|███▉      | 2587/6500 [17:07:03<20:39:07, 19.00s/it]                                                          40%|███▉      | 2587/6500 [17:07:03<20:39:07, 19.00s/it] 40%|███▉      | 2588/6500 [17:07:21<20:21:48, 18.74s/it]                                                          40%|███▉      | 2588/6500 [17:07:21<20:21:48, 18.74s/it] 40%|███▉      | 2589/6500 [17:07:39<20:09:50, 18.56s/it]                                                          40%|███▉      | 2589/6500 [17:07:39<20:09:50, 18.56s/it] 40%|███▉      | 2590/6500 [17:07:58<20:01:10, 18.43s/it]                                                          40%|███▉      | 2590/6500 [17:07:58<20:01:10, 18.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8144917488098145, 'eval_runtime': 5.5868, 'eval_samples_per_second': 4.117, 'eval_steps_per_second': 1.074, 'epoch': 0.4}
                                                          40%|███▉      | 2590/6500 [17:08:03<20:01:10, 18.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2590
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2590 the checkpoint model will be saved in 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2590
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2590

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2590
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2590/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2590/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3186, 'learning_rate': 6.568516295902788e-05, 'epoch': 0.4}
{'loss': 0.3119, 'learning_rate': 6.566220784558304e-05, 'epoch': 0.4}
{'loss': 0.3058, 'learning_rate': 6.563924907119234e-05, 'epoch': 0.4}
{'loss': 0.3027, 'learning_rate': 6.561628664122226e-05, 'epoch': 0.4}
{'loss': 0.3311, 'learning_rate': 6.559332056104012e-05, 'epoch': 0.4}
 40%|███▉      | 2591/6500 [17:09:50<50:46:53, 46.77s/it]                                                          40%|███▉      | 2591/6500 [17:09:50<50:46:53, 46.77s/it] 40%|███▉      | 2592/6500 [17:10:09<41:33:34, 38.28s/it]                                                          40%|███▉      | 2592/6500 [17:10:09<41:33:34, 38.28s/it] 40%|███▉      | 2593/6500 [17:10:27<34:56:16, 32.19s/it]                                                          40%|███▉      | 2593/6500 [17:10:27<34:56:16, 32.19s/it] 40%|███▉      | 2594/6500 [17:10:45<30:18:25, 27.93s/it]                                                          40%|███▉      | 2594/6500 [17:10:45<30:18:25, 27.93s/it] 40%|███▉      | 2595/6500 [17:11:03<27:04:19, 24.96s/it]                                                          40%|███▉      | 2595/6500 [17:11:03<27:04:19, 24.96s/it] 40%|███▉      | 2596/6500 [17:11:21<24:48:59, 22.88s/it]  {'loss': 0.3015, 'learning_rate': 6.557035083601413e-05, 'epoch': 0.4}
{'loss': 0.3076, 'learning_rate': 6.554737747151328e-05, 'epoch': 0.4}
{'loss': 0.3031, 'learning_rate': 6.552440047290747e-05, 'epoch': 0.4}
{'loss': 0.3068, 'learning_rate': 6.550141984556747e-05, 'epoch': 0.4}
{'loss': 0.3116, 'learning_rate': 6.547843559486481e-05, 'epoch': 0.4}
                                                        40%|███▉      | 2596/6500 [17:11:21<24:48:59, 22.88s/it] 40%|███▉      | 2597/6500 [17:11:39<23:14:48, 21.44s/it]                                                          40%|███▉      | 2597/6500 [17:11:39<23:14:48, 21.44s/it] 40%|███▉      | 2598/6500 [17:11:57<22:09:11, 20.44s/it]                                                          40%|███▉      | 2598/6500 [17:11:57<22:09:11, 20.44s/it] 40%|███▉      | 2599/6500 [17:12:15<21:23:33, 19.74s/it]                                                          40%|███▉      | 2599/6500 [17:12:15<21:23:33, 19.74s/it] 40%|████      | 2600/6500 [17:12:33<20:52:12, 19.26s/it]                                                          40%|████      | 2600/6500 [17:12:33<20:52:12, 19.26s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8199318051338196, 'eval_runtime': 5.3343, 'eval_samples_per_second': 4.312, 'eval_steps_per_second': 1.125, 'epoch': 0.4}
                                                          40%|████      | 2600/6500 [17:12:39<20:52:12, 19.26s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2600
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2600
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2600
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2600
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2600/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2600/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3049, 'learning_rate': 6.545544772617194e-05, 'epoch': 0.4}
{'loss': 0.3231, 'learning_rate': 6.543245624486214e-05, 'epoch': 0.4}
{'loss': 0.3094, 'learning_rate': 6.540946115630952e-05, 'epoch': 0.4}
{'loss': 0.3246, 'learning_rate': 6.538646246588906e-05, 'epoch': 0.4}
{'loss': 0.3036, 'learning_rate': 6.536346017897653e-05, 'epoch': 0.4}
 40%|████      | 2601/6500 [17:13:53<40:22:12, 37.27s/it]                                                          40%|████      | 2601/6500 [17:13:53<40:22:12, 37.27s/it] 40%|████      | 2602/6500 [17:14:11<34:14:45, 31.63s/it]                                                          40%|████      | 2602/6500 [17:14:11<34:14:45, 31.63s/it] 40%|████      | 2603/6500 [17:14:29<29:48:33, 27.54s/it]                                                          40%|████      | 2603/6500 [17:14:29<29:48:33, 27.54s/it] 40%|████      | 2604/6500 [17:14:47<26:43:08, 24.69s/it]                                                          40%|████      | 2604/6500 [17:14:47<26:43:08, 24.69s/it] 40%|████      | 2605/6500 [17:15:05<24:33:33, 22.70s/it]                                                          40%|████      | 2605/6500 [17:15:05<24:33:33, 22.70s/it] 40%|████      | 2606/6500 [17:15:23<23:03:32, 21.32s/it]  {'loss': 0.3013, 'learning_rate': 6.53404543009486e-05, 'epoch': 0.4}
{'loss': 0.307, 'learning_rate': 6.531744483718274e-05, 'epoch': 0.4}
{'loss': 0.3062, 'learning_rate': 6.529443179305728e-05, 'epoch': 0.4}
{'loss': 0.3132, 'learning_rate': 6.52714151739514e-05, 'epoch': 0.4}
{'loss': 0.311, 'learning_rate': 6.524839498524508e-05, 'epoch': 0.4}
                                                        40%|████      | 2606/6500 [17:15:23<23:03:32, 21.32s/it] 40%|████      | 2607/6500 [17:15:42<22:03:34, 20.40s/it]                                                          40%|████      | 2607/6500 [17:15:42<22:03:34, 20.40s/it] 40%|████      | 2608/6500 [17:16:00<21:19:20, 19.72s/it]                                                          40%|████      | 2608/6500 [17:16:00<21:19:20, 19.72s/it] 40%|████      | 2609/6500 [17:16:18<20:48:43, 19.26s/it]                                                          40%|████      | 2609/6500 [17:16:18<20:48:43, 19.26s/it] 40%|████      | 2610/6500 [17:16:36<20:27:18, 18.93s/it]                                                          40%|████      | 2610/6500 [17:16:36<20:27:18, 18.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8348593711853027, 'eval_runtime': 5.336, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.4}
                                                          40%|████      | 2610/6500 [17:16:41<20:27:18, 18.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2610
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2610
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2610
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2610

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2610
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2610/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3291, 'learning_rate': 6.522537123231912e-05, 'epoch': 0.4}
{'loss': 0.2985, 'learning_rate': 6.520234392055522e-05, 'epoch': 0.4}
{'loss': 0.311, 'learning_rate': 6.517931305533584e-05, 'epoch': 0.4}
{'loss': 0.2923, 'learning_rate': 6.515627864204434e-05, 'epoch': 0.4}
{'loss': 0.3429, 'learning_rate': 6.513324068606488e-05, 'epoch': 0.4}
 40%|████      | 2611/6500 [17:18:03<42:38:54, 39.48s/it]                                                          40%|████      | 2611/6500 [17:18:04<42:38:54, 39.48s/it] 40%|████      | 2612/6500 [17:18:22<35:42:06, 33.06s/it]                                                          40%|████      | 2612/6500 [17:18:22<35:42:06, 33.06s/it] 40%|████      | 2613/6500 [17:18:40<30:49:10, 28.54s/it]                                                          40%|████      | 2613/6500 [17:18:40<30:49:10, 28.54s/it] 40%|████      | 2614/6500 [17:18:58<27:24:13, 25.39s/it]                                                          40%|████      | 2614/6500 [17:18:58<27:24:13, 25.39s/it] 40%|████      | 2615/6500 [17:19:16<25:01:18, 23.19s/it]                                                          40%|████      | 2615/6500 [17:19:16<25:01:18, 23.19s/it] 40%|████      | 2616/6500 [17:19:34<23:20:56, 21.64s/it]  {'loss': 0.3578, 'learning_rate': 6.511019919278239e-05, 'epoch': 0.4}
{'loss': 0.3011, 'learning_rate': 6.508715416758273e-05, 'epoch': 0.4}
{'loss': 0.3182, 'learning_rate': 6.50641056158525e-05, 'epoch': 0.4}
{'loss': 0.3199, 'learning_rate': 6.504105354297918e-05, 'epoch': 0.4}
{'loss': 0.8397, 'learning_rate': 6.501799795435104e-05, 'epoch': 0.4}
                                                        40%|████      | 2616/6500 [17:19:34<23:20:56, 21.64s/it] 40%|████      | 2617/6500 [17:19:52<22:11:03, 20.57s/it]                                                          40%|████      | 2617/6500 [17:19:52<22:11:03, 20.57s/it] 40%|████      | 2618/6500 [17:20:10<21:28:45, 19.92s/it]                                                          40%|████      | 2618/6500 [17:20:10<21:28:45, 19.92s/it] 40%|████      | 2619/6500 [17:20:28<20:53:12, 19.37s/it]                                                          40%|████      | 2619/6500 [17:20:28<20:53:12, 19.37s/it] 40%|████      | 2620/6500 [17:20:46<20:28:15, 18.99s/it]                                                          40%|████      | 2620/6500 [17:20:46<20:28:15, 18.99s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8230561017990112, 'eval_runtime': 5.3454, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.122, 'epoch': 0.4}
                                                          40%|████      | 2620/6500 [17:20:52<20:28:15, 18.99s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2620
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2620 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2620
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2620

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2620
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2620/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2620/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3154, 'learning_rate': 6.499493885535721e-05, 'epoch': 0.4}
{'loss': 0.3253, 'learning_rate': 6.497187625138757e-05, 'epoch': 0.4}
{'loss': 0.2947, 'learning_rate': 6.49488101478329e-05, 'epoch': 0.4}
{'loss': 0.3232, 'learning_rate': 6.492574055008473e-05, 'epoch': 0.4}
{'loss': 0.3069, 'learning_rate': 6.490266746353547e-05, 'epoch': 0.4}
 40%|████      | 2621/6500 [17:21:58<37:38:33, 34.94s/it]                                                          40%|████      | 2621/6500 [17:21:58<37:38:33, 34.94s/it] 40%|████      | 2622/6500 [17:22:16<32:09:33, 29.85s/it]                                                          40%|████      | 2622/6500 [17:22:16<32:09:33, 29.85s/it] 40%|████      | 2623/6500 [17:22:34<28:18:27, 26.29s/it]                                                          40%|████      | 2623/6500 [17:22:34<28:18:27, 26.29s/it] 40%|████      | 2624/6500 [17:22:53<25:44:50, 23.91s/it]                                                          40%|████      | 2624/6500 [17:22:53<25:44:50, 23.91s/it] 40%|████      | 2625/6500 [17:23:11<23:51:38, 22.17s/it]                                                          40%|████      | 2625/6500 [17:23:11<23:51:38, 22.17s/it] 40%|████      | 2626/6500 [17:23:29<22:31:01, 20.92s/it]  {'loss': 0.2877, 'learning_rate': 6.48795908935783e-05, 'epoch': 0.4}
{'loss': 0.3132, 'learning_rate': 6.485651084560723e-05, 'epoch': 0.4}
{'loss': 0.302, 'learning_rate': 6.483342732501707e-05, 'epoch': 0.4}
{'loss': 0.3124, 'learning_rate': 6.481034033720347e-05, 'epoch': 0.4}
{'loss': 0.3084, 'learning_rate': 6.478724988756285e-05, 'epoch': 0.4}
                                                        40%|████      | 2626/6500 [17:23:29<22:31:01, 20.92s/it] 40%|████      | 2627/6500 [17:23:47<21:43:49, 20.20s/it]                                                          40%|████      | 2627/6500 [17:23:47<21:43:49, 20.20s/it] 40%|████      | 2628/6500 [17:24:05<21:02:17, 19.56s/it]                                                          40%|████      | 2628/6500 [17:24:05<21:02:17, 19.56s/it] 40%|████      | 2629/6500 [17:24:24<20:33:33, 19.12s/it]                                                          40%|████      | 2629/6500 [17:24:24<20:33:33, 19.12s/it] 40%|████      | 2630/6500 [17:24:42<20:13:50, 18.82s/it]                                                          40%|████      | 2630/6500 [17:24:42<20:13:50, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.820977509021759, 'eval_runtime': 5.42, 'eval_samples_per_second': 4.244, 'eval_steps_per_second': 1.107, 'epoch': 0.4}
                                                          40%|████      | 2630/6500 [17:24:47<20:13:50, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2630
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2630
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2630
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2630
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2630/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2630/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.312, 'learning_rate': 6.47641559814925e-05, 'epoch': 0.4}
{'loss': 0.3054, 'learning_rate': 6.474105862439045e-05, 'epoch': 0.4}
{'loss': 0.3071, 'learning_rate': 6.471795782165556e-05, 'epoch': 0.41}
{'loss': 0.3373, 'learning_rate': 6.469485357868754e-05, 'epoch': 0.41}
{'loss': 0.3119, 'learning_rate': 6.467174590088681e-05, 'epoch': 0.41}
 40%|████      | 2631/6500 [17:26:00<39:17:15, 36.56s/it]                                                          40%|████      | 2631/6500 [17:26:00<39:17:15, 36.56s/it] 40%|████      | 2632/6500 [17:26:18<33:17:56, 30.99s/it]                                                          40%|████      | 2632/6500 [17:26:18<33:17:56, 30.99s/it] 41%|████      | 2633/6500 [17:26:36<29:06:08, 27.09s/it]                                                          41%|████      | 2633/6500 [17:26:36<29:06:08, 27.09s/it] 41%|████      | 2634/6500 [17:26:54<26:09:56, 24.37s/it]                                                          41%|████      | 2634/6500 [17:26:54<26:09:56, 24.37s/it] 41%|████      | 2635/6500 [17:27:12<24:15:12, 22.59s/it]                                                          41%|████      | 2635/6500 [17:27:12<24:15:12, 22.59s/it] 41%|████      | 2636/6500 [17:27:30<22:50:13, 21.28s/it]  {'loss': 0.3182, 'learning_rate': 6.46486347936547e-05, 'epoch': 0.41}
{'loss': 0.3286, 'learning_rate': 6.462552026239328e-05, 'epoch': 0.41}
{'loss': 0.3025, 'learning_rate': 6.46024023125054e-05, 'epoch': 0.41}
{'loss': 0.3143, 'learning_rate': 6.457928094939478e-05, 'epoch': 0.41}
{'loss': 0.3161, 'learning_rate': 6.455615617846588e-05, 'epoch': 0.41}
                                                        41%|████      | 2636/6500 [17:27:30<22:50:13, 21.28s/it] 41%|████      | 2637/6500 [17:27:48<21:48:07, 20.32s/it]                                                          41%|████      | 2637/6500 [17:27:48<21:48:07, 20.32s/it] 41%|████      | 2638/6500 [17:28:06<21:05:09, 19.66s/it]                                                          41%|████      | 2638/6500 [17:28:06<21:05:09, 19.66s/it] 41%|████      | 2639/6500 [17:28:25<20:35:23, 19.20s/it]                                                          41%|████      | 2639/6500 [17:28:25<20:35:23, 19.20s/it] 41%|████      | 2640/6500 [17:28:43<20:16:55, 18.92s/it]                                                          41%|████      | 2640/6500 [17:28:43<20:16:55, 18.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8321178555488586, 'eval_runtime': 5.4867, 'eval_samples_per_second': 4.192, 'eval_steps_per_second': 1.094, 'epoch': 0.41}
                                                          41%|████      | 2640/6500 [17:28:48<20:16:55, 18.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2640
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2640/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2640

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2640

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2640
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2640/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2640/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3158, 'learning_rate': 6.453302800512398e-05, 'epoch': 0.41}
{'loss': 0.2988, 'learning_rate': 6.450989643477515e-05, 'epoch': 0.41}
{'loss': 0.2977, 'learning_rate': 6.448676147282625e-05, 'epoch': 0.41}
{'loss': 0.2944, 'learning_rate': 6.446362312468492e-05, 'epoch': 0.41}
{'loss': 0.3979, 'learning_rate': 6.444048139575963e-05, 'epoch': 0.41}
 41%|████      | 2641/6500 [17:30:31<49:07:02, 45.82s/it]                                                          41%|████      | 2641/6500 [17:30:31<49:07:02, 45.82s/it] 41%|████      | 2642/6500 [17:30:49<40:09:41, 37.48s/it]                                                          41%|████      | 2642/6500 [17:30:49<40:09:41, 37.48s/it] 41%|████      | 2643/6500 [17:31:07<33:53:34, 31.63s/it]                                                          41%|████      | 2643/6500 [17:31:07<33:53:34, 31.63s/it] 41%|████      | 2644/6500 [17:31:26<29:30:44, 27.55s/it]                                                          41%|████      | 2644/6500 [17:31:26<29:30:44, 27.55s/it] 41%|████      | 2645/6500 [17:31:44<26:27:51, 24.71s/it]                                                          41%|████      | 2645/6500 [17:31:44<26:27:51, 24.71s/it] 41%|████      | 2646/6500 [17:32:02<24:19:42, 22.73s/it]  {'loss': 0.3111, 'learning_rate': 6.441733629145961e-05, 'epoch': 0.41}
{'loss': 0.3104, 'learning_rate': 6.43941878171949e-05, 'epoch': 0.41}
{'loss': 0.3246, 'learning_rate': 6.437103597837631e-05, 'epoch': 0.41}
{'loss': 0.841, 'learning_rate': 6.434788078041543e-05, 'epoch': 0.41}
{'loss': 0.3209, 'learning_rate': 6.432472222872465e-05, 'epoch': 0.41}
                                                        41%|████      | 2646/6500 [17:32:02<24:19:42, 22.73s/it] 41%|████      | 2647/6500 [17:32:20<22:50:36, 21.34s/it]                                                          41%|████      | 2647/6500 [17:32:20<22:50:36, 21.34s/it] 41%|████      | 2648/6500 [17:32:38<21:48:22, 20.38s/it]                                                          41%|████      | 2648/6500 [17:32:38<21:48:22, 20.38s/it] 41%|████      | 2649/6500 [17:32:56<21:04:39, 19.70s/it]                                                          41%|████      | 2649/6500 [17:32:56<21:04:39, 19.70s/it] 41%|████      | 2650/6500 [17:33:14<20:34:44, 19.24s/it]                                                          41%|████      | 2650/6500 [17:33:14<20:34:44, 19.24s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8165062069892883, 'eval_runtime': 5.3501, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.121, 'epoch': 0.41}
                                                          41%|████      | 2650/6500 [17:33:20<20:34:44, 19.24s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2650
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2650
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2650
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2650
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2650/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2650/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2650/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3004, 'learning_rate': 6.430156032871715e-05, 'epoch': 0.41}
{'loss': 0.311, 'learning_rate': 6.427839508580687e-05, 'epoch': 0.41}
{'loss': 0.2891, 'learning_rate': 6.425522650540857e-05, 'epoch': 0.41}
{'loss': 0.3237, 'learning_rate': 6.423205459293773e-05, 'epoch': 0.41}
{'loss': 0.294, 'learning_rate': 6.420887935381067e-05, 'epoch': 0.41}
 41%|████      | 2651/6500 [17:34:44<43:16:03, 40.47s/it]                                                          41%|████      | 2651/6500 [17:34:44<43:16:03, 40.47s/it] 41%|████      | 2652/6500 [17:35:02<36:03:30, 33.73s/it]                                                          41%|████      | 2652/6500 [17:35:02<36:03:30, 33.73s/it] 41%|████      | 2653/6500 [17:35:20<31:00:57, 29.02s/it]                                                          41%|████      | 2653/6500 [17:35:20<31:00:57, 29.02s/it] 41%|████      | 2654/6500 [17:35:38<27:29:32, 25.73s/it]                                                          41%|████      | 2654/6500 [17:35:38<27:29:32, 25.73s/it] 41%|████      | 2655/6500 [17:35:56<25:01:51, 23.44s/it]                                                          41%|████      | 2655/6500 [17:35:56<25:01:51, 23.44s/it] 41%|████      | 2656/6500 [17:36:15<23:19:52, 21.85s/it]  {'loss': 0.2854, 'learning_rate': 6.418570079344444e-05, 'epoch': 0.41}
{'loss': 0.3068, 'learning_rate': 6.416251891725692e-05, 'epoch': 0.41}
{'loss': 0.2972, 'learning_rate': 6.413933373066671e-05, 'epoch': 0.41}
{'loss': 0.309, 'learning_rate': 6.411614523909321e-05, 'epoch': 0.41}
{'loss': 0.2981, 'learning_rate': 6.409295344795657e-05, 'epoch': 0.41}
                                                        41%|████      | 2656/6500 [17:36:15<23:19:52, 21.85s/it] 41%|████      | 2657/6500 [17:36:33<22:10:41, 20.78s/it]                                                          41%|████      | 2657/6500 [17:36:33<22:10:41, 20.78s/it] 41%|████      | 2658/6500 [17:36:51<21:20:47, 20.00s/it]                                                          41%|████      | 2658/6500 [17:36:51<21:20:47, 20.00s/it] 41%|████      | 2659/6500 [17:37:09<20:45:44, 19.46s/it]                                                          41%|████      | 2659/6500 [17:37:09<20:45:44, 19.46s/it] 41%|████      | 2660/6500 [17:37:27<20:21:13, 19.08s/it]                                                          41%|████      | 2660/6500 [17:37:27<20:21:13, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8250002861022949, 'eval_runtime': 6.0357, 'eval_samples_per_second': 3.811, 'eval_steps_per_second': 0.994, 'epoch': 0.41}
                                                          41%|████      | 2660/6500 [17:37:33<20:21:13, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2660
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2660
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2660
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2660

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2660
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2660/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2660/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3202, 'learning_rate': 6.406975836267776e-05, 'epoch': 0.41}
{'loss': 0.3008, 'learning_rate': 6.404655998867848e-05, 'epoch': 0.41}
{'loss': 0.3134, 'learning_rate': 6.40233583313812e-05, 'epoch': 0.41}
{'loss': 0.3042, 'learning_rate': 6.400015339620917e-05, 'epoch': 0.41}
{'loss': 0.3044, 'learning_rate': 6.397694518858643e-05, 'epoch': 0.41}
 41%|████      | 2661/6500 [17:38:44<38:36:57, 36.21s/it]                                                          41%|████      | 2661/6500 [17:38:44<38:36:57, 36.21s/it] 41%|████      | 2662/6500 [17:39:02<32:47:23, 30.76s/it]                                                          41%|████      | 2662/6500 [17:39:02<32:47:23, 30.76s/it] 41%|████      | 2663/6500 [17:39:20<28:42:59, 26.94s/it]                                                          41%|████      | 2663/6500 [17:39:20<28:42:59, 26.94s/it] 41%|████      | 2664/6500 [17:39:38<25:52:24, 24.28s/it]                                                          41%|████      | 2664/6500 [17:39:38<25:52:24, 24.28s/it] 41%|████      | 2665/6500 [17:39:56<23:53:26, 22.43s/it]                                                          41%|████      | 2665/6500 [17:39:56<23:53:26, 22.43s/it] 41%|████      | 2666/6500 [17:40:14<22:30:29, 21.13s/it]  {'loss': 0.3165, 'learning_rate': 6.39537337139377e-05, 'epoch': 0.41}
{'loss': 0.3038, 'learning_rate': 6.393051897768858e-05, 'epoch': 0.41}
{'loss': 0.3065, 'learning_rate': 6.390730098526533e-05, 'epoch': 0.41}
{'loss': 0.3071, 'learning_rate': 6.388407974209505e-05, 'epoch': 0.41}
{'loss': 0.3284, 'learning_rate': 6.386085525360553e-05, 'epoch': 0.41}
                                                        41%|████      | 2666/6500 [17:40:14<22:30:29, 21.13s/it] 41%|████      | 2667/6500 [17:40:32<21:37:45, 20.31s/it]                                                          41%|████      | 2667/6500 [17:40:32<21:37:45, 20.31s/it] 41%|████      | 2668/6500 [17:40:51<20:56:28, 19.67s/it]                                                          41%|████      | 2668/6500 [17:40:51<20:56:28, 19.67s/it] 41%|████      | 2669/6500 [17:41:09<20:26:58, 19.22s/it]                                                          41%|████      | 2669/6500 [17:41:09<20:26:58, 19.22s/it] 41%|████      | 2670/6500 [17:41:27<20:06:45, 18.90s/it]                                                          41%|████      | 2670/6500 [17:41:27<20:06:45, 18.90s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.837264358997345, 'eval_runtime': 5.6881, 'eval_samples_per_second': 4.044, 'eval_steps_per_second': 1.055, 'epoch': 0.41}
                                                          41%|████      | 2670/6500 [17:41:33<20:06:45, 18.90s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2670
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2670 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2670
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2670

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2670
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2670/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2966, 'learning_rate': 6.383762752522539e-05, 'epoch': 0.41}
{'loss': 0.3, 'learning_rate': 6.381439656238393e-05, 'epoch': 0.41}
{'loss': 0.2944, 'learning_rate': 6.379116237051127e-05, 'epoch': 0.41}
{'loss': 0.3277, 'learning_rate': 6.376792495503828e-05, 'epoch': 0.41}
{'loss': 0.3712, 'learning_rate': 6.374468432139652e-05, 'epoch': 0.41}
 41%|████      | 2671/6500 [17:42:06<26:34:08, 24.98s/it]                                                          41%|████      | 2671/6500 [17:42:06<26:34:08, 24.98s/it] 41%|████      | 2672/6500 [17:42:24<24:21:38, 22.91s/it]                                                          41%|████      | 2672/6500 [17:42:24<24:21:38, 22.91s/it] 41%|████      | 2673/6500 [17:42:42<22:48:19, 21.45s/it]                                                          41%|████      | 2673/6500 [17:42:42<22:48:19, 21.45s/it] 41%|████      | 2674/6500 [17:43:00<21:43:37, 20.44s/it]                                                          41%|████      | 2674/6500 [17:43:00<21:43:37, 20.44s/it] 41%|████      | 2675/6500 [17:43:18<20:58:30, 19.74s/it]                                                          41%|████      | 2675/6500 [17:43:18<20:58:30, 19.74s/it] 41%|████      | 2676/6500 [17:43:36<20:27:22, 19.26s/it]  {'loss': 0.301, 'learning_rate': 6.372144047501837e-05, 'epoch': 0.41}
{'loss': 0.2999, 'learning_rate': 6.369819342133694e-05, 'epoch': 0.41}
{'loss': 0.3118, 'learning_rate': 6.367494316578609e-05, 'epoch': 0.41}
{'loss': 0.8289, 'learning_rate': 6.36516897138004e-05, 'epoch': 0.41}
{'loss': 0.3253, 'learning_rate': 6.362843307081527e-05, 'epoch': 0.41}
                                                        41%|████      | 2676/6500 [17:43:36<20:27:22, 19.26s/it] 41%|████      | 2677/6500 [17:43:55<20:05:30, 18.92s/it]                                                          41%|████      | 2677/6500 [17:43:55<20:05:30, 18.92s/it] 41%|████      | 2678/6500 [17:44:13<19:50:43, 18.69s/it]                                                          41%|████      | 2678/6500 [17:44:13<19:50:43, 18.69s/it] 41%|████      | 2679/6500 [17:44:31<19:39:50, 18.53s/it]                                                          41%|████      | 2679/6500 [17:44:31<19:39:50, 18.53s/it] 41%|████      | 2680/6500 [17:44:49<19:32:56, 18.42s/it]                                                          41%|████      | 2680/6500 [17:44:49<19:32:56, 18.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8243395686149597, 'eval_runtime': 5.3593, 'eval_samples_per_second': 4.292, 'eval_steps_per_second': 1.12, 'epoch': 0.41}
                                                          41%|████      | 2680/6500 [17:44:54<19:32:56, 18.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2680
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2680
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2680
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2680

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2680
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2680/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2680/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3, 'learning_rate': 6.360517324226676e-05, 'epoch': 0.41}
{'loss': 0.2954, 'learning_rate': 6.358191023359172e-05, 'epoch': 0.41}
{'loss': 0.2946, 'learning_rate': 6.355864405022774e-05, 'epoch': 0.41}
{'loss': 0.3231, 'learning_rate': 6.353537469761315e-05, 'epoch': 0.41}
{'loss': 0.292, 'learning_rate': 6.351210218118704e-05, 'epoch': 0.41}
 41%|████      | 2681/6500 [17:46:13<40:17:21, 37.98s/it]                                                          41%|████      | 2681/6500 [17:46:13<40:17:21, 37.98s/it] 41%|████▏     | 2682/6500 [17:46:31<33:55:15, 31.98s/it]                                                          41%|████▏     | 2682/6500 [17:46:31<33:55:15, 31.98s/it] 41%|████▏     | 2683/6500 [17:46:49<29:38:27, 27.96s/it]                                                          41%|████▏     | 2683/6500 [17:46:49<29:38:27, 27.96s/it] 41%|████▏     | 2684/6500 [17:47:07<26:27:52, 24.97s/it]                                                          41%|████▏     | 2684/6500 [17:47:07<26:27:52, 24.97s/it] 41%|████▏     | 2685/6500 [17:47:25<24:15:17, 22.89s/it]                                                          41%|████▏     | 2685/6500 [17:47:25<24:15:17, 22.89s/it] 41%|████▏     | 2686/6500 [17:47:43<22:42{'loss': 0.2962, 'learning_rate': 6.34888265063892e-05, 'epoch': 0.41}
{'loss': 0.2906, 'learning_rate': 6.346554767866017e-05, 'epoch': 0.41}
{'loss': 0.3003, 'learning_rate': 6.344226570344123e-05, 'epoch': 0.41}
{'loss': 0.3075, 'learning_rate': 6.341898058617442e-05, 'epoch': 0.41}
{'loss': 0.2985, 'learning_rate': 6.339569233230249e-05, 'epoch': 0.41}
:42, 21.44s/it]                                                          41%|████▏     | 2686/6500 [17:47:43<22:42:42, 21.44s/it] 41%|████▏     | 2687/6500 [17:48:01<21:38:45, 20.44s/it]                                                          41%|████▏     | 2687/6500 [17:48:01<21:38:45, 20.44s/it] 41%|████▏     | 2688/6500 [17:48:20<20:53:44, 19.73s/it]                                                          41%|████▏     | 2688/6500 [17:48:20<20:53:44, 19.73s/it] 41%|████▏     | 2689/6500 [17:48:38<20:22:41, 19.25s/it]                                                          41%|████▏     | 2689/6500 [17:48:38<20:22:41, 19.25s/it] 41%|████▏     | 2690/6500 [17:48:56<20:01:02, 18.91s/it]                                                          41%|████▏     | 2690/6500 [17:48:56<20:01:02, 18.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8298940062522888, 'eval_runtime': 5.3651, 'eval_samples_per_second': 4.287, 'eval_steps_per_second': 1.118, 'epoch': 0.41}
                                                          41%|████▏     | 2690/6500 [17:49:01<20:01:02, 18.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2690
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2690
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2690
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2690
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2690/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2690/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3103, 'learning_rate': 6.337240094726893e-05, 'epoch': 0.41}
{'loss': 0.2981, 'learning_rate': 6.334910643651794e-05, 'epoch': 0.41}
{'loss': 0.317, 'learning_rate': 6.33258088054945e-05, 'epoch': 0.41}
{'loss': 0.3038, 'learning_rate': 6.330250805964425e-05, 'epoch': 0.41}
{'loss': 0.3041, 'learning_rate': 6.327920420441365e-05, 'epoch': 0.41}
 41%|████▏     | 2691/6500 [17:49:49<30:49:56, 29.14s/it]                                                          41%|████▏     | 2691/6500 [17:49:49<30:49:56, 29.14s/it] 41%|████▏     | 2692/6500 [17:50:07<27:17:34, 25.80s/it]                                                          41%|████▏     | 2692/6500 [17:50:07<27:17:34, 25.80s/it] 41%|████▏     | 2693/6500 [17:50:26<25:02:16, 23.68s/it]                                                          41%|████▏     | 2693/6500 [17:50:26<25:02:16, 23.68s/it] 41%|████▏     | 2694/6500 [17:50:44<23:15:19, 22.00s/it]                                                          41%|████▏     | 2694/6500 [17:50:44<23:15:19, 22.00s/it] 41%|████▏     | 2695/6500 [17:51:02<22:00:54, 20.83s/it]                                                          41%|████▏     | 2695/6500 [17:51:02<22:00:54, 20.83s/it] 41%|████▏     | 2696/6500 [17:51:20<2{'loss': 0.3165, 'learning_rate': 6.325589724524978e-05, 'epoch': 0.41}
{'loss': 0.3053, 'learning_rate': 6.323258718760055e-05, 'epoch': 0.41}
{'loss': 0.3097, 'learning_rate': 6.32092740369145e-05, 'epoch': 0.42}
{'loss': 0.3026, 'learning_rate': 6.318595779864098e-05, 'epoch': 0.42}
{'loss': 0.3187, 'learning_rate': 6.316263847822997e-05, 'epoch': 0.42}
1:09:21, 20.02s/it]                                                          41%|████▏     | 2696/6500 [17:51:20<21:09:21, 20.02s/it] 41%|████▏     | 2697/6500 [17:51:38<20:33:27, 19.46s/it]                                                          41%|████▏     | 2697/6500 [17:51:38<20:33:27, 19.46s/it] 42%|████▏     | 2698/6500 [17:51:56<20:08:29, 19.07s/it]                                                          42%|████▏     | 2698/6500 [17:51:56<20:08:29, 19.07s/it] 42%|████▏     | 2699/6500 [17:52:16<20:17:26, 19.22s/it]                                                          42%|████▏     | 2699/6500 [17:52:16<20:17:26, 19.22s/it] 42%|████▏     | 2700/6500 [17:52:34<19:56:55, 18.90s/it]                                                          42%|████▏     | 2700/6500 [17:52:34<19:56:55, 18.90s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8377209305763245, 'eval_runtime': 5.7362, 'eval_samples_per_second': 4.01, 'eval_steps_per_second': 1.046, 'epoch': 0.42}
                                                          42%|████▏     | 2700/6500 [17:52:40<19:56:55, 18.90s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2700
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2700
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2700 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2700

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2700
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2700/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2876, 'learning_rate': 6.313931608113226e-05, 'epoch': 0.42}
{'loss': 0.2945, 'learning_rate': 6.311599061279932e-05, 'epoch': 0.42}
{'loss': 0.2859, 'learning_rate': 6.30926620786833e-05, 'epoch': 0.42}
{'loss': 0.3379, 'learning_rate': 6.30693304842371e-05, 'epoch': 0.42}
{'loss': 0.3476, 'learning_rate': 6.30459958349144e-05, 'epoch': 0.42}
 42%|████▏     | 2701/6500 [17:53:45<36:37:38, 34.71s/it]                                                          42%|████▏     | 2701/6500 [17:53:45<36:37:38, 34.71s/it] 42%|████▏     | 2702/6500 [17:54:03<31:19:40, 29.69s/it]                                                          42%|████▏     | 2702/6500 [17:54:03<31:19:40, 29.69s/it] 42%|████▏     | 2703/6500 [17:54:22<27:37:51, 26.20s/it]                                                          42%|████▏     | 2703/6500 [17:54:22<27:37:51, 26.20s/it] 42%|████▏     | 2704/6500 [17:54:40<25:03:13, 23.76s/it]                                                          42%|████▏     | 2704/6500 [17:54:40<25:03:13, 23.76s/it] 42%|████▏     | 2705/6500 [17:54:58<23:14:59, 22.06s/it]                                                          42%|████▏     | 2705/6500 [17:54:58<23:14:59, 22.06s/it] 42%|████▏     | 2706/6500 [17:55:16<2{'loss': 0.2923, 'learning_rate': 6.302265813616947e-05, 'epoch': 0.42}
{'loss': 0.3167, 'learning_rate': 6.299931739345741e-05, 'epoch': 0.42}
{'loss': 0.4179, 'learning_rate': 6.297597361223392e-05, 'epoch': 0.42}
{'loss': 0.726, 'learning_rate': 6.29526267979555e-05, 'epoch': 0.42}
{'loss': 0.2971, 'learning_rate': 6.292927695607933e-05, 'epoch': 0.42}
1:59:32, 20.87s/it]                                                          42%|████▏     | 2706/6500 [17:55:16<21:59:32, 20.87s/it] 42%|████▏     | 2707/6500 [17:55:34<21:09:23, 20.08s/it]                                                          42%|████▏     | 2707/6500 [17:55:34<21:09:23, 20.08s/it] 42%|████▏     | 2708/6500 [17:55:52<20:33:43, 19.52s/it]                                                          42%|████▏     | 2708/6500 [17:55:52<20:33:43, 19.52s/it] 42%|████▏     | 2709/6500 [17:56:10<20:08:13, 19.12s/it]                                                          42%|████▏     | 2709/6500 [17:56:10<20:08:13, 19.12s/it] 42%|████▏     | 2710/6500 [17:56:29<19:49:55, 18.84s/it]                                                          42%|████▏     | 2710/6500 [17:56:29<19:49:55, 18.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8264240026473999, 'eval_runtime': 5.9154, 'eval_samples_per_second': 3.888, 'eval_steps_per_second': 1.014, 'epoch': 0.42}
                                                          42%|████▏     | 2710/6500 [17:56:34<19:49:55, 18.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2710
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2710

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2710
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2710

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2710
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2710/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2710/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.316, 'learning_rate': 6.290592409206327e-05, 'epoch': 0.42}
{'loss': 0.2892, 'learning_rate': 6.288256821136594e-05, 'epoch': 0.42}
{'loss': 0.3095, 'learning_rate': 6.285920931944661e-05, 'epoch': 0.42}
{'loss': 0.306, 'learning_rate': 6.283584742176528e-05, 'epoch': 0.42}
{'loss': 0.2724, 'learning_rate': 6.281248252378267e-05, 'epoch': 0.42}
 42%|████▏     | 2711/6500 [17:57:49<39:18:09, 37.34s/it]                                                          42%|████▏     | 2711/6500 [17:57:49<39:18:09, 37.34s/it] 42%|████▏     | 2712/6500 [17:58:07<33:11:10, 31.54s/it]                                                          42%|████▏     | 2712/6500 [17:58:07<33:11:10, 31.54s/it] 42%|████▏     | 2713/6500 [17:58:25<28:55:12, 27.49s/it]                                                          42%|████▏     | 2713/6500 [17:58:25<28:55:12, 27.49s/it] 42%|████▏     | 2714/6500 [17:58:43<25:56:25, 24.67s/it]                                                          42%|████▏     | 2714/6500 [17:58:43<25:56:25, 24.67s/it] 42%|████▏     | 2715/6500 [17:59:02<24:01:13, 22.85s/it]                                                          42%|████▏     | 2715/6500 [17:59:02<24:01:13, 22.85s/it] 42%|████▏     | 2716/6500 [17:59:20<2{'loss': 0.318, 'learning_rate': 6.278911463096016e-05, 'epoch': 0.42}
{'loss': 0.2954, 'learning_rate': 6.276574374875986e-05, 'epoch': 0.42}
{'loss': 0.309, 'learning_rate': 6.274236988264459e-05, 'epoch': 0.42}
{'loss': 0.3111, 'learning_rate': 6.271899303807783e-05, 'epoch': 0.42}
{'loss': 0.3132, 'learning_rate': 6.269561322052378e-05, 'epoch': 0.42}
2:31:25, 21.43s/it]                                                          42%|████▏     | 2716/6500 [17:59:20<22:31:25, 21.43s/it] 42%|████▏     | 2717/6500 [17:59:38<21:28:35, 20.44s/it]                                                          42%|████▏     | 2717/6500 [17:59:38<21:28:35, 20.44s/it] 42%|████▏     | 2718/6500 [17:59:56<20:45:19, 19.76s/it]                                                          42%|████▏     | 2718/6500 [17:59:56<20:45:19, 19.76s/it] 42%|████▏     | 2719/6500 [18:00:14<20:14:56, 19.28s/it]                                                          42%|████▏     | 2719/6500 [18:00:14<20:14:56, 19.28s/it] 42%|████▏     | 2720/6500 [18:00:33<19:53:46, 18.95s/it]                                                          42%|████▏     | 2720/6500 [18:00:33<19:53:46, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8247687220573425, 'eval_runtime': 5.3632, 'eval_samples_per_second': 4.289, 'eval_steps_per_second': 1.119, 'epoch': 0.42}
                                                          42%|████▏     | 2720/6500 [18:00:38<19:53:46, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2720
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2720
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2720
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2720

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2720
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2720/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2720/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.303, 'learning_rate': 6.267223043544732e-05, 'epoch': 0.42}
{'loss': 0.3083, 'learning_rate': 6.264884468831405e-05, 'epoch': 0.42}
{'loss': 0.3267, 'learning_rate': 6.262545598459025e-05, 'epoch': 0.42}
{'loss': 0.302, 'learning_rate': 6.260206432974288e-05, 'epoch': 0.42}
{'loss': 0.3159, 'learning_rate': 6.257866972923956e-05, 'epoch': 0.42}
 42%|████▏     | 2721/6500 [18:01:34<33:16:14, 31.69s/it]                                                          42%|████▏     | 2721/6500 [18:01:34<33:16:14, 31.69s/it] 42%|████▏     | 2722/6500 [18:01:52<28:57:30, 27.59s/it]                                                          42%|████▏     | 2722/6500 [18:01:52<28:57:30, 27.59s/it] 42%|████▏     | 2723/6500 [18:02:10<25:57:08, 24.74s/it]                                                          42%|████▏     | 2723/6500 [18:02:10<25:57:08, 24.74s/it] 42%|████▏     | 2724/6500 [18:02:30<24:26:36, 23.30s/it]                                                          42%|████▏     | 2724/6500 [18:02:30<24:26:36, 23.30s/it] 42%|████▏     | 2725/6500 [18:02:48<22:48:25, 21.75s/it]                                                          42%|████▏     | 2725/6500 [18:02:48<22:48:25, 21.75s/it] 42%|████▏     | 2726/6500 [18:03:06<2{'loss': 0.3108, 'learning_rate': 6.25552721885487e-05, 'epoch': 0.42}
{'loss': 0.3045, 'learning_rate': 6.25318717131393e-05, 'epoch': 0.42}
{'loss': 0.3081, 'learning_rate': 6.250846830848108e-05, 'epoch': 0.42}
{'loss': 0.3214, 'learning_rate': 6.248506198004445e-05, 'epoch': 0.42}
{'loss': 0.31, 'learning_rate': 6.246165273330049e-05, 'epoch': 0.42}
1:39:15, 20.66s/it]                                                          42%|████▏     | 2726/6500 [18:03:06<21:39:15, 20.66s/it] 42%|████▏     | 2727/6500 [18:03:24<20:51:11, 19.90s/it]                                                          42%|████▏     | 2727/6500 [18:03:24<20:51:11, 19.90s/it] 42%|████▏     | 2728/6500 [18:03:43<20:17:26, 19.37s/it]                                                          42%|████▏     | 2728/6500 [18:03:43<20:17:26, 19.37s/it] 42%|████▏     | 2729/6500 [18:04:01<19:54:00, 19.00s/it]                                                          42%|████▏     | 2729/6500 [18:04:01<19:54:00, 19.00s/it] 42%|████▏     | 2730/6500 [18:04:19<19:38:27, 18.76s/it]                                                          42%|████▏     | 2730/6500 [18:04:19<19:38:27, 18.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8340196013450623, 'eval_runtime': 5.4054, 'eval_samples_per_second': 4.255, 'eval_steps_per_second': 1.11, 'epoch': 0.42}
                                                          42%|████▏     | 2730/6500 [18:04:24<19:38:27, 18.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2730
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2730/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2730
 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2730

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2730
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2730/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2730/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2730/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3, 'learning_rate': 6.243824057372098e-05, 'epoch': 0.42}
{'loss': 0.2906, 'learning_rate': 6.241482550677833e-05, 'epoch': 0.42}
{'loss': 0.3078, 'learning_rate': 6.239140753794574e-05, 'epoch': 0.42}
{'loss': 0.3705, 'learning_rate': 6.236798667269695e-05, 'epoch': 0.42}
{'loss': 0.2962, 'learning_rate': 6.23445629165065e-05, 'epoch': 0.42}
 42%|████▏     | 2731/6500 [18:05:41<39:31:35, 37.75s/it]                                                          42%|████▏     | 2731/6500 [18:05:41<39:31:35, 37.75s/it] 42%|████▏     | 2732/6500 [18:05:59<33:24:01, 31.91s/it]                                                          42%|████▏     | 2732/6500 [18:05:59<33:24:01, 31.91s/it] 42%|████▏     | 2733/6500 [18:06:17<29:01:43, 27.74s/it]                                                          42%|████▏     | 2733/6500 [18:06:17<29:01:43, 27.74s/it] 42%|████▏     | 2734/6500 [18:06:35<25:58:37, 24.83s/it]                                                          42%|████▏     | 2734/6500 [18:06:35<25:58:37, 24.83s/it] 42%|████▏     | 2735/6500 [18:06:53<23:51:00, 22.80s/it]                                                          42%|████▏     | 2735/6500 [18:06:53<23:51:00, 22.80s/it] 42%|████▏     | 2736/6500 [18:07:12<2{'loss': 0.2891, 'learning_rate': 6.23211362748495e-05, 'epoch': 0.42}
{'loss': 0.3209, 'learning_rate': 6.229770675320184e-05, 'epoch': 0.42}
{'loss': 0.8267, 'learning_rate': 6.227427435703997e-05, 'epoch': 0.42}
{'loss': 0.3165, 'learning_rate': 6.225083909184109e-05, 'epoch': 0.42}
{'loss': 0.2988, 'learning_rate': 6.222740096308309e-05, 'epoch': 0.42}
2:27:00, 21.47s/it]                                                          42%|████▏     | 2736/6500 [18:07:12<22:27:00, 21.47s/it] 42%|████▏     | 2737/6500 [18:07:30<21:23:50, 20.47s/it]                                                          42%|████▏     | 2737/6500 [18:07:30<21:23:50, 20.47s/it] 42%|████▏     | 2738/6500 [18:07:48<20:39:34, 19.77s/it]                                                          42%|████▏     | 2738/6500 [18:07:48<20:39:34, 19.77s/it] 42%|████▏     | 2739/6500 [18:08:06<20:09:02, 19.29s/it]                                                          42%|████▏     | 2739/6500 [18:08:06<20:09:02, 19.29s/it] 42%|████▏     | 2740/6500 [18:08:24<19:47:55, 18.96s/it]                                                          42%|████▏     | 2740/6500 [18:08:24<19:47:55, 18.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8273020386695862, 'eval_runtime': 5.3558, 'eval_samples_per_second': 4.294, 'eval_steps_per_second': 1.12, 'epoch': 0.42}
                                                          42%|████▏     | 2740/6500 [18:08:30<19:47:55, 18.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2740
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2740
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2740
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2740

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2740
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2740/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3097, 'learning_rate': 6.220395997624443e-05, 'epoch': 0.42}
{'loss': 0.2742, 'learning_rate': 6.218051613680435e-05, 'epoch': 0.42}
{'loss': 0.319, 'learning_rate': 6.215706945024267e-05, 'epoch': 0.42}
{'loss': 0.2836, 'learning_rate': 6.213361992203991e-05, 'epoch': 0.42}
{'loss': 0.2859, 'learning_rate': 6.211016755767729e-05, 'epoch': 0.42}
 42%|████▏     | 2741/6500 [18:09:34<35:44:40, 34.23s/it]                                                          42%|████▏     | 2741/6500 [18:09:34<35:44:40, 34.23s/it] 42%|████▏     | 2742/6500 [18:09:52<30:39:23, 29.37s/it]                                                          42%|████▏     | 2742/6500 [18:09:52<30:39:23, 29.37s/it] 42%|████▏     | 2743/6500 [18:10:10<27:05:50, 25.96s/it]                                                          42%|████▏     | 2743/6500 [18:10:10<27:05:50, 25.96s/it] 42%|████▏     | 2744/6500 [18:10:28<24:35:53, 23.58s/it]                                                          42%|████▏     | 2744/6500 [18:10:28<24:35:53, 23.58s/it] 42%|████▏     | 2745/6500 [18:10:46<22:52:15, 21.93s/it]                                                          42%|████▏     | 2745/6500 [18:10:46<22:52:15, 21.93s/it] 42%|████▏     | 2746/6500 [18:11:04<2{'loss': 0.2958, 'learning_rate': 6.208671236263663e-05, 'epoch': 0.42}
{'loss': 0.2946, 'learning_rate': 6.206325434240043e-05, 'epoch': 0.42}
{'loss': 0.309, 'learning_rate': 6.203979350245188e-05, 'epoch': 0.42}
{'loss': 0.2935, 'learning_rate': 6.20163298482748e-05, 'epoch': 0.42}
{'loss': 0.3269, 'learning_rate': 6.199286338535369e-05, 'epoch': 0.42}
1:39:41, 20.77s/it]                                                          42%|████▏     | 2746/6500 [18:11:04<21:39:41, 20.77s/it] 42%|████▏     | 2747/6500 [18:11:23<20:49:13, 19.97s/it]                                                          42%|████▏     | 2747/6500 [18:11:23<20:49:13, 19.97s/it] 42%|████▏     | 2748/6500 [18:11:41<20:23:25, 19.56s/it]                                                          42%|████▏     | 2748/6500 [18:11:41<20:23:25, 19.56s/it] 42%|████▏     | 2749/6500 [18:11:59<19:56:20, 19.14s/it]                                                          42%|████▏     | 2749/6500 [18:11:59<19:56:20, 19.14s/it] 42%|████▏     | 2750/6500 [18:12:17<19:38:02, 18.85s/it]                                                          42%|████▏     | 2750/6500 [18:12:17<19:38:02, 18.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8293542265892029, 'eval_runtime': 5.5988, 'eval_samples_per_second': 4.108, 'eval_steps_per_second': 1.072, 'epoch': 0.42}
                                                          42%|████▏     | 2750/6500 [18:12:23<19:38:02, 18.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2750
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2750
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2750 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2750

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2750
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2750/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2750/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2953, 'learning_rate': 6.196939411917369e-05, 'epoch': 0.42}
{'loss': 0.3275, 'learning_rate': 6.194592205522056e-05, 'epoch': 0.42}
{'loss': 0.3128, 'learning_rate': 6.192244719898079e-05, 'epoch': 0.42}
{'loss': 0.3105, 'learning_rate': 6.189896955594148e-05, 'epoch': 0.42}
{'loss': 0.3139, 'learning_rate': 6.187548913159039e-05, 'epoch': 0.42}
 42%|████▏     | 2751/6500 [18:13:15<31:42:40, 30.45s/it]                                                          42%|████▏     | 2751/6500 [18:13:15<31:42:40, 30.45s/it] 42%|████▏     | 2752/6500 [18:13:33<27:49:31, 26.73s/it]                                                          42%|████▏     | 2752/6500 [18:13:33<27:49:31, 26.73s/it] 42%|████▏     | 2753/6500 [18:13:51<25:05:54, 24.11s/it]                                                          42%|████▏     | 2753/6500 [18:13:51<25:05:54, 24.11s/it] 42%|████▏     | 2754/6500 [18:14:09<23:11:40, 22.29s/it]                                                          42%|████▏     | 2754/6500 [18:14:09<23:11:40, 22.29s/it] 42%|████▏     | 2755/6500 [18:14:27<21:51:59, 21.02s/it]                                                          42%|████▏     | 2755/6500 [18:14:27<21:51:59, 21.02s/it] 42%|████▏     | 2756/6500 [18:14:45<2{'loss': 0.3124, 'learning_rate': 6.185200593141593e-05, 'epoch': 0.42}
{'loss': 0.3057, 'learning_rate': 6.182851996090713e-05, 'epoch': 0.42}
{'loss': 0.3117, 'learning_rate': 6.18050312255537e-05, 'epoch': 0.42}
{'loss': 0.3243, 'learning_rate': 6.1781539730846e-05, 'epoch': 0.42}
{'loss': 0.2856, 'learning_rate': 6.175804548227502e-05, 'epoch': 0.42}
0:56:13, 20.13s/it]                                                          42%|████▏     | 2756/6500 [18:14:45<20:56:13, 20.13s/it] 42%|████▏     | 2757/6500 [18:15:03<20:17:19, 19.51s/it]                                                          42%|████▏     | 2757/6500 [18:15:03<20:17:19, 19.51s/it] 42%|████▏     | 2758/6500 [18:15:21<19:51:58, 19.11s/it]                                                          42%|████▏     | 2758/6500 [18:15:21<19:51:58, 19.11s/it] 42%|████▏     | 2759/6500 [18:15:40<19:33:04, 18.81s/it]                                                          42%|████▏     | 2759/6500 [18:15:40<19:33:04, 18.81s/it] 42%|████▏     | 2760/6500 [18:15:58<19:20:01, 18.61s/it]                                                          42%|████▏     | 2760/6500 [18:15:58<19:20:01, 18.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8378153443336487, 'eval_runtime': 5.7674, 'eval_samples_per_second': 3.988, 'eval_steps_per_second': 1.04, 'epoch': 0.42}
                                                          42%|████▏     | 2760/6500 [18:16:03<19:20:01, 18.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2760
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2760
 I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2760

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2760

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2760
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2760/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2760/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3004, 'learning_rate': 6.173454848533242e-05, 'epoch': 0.42}
{'loss': 0.2879, 'learning_rate': 6.171104874551044e-05, 'epoch': 0.42}
{'loss': 0.3294, 'learning_rate': 6.168754626830202e-05, 'epoch': 0.43}
{'loss': 0.3578, 'learning_rate': 6.166404105920073e-05, 'epoch': 0.43}
{'loss': 0.2941, 'learning_rate': 6.164053312370074e-05, 'epoch': 0.43}
 42%|████▏     | 2761/6500 [18:17:30<42:23:56, 40.82s/it]                                                          42%|████▏     | 2761/6500 [18:17:30<42:23:56, 40.82s/it] 42%|████▏     | 2762/6500 [18:17:48<35:16:59, 33.98s/it]                                                          42%|████▏     | 2762/6500 [18:17:48<35:16:59, 33.98s/it] 43%|████▎     | 2763/6500 [18:18:06<30:18:31, 29.20s/it]                                                          43%|████▎     | 2763/6500 [18:18:06<30:18:31, 29.20s/it] 43%|████▎     | 2764/6500 [18:18:25<26:55:19, 25.94s/it]                                                          43%|████▎     | 2764/6500 [18:18:25<26:55:19, 25.94s/it] 43%|████▎     | 2765/6500 [18:18:43<24:27:43, 23.58s/it]                                                          43%|████▎     | 2765/6500 [18:18:43<24:27:43, 23.58s/it] 43%|████▎     | 2766/6500 [18:19:01<2{'loss': 0.315, 'learning_rate': 6.161702246729692e-05, 'epoch': 0.43}
{'loss': 0.2932, 'learning_rate': 6.159350909548475e-05, 'epoch': 0.43}
{'loss': 0.8221, 'learning_rate': 6.156999301376031e-05, 'epoch': 0.43}
{'loss': 0.3041, 'learning_rate': 6.154647422762033e-05, 'epoch': 0.43}
{'loss': 0.3081, 'learning_rate': 6.152295274256222e-05, 'epoch': 0.43}
2:45:15, 21.94s/it]                                                          43%|████▎     | 2766/6500 [18:19:01<22:45:15, 21.94s/it] 43%|████▎     | 2767/6500 [18:19:19<21:33:02, 20.78s/it]                                                          43%|████▎     | 2767/6500 [18:19:19<21:33:02, 20.78s/it] 43%|████▎     | 2768/6500 [18:19:37<20:42:40, 19.98s/it]                                                          43%|████▎     | 2768/6500 [18:19:37<20:42:40, 19.98s/it] 43%|████▎     | 2769/6500 [18:19:55<20:08:01, 19.43s/it]                                                          43%|████▎     | 2769/6500 [18:19:55<20:08:01, 19.43s/it] 43%|████▎     | 2770/6500 [18:20:14<19:52:33, 19.18s/it]                                                          43%|████▎     | 2770/6500 [18:20:14<19:52:33, 19.18s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8311113119125366, 'eval_runtime': 5.357, 'eval_samples_per_second': 4.293, 'eval_steps_per_second': 1.12, 'epoch': 0.43}
                                                          43%|████▎     | 2770/6500 [18:20:19<19:52:33, 19.18s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2770
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2770/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2770

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2770

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2770
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2770/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2895, 'learning_rate': 6.149942856408396e-05, 'epoch': 0.43}
{'loss': 0.2922, 'learning_rate': 6.147590169768419e-05, 'epoch': 0.43}
{'loss': 0.315, 'learning_rate': 6.145237214886219e-05, 'epoch': 0.43}
{'loss': 0.2818, 'learning_rate': 6.142883992311781e-05, 'epoch': 0.43}
{'loss': 0.3031, 'learning_rate': 6.14053050259516e-05, 'epoch': 0.43}
 43%|████▎     | 2771/6500 [18:21:22<35:01:02, 33.81s/it]                                                          43%|████▎     | 2771/6500 [18:21:22<35:01:02, 33.81s/it] 43%|████▎     | 2772/6500 [18:21:40<30:05:47, 29.06s/it]                                                          43%|████▎     | 2772/6500 [18:21:40<30:05:47, 29.06s/it] 43%|████▎     | 2773/6500 [18:21:58<26:39:17, 25.75s/it]                                                          43%|████▎     | 2773/6500 [18:21:58<26:39:17, 25.75s/it] 43%|████▎     | 2774/6500 [18:22:16<24:14:45, 23.43s/it]                                                          43%|████▎     | 2774/6500 [18:22:16<24:14:45, 23.43s/it] 43%|████▎     | 2775/6500 [18:22:34<22:34:32, 21.82s/it]                                                          43%|████▎     | 2775/6500 [18:22:34<22:34:32, 21.82s/it] 43%|████▎     | 2776/6500 [18:22:52<2{'loss': 0.2975, 'learning_rate': 6.138176746286468e-05, 'epoch': 0.43}
{'loss': 0.2925, 'learning_rate': 6.135822723935882e-05, 'epoch': 0.43}
{'loss': 0.3064, 'learning_rate': 6.13346843609364e-05, 'epoch': 0.43}
{'loss': 0.295, 'learning_rate': 6.131113883310041e-05, 'epoch': 0.43}
{'loss': 0.3031, 'learning_rate': 6.128759066135451e-05, 'epoch': 0.43}
1:24:24, 20.69s/it]                                                          43%|████▎     | 2776/6500 [18:22:52<21:24:24, 20.69s/it] 43%|████▎     | 2777/6500 [18:23:10<20:35:51, 19.92s/it]                                                          43%|████▎     | 2777/6500 [18:23:10<20:35:51, 19.92s/it] 43%|████▎     | 2778/6500 [18:23:28<20:01:53, 19.37s/it]                                                          43%|████▎     | 2778/6500 [18:23:28<20:01:53, 19.37s/it] 43%|████▎     | 2779/6500 [18:23:46<19:40:12, 19.03s/it]                                                          43%|████▎     | 2779/6500 [18:23:46<19:40:12, 19.03s/it] 43%|████▎     | 2780/6500 [18:24:05<19:30:56, 18.89s/it]                                                          43%|████▎     | 2780/6500 [18:24:05<19:30:56, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.83010333776474, 'eval_runtime': 5.4832, 'eval_samples_per_second': 4.195, 'eval_steps_per_second': 1.094, 'epoch': 0.43}
                                                          43%|████▎     | 2780/6500 [18:24:10<19:30:56, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2780
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2780
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2780
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2780

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2780
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2780/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2780/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2780/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3007, 'learning_rate': 6.126403985120292e-05, 'epoch': 0.43}
{'loss': 0.3154, 'learning_rate': 6.12404864081505e-05, 'epoch': 0.43}
{'loss': 0.2804, 'learning_rate': 6.121693033770274e-05, 'epoch': 0.43}
{'loss': 0.2966, 'learning_rate': 6.11933716453657e-05, 'epoch': 0.43}
{'loss': 0.311, 'learning_rate': 6.116981033664609e-05, 'epoch': 0.43}
 43%|████▎     | 2781/6500 [18:25:33<40:58:48, 39.67s/it]                                                          43%|████▎     | 2781/6500 [18:25:33<40:58:48, 39.67s/it] 43%|████▎     | 2782/6500 [18:25:51<34:16:08, 33.18s/it]                                                          43%|████▎     | 2782/6500 [18:25:51<34:16:08, 33.18s/it] 43%|████▎     | 2783/6500 [18:26:09<29:34:40, 28.65s/it]                                                          43%|████▎     | 2783/6500 [18:26:09<29:34:40, 28.65s/it] 43%|████▎     | 2784/6500 [18:26:27<26:16:50, 25.46s/it]                                                          43%|████▎     | 2784/6500 [18:26:27<26:16:50, 25.46s/it] 43%|████▎     | 2785/6500 [18:26:45<23:59:10, 23.24s/it]                                                          43%|████▎     | 2785/6500 [18:26:45<23:59:10, 23.24s/it] 43%|████▎     | 2786/6500 [18:27:03<2{'loss': 0.2875, 'learning_rate': 6.114624641705122e-05, 'epoch': 0.43}
{'loss': 0.3095, 'learning_rate': 6.112267989208904e-05, 'epoch': 0.43}
{'loss': 0.3023, 'learning_rate': 6.109911076726806e-05, 'epoch': 0.43}
{'loss': 0.3098, 'learning_rate': 6.107553904809741e-05, 'epoch': 0.43}
{'loss': 0.2917, 'learning_rate': 6.105196474008686e-05, 'epoch': 0.43}
2:22:58, 21.70s/it]                                                          43%|████▎     | 2786/6500 [18:27:03<22:22:58, 21.70s/it] 43%|████▎     | 2787/6500 [18:27:21<21:15:24, 20.61s/it]                                                          43%|████▎     | 2787/6500 [18:27:21<21:15:24, 20.61s/it] 43%|████▎     | 2788/6500 [18:27:40<20:28:47, 19.86s/it]                                                          43%|████▎     | 2788/6500 [18:27:40<20:28:47, 19.86s/it] 43%|████▎     | 2789/6500 [18:27:58<19:56:29, 19.35s/it]                                                          43%|████▎     | 2789/6500 [18:27:58<19:56:29, 19.35s/it] 43%|████▎     | 2790/6500 [18:28:16<19:33:54, 18.98s/it]                                                          43%|████▎     | 2790/6500 [18:28:16<19:33:54, 18.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.841331958770752, 'eval_runtime': 5.3368, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.43}
                                                          43%|████▎     | 2790/6500 [18:28:21<19:33:54, 18.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2790
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2790
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2790
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2790

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2790
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2790/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2949, 'learning_rate': 6.1028387848746737e-05, 'epoch': 0.43}
{'loss': 0.2825, 'learning_rate': 6.100480837958802e-05, 'epoch': 0.43}
{'loss': 0.3737, 'learning_rate': 6.098122633812225e-05, 'epoch': 0.43}
{'loss': 0.3049, 'learning_rate': 6.095764172986159e-05, 'epoch': 0.43}
{'loss': 0.2863, 'learning_rate': 6.09340545603188e-05, 'epoch': 0.43}
 43%|████▎     | 2791/6500 [18:29:34<37:42:22, 36.60s/it]                                                          43%|████▎     | 2791/6500 [18:29:34<37:42:22, 36.60s/it] 43%|████▎     | 2792/6500 [18:29:52<31:57:23, 31.03s/it]                                                          43%|████▎     | 2792/6500 [18:29:52<31:57:23, 31.03s/it] 43%|████▎     | 2793/6500 [18:30:10<27:55:05, 27.11s/it]                                                          43%|████▎     | 2793/6500 [18:30:10<27:55:05, 27.11s/it] 43%|████▎     | 2794/6500 [18:30:28<25:06:31, 24.39s/it]                                                          43%|████▎     | 2794/6500 [18:30:28<25:06:31, 24.39s/it] 43%|████▎     | 2795/6500 [18:30:46<23:08:55, 22.49s/it]                                                          43%|████▎     | 2795/6500 [18:30:46<23:08:55, 22.49s/it] 43%|████▎     | 2796/6500 [18:31:04<2{'loss': 0.3102, 'learning_rate': 6.091046483500723e-05, 'epoch': 0.43}
{'loss': 0.8261, 'learning_rate': 6.0886872559440845e-05, 'epoch': 0.43}
{'loss': 0.3108, 'learning_rate': 6.086327773913419e-05, 'epoch': 0.43}
{'loss': 0.3076, 'learning_rate': 6.083968037960243e-05, 'epoch': 0.43}
{'loss': 0.3087, 'learning_rate': 6.081608048636127e-05, 'epoch': 0.43}
1:50:29, 21.23s/it]                                                          43%|████▎     | 2796/6500 [18:31:04<21:50:29, 21.23s/it] 43%|████▎     | 2797/6500 [18:31:22<20:51:21, 20.28s/it]                                                          43%|████▎     | 2797/6500 [18:31:22<20:51:21, 20.28s/it] 43%|████▎     | 2798/6500 [18:31:40<20:11:23, 19.63s/it]                                                          43%|████▎     | 2798/6500 [18:31:40<20:11:23, 19.63s/it] 43%|████▎     | 2799/6500 [18:31:58<19:43:27, 19.19s/it]                                                          43%|████▎     | 2799/6500 [18:31:58<19:43:27, 19.19s/it] 43%|████▎     | 2800/6500 [18:32:16<19:24:23, 18.88s/it]                                                          43%|████▎     | 2800/6500 [18:32:16<19:24:23, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8339803814888, 'eval_runtime': 5.8119, 'eval_samples_per_second': 3.957, 'eval_steps_per_second': 1.032, 'epoch': 0.43}
                                                          43%|████▎     | 2800/6500 [18:32:22<19:24:23, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2800
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2800
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2800 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2800

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2800
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2800/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.28, 'learning_rate': 6.079247806492707e-05, 'epoch': 0.43}
{'loss': 0.3073, 'learning_rate': 6.076887312081673e-05, 'epoch': 0.43}
{'loss': 0.2963, 'learning_rate': 6.074526565954778e-05, 'epoch': 0.43}
{'loss': 0.2749, 'learning_rate': 6.072165568663831e-05, 'epoch': 0.43}
{'loss': 0.3089, 'learning_rate': 6.069804320760703e-05, 'epoch': 0.43}
 43%|████▎     | 2801/6500 [18:33:18<32:33:58, 31.69s/it]                                                          43%|████▎     | 2801/6500 [18:33:18<32:33:58, 31.69s/it] 43%|████▎     | 2802/6500 [18:33:36<28:20:40, 27.59s/it]                                                          43%|████▎     | 2802/6500 [18:33:36<28:20:40, 27.59s/it] 43%|████▎     | 2803/6500 [18:33:54<25:23:34, 24.73s/it]                                                          43%|████▎     | 2803/6500 [18:33:54<25:23:34, 24.73s/it] 43%|████▎     | 2804/6500 [18:34:12<23:19:37, 22.72s/it]                                                          43%|████▎     | 2804/6500 [18:34:12<23:19:37, 22.72s/it] 43%|████▎     | 2805/6500 [18:34:30<21:53:15, 21.32s/it]                                                          43%|████▎     | 2805/6500 [18:34:30<21:53:15, 21.32s/it] 43%|████▎     | 2806/6500 [18:34:48<2{'loss': 0.2897, 'learning_rate': 6.067442822797318e-05, 'epoch': 0.43}
{'loss': 0.3052, 'learning_rate': 6.065081075325663e-05, 'epoch': 0.43}
{'loss': 0.2915, 'learning_rate': 6.0627190788977825e-05, 'epoch': 0.43}
{'loss': 0.3035, 'learning_rate': 6.060356834065779e-05, 'epoch': 0.43}
{'loss': 0.2886, 'learning_rate': 6.057994341381813e-05, 'epoch': 0.43}
0:52:57, 20.35s/it]                                                          43%|████▎     | 2806/6500 [18:34:48<20:52:57, 20.35s/it] 43%|████▎     | 2807/6500 [18:35:06<20:11:12, 19.68s/it]                                                          43%|████▎     | 2807/6500 [18:35:06<20:11:12, 19.68s/it] 43%|████▎     | 2808/6500 [18:35:24<19:42:01, 19.21s/it]                                                          43%|████▎     | 2808/6500 [18:35:24<19:42:01, 19.21s/it] 43%|████▎     | 2809/6500 [18:35:43<19:21:37, 18.88s/it]                                                          43%|████▎     | 2809/6500 [18:35:43<19:21:37, 18.88s/it] 43%|████▎     | 2810/6500 [18:36:01<19:07:28, 18.66s/it]                                                          43%|████▎     | 2810/6500 [18:36:01<19:07:28, 18.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8262129426002502, 'eval_runtime': 5.3281, 'eval_samples_per_second': 4.317, 'eval_steps_per_second': 1.126, 'epoch': 0.43}
                                                          43%|████▎     | 2810/6500 [18:36:06<19:07:28, 18.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2810
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2810
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2810
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2810

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2810
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2810/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2810/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3183, 'learning_rate': 6.055631601398103e-05, 'epoch': 0.43}
{'loss': 0.3036, 'learning_rate': 6.0532686146669236e-05, 'epoch': 0.43}
{'loss': 0.3062, 'learning_rate': 6.050905381740609e-05, 'epoch': 0.43}
{'loss': 0.3144, 'learning_rate': 6.0485419031715516e-05, 'epoch': 0.43}
{'loss': 0.3021, 'learning_rate': 6.0461781795122e-05, 'epoch': 0.43}
 43%|████▎     | 2811/6500 [18:37:56<48:40:41, 47.50s/it]                                                          43%|████▎     | 2811/6500 [18:37:56<48:40:41, 47.50s/it] 43%|████▎     | 2812/6500 [18:38:14<39:44:31, 38.79s/it]                                                          43%|████▎     | 2812/6500 [18:38:14<39:44:31, 38.79s/it] 43%|████▎     | 2813/6500 [18:38:32<33:19:37, 32.54s/it]                                                          43%|████▎     | 2813/6500 [18:38:32<33:19:37, 32.54s/it] 43%|████▎     | 2814/6500 [18:38:50<28:49:56, 28.16s/it]                                                          43%|████▎     | 2814/6500 [18:38:50<28:49:56, 28.16s/it] 43%|████▎     | 2815/6500 [18:39:08<25:41:37, 25.10s/it]                                                          43%|████▎     | 2815/6500 [18:39:08<25:41:37, 25.10s/it] 43%|████▎     | 2816/6500 [18:39:26<2{'loss': 0.3028, 'learning_rate': 6.04381421131506e-05, 'epoch': 0.43}
{'loss': 0.2959, 'learning_rate': 6.0414499991326934e-05, 'epoch': 0.43}
{'loss': 0.3128, 'learning_rate': 6.039085543517722e-05, 'epoch': 0.43}
{'loss': 0.2982, 'learning_rate': 6.036720845022823e-05, 'epoch': 0.43}
{'loss': 0.2915, 'learning_rate': 6.034355904200729e-05, 'epoch': 0.43}
3:30:29, 22.97s/it]                                                          43%|████▎     | 2816/6500 [18:39:26<23:30:29, 22.97s/it] 43%|████▎     | 2817/6500 [18:39:44<21:58:59, 21.49s/it]                                                          43%|████▎     | 2817/6500 [18:39:44<21:58:59, 21.49s/it] 43%|████▎     | 2818/6500 [18:40:02<20:55:17, 20.46s/it]                                                          43%|████▎     | 2818/6500 [18:40:02<20:55:17, 20.46s/it] 43%|████▎     | 2819/6500 [18:40:20<20:11:21, 19.75s/it]                                                          43%|████▎     | 2819/6500 [18:40:20<20:11:21, 19.75s/it] 43%|████▎     | 2820/6500 [18:40:38<19:40:45, 19.25s/it]                                                          43%|████▎     | 2820/6500 [18:40:38<19:40:45, 19.25s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.843960165977478, 'eval_runtime': 5.3424, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.43}
                                                          43%|████▎     | 2820/6500 [18:40:43<19:40:45, 19.25s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2820
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2820
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2820
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2820

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2820
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2820/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2884, 'learning_rate': 6.0319907216042324e-05, 'epoch': 0.43}
{'loss': 0.3107, 'learning_rate': 6.029625297786179e-05, 'epoch': 0.43}
{'loss': 0.3574, 'learning_rate': 6.0272596332994725e-05, 'epoch': 0.43}
{'loss': 0.3122, 'learning_rate': 6.024893728697072e-05, 'epoch': 0.43}
{'loss': 0.2936, 'learning_rate': 6.022527584531994e-05, 'epoch': 0.43}
 43%|████▎     | 2821/6500 [18:41:35<31:02:58, 30.38s/it]                                                          43%|████▎     | 2821/6500 [18:41:35<31:02:58, 30.38s/it] 43%|████▎     | 2822/6500 [18:41:53<27:14:50, 26.67s/it]                                                          43%|████▎     | 2822/6500 [18:41:53<27:14:50, 26.67s/it] 43%|████▎     | 2823/6500 [18:42:11<24:35:58, 24.08s/it]                                                          43%|████▎     | 2823/6500 [18:42:11<24:35:58, 24.08s/it] 43%|████▎     | 2824/6500 [18:42:29<22:44:09, 22.27s/it]                                                          43%|████▎     | 2824/6500 [18:42:29<22:44:09, 22.27s/it] 43%|████▎     | 2825/6500 [18:42:47<21:26:06, 21.00s/it]                                                          43%|████▎     | 2825/6500 [18:42:47<21:26:06, 21.00s/it] 43%|████▎     | 2826/6500 [18:43:05<2{'loss': 0.3062, 'learning_rate': 6.0201612013573116e-05, 'epoch': 0.43}
{'loss': 0.8294, 'learning_rate': 6.017794579726149e-05, 'epoch': 0.43}
{'loss': 0.315, 'learning_rate': 6.015427720191693e-05, 'epoch': 0.44}
{'loss': 0.2925, 'learning_rate': 6.013060623307181e-05, 'epoch': 0.44}
{'loss': 0.2898, 'learning_rate': 6.010693289625907e-05, 'epoch': 0.44}
0:32:17, 20.12s/it]                                                          43%|████▎     | 2826/6500 [18:43:05<20:32:17, 20.12s/it] 43%|████▎     | 2827/6500 [18:43:23<19:54:02, 19.51s/it]                                                          43%|████▎     | 2827/6500 [18:43:23<19:54:02, 19.51s/it] 44%|████▎     | 2828/6500 [18:43:41<19:28:04, 19.09s/it]                                                          44%|████▎     | 2828/6500 [18:43:41<19:28:04, 19.09s/it] 44%|████▎     | 2829/6500 [18:43:59<19:19:07, 18.95s/it]                                                          44%|████▎     | 2829/6500 [18:43:59<19:19:07, 18.95s/it] 44%|████▎     | 2830/6500 [18:44:18<19:04:13, 18.71s/it]                                                          44%|████▎     | 2830/6500 [18:44:18<19:04:13, 18.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8366373181343079, 'eval_runtime': 5.6096, 'eval_samples_per_second': 4.1, 'eval_steps_per_second': 1.07, 'epoch': 0.44}
                                                          44%|████▎     | 2830/6500 [18:44:23<19:04:13, 18.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2830
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2830/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2830

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2830

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2830
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2830/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2829, 'learning_rate': 6.0083257197012224e-05, 'epoch': 0.44}
{'loss': 0.3137, 'learning_rate': 6.005957914086533e-05, 'epoch': 0.44}
{'loss': 0.2791, 'learning_rate': 6.003589873335296e-05, 'epoch': 0.44}
{'loss': 0.2902, 'learning_rate': 6.001221598001028e-05, 'epoch': 0.44}
{'loss': 0.2927, 'learning_rate': 5.9988530886372985e-05, 'epoch': 0.44}
 44%|████▎     | 2831/6500 [18:45:15<30:52:56, 30.30s/it]                                                          44%|████▎     | 2831/6500 [18:45:15<30:52:56, 30.30s/it] 44%|████▎     | 2832/6500 [18:45:33<27:06:59, 26.61s/it]                                                          44%|████▎     | 2832/6500 [18:45:33<27:06:59, 26.61s/it] 44%|████▎     | 2833/6500 [18:45:51<24:29:19, 24.04s/it]                                                          44%|████▎     | 2833/6500 [18:45:51<24:29:19, 24.04s/it] 44%|████▎     | 2834/6500 [18:46:09<22:39:41, 22.25s/it]                                                          44%|████▎     | 2834/6500 [18:46:09<22:39:41, 22.25s/it] 44%|████▎     | 2835/6500 [18:46:27<21:22:49, 21.00s/it]                                                          44%|████▎     | 2835/6500 [18:46:27<21:22:49, 21.00s/it] 44%|████▎     | 2836/6500 [18:46:46<2{'loss': 0.2974, 'learning_rate': 5.996484345797733e-05, 'epoch': 0.44}
{'loss': 0.2963, 'learning_rate': 5.994115370036011e-05, 'epoch': 0.44}
{'loss': 0.2869, 'learning_rate': 5.991746161905865e-05, 'epoch': 0.44}
{'loss': 0.3122, 'learning_rate': 5.9893767219610844e-05, 'epoch': 0.44}
{'loss': 0.2936, 'learning_rate': 5.9870070507555084e-05, 'epoch': 0.44}
0:34:32, 20.22s/it]                                                          44%|████▎     | 2836/6500 [18:46:46<20:34:32, 20.22s/it] 44%|████▎     | 2837/6500 [18:47:04<19:56:02, 19.59s/it]                                                          44%|████▎     | 2837/6500 [18:47:04<19:56:02, 19.59s/it] 44%|████▎     | 2838/6500 [18:47:22<19:28:46, 19.15s/it]                                                          44%|████▎     | 2838/6500 [18:47:22<19:28:46, 19.15s/it] 44%|████▎     | 2839/6500 [18:47:40<19:09:51, 18.84s/it]                                                          44%|████▎     | 2839/6500 [18:47:40<19:09:51, 18.84s/it] 44%|████▎     | 2840/6500 [18:47:58<18:56:26, 18.63s/it]                                                          44%|████▎     | 2840/6500 [18:47:58<18:56:26, 18.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8302338719367981, 'eval_runtime': 6.8203, 'eval_samples_per_second': 3.372, 'eval_steps_per_second': 0.88, 'epoch': 0.44}
                                                          44%|████▎     | 2840/6500 [18:48:05<18:56:26, 18.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2840
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2840/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2840
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2840/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2840/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3064, 'learning_rate': 5.984637148843037e-05, 'epoch': 0.44}
{'loss': 0.2959, 'learning_rate': 5.9822670167776183e-05, 'epoch': 0.44}
{'loss': 0.2915, 'learning_rate': 5.979896655113259e-05, 'epoch': 0.44}
{'loss': 0.3023, 'learning_rate': 5.977526064404012e-05, 'epoch': 0.44}
{'loss': 0.2983, 'learning_rate': 5.9751552452039916e-05, 'epoch': 0.44}
 44%|████▎     | 2841/6500 [18:49:27<40:25:46, 39.78s/it]                                                          44%|████▎     | 2841/6500 [18:49:27<40:25:46, 39.78s/it] 44%|████▎     | 2842/6500 [18:49:45<33:45:51, 33.23s/it]                                                          44%|████▎     | 2842/6500 [18:49:45<33:45:51, 33.23s/it] 44%|████▎     | 2843/6500 [18:50:03<29:05:51, 28.64s/it]                                                          44%|████▎     | 2843/6500 [18:50:03<29:05:51, 28.64s/it] 44%|████▍     | 2844/6500 [18:50:21<25:49:59, 25.44s/it]                                                          44%|████▍     | 2844/6500 [18:50:21<25:49:59, 25.44s/it] 44%|████▍     | 2845/6500 [18:50:39<23:40:04, 23.31s/it]                                                          44%|████▍     | 2845/6500 [18:50:39<23:40:04, 23.31s/it] 44%|████▍     | 2846/6500 [18:50:57<2{'loss': 0.2966, 'learning_rate': 5.9727841980673604e-05, 'epoch': 0.44}
{'loss': 0.2998, 'learning_rate': 5.970412923548339e-05, 'epoch': 0.44}
{'loss': 0.3205, 'learning_rate': 5.9680414222011974e-05, 'epoch': 0.44}
{'loss': 0.2811, 'learning_rate': 5.965669694580258e-05, 'epoch': 0.44}
{'loss': 0.2936, 'learning_rate': 5.9632977412399e-05, 'epoch': 0.44}
2:03:06, 21.73s/it]                                                          44%|████▍     | 2846/6500 [18:50:57<22:03:06, 21.73s/it] 44%|████▍     | 2847/6500 [18:51:16<20:55:29, 20.62s/it]                                                          44%|████▍     | 2847/6500 [18:51:16<20:55:29, 20.62s/it] 44%|████▍     | 2848/6500 [18:51:34<20:08:28, 19.85s/it]                                                          44%|████▍     | 2848/6500 [18:51:34<20:08:28, 19.85s/it] 44%|████▍     | 2849/6500 [18:51:52<19:36:01, 19.33s/it]                                                          44%|████▍     | 2849/6500 [18:51:52<19:36:01, 19.33s/it] 44%|████▍     | 2850/6500 [18:52:10<19:16:44, 19.02s/it]                                                          44%|████▍     | 2850/6500 [18:52:10<19:16:44, 19.02s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.843349814414978, 'eval_runtime': 5.4786, 'eval_samples_per_second': 4.198, 'eval_steps_per_second': 1.095, 'epoch': 0.44}
                                                          44%|████▍     | 2850/6500 [18:52:15<19:16:44, 19.02s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2850
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2850I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2850
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2850

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2850
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2850/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2850/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2812, 'learning_rate': 5.9609255627345495e-05, 'epoch': 0.44}
{'loss': 0.3276, 'learning_rate': 5.958553159618693e-05, 'epoch': 0.44}
{'loss': 0.3497, 'learning_rate': 5.956180532446863e-05, 'epoch': 0.44}
{'loss': 0.2885, 'learning_rate': 5.953807681773649e-05, 'epoch': 0.44}
{'loss': 0.3059, 'learning_rate': 5.9514346081536855e-05, 'epoch': 0.44}
 44%|████▍     | 2851/6500 [18:53:17<33:51:52, 33.41s/it]                                                          44%|████▍     | 2851/6500 [18:53:17<33:51:52, 33.41s/it] 44%|████▍     | 2852/6500 [18:53:35<29:10:33, 28.79s/it]                                                          44%|████▍     | 2852/6500 [18:53:35<29:10:33, 28.79s/it] 44%|████▍     | 2853/6500 [18:53:53<25:54:19, 25.57s/it]                                                          44%|████▍     | 2853/6500 [18:53:53<25:54:19, 25.57s/it] 44%|████▍     | 2854/6500 [18:54:11<23:38:46, 23.35s/it]                                                          44%|████▍     | 2854/6500 [18:54:11<23:38:46, 23.35s/it] 44%|████▍     | 2855/6500 [18:54:29<22:02:29, 21.77s/it]                                                          44%|████▍     | 2855/6500 [18:54:29<22:02:29, 21.77s/it] 44%|████▍     | 2856/6500 [18:54:47<2{'loss': 0.295, 'learning_rate': 5.949061312141668e-05, 'epoch': 0.44}
{'loss': 0.822, 'learning_rate': 5.946687794292341e-05, 'epoch': 0.44}
{'loss': 0.2969, 'learning_rate': 5.944314055160497e-05, 'epoch': 0.44}
{'loss': 0.3, 'learning_rate': 5.941940095300984e-05, 'epoch': 0.44}
{'loss': 0.2815, 'learning_rate': 5.939565915268701e-05, 'epoch': 0.44}
0:54:36, 20.66s/it]                                                          44%|████▍     | 2856/6500 [18:54:47<20:54:36, 20.66s/it] 44%|████▍     | 2857/6500 [18:55:05<20:07:23, 19.89s/it]                                                          44%|████▍     | 2857/6500 [18:55:05<20:07:23, 19.89s/it] 44%|████▍     | 2858/6500 [18:55:24<19:34:55, 19.36s/it]                                                          44%|████▍     | 2858/6500 [18:55:24<19:34:55, 19.36s/it] 44%|████▍     | 2859/6500 [18:55:42<19:12:21, 18.99s/it]                                                          44%|████▍     | 2859/6500 [18:55:42<19:12:21, 18.99s/it] 44%|████▍     | 2860/6500 [18:56:00<18:56:25, 18.73s/it]                                                          44%|████▍     | 2860/6500 [18:56:00<18:56:25, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8333229422569275, 'eval_runtime': 5.3448, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.123, 'epoch': 0.44}
                                                          44%|████▍     | 2860/6500 [18:56:05<18:56:25, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2860
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2860

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2860
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2860/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2860/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3034, 'learning_rate': 5.937191515618598e-05, 'epoch': 0.44}
{'loss': 0.2916, 'learning_rate': 5.934816896905676e-05, 'epoch': 0.44}
{'loss': 0.2698, 'learning_rate': 5.9324420596849886e-05, 'epoch': 0.44}
{'loss': 0.292, 'learning_rate': 5.93006700451164e-05, 'epoch': 0.44}
{'loss': 0.2841, 'learning_rate': 5.927691731940783e-05, 'epoch': 0.44}
 44%|████▍     | 2861/6500 [18:57:12<34:59:57, 34.62s/it]                                                          44%|████▍     | 2861/6500 [18:57:12<34:59:57, 34.62s/it] 44%|████▍     | 2862/6500 [18:57:30<29:56:56, 29.64s/it]                                                          44%|████▍     | 2862/6500 [18:57:30<29:56:56, 29.64s/it] 44%|████▍     | 2863/6500 [18:57:47<26:24:41, 26.14s/it]                                                          44%|████▍     | 2863/6500 [18:57:48<26:24:41, 26.14s/it] 44%|████▍     | 2864/6500 [18:58:06<23:57:47, 23.73s/it]                                                          44%|████▍     | 2864/6500 [18:58:06<23:57:47, 23.73s/it] 44%|████▍     | 2865/6500 [18:58:24<22:13:57, 22.02s/it]                                                          44%|████▍     | 2865/6500 [18:58:24<22:13:57, 22.02s/it] 44%|████▍     | 2866/6500 [18:58:42<2{'loss': 0.2932, 'learning_rate': 5.925316242527623e-05, 'epoch': 0.44}
{'loss': 0.2977, 'learning_rate': 5.922940536827419e-05, 'epoch': 0.44}
{'loss': 0.3016, 'learning_rate': 5.920564615395475e-05, 'epoch': 0.44}
{'loss': 0.2948, 'learning_rate': 5.91818847878715e-05, 'epoch': 0.44}
{'loss': 0.2843, 'learning_rate': 5.915812127557851e-05, 'epoch': 0.44}
1:14:19, 21.04s/it]                                                          44%|████▍     | 2866/6500 [18:58:42<21:14:19, 21.04s/it] 44%|████▍     | 2867/6500 [18:59:00<20:20:23, 20.15s/it]                                                          44%|████▍     | 2867/6500 [18:59:00<20:20:23, 20.15s/it] 44%|████▍     | 2868/6500 [18:59:19<19:42:59, 19.54s/it]                                                          44%|████▍     | 2868/6500 [18:59:19<19:42:59, 19.54s/it] 44%|████▍     | 2869/6500 [18:59:37<19:16:46, 19.11s/it]                                                          44%|████▍     | 2869/6500 [18:59:37<19:16:46, 19.11s/it] 44%|████▍     | 2870/6500 [18:59:55<18:58:46, 18.82s/it]                                                          44%|████▍     | 2870/6500 [18:59:55<18:58:46, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8316105604171753, 'eval_runtime': 5.7604, 'eval_samples_per_second': 3.993, 'eval_steps_per_second': 1.042, 'epoch': 0.44}
                                                          44%|████▍     | 2870/6500 [19:00:01<18:58:46, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2870
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2870

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2870
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2870

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2870
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2870/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2870/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3158, 'learning_rate': 5.9134355622630356e-05, 'epoch': 0.44}
{'loss': 0.2807, 'learning_rate': 5.91105878345821e-05, 'epoch': 0.44}
{'loss': 0.3016, 'learning_rate': 5.9086817916989335e-05, 'epoch': 0.44}
{'loss': 0.3121, 'learning_rate': 5.906304587540813e-05, 'epoch': 0.44}
{'loss': 0.2849, 'learning_rate': 5.903927171539507e-05, 'epoch': 0.44}
 44%|████▍     | 2871/6500 [19:01:37<44:04:14, 43.72s/it]                                                          44%|████▍     | 2871/6500 [19:01:37<44:04:14, 43.72s/it] 44%|████▍     | 2872/6500 [19:01:55<36:16:14, 35.99s/it]                                                          44%|████▍     | 2872/6500 [19:01:55<36:16:14, 35.99s/it] 44%|████▍     | 2873/6500 [19:02:13<30:48:41, 30.58s/it]                                                          44%|████▍     | 2873/6500 [19:02:13<30:48:41, 30.58s/it] 44%|████▍     | 2874/6500 [19:02:31<26:59:22, 26.80s/it]                                                          44%|████▍     | 2874/6500 [19:02:31<26:59:22, 26.80s/it] 44%|████▍     | 2875/6500 [19:02:49<24:19:53, 24.16s/it]                                                          44%|████▍     | 2875/6500 [19:02:49<24:19:53, 24.16s/it] 44%|████▍     | 2876/6500 [19:03:07<2{'loss': 0.2996, 'learning_rate': 5.9015495442507194e-05, 'epoch': 0.44}
{'loss': 0.3086, 'learning_rate': 5.899171706230208e-05, 'epoch': 0.44}
{'loss': 0.2939, 'learning_rate': 5.896793658033776e-05, 'epoch': 0.44}
{'loss': 0.278, 'learning_rate': 5.89441540021728e-05, 'epoch': 0.44}
{'loss': 0.2835, 'learning_rate': 5.892036933336622e-05, 'epoch': 0.44}
2:28:59, 22.33s/it]                                                          44%|████▍     | 2876/6500 [19:03:07<22:28:59, 22.33s/it] 44%|████▍     | 2877/6500 [19:03:25<21:17:48, 21.16s/it]                                                          44%|████▍     | 2877/6500 [19:03:25<21:17:48, 21.16s/it] 44%|████▍     | 2878/6500 [19:03:43<20:21:50, 20.24s/it]                                                          44%|████▍     | 2878/6500 [19:03:43<20:21:50, 20.24s/it] 44%|████▍     | 2879/6500 [19:04:01<19:43:04, 19.60s/it]                                                          44%|████▍     | 2879/6500 [19:04:01<19:43:04, 19.60s/it] 44%|████▍     | 2880/6500 [19:04:19<19:16:15, 19.16s/it]                                                          44%|████▍     | 2880/6500 [19:04:19<19:16:15, 19.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8473964333534241, 'eval_runtime': 6.1816, 'eval_samples_per_second': 3.721, 'eval_steps_per_second': 0.971, 'epoch': 0.44}
                                                          44%|████▍     | 2880/6500 [19:04:26<19:16:15, 19.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2880
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2880
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2880 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2880

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2880
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2880/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2880/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2754, 'learning_rate': 5.889658257947755e-05, 'epoch': 0.44}
{'loss': 0.3802, 'learning_rate': 5.887279374606679e-05, 'epoch': 0.44}
{'loss': 0.2867, 'learning_rate': 5.884900283869445e-05, 'epoch': 0.44}
{'loss': 0.281, 'learning_rate': 5.882520986292148e-05, 'epoch': 0.44}
{'loss': 0.3061, 'learning_rate': 5.8801414824309365e-05, 'epoch': 0.44}
 44%|████▍     | 2881/6500 [19:06:15<48:26:57, 48.20s/it]                                                          44%|████▍     | 2881/6500 [19:06:15<48:26:57, 48.20s/it] 44%|████▍     | 2882/6500 [19:06:33<39:19:05, 39.12s/it]                                                          44%|████▍     | 2882/6500 [19:06:33<39:19:05, 39.12s/it] 44%|████▍     | 2883/6500 [19:06:51<32:55:34, 32.77s/it]                                                          44%|████▍     | 2883/6500 [19:06:51<32:55:34, 32.77s/it] 44%|████▍     | 2884/6500 [19:07:09<28:27:11, 28.33s/it]                                                          44%|████▍     | 2884/6500 [19:07:09<28:27:11, 28.33s/it] 44%|████▍     | 2885/6500 [19:07:27<25:20:00, 25.23s/it]                                                          44%|████▍     | 2885/6500 [19:07:27<25:20:00, 25.23s/it] 44%|████▍     | 2886/6500 [19:07:45<2{'loss': 0.8163, 'learning_rate': 5.8777617728420075e-05, 'epoch': 0.44}
{'loss': 0.3005, 'learning_rate': 5.875381858081599e-05, 'epoch': 0.44}
{'loss': 0.289, 'learning_rate': 5.8730017387060035e-05, 'epoch': 0.44}
{'loss': 0.3062, 'learning_rate': 5.870621415271559e-05, 'epoch': 0.44}
{'loss': 0.2794, 'learning_rate': 5.868240888334653e-05, 'epoch': 0.44}
3:09:02, 23.06s/it]                                                          44%|████▍     | 2886/6500 [19:07:45<23:09:02, 23.06s/it] 44%|████▍     | 2887/6500 [19:08:03<21:38:17, 21.56s/it]                                                          44%|████▍     | 2887/6500 [19:08:03<21:38:17, 21.56s/it] 44%|████▍     | 2888/6500 [19:08:21<20:35:06, 20.52s/it]                                                          44%|████▍     | 2888/6500 [19:08:21<20:35:06, 20.52s/it] 44%|████▍     | 2889/6500 [19:08:39<19:51:07, 19.79s/it]                                                          44%|████▍     | 2889/6500 [19:08:39<19:51:07, 19.79s/it] 44%|████▍     | 2890/6500 [19:08:58<19:20:30, 19.29s/it]                                                          44%|████▍     | 2890/6500 [19:08:58<19:20:30, 19.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.837075412273407, 'eval_runtime': 5.3531, 'eval_samples_per_second': 4.297, 'eval_steps_per_second': 1.121, 'epoch': 0.44}
                                                          44%|████▍     | 2890/6500 [19:09:03<19:20:30, 19.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2890
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2890/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2890
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2890/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3065, 'learning_rate': 5.865860158451717e-05, 'epoch': 0.44}
{'loss': 0.2758, 'learning_rate': 5.863479226179236e-05, 'epoch': 0.44}
{'loss': 0.2802, 'learning_rate': 5.861098092073733e-05, 'epoch': 0.45}
{'loss': 0.2963, 'learning_rate': 5.8587167566917874e-05, 'epoch': 0.45}
{'loss': 0.2946, 'learning_rate': 5.856335220590022e-05, 'epoch': 0.45}
 44%|████▍     | 2891/6500 [19:10:14<36:32:08, 36.44s/it]                                                          44%|████▍     | 2891/6500 [19:10:14<36:32:08, 36.44s/it] 44%|████▍     | 2892/6500 [19:10:32<30:59:04, 30.92s/it]                                                          44%|████▍     | 2892/6500 [19:10:32<30:59:04, 30.92s/it] 45%|████▍     | 2893/6500 [19:10:51<27:16:47, 27.23s/it]                                                          45%|████▍     | 2893/6500 [19:10:51<27:16:47, 27.23s/it] 45%|████▍     | 2894/6500 [19:11:09<24:35:41, 24.55s/it]                                                          45%|████▍     | 2894/6500 [19:11:09<24:35:41, 24.55s/it] 45%|████▍     | 2895/6500 [19:11:27<22:38:53, 22.62s/it]                                                          45%|████▍     | 2895/6500 [19:11:27<22:38:53, 22.62s/it] 45%|████▍     | 2896/6500 [19:11:45<2{'loss': 0.3087, 'learning_rate': 5.8539534843251064e-05, 'epoch': 0.45}
{'loss': 0.2973, 'learning_rate': 5.8515715484537534e-05, 'epoch': 0.45}
{'loss': 0.3106, 'learning_rate': 5.849189413532731e-05, 'epoch': 0.45}
{'loss': 0.2876, 'learning_rate': 5.846807080118845e-05, 'epoch': 0.45}
{'loss': 0.3111, 'learning_rate': 5.844424548768952e-05, 'epoch': 0.45}
1:17:23, 21.27s/it]                                                          45%|████▍     | 2896/6500 [19:11:45<21:17:23, 21.27s/it] 45%|████▍     | 2897/6500 [19:12:03<20:20:47, 20.33s/it]                                                          45%|████▍     | 2897/6500 [19:12:03<20:20:47, 20.33s/it] 45%|████▍     | 2898/6500 [19:12:22<19:44:01, 19.72s/it]                                                          45%|████▍     | 2898/6500 [19:12:22<19:44:01, 19.72s/it] 45%|████▍     | 2899/6500 [19:12:40<19:15:52, 19.26s/it]                                                          45%|████▍     | 2899/6500 [19:12:40<19:15:52, 19.26s/it] 45%|████▍     | 2900/6500 [19:12:58<18:56:04, 18.93s/it]                                                          45%|████▍     | 2900/6500 [19:12:58<18:56:04, 18.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8318206071853638, 'eval_runtime': 5.3488, 'eval_samples_per_second': 4.3, 'eval_steps_per_second': 1.122, 'epoch': 0.45}
                                                          45%|████▍     | 2900/6500 [19:13:03<18:56:04, 18.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2900
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2900
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2900the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2900
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2900/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2900/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2996, 'learning_rate': 5.842041820039956e-05, 'epoch': 0.45}
{'loss': 0.2927, 'learning_rate': 5.8396588944888044e-05, 'epoch': 0.45}
{'loss': 0.3062, 'learning_rate': 5.8372757726724914e-05, 'epoch': 0.45}
{'loss': 0.3064, 'learning_rate': 5.8348924551480565e-05, 'epoch': 0.45}
{'loss': 0.3031, 'learning_rate': 5.8325089424725865e-05, 'epoch': 0.45}
 45%|████▍     | 2901/6500 [19:14:12<35:18:53, 35.32s/it]                                                          45%|████▍     | 2901/6500 [19:14:12<35:18:53, 35.32s/it] 45%|████▍     | 2902/6500 [19:14:30<30:06:09, 30.12s/it]                                                          45%|████▍     | 2902/6500 [19:14:30<30:06:09, 30.12s/it] 45%|████▍     | 2903/6500 [19:14:48<26:27:17, 26.48s/it]                                                          45%|████▍     | 2903/6500 [19:14:48<26:27:17, 26.48s/it] 45%|████▍     | 2904/6500 [19:15:06<23:54:59, 23.94s/it]                                                          45%|████▍     | 2904/6500 [19:15:06<23:54:59, 23.94s/it] 45%|████▍     | 2905/6500 [19:15:24<22:08:31, 22.17s/it]                                                          45%|████▍     | 2905/6500 [19:15:24<22:08:31, 22.17s/it] 45%|████▍     | 2906/6500 [19:15:42<2{'loss': 0.2856, 'learning_rate': 5.830125235203213e-05, 'epoch': 0.45}
{'loss': 0.3168, 'learning_rate': 5.8277413338971135e-05, 'epoch': 0.45}
{'loss': 0.2879, 'learning_rate': 5.825357239111511e-05, 'epoch': 0.45}
{'loss': 0.2887, 'learning_rate': 5.8229729514036705e-05, 'epoch': 0.45}
{'loss': 0.2811, 'learning_rate': 5.820588471330906e-05, 'epoch': 0.45}
0:53:57, 20.93s/it]                                                          45%|████▍     | 2906/6500 [19:15:42<20:53:57, 20.93s/it] 45%|████▍     | 2907/6500 [19:16:00<20:02:07, 20.07s/it]                                                          45%|████▍     | 2907/6500 [19:16:00<20:02:07, 20.07s/it] 45%|████▍     | 2908/6500 [19:16:18<19:26:18, 19.48s/it]                                                          45%|████▍     | 2908/6500 [19:16:18<19:26:18, 19.48s/it] 45%|████▍     | 2909/6500 [19:16:36<19:05:55, 19.15s/it]                                                          45%|████▍     | 2909/6500 [19:16:36<19:05:55, 19.15s/it] 45%|████▍     | 2910/6500 [19:16:54<18:47:48, 18.85s/it]                                                          45%|████▍     | 2910/6500 [19:16:54<18:47:48, 18.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8440751433372498, 'eval_runtime': 5.3427, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.45}
                                                          45%|████▍     | 2910/6500 [19:17:00<18:47:48, 18.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2910
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2910the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2910
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2910

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2910
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2910/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2910/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3098, 'learning_rate': 5.818203799450577e-05, 'epoch': 0.45}
{'loss': 0.3539, 'learning_rate': 5.8158189363200834e-05, 'epoch': 0.45}
{'loss': 0.2802, 'learning_rate': 5.813433882496875e-05, 'epoch': 0.45}
{'loss': 0.2927, 'learning_rate': 5.811048638538441e-05, 'epoch': 0.45}
{'loss': 0.3027, 'learning_rate': 5.808663205002319e-05, 'epoch': 0.45}
 45%|████▍     | 2911/6500 [19:18:40<44:54:03, 45.04s/it]                                                          45%|████▍     | 2911/6500 [19:18:41<44:54:03, 45.04s/it] 45%|████▍     | 2912/6500 [19:18:58<36:48:29, 36.93s/it]                                                          45%|████▍     | 2912/6500 [19:18:58<36:48:29, 36.93s/it] 45%|████▍     | 2913/6500 [19:19:16<31:07:34, 31.24s/it]                                                          45%|████▍     | 2913/6500 [19:19:16<31:07:34, 31.24s/it] 45%|████▍     | 2914/6500 [19:19:34<27:09:25, 27.26s/it]                                                          45%|████▍     | 2914/6500 [19:19:34<27:09:25, 27.26s/it] 45%|████▍     | 2915/6500 [19:19:52<24:23:19, 24.49s/it]                                                          45%|████▍     | 2915/6500 [19:19:52<24:23:19, 24.49s/it] 45%|████▍     | 2916/6500 [19:20:10<2{'loss': 0.8096, 'learning_rate': 5.80627758244609e-05, 'epoch': 0.45}
{'loss': 0.3096, 'learning_rate': 5.803891771427379e-05, 'epoch': 0.45}
{'loss': 0.2876, 'learning_rate': 5.8015057725038534e-05, 'epoch': 0.45}
{'loss': 0.2835, 'learning_rate': 5.799119586233228e-05, 'epoch': 0.45}
{'loss': 0.2802, 'learning_rate': 5.796733213173257e-05, 'epoch': 0.45}
2:26:58, 22.55s/it]                                                          45%|████▍     | 2916/6500 [19:20:10<22:26:58, 22.55s/it] 45%|████▍     | 2917/6500 [19:20:29<21:06:23, 21.21s/it]                                                          45%|████▍     | 2917/6500 [19:20:29<21:06:23, 21.21s/it] 45%|████▍     | 2918/6500 [19:20:47<20:10:00, 20.27s/it]                                                          45%|████▍     | 2918/6500 [19:20:47<20:10:00, 20.27s/it] 45%|████▍     | 2919/6500 [19:21:05<19:36:21, 19.71s/it]                                                          45%|████▍     | 2919/6500 [19:21:05<19:36:21, 19.71s/it] 45%|████▍     | 2920/6500 [19:21:23<19:08:01, 19.24s/it]                                                          45%|████▍     | 2920/6500 [19:21:23<19:08:01, 19.24s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8328397870063782, 'eval_runtime': 6.2415, 'eval_samples_per_second': 3.685, 'eval_steps_per_second': 0.961, 'epoch': 0.45}
                                                          45%|████▍     | 2920/6500 [19:21:29<19:08:01, 19.24s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2920
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2920/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2920
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2920/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2920/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3073, 'learning_rate': 5.7943466538817416e-05, 'epoch': 0.45}
{'loss': 0.2713, 'learning_rate': 5.791959908916526e-05, 'epoch': 0.45}
{'loss': 0.2817, 'learning_rate': 5.789572978835496e-05, 'epoch': 0.45}
{'loss': 0.2788, 'learning_rate': 5.787185864196584e-05, 'epoch': 0.45}
{'loss': 0.2876, 'learning_rate': 5.784798565557762e-05, 'epoch': 0.45}
 45%|████▍     | 2921/6500 [19:23:02<42:46:18, 43.02s/it]                                                          45%|████▍     | 2921/6500 [19:23:02<42:46:18, 43.02s/it] 45%|████▍     | 2922/6500 [19:23:20<35:21:01, 35.57s/it]                                                          45%|████▍     | 2922/6500 [19:23:20<35:21:01, 35.57s/it] 45%|████▍     | 2923/6500 [19:23:38<30:05:29, 30.29s/it]                                                          45%|████▍     | 2923/6500 [19:23:38<30:05:29, 30.29s/it] 45%|████▍     | 2924/6500 [19:23:56<26:26:03, 26.61s/it]                                                          45%|████▍     | 2924/6500 [19:23:56<26:26:03, 26.61s/it] 45%|████▌     | 2925/6500 [19:24:14<24:00:17, 24.17s/it]                                                          45%|████▌     | 2925/6500 [19:24:14<24:00:17, 24.17s/it] 45%|████▌     | 2926/6500 [19:24:32<2{'loss': 0.2967, 'learning_rate': 5.782411083477046e-05, 'epoch': 0.45}
{'loss': 0.2897, 'learning_rate': 5.780023418512497e-05, 'epoch': 0.45}
{'loss': 0.3117, 'learning_rate': 5.7776355712222166e-05, 'epoch': 0.45}
{'loss': 0.2977, 'learning_rate': 5.775247542164349e-05, 'epoch': 0.45}
{'loss': 0.3126, 'learning_rate': 5.7728593318970825e-05, 'epoch': 0.45}
2:10:10, 22.33s/it]                                                          45%|████▌     | 2926/6500 [19:24:32<22:10:10, 22.33s/it] 45%|████▌     | 2927/6500 [19:24:50<20:53:32, 21.05s/it]                                                          45%|████▌     | 2927/6500 [19:24:50<20:53:32, 21.05s/it] 45%|████▌     | 2928/6500 [19:25:09<20:00:27, 20.16s/it]                                                          45%|████▌     | 2928/6500 [19:25:09<20:00:27, 20.16s/it] 45%|████▌     | 2929/6500 [19:25:27<19:22:58, 19.54s/it]                                                          45%|████▌     | 2929/6500 [19:25:27<19:22:58, 19.54s/it] 45%|████▌     | 2930/6500 [19:25:45<18:56:44, 19.10s/it]                                                          45%|████▌     | 2930/6500 [19:25:45<18:56:44, 19.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8368350863456726, 'eval_runtime': 5.8534, 'eval_samples_per_second': 3.929, 'eval_steps_per_second': 1.025, 'epoch': 0.45}
                                                          45%|████▌     | 2930/6500 [19:25:51<18:56:44, 19.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2930
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2930
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2930the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2930

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2930
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2930/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2930/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3012, 'learning_rate': 5.7704709409786464e-05, 'epoch': 0.45}
{'loss': 0.2995, 'learning_rate': 5.768082369967312e-05, 'epoch': 0.45}
{'loss': 0.3053, 'learning_rate': 5.765693619421394e-05, 'epoch': 0.45}
{'loss': 0.2976, 'learning_rate': 5.763304689899249e-05, 'epoch': 0.45}
{'loss': 0.2962, 'learning_rate': 5.760915581959272e-05, 'epoch': 0.45}
 45%|████▌     | 2931/6500 [19:26:55<34:11:56, 34.50s/it]                                                          45%|████▌     | 2931/6500 [19:26:55<34:11:56, 34.50s/it] 45%|████▌     | 2932/6500 [19:27:13<29:17:40, 29.56s/it]                                                          45%|████▌     | 2932/6500 [19:27:13<29:17:40, 29.56s/it] 45%|████▌     | 2933/6500 [19:27:31<25:51:54, 26.10s/it]                                                          45%|████▌     | 2933/6500 [19:27:31<25:51:54, 26.10s/it] 45%|████▌     | 2934/6500 [19:27:49<23:27:26, 23.68s/it]                                                          45%|████▌     | 2934/6500 [19:27:49<23:27:26, 23.68s/it] 45%|████▌     | 2935/6500 [19:28:07<21:46:59, 22.00s/it]                                                          45%|████▌     | 2935/6500 [19:28:07<21:46:59, 22.00s/it] 45%|████▌     | 2936/6500 [19:28:25<2{'loss': 0.3005, 'learning_rate': 5.7585262961599054e-05, 'epoch': 0.45}
{'loss': 0.3101, 'learning_rate': 5.7561368330596275e-05, 'epoch': 0.45}
{'loss': 0.2714, 'learning_rate': 5.753747193216963e-05, 'epoch': 0.45}
{'loss': 0.2936, 'learning_rate': 5.751357377190475e-05, 'epoch': 0.45}
{'loss': 0.2867, 'learning_rate': 5.748967385538769e-05, 'epoch': 0.45}
0:37:05, 20.83s/it]                                                          45%|████▌     | 2936/6500 [19:28:25<20:37:05, 20.83s/it] 45%|████▌     | 2937/6500 [19:28:43<19:48:16, 20.01s/it]                                                          45%|████▌     | 2937/6500 [19:28:43<19:48:16, 20.01s/it] 45%|████▌     | 2938/6500 [19:29:02<19:15:44, 19.47s/it]                                                          45%|████▌     | 2938/6500 [19:29:02<19:15:44, 19.47s/it] 45%|████▌     | 2939/6500 [19:29:20<18:52:17, 19.08s/it]                                                          45%|████▌     | 2939/6500 [19:29:20<18:52:17, 19.08s/it] 45%|████▌     | 2940/6500 [19:29:38<18:35:47, 18.81s/it]                                                          45%|████▌     | 2940/6500 [19:29:38<18:35:47, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8371372222900391, 'eval_runtime': 5.592, 'eval_samples_per_second': 4.113, 'eval_steps_per_second': 1.073, 'epoch': 0.45}
                                                          45%|████▌     | 2940/6500 [19:29:44<18:35:47, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2940
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2940
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2940
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2940/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3218, 'learning_rate': 5.7465772188204905e-05, 'epoch': 0.45}
{'loss': 0.3379, 'learning_rate': 5.744186877594325e-05, 'epoch': 0.45}
{'loss': 0.281, 'learning_rate': 5.741796362419003e-05, 'epoch': 0.45}
{'loss': 0.3057, 'learning_rate': 5.73940567385329e-05, 'epoch': 0.45}
{'loss': 0.3183, 'learning_rate': 5.737014812455999e-05, 'epoch': 0.45}
 45%|████▌     | 2941/6500 [19:30:44<32:41:16, 33.06s/it]                                                          45%|████▌     | 2941/6500 [19:30:44<32:41:16, 33.06s/it] 45%|████▌     | 2942/6500 [19:31:03<28:18:37, 28.64s/it]                                                          45%|████▌     | 2942/6500 [19:31:03<28:18:37, 28.64s/it] 45%|████▌     | 2943/6500 [19:31:21<25:09:36, 25.46s/it]                                                          45%|████▌     | 2943/6500 [19:31:21<25:09:36, 25.46s/it] 45%|████▌     | 2944/6500 [19:31:39<22:57:18, 23.24s/it]                                                          45%|████▌     | 2944/6500 [19:31:39<22:57:18, 23.24s/it] 45%|████▌     | 2945/6500 [19:31:57<21:25:09, 21.69s/it]                                                          45%|████▌     | 2945/6500 [19:31:57<21:25:09, 21.69s/it] 45%|████▌     | 2946/6500 [19:32:15<2{'loss': 0.7905, 'learning_rate': 5.7346237787859745e-05, 'epoch': 0.45}
{'loss': 0.2877, 'learning_rate': 5.7322325734021086e-05, 'epoch': 0.45}
{'loss': 0.2987, 'learning_rate': 5.7298411968633306e-05, 'epoch': 0.45}
{'loss': 0.276, 'learning_rate': 5.72744964972861e-05, 'epoch': 0.45}
{'loss': 0.3032, 'learning_rate': 5.7250579325569574e-05, 'epoch': 0.45}
0:20:15, 20.60s/it]                                                          45%|████▌     | 2946/6500 [19:32:15<20:20:15, 20.60s/it] 45%|████▌     | 2947/6500 [19:32:33<19:36:03, 19.86s/it]                                                          45%|████▌     | 2947/6500 [19:32:33<19:36:03, 19.86s/it] 45%|████▌     | 2948/6500 [19:32:51<19:05:26, 19.35s/it]                                                          45%|████▌     | 2948/6500 [19:32:51<19:05:26, 19.35s/it] 45%|████▌     | 2949/6500 [19:33:09<18:43:59, 18.99s/it]                                                          45%|████▌     | 2949/6500 [19:33:09<18:43:59, 18.99s/it] 45%|████▌     | 2950/6500 [19:33:28<18:29:12, 18.75s/it]                                                          45%|████▌     | 2950/6500 [19:33:28<18:29:12, 18.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8399662971496582, 'eval_runtime': 5.5757, 'eval_samples_per_second': 4.125, 'eval_steps_per_second': 1.076, 'epoch': 0.45}
                                                          45%|████▌     | 2950/6500 [19:33:33<18:29:12, 18.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2950
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2950

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2950
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2950

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2950
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2950/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2928, 'learning_rate': 5.722666045907422e-05, 'epoch': 0.45}
{'loss': 0.2621, 'learning_rate': 5.720273990339092e-05, 'epoch': 0.45}
{'loss': 0.2919, 'learning_rate': 5.717881766411095e-05, 'epoch': 0.45}
{'loss': 0.2839, 'learning_rate': 5.7154893746826014e-05, 'epoch': 0.45}
{'loss': 0.2897, 'learning_rate': 5.7130968157128154e-05, 'epoch': 0.45}
 45%|████▌     | 2951/6500 [19:35:05<41:53:12, 42.49s/it]                                                          45%|████▌     | 2951/6500 [19:35:05<41:53:12, 42.49s/it] 45%|████▌     | 2952/6500 [19:35:23<34:37:50, 35.14s/it]                                                          45%|████▌     | 2952/6500 [19:35:23<34:37:50, 35.14s/it] 45%|████▌     | 2953/6500 [19:35:41<29:33:11, 29.99s/it]                                                          45%|████▌     | 2953/6500 [19:35:41<29:33:11, 29.99s/it] 45%|████▌     | 2954/6500 [19:35:59<26:00:23, 26.40s/it]                                                          45%|████▌     | 2954/6500 [19:35:59<26:00:23, 26.40s/it] 45%|████▌     | 2955/6500 [19:36:18<23:33:04, 23.92s/it]                                                          45%|████▌     | 2955/6500 [19:36:18<23:33:04, 23.92s/it] 45%|████▌     | 2956/6500 [19:36:36<2{'loss': 0.2944, 'learning_rate': 5.710704090060985e-05, 'epoch': 0.45}
{'loss': 0.2937, 'learning_rate': 5.7083111982863956e-05, 'epoch': 0.45}
{'loss': 0.2796, 'learning_rate': 5.7059181409483684e-05, 'epoch': 0.46}
{'loss': 0.2959, 'learning_rate': 5.703524918606269e-05, 'epoch': 0.46}
{'loss': 0.3029, 'learning_rate': 5.701131531819497e-05, 'epoch': 0.46}
1:48:58, 22.16s/it]                                                          45%|████▌     | 2956/6500 [19:36:36<21:48:58, 22.16s/it] 45%|████▌     | 2957/6500 [19:36:54<20:36:11, 20.93s/it]                                                          45%|████▌     | 2957/6500 [19:36:54<20:36:11, 20.93s/it] 46%|████▌     | 2958/6500 [19:37:12<19:53:26, 20.22s/it]                                                          46%|████▌     | 2958/6500 [19:37:12<19:53:26, 20.22s/it] 46%|████▌     | 2959/6500 [19:37:32<19:51:52, 20.20s/it]                                                          46%|████▌     | 2959/6500 [19:37:32<19:51:52, 20.20s/it] 46%|████▌     | 2960/6500 [19:37:50<19:14:47, 19.57s/it]                                                          46%|████▌     | 2960/6500 [19:37:50<19:14:47, 19.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8381724953651428, 'eval_runtime': 5.5105, 'eval_samples_per_second': 4.174, 'eval_steps_per_second': 1.089, 'epoch': 0.46}
                                                          46%|████▌     | 2960/6500 [19:37:56<19:14:47, 19.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2960
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2960the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2960
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2960

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2960
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2960/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2960/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2822, 'learning_rate': 5.698737981147493e-05, 'epoch': 0.46}
{'loss': 0.2991, 'learning_rate': 5.696344267149735e-05, 'epoch': 0.46}
{'loss': 0.2852, 'learning_rate': 5.693950390385736e-05, 'epoch': 0.46}
{'loss': 0.2861, 'learning_rate': 5.691556351415054e-05, 'epoch': 0.46}
{'loss': 0.2868, 'learning_rate': 5.6891621507972794e-05, 'epoch': 0.46}
 46%|████▌     | 2961/6500 [19:38:44<29:15:12, 29.76s/it]                                                          46%|████▌     | 2961/6500 [19:38:44<29:15:12, 29.76s/it] 46%|████▌     | 2962/6500 [19:39:02<25:46:55, 26.23s/it]                                                          46%|████▌     | 2962/6500 [19:39:02<25:46:55, 26.23s/it] 46%|████▌     | 2963/6500 [19:39:20<23:21:25, 23.77s/it]                                                          46%|████▌     | 2963/6500 [19:39:20<23:21:25, 23.77s/it] 46%|████▌     | 2964/6500 [19:39:38<21:39:18, 22.05s/it]                                                          46%|████▌     | 2964/6500 [19:39:38<21:39:18, 22.05s/it] 46%|████▌     | 2965/6500 [19:39:57<20:36:48, 20.99s/it]                                                          46%|████▌     | 2965/6500 [19:39:57<20:36:48, 20.99s/it] 46%|████▌     | 2966/6500 [19:40:15<1{'loss': 0.3021, 'learning_rate': 5.686767789092041e-05, 'epoch': 0.46}
{'loss': 0.2884, 'learning_rate': 5.684373266859009e-05, 'epoch': 0.46}
{'loss': 0.2813, 'learning_rate': 5.681978584657886e-05, 'epoch': 0.46}
{'loss': 0.2829, 'learning_rate': 5.679583743048416e-05, 'epoch': 0.46}
{'loss': 0.2919, 'learning_rate': 5.677188742590378e-05, 'epoch': 0.46}
9:44:17, 20.11s/it]                                                          46%|████▌     | 2966/6500 [19:40:15<19:44:17, 20.11s/it] 46%|████▌     | 2967/6500 [19:40:33<19:07:56, 19.50s/it]                                                          46%|████▌     | 2967/6500 [19:40:33<19:07:56, 19.50s/it] 46%|████▌     | 2968/6500 [19:40:51<18:43:04, 19.08s/it]                                                          46%|████▌     | 2968/6500 [19:40:51<18:43:04, 19.08s/it] 46%|████▌     | 2969/6500 [19:41:09<18:25:27, 18.78s/it]                                                          46%|████▌     | 2969/6500 [19:41:09<18:25:27, 18.78s/it] 46%|████▌     | 2970/6500 [19:41:27<18:13:29, 18.59s/it]                                                          46%|████▌     | 2970/6500 [19:41:27<18:13:29, 18.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8412386775016785, 'eval_runtime': 5.3219, 'eval_samples_per_second': 4.322, 'eval_steps_per_second': 1.127, 'epoch': 0.46}
                                                          46%|████▌     | 2970/6500 [19:41:32<18:13:29, 18.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2970
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2970
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2970
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2970

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2970
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2970/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2970/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3582, 'learning_rate': 5.674793583843588e-05, 'epoch': 0.46}
{'loss': 0.2853, 'learning_rate': 5.672398267367902e-05, 'epoch': 0.46}
{'loss': 0.2681, 'learning_rate': 5.670002793723209e-05, 'epoch': 0.46}
{'loss': 0.3086, 'learning_rate': 5.667607163469436e-05, 'epoch': 0.46}
{'loss': 0.8094, 'learning_rate': 5.665211377166548e-05, 'epoch': 0.46}
 46%|████▌     | 2971/6500 [19:42:31<31:36:46, 32.25s/it]                                                          46%|████▌     | 2971/6500 [19:42:31<31:36:46, 32.25s/it] 46%|████▌     | 2972/6500 [19:42:49<27:24:38, 27.97s/it]                                                          46%|████▌     | 2972/6500 [19:42:49<27:24:38, 27.97s/it] 46%|████▌     | 2973/6500 [19:43:07<24:28:20, 24.98s/it]                                                          46%|████▌     | 2973/6500 [19:43:07<24:28:20, 24.98s/it] 46%|████▌     | 2974/6500 [19:43:25<22:28:46, 22.95s/it]                                                          46%|████▌     | 2974/6500 [19:43:25<22:28:46, 22.95s/it] 46%|████▌     | 2975/6500 [19:43:43<21:01:03, 21.46s/it]                                                          46%|████▌     | 2975/6500 [19:43:43<21:01:03, 21.46s/it] 46%|████▌     | 2976/6500 [19:44:01<1{'loss': 0.3016, 'learning_rate': 5.662815435374544e-05, 'epoch': 0.46}
{'loss': 0.2937, 'learning_rate': 5.660419338653463e-05, 'epoch': 0.46}
{'loss': 0.3008, 'learning_rate': 5.658023087563379e-05, 'epoch': 0.46}
{'loss': 0.2628, 'learning_rate': 5.655626682664397e-05, 'epoch': 0.46}
{'loss': 0.3025, 'learning_rate': 5.653230124516663e-05, 'epoch': 0.46}
9:59:55, 20.43s/it]                                                          46%|████▌     | 2976/6500 [19:44:01<19:59:55, 20.43s/it] 46%|████▌     | 2977/6500 [19:44:19<19:17:41, 19.72s/it]                                                          46%|████▌     | 2977/6500 [19:44:19<19:17:41, 19.72s/it] 46%|████▌     | 2978/6500 [19:44:37<18:48:04, 19.22s/it]                                                          46%|████▌     | 2978/6500 [19:44:37<18:48:04, 19.22s/it] 46%|████▌     | 2979/6500 [19:44:56<18:27:44, 18.88s/it]                                                          46%|████▌     | 2979/6500 [19:44:56<18:27:44, 18.88s/it] 46%|████▌     | 2980/6500 [19:45:14<18:14:00, 18.65s/it]                                                          46%|████▌     | 2980/6500 [19:45:14<18:14:00, 18.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.836660623550415, 'eval_runtime': 5.3406, 'eval_samples_per_second': 4.307, 'eval_steps_per_second': 1.123, 'epoch': 0.46}
                                                          46%|████▌     | 2980/6500 [19:45:19<18:14:00, 18.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2980
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2980/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2980

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2980

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2980/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2980/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2980/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2802, 'learning_rate': 5.650833413680361e-05, 'epoch': 0.46}
{'loss': 0.2722, 'learning_rate': 5.648436550715704e-05, 'epoch': 0.46}
{'loss': 0.2913, 'learning_rate': 5.646039536182949e-05, 'epoch': 0.46}
{'loss': 0.2769, 'learning_rate': 5.643642370642378e-05, 'epoch': 0.46}
{'loss': 0.2922, 'learning_rate': 5.641245054654316e-05, 'epoch': 0.46}
 46%|████▌     | 2981/6500 [19:46:50<40:53:15, 41.83s/it]                                                          46%|████▌     | 2981/6500 [19:46:50<40:53:15, 41.83s/it] 46%|████▌     | 2982/6500 [19:47:08<33:52:27, 34.66s/it]                                                          46%|████▌     | 2982/6500 [19:47:08<33:52:27, 34.66s/it] 46%|████▌     | 2983/6500 [19:47:25<28:57:28, 29.64s/it]                                                          46%|████▌     | 2983/6500 [19:47:25<28:57:28, 29.64s/it] 46%|████▌     | 2984/6500 [19:47:43<25:31:26, 26.13s/it]                                                          46%|████▌     | 2984/6500 [19:47:43<25:31:26, 26.13s/it] 46%|████▌     | 2985/6500 [19:48:01<23:07:49, 23.69s/it]                                                          46%|████▌     | 2985/6500 [19:48:01<23:07:49, 23.69s/it] 46%|████▌     | 2986/6500 [19:48:19<2{'loss': 0.282, 'learning_rate': 5.638847588779121e-05, 'epoch': 0.46}
{'loss': 0.2959, 'learning_rate': 5.636449973577188e-05, 'epoch': 0.46}
{'loss': 0.2789, 'learning_rate': 5.6340522096089424e-05, 'epoch': 0.46}
{'loss': 0.3119, 'learning_rate': 5.631654297434849e-05, 'epoch': 0.46}
{'loss': 0.2929, 'learning_rate': 5.6292562376154037e-05, 'epoch': 0.46}
1:27:55, 21.99s/it]                                                          46%|████▌     | 2986/6500 [19:48:19<21:27:55, 21.99s/it] 46%|████▌     | 2987/6500 [19:48:37<20:17:52, 20.80s/it]                                                          46%|████▌     | 2987/6500 [19:48:37<20:17:52, 20.80s/it] 46%|████▌     | 2988/6500 [19:48:56<19:29:40, 19.98s/it]                                                          46%|████▌     | 2988/6500 [19:48:56<19:29:40, 19.98s/it] 46%|████▌     | 2989/6500 [19:49:14<18:56:09, 19.42s/it]                                                          46%|████▌     | 2989/6500 [19:49:14<18:56:09, 19.42s/it] 46%|████▌     | 2990/6500 [19:49:33<18:51:41, 19.35s/it]                                                          46%|████▌     | 2990/6500 [19:49:33<18:51:41, 19.35s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8375704288482666, 'eval_runtime': 5.3263, 'eval_samples_per_second': 4.318, 'eval_steps_per_second': 1.126, 'epoch': 0.46}
                                                          46%|████▌     | 2990/6500 [19:49:38<18:51:41, 19.35s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-2990
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2990 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2990
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2990

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2990
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2990/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-2990/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2834, 'learning_rate': 5.6268580307111376e-05, 'epoch': 0.46}
{'loss': 0.2959, 'learning_rate': 5.624459677282619e-05, 'epoch': 0.46}
{'loss': 0.2927, 'learning_rate': 5.622061177890447e-05, 'epoch': 0.46}
{'loss': 0.2915, 'learning_rate': 5.619662533095257e-05, 'epoch': 0.46}
{'loss': 0.2942, 'learning_rate': 5.617263743457719e-05, 'epoch': 0.46}
 46%|████▌     | 2991/6500 [19:50:44<33:58:43, 34.86s/it]                                                          46%|████▌     | 2991/6500 [19:50:44<33:58:43, 34.86s/it] 46%|████▌     | 2992/6500 [19:51:02<29:06:19, 29.87s/it]                                                          46%|████▌     | 2992/6500 [19:51:02<29:06:19, 29.87s/it] 46%|████▌     | 2993/6500 [19:51:20<25:39:01, 26.33s/it]                                                          46%|████▌     | 2993/6500 [19:51:20<25:39:01, 26.33s/it] 46%|████▌     | 2994/6500 [19:51:38<23:11:42, 23.82s/it]                                                          46%|████▌     | 2994/6500 [19:51:38<23:11:42, 23.82s/it] 46%|████▌     | 2995/6500 [19:51:56<21:29:25, 22.07s/it]                                                          46%|████▌     | 2995/6500 [19:51:56<21:29:25, 22.07s/it] 46%|████▌     | 2996/6500 [19:52:14<2{'loss': 0.301, 'learning_rate': 5.6148648095385327e-05, 'epoch': 0.46}
{'loss': 0.2797, 'learning_rate': 5.612465731898435e-05, 'epoch': 0.46}
{'loss': 0.2773, 'learning_rate': 5.610066511098198e-05, 'epoch': 0.46}
{'loss': 0.2692, 'learning_rate': 5.607667147698622e-05, 'epoch': 0.46}
{'loss': 0.3145, 'learning_rate': 5.6052676422605467e-05, 'epoch': 0.46}
0:19:42, 20.89s/it]                                                          46%|████▌     | 2996/6500 [19:52:14<20:19:42, 20.89s/it] 46%|████▌     | 2997/6500 [19:52:32<19:31:50, 20.07s/it]                                                          46%|████▌     | 2997/6500 [19:52:32<19:31:50, 20.07s/it] 46%|████▌     | 2998/6500 [19:52:50<18:56:25, 19.47s/it]                                                          46%|████▌     | 2998/6500 [19:52:50<18:56:25, 19.47s/it] 46%|████▌     | 2999/6500 [19:53:09<18:32:04, 19.06s/it]                                                          46%|████▌     | 2999/6500 [19:53:09<18:32:04, 19.06s/it] 46%|████▌     | 3000/6500 [19:53:27<18:15:14, 18.78s/it]                                                          46%|████▌     | 3000/6500 [19:53:27<18:15:14, 18.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8436397910118103, 'eval_runtime': 5.3352, 'eval_samples_per_second': 4.311, 'eval_steps_per_second': 1.125, 'epoch': 0.46}
                                                          46%|████▌     | 3000/6500 [19:53:32<18:15:14, 18.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3000
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3000

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3000
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3000

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3000
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3000/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3415, 'learning_rate': 5.6028679953448384e-05, 'epoch': 0.46}
{'loss': 0.2952, 'learning_rate': 5.6004682075124016e-05, 'epoch': 0.46}
{'loss': 0.2974, 'learning_rate': 5.598068279324172e-05, 'epoch': 0.46}
{'loss': 0.2868, 'learning_rate': 5.595668211341118e-05, 'epoch': 0.46}
{'loss': 0.8238, 'learning_rate': 5.593268004124243e-05, 'epoch': 0.46}
 46%|████▌     | 3001/6500 [19:54:42<34:39:54, 35.67s/it]                                                          46%|████▌     | 3001/6500 [19:54:42<34:39:54, 35.67s/it] 46%|████▌     | 3002/6500 [19:55:00<29:29:37, 30.35s/it]                                                          46%|████▌     | 3002/6500 [19:55:00<29:29:37, 30.35s/it] 46%|████▌     | 3003/6500 [19:55:18<25:52:27, 26.64s/it]                                                          46%|████▌     | 3003/6500 [19:55:18<25:52:27, 26.64s/it] 46%|████▌     | 3004/6500 [19:55:36<23:21:01, 24.04s/it]                                                          46%|████▌     | 3004/6500 [19:55:36<23:21:01, 24.04s/it] 46%|████▌     | 3005/6500 [19:55:54<21:35:00, 22.23s/it]                                                          46%|████▌     | 3005/6500 [19:55:54<21:35:00, 22.23s/it] 46%|████▌     | 3006/6500 [19:56:12<2{'loss': 0.2972, 'learning_rate': 5.5908676582345786e-05, 'epoch': 0.46}
{'loss': 0.2907, 'learning_rate': 5.588467174233192e-05, 'epoch': 0.46}
{'loss': 0.2709, 'learning_rate': 5.586066552681179e-05, 'epoch': 0.46}
{'loss': 0.2717, 'learning_rate': 5.583665794139675e-05, 'epoch': 0.46}
{'loss': 0.2966, 'learning_rate': 5.5812648991698415e-05, 'epoch': 0.46}
0:28:22, 21.09s/it]                                                          46%|████▌     | 3006/6500 [19:56:12<20:28:22, 21.09s/it] 46%|████▋     | 3007/6500 [19:56:30<19:35:35, 20.19s/it]                                                          46%|████▋     | 3007/6500 [19:56:30<19:35:35, 20.19s/it] 46%|████▋     | 3008/6500 [19:56:48<18:58:40, 19.56s/it]                                                          46%|████▋     | 3008/6500 [19:56:48<18:58:40, 19.56s/it] 46%|████▋     | 3009/6500 [19:57:06<18:33:00, 19.13s/it]                                                          46%|████▋     | 3009/6500 [19:57:06<18:33:00, 19.13s/it] 46%|████▋     | 3010/6500 [19:57:26<18:41:45, 19.29s/it]                                                          46%|████▋     | 3010/6500 [19:57:26<18:41:45, 19.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.84494549036026, 'eval_runtime': 5.8973, 'eval_samples_per_second': 3.9, 'eval_steps_per_second': 1.017, 'epoch': 0.46}
                                                          46%|████▋     | 3010/6500 [19:57:32<18:41:45, 19.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3010
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3010/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3010 

/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3010

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3010
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3010/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3010/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2673, 'learning_rate': 5.57886386833287e-05, 'epoch': 0.46}
{'loss': 0.2807, 'learning_rate': 5.576462702189989e-05, 'epoch': 0.46}
{'loss': 0.2753, 'learning_rate': 5.574061401302456e-05, 'epoch': 0.46}
{'loss': 0.2866, 'learning_rate': 5.571659966231562e-05, 'epoch': 0.46}
{'loss': 0.2922, 'learning_rate': 5.569258397538626e-05, 'epoch': 0.46}
 46%|████▋     | 3011/6500 [19:58:06<24:33:13, 25.33s/it]                                                          46%|████▋     | 3011/6500 [19:58:06<24:33:13, 25.33s/it] 46%|████▋     | 3012/6500 [19:58:24<22:25:36, 23.15s/it]                                                          46%|████▋     | 3012/6500 [19:58:24<22:25:36, 23.15s/it] 46%|████▋     | 3013/6500 [19:58:42<20:56:02, 21.61s/it]                                                          46%|████▋     | 3013/6500 [19:58:42<20:56:02, 21.61s/it] 46%|████▋     | 3014/6500 [19:59:00<19:53:39, 20.54s/it]                                                          46%|████▋     | 3014/6500 [19:59:00<19:53:39, 20.54s/it] 46%|████▋     | 3015/6500 [19:59:18<19:10:36, 19.81s/it]                                                          46%|████▋     | 3015/6500 [19:59:18<19:10:36, 19.81s/it] 46%|████▋     | 3016/6500 [19:59:36<1{'loss': 0.2694, 'learning_rate': 5.566856695785001e-05, 'epoch': 0.46}
{'loss': 0.2926, 'learning_rate': 5.564454861532069e-05, 'epoch': 0.46}
{'loss': 0.2825, 'learning_rate': 5.5620528953412456e-05, 'epoch': 0.46}
{'loss': 0.3002, 'learning_rate': 5.5596507977739755e-05, 'epoch': 0.46}
{'loss': 0.2763, 'learning_rate': 5.5572485693917345e-05, 'epoch': 0.46}
8:40:29, 19.30s/it]                                                          46%|████▋     | 3016/6500 [19:59:36<18:40:29, 19.30s/it] 46%|████▋     | 3017/6500 [19:59:54<18:19:43, 18.94s/it]                                                          46%|████▋     | 3017/6500 [19:59:54<18:19:43, 18.94s/it] 46%|████▋     | 3018/6500 [20:00:12<18:05:17, 18.70s/it]                                                          46%|████▋     | 3018/6500 [20:00:12<18:05:17, 18.70s/it] 46%|████▋     | 3019/6500 [20:00:30<17:55:22, 18.54s/it]                                                          46%|████▋     | 3019/6500 [20:00:30<17:55:22, 18.54s/it] 46%|████▋     | 3020/6500 [20:00:48<17:48:16, 18.42s/it]                                                          46%|████▋     | 3020/6500 [20:00:48<17:48:16, 18.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8417324423789978, 'eval_runtime': 5.3356, 'eval_samples_per_second': 4.311, 'eval_steps_per_second': 1.125, 'epoch': 0.46}
                                                          46%|████▋     | 3020/6500 [20:00:54<17:48:16, 18.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3020
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3020I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3020
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3020

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3020
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3020/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2882, 'learning_rate': 5.5548462107560284e-05, 'epoch': 0.46}
{'loss': 0.2958, 'learning_rate': 5.552443722428393e-05, 'epoch': 0.46}
{'loss': 0.2789, 'learning_rate': 5.550041104970397e-05, 'epoch': 0.47}
{'loss': 0.2952, 'learning_rate': 5.547638358943637e-05, 'epoch': 0.47}
{'loss': 0.2863, 'learning_rate': 5.5452354849097396e-05, 'epoch': 0.47}
 46%|████▋     | 3021/6500 [20:01:41<27:48:08, 28.77s/it]                                                          46%|████▋     | 3021/6500 [20:01:41<27:48:08, 28.77s/it] 46%|████▋     | 3022/6500 [20:02:00<24:46:16, 25.64s/it]                                                          46%|████▋     | 3022/6500 [20:02:00<24:46:16, 25.64s/it] 47%|████▋     | 3023/6500 [20:02:18<22:33:41, 23.36s/it]                                                          47%|████▋     | 3023/6500 [20:02:18<22:33:41, 23.36s/it] 47%|████▋     | 3024/6500 [20:02:36<21:00:38, 21.76s/it]                                                          47%|████▋     | 3024/6500 [20:02:36<21:00:38, 21.76s/it] 47%|████▋     | 3025/6500 [20:02:54<19:55:43, 20.65s/it]                                                          47%|████▋     | 3025/6500 [20:02:54<19:55:43, 20.65s/it] 47%|████▋     | 3026/6500 [20:03:12<1{'loss': 0.2986, 'learning_rate': 5.542832483430363e-05, 'epoch': 0.47}
{'loss': 0.2729, 'learning_rate': 5.540429355067196e-05, 'epoch': 0.47}
{'loss': 0.2811, 'learning_rate': 5.538026100381951e-05, 'epoch': 0.47}
{'loss': 0.2744, 'learning_rate': 5.5356227199363764e-05, 'epoch': 0.47}
{'loss': 0.3606, 'learning_rate': 5.533219214292248e-05, 'epoch': 0.47}
9:11:25, 19.89s/it]                                                          47%|████▋     | 3026/6500 [20:03:12<19:11:25, 19.89s/it] 47%|████▋     | 3027/6500 [20:03:30<18:40:04, 19.35s/it]                                                          47%|████▋     | 3027/6500 [20:03:30<18:40:04, 19.35s/it] 47%|████▋     | 3028/6500 [20:03:48<18:18:20, 18.98s/it]                                                          47%|████▋     | 3028/6500 [20:03:48<18:18:20, 18.98s/it] 47%|████▋     | 3029/6500 [20:04:06<18:03:32, 18.73s/it]                                                          47%|████▋     | 3029/6500 [20:04:06<18:03:32, 18.73s/it] 47%|████▋     | 3030/6500 [20:04:24<17:53:04, 18.55s/it]                                                          47%|████▋     | 3030/6500 [20:04:24<17:53:04, 18.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8402863144874573, 'eval_runtime': 5.479, 'eval_samples_per_second': 4.198, 'eval_steps_per_second': 1.095, 'epoch': 0.47}
                                                          47%|████▋     | 3030/6500 [20:04:30<17:53:04, 18.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3030
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3030 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3030

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3030

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3030
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3030/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3030/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2897, 'learning_rate': 5.530815584011371e-05, 'epoch': 0.47}
{'loss': 0.2748, 'learning_rate': 5.528411829655579e-05, 'epoch': 0.47}
{'loss': 0.291, 'learning_rate': 5.5260079517867323e-05, 'epoch': 0.47}
{'loss': 0.7952, 'learning_rate': 5.523603950966726e-05, 'epoch': 0.47}
{'loss': 0.2909, 'learning_rate': 5.5211998277574805e-05, 'epoch': 0.47}
 47%|████▋     | 3031/6500 [20:05:51<37:31:17, 38.94s/it]                                                          47%|████▋     | 3031/6500 [20:05:51<37:31:17, 38.94s/it] 47%|████▋     | 3032/6500 [20:06:09<31:26:43, 32.64s/it]                                                          47%|████▋     | 3032/6500 [20:06:09<31:26:43, 32.64s/it] 47%|████▋     | 3033/6500 [20:06:27<27:12:05, 28.25s/it]                                                          47%|████▋     | 3033/6500 [20:06:27<27:12:05, 28.25s/it] 47%|████▋     | 3034/6500 [20:06:45<24:14:02, 25.17s/it]                                                          47%|████▋     | 3034/6500 [20:06:45<24:14:02, 25.17s/it] 47%|████▋     | 3035/6500 [20:07:03<22:09:56, 23.03s/it]                                                          47%|████▋     | 3035/6500 [20:07:03<22:09:56, 23.03s/it] 47%|████▋     | 3036/6500 [20:07:21<2{'loss': 0.285, 'learning_rate': 5.518795582720944e-05, 'epoch': 0.47}
{'loss': 0.2905, 'learning_rate': 5.5163912164190935e-05, 'epoch': 0.47}
{'loss': 0.2746, 'learning_rate': 5.513986729413937e-05, 'epoch': 0.47}
{'loss': 0.291, 'learning_rate': 5.511582122267507e-05, 'epoch': 0.47}
{'loss': 0.2776, 'learning_rate': 5.509177395541866e-05, 'epoch': 0.47}
0:43:23, 21.54s/it]                                                          47%|████▋     | 3036/6500 [20:07:21<20:43:23, 21.54s/it] 47%|████▋     | 3037/6500 [20:07:39<19:43:20, 20.50s/it]                                                          47%|████▋     | 3037/6500 [20:07:39<19:43:20, 20.50s/it] 47%|████▋     | 3038/6500 [20:07:57<19:01:27, 19.78s/it]                                                          47%|████▋     | 3038/6500 [20:07:57<19:01:27, 19.78s/it] 47%|████▋     | 3039/6500 [20:08:16<18:43:10, 19.47s/it]                                                          47%|████▋     | 3039/6500 [20:08:16<18:43:10, 19.47s/it] 47%|████▋     | 3040/6500 [20:08:34<18:19:24, 19.06s/it]                                                          47%|████▋     | 3040/6500 [20:08:34<18:19:24, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8481442928314209, 'eval_runtime': 5.3332, 'eval_samples_per_second': 4.313, 'eval_steps_per_second': 1.125, 'epoch': 0.47}
                                                          47%|████▋     | 3040/6500 [20:08:39<18:19:24, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3040
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3040/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3040
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3040/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3040/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2583, 'learning_rate': 5.506772549799105e-05, 'epoch': 0.47}
{'loss': 0.2869, 'learning_rate': 5.504367585601342e-05, 'epoch': 0.47}
{'loss': 0.2733, 'learning_rate': 5.501962503510721e-05, 'epoch': 0.47}
{'loss': 0.2835, 'learning_rate': 5.499557304089419e-05, 'epoch': 0.47}
{'loss': 0.2827, 'learning_rate': 5.497151987899634e-05, 'epoch': 0.47}
 47%|████▋     | 3041/6500 [20:09:35<30:25:26, 31.66s/it]                                                          47%|████▋     | 3041/6500 [20:09:35<30:25:26, 31.66s/it] 47%|████▋     | 3042/6500 [20:09:53<26:28:36, 27.56s/it]                                                          47%|████▋     | 3042/6500 [20:09:53<26:28:36, 27.56s/it] 47%|████▋     | 3043/6500 [20:10:11<23:42:38, 24.69s/it]                                                          47%|████▋     | 3043/6500 [20:10:11<23:42:38, 24.69s/it] 47%|████▋     | 3044/6500 [20:10:30<22:00:59, 22.93s/it]                                                          47%|████▋     | 3044/6500 [20:10:30<22:00:59, 22.93s/it] 47%|████▋     | 3045/6500 [20:10:48<20:35:55, 21.46s/it]                                                          47%|████▋     | 3045/6500 [20:10:48<20:35:55, 21.46s/it] 47%|████▋     | 3046/6500 [20:11:06<1{'loss': 0.2815, 'learning_rate': 5.494746555503593e-05, 'epoch': 0.47}
{'loss': 0.2817, 'learning_rate': 5.492341007463554e-05, 'epoch': 0.47}
{'loss': 0.2932, 'learning_rate': 5.489935344341799e-05, 'epoch': 0.47}
{'loss': 0.2847, 'learning_rate': 5.4875295667006346e-05, 'epoch': 0.47}
{'loss': 0.2828, 'learning_rate': 5.4851236751023985e-05, 'epoch': 0.47}
9:36:33, 20.44s/it]                                                          47%|████▋     | 3046/6500 [20:11:06<19:36:33, 20.44s/it] 47%|████▋     | 3047/6500 [20:11:24<18:55:16, 19.73s/it]                                                          47%|████▋     | 3047/6500 [20:11:24<18:55:16, 19.73s/it] 47%|████▋     | 3048/6500 [20:11:42<18:26:56, 19.24s/it]                                                          47%|████▋     | 3048/6500 [20:11:42<18:26:56, 19.24s/it] 47%|████▋     | 3049/6500 [20:12:00<18:06:46, 18.90s/it]                                                          47%|████▋     | 3049/6500 [20:12:00<18:06:46, 18.90s/it] 47%|████▋     | 3050/6500 [20:12:18<17:53:09, 18.66s/it]                                                          47%|████▋     | 3050/6500 [20:12:18<17:53:09, 18.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.843390941619873, 'eval_runtime': 5.3294, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.47}
                                                          47%|████▋     | 3050/6500 [20:12:24<17:53:09, 18.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3050
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3050/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3050

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3050
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3050/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2929, 'learning_rate': 5.482717670109453e-05, 'epoch': 0.47}
{'loss': 0.2878, 'learning_rate': 5.4803115522841866e-05, 'epoch': 0.47}
{'loss': 0.2857, 'learning_rate': 5.477905322189015e-05, 'epoch': 0.47}
{'loss': 0.2813, 'learning_rate': 5.475498980386382e-05, 'epoch': 0.47}
{'loss': 0.2994, 'learning_rate': 5.4730925274387524e-05, 'epoch': 0.47}
 47%|████▋     | 3051/6500 [20:13:19<29:53:07, 31.19s/it]                                                          47%|████▋     | 3051/6500 [20:13:19<29:53:07, 31.19s/it] 47%|████▋     | 3052/6500 [20:13:37<26:05:36, 27.24s/it]                                                          47%|████▋     | 3052/6500 [20:13:37<26:05:36, 27.24s/it] 47%|████▋     | 3053/6500 [20:13:55<23:26:03, 24.47s/it]                                                          47%|████▋     | 3053/6500 [20:13:55<23:26:03, 24.47s/it] 47%|████▋     | 3054/6500 [20:14:13<21:34:52, 22.55s/it]                                                          47%|████▋     | 3054/6500 [20:14:13<21:34:52, 22.55s/it] 47%|████▋     | 3055/6500 [20:14:31<20:21:10, 21.27s/it]                                                          47%|████▋     | 3055/6500 [20:14:31<20:21:10, 21.27s/it] 47%|████▋     | 3056/6500 [20:14:49<1{'loss': 0.2806, 'learning_rate': 5.470685963908621e-05, 'epoch': 0.47}
{'loss': 0.2687, 'learning_rate': 5.468279290358507e-05, 'epoch': 0.47}
{'loss': 0.2682, 'learning_rate': 5.465872507350955e-05, 'epoch': 0.47}
{'loss': 0.284, 'learning_rate': 5.46346561544854e-05, 'epoch': 0.47}
{'loss': 0.344, 'learning_rate': 5.461058615213852e-05, 'epoch': 0.47}
9:25:17, 20.30s/it]                                                          47%|████▋     | 3056/6500 [20:14:49<19:25:17, 20.30s/it] 47%|████▋     | 3057/6500 [20:15:07<18:46:43, 19.64s/it]                                                          47%|████▋     | 3057/6500 [20:15:07<18:46:43, 19.64s/it] 47%|████▋     | 3058/6500 [20:15:25<18:19:53, 19.17s/it]                                                          47%|████▋     | 3058/6500 [20:15:25<18:19:53, 19.17s/it] 47%|████▋     | 3059/6500 [20:15:43<18:01:22, 18.86s/it]                                                          47%|████▋     | 3059/6500 [20:15:43<18:01:22, 18.86s/it] 47%|████▋     | 3060/6500 [20:16:04<18:23:30, 19.25s/it]                                                          47%|████▋     | 3060/6500 [20:16:04<18:23:30, 19.25s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8487369418144226, 'eval_runtime': 6.6097, 'eval_samples_per_second': 3.48, 'eval_steps_per_second': 0.908, 'epoch': 0.47}
                                                          47%|████▋     | 3060/6500 [20:16:10<18:23:30, 19.25s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3060
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3060/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3060

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3060

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3060
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3060/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3060/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2811, 'learning_rate': 5.458651507209518e-05, 'epoch': 0.47}
{'loss': 0.2749, 'learning_rate': 5.4562442919981816e-05, 'epoch': 0.47}
{'loss': 0.291, 'learning_rate': 5.453836970142516e-05, 'epoch': 0.47}
{'loss': 0.8023, 'learning_rate': 5.45142954220522e-05, 'epoch': 0.47}
{'loss': 0.2886, 'learning_rate': 5.449022008749012e-05, 'epoch': 0.47}
 47%|████▋     | 3061/6500 [20:17:34<38:44:48, 40.56s/it]                                                          47%|████▋     | 3061/6500 [20:17:34<38:44:48, 40.56s/it] 47%|████▋     | 3062/6500 [20:17:52<32:15:37, 33.78s/it]                                                          47%|████▋     | 3062/6500 [20:17:52<32:15:37, 33.78s/it] 47%|████▋     | 3063/6500 [20:18:10<27:43:19, 29.04s/it]                                                          47%|████▋     | 3063/6500 [20:18:10<27:43:19, 29.04s/it] 47%|████▋     | 3064/6500 [20:18:28<24:34:37, 25.75s/it]                                                          47%|████▋     | 3064/6500 [20:18:28<24:34:37, 25.75s/it] 47%|████▋     | 3065/6500 [20:18:46<22:23:33, 23.47s/it]                                                          47%|████▋     | 3065/6500 [20:18:46<22:23:33, 23.47s/it] 47%|████▋     | 3066/6500 [20:19:04<2{'loss': 0.2834, 'learning_rate': 5.446614370336639e-05, 'epoch': 0.47}
{'loss': 0.283, 'learning_rate': 5.444206627530873e-05, 'epoch': 0.47}
{'loss': 0.2682, 'learning_rate': 5.441798780894508e-05, 'epoch': 0.47}
{'loss': 0.3022, 'learning_rate': 5.439390830990365e-05, 'epoch': 0.47}
{'loss': 0.2608, 'learning_rate': 5.4369827783812864e-05, 'epoch': 0.47}
0:50:13, 21.84s/it]                                                          47%|████▋     | 3066/6500 [20:19:04<20:50:13, 21.84s/it] 47%|████▋     | 3067/6500 [20:19:22<19:45:02, 20.71s/it]                                                          47%|████▋     | 3067/6500 [20:19:22<19:45:02, 20.71s/it] 47%|████▋     | 3068/6500 [20:19:40<19:01:33, 19.96s/it]                                                          47%|████▋     | 3068/6500 [20:19:40<19:01:33, 19.96s/it] 47%|████▋     | 3069/6500 [20:19:58<18:29:40, 19.41s/it]                                                          47%|████▋     | 3069/6500 [20:19:59<18:29:40, 19.41s/it] 47%|████▋     | 3070/6500 [20:20:17<18:07:26, 19.02s/it]                                                          47%|████▋     | 3070/6500 [20:20:17<18:07:26, 19.02s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8457154035568237, 'eval_runtime': 5.3565, 'eval_samples_per_second': 4.294, 'eval_steps_per_second': 1.12, 'epoch': 0.47}
                                                          47%|████▋     | 3070/6500 [20:20:22<18:07:26, 19.02s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3070
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3070the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3070
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3070

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3070
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3070/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2792, 'learning_rate': 5.434574623630141e-05, 'epoch': 0.47}
{'loss': 0.2832, 'learning_rate': 5.432166367299818e-05, 'epoch': 0.47}
{'loss': 0.2865, 'learning_rate': 5.429758009953235e-05, 'epoch': 0.47}
{'loss': 0.284, 'learning_rate': 5.4273495521533304e-05, 'epoch': 0.47}
{'loss': 0.2881, 'learning_rate': 5.424940994463066e-05, 'epoch': 0.47}
 47%|████▋     | 3071/6500 [20:21:46<38:09:57, 40.07s/it]                                                          47%|████▋     | 3071/6500 [20:21:46<38:09:57, 40.07s/it] 47%|████▋     | 3072/6500 [20:22:04<31:50:15, 33.43s/it]                                                          47%|████▋     | 3072/6500 [20:22:04<31:50:15, 33.43s/it] 47%|████▋     | 3073/6500 [20:22:22<27:24:33, 28.79s/it]                                                          47%|████▋     | 3073/6500 [20:22:22<27:24:33, 28.79s/it] 47%|████▋     | 3074/6500 [20:22:40<24:19:03, 25.55s/it]                                                          47%|████▋     | 3074/6500 [20:22:40<24:19:03, 25.55s/it] 47%|████▋     | 3075/6500 [20:22:58<22:09:47, 23.30s/it]                                                          47%|████▋     | 3075/6500 [20:22:58<22:09:47, 23.30s/it] 47%|████▋     | 3076/6500 [20:23:16<2{'loss': 0.2992, 'learning_rate': 5.4225323374454286e-05, 'epoch': 0.47}
{'loss': 0.2843, 'learning_rate': 5.420123581663426e-05, 'epoch': 0.47}
{'loss': 0.2973, 'learning_rate': 5.4177147276800896e-05, 'epoch': 0.47}
{'loss': 0.2833, 'learning_rate': 5.4153057760584755e-05, 'epoch': 0.47}
{'loss': 0.2804, 'learning_rate': 5.4128967273616625e-05, 'epoch': 0.47}
0:39:26, 21.72s/it]                                                          47%|████▋     | 3076/6500 [20:23:16<20:39:26, 21.72s/it] 47%|████▋     | 3077/6500 [20:23:34<19:36:43, 20.63s/it]                                                          47%|████▋     | 3077/6500 [20:23:34<19:36:43, 20.63s/it] 47%|████▋     | 3078/6500 [20:23:52<18:53:13, 19.87s/it]                                                          47%|████▋     | 3078/6500 [20:23:52<18:53:13, 19.87s/it] 47%|████▋     | 3079/6500 [20:24:10<18:22:56, 19.34s/it]                                                          47%|████▋     | 3079/6500 [20:24:10<18:22:56, 19.34s/it] 47%|████▋     | 3080/6500 [20:24:28<18:02:01, 18.98s/it]                                                          47%|████▋     | 3080/6500 [20:24:28<18:02:01, 18.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8391318321228027, 'eval_runtime': 5.3459, 'eval_samples_per_second': 4.302, 'eval_steps_per_second': 1.122, 'epoch': 0.47}
                                                          47%|████▋     | 3080/6500 [20:24:34<18:02:01, 18.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3080
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3080/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3080

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3080

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3080
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3080/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3080/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2905, 'learning_rate': 5.410487582152749e-05, 'epoch': 0.47}
{'loss': 0.2885, 'learning_rate': 5.408078340994859e-05, 'epoch': 0.47}
{'loss': 0.2851, 'learning_rate': 5.4056690044511385e-05, 'epoch': 0.47}
{'loss': 0.292, 'learning_rate': 5.403259573084753e-05, 'epoch': 0.47}
{'loss': 0.302, 'learning_rate': 5.4008500474588965e-05, 'epoch': 0.47}
 47%|████▋     | 3081/6500 [20:25:31<30:28:38, 32.09s/it]                                                          47%|████▋     | 3081/6500 [20:25:31<30:28:38, 32.09s/it] 47%|████▋     | 3082/6500 [20:25:49<26:27:22, 27.87s/it]                                                          47%|████▋     | 3082/6500 [20:25:49<26:27:22, 27.87s/it] 47%|████▋     | 3083/6500 [20:26:07<23:38:46, 24.91s/it]                                                          47%|████▋     | 3083/6500 [20:26:07<23:38:46, 24.91s/it] 47%|████▋     | 3084/6500 [20:26:25<21:42:16, 22.87s/it]                                                          47%|████▋     | 3084/6500 [20:26:25<21:42:16, 22.87s/it] 47%|████▋     | 3085/6500 [20:26:43<20:19:21, 21.42s/it]                                                          47%|████▋     | 3085/6500 [20:26:43<20:19:21, 21.42s/it] 47%|████▋     | 3086/6500 [20:27:01<1{'loss': 0.2689, 'learning_rate': 5.3984404281367786e-05, 'epoch': 0.47}
{'loss': 0.2779, 'learning_rate': 5.3960307156816324e-05, 'epoch': 0.47}
{'loss': 0.2576, 'learning_rate': 5.393620910656714e-05, 'epoch': 0.48}
{'loss': 0.3086, 'learning_rate': 5.391211013625301e-05, 'epoch': 0.48}
{'loss': 0.327, 'learning_rate': 5.3888010251506915e-05, 'epoch': 0.48}
9:21:30, 20.41s/it]                                                          47%|████▋     | 3086/6500 [20:27:01<19:21:30, 20.41s/it] 47%|████▋     | 3087/6500 [20:27:20<18:47:03, 19.81s/it]                                                          47%|████▋     | 3087/6500 [20:27:20<18:47:03, 19.81s/it] 48%|████▊     | 3088/6500 [20:27:38<18:17:40, 19.30s/it]                                                          48%|████▊     | 3088/6500 [20:27:38<18:17:40, 19.30s/it] 48%|████▊     | 3089/6500 [20:27:56<17:57:23, 18.95s/it]                                                          48%|████▊     | 3089/6500 [20:27:56<17:57:23, 18.95s/it] 48%|████▊     | 3090/6500 [20:28:14<17:43:14, 18.71s/it]                                                          48%|████▊     | 3090/6500 [20:28:14<17:43:14, 18.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8419195413589478, 'eval_runtime': 5.623, 'eval_samples_per_second': 4.09, 'eval_steps_per_second': 1.067, 'epoch': 0.48}
                                                          48%|████▊     | 3090/6500 [20:28:20<17:43:14, 18.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3090
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3090

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3090 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3090

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3090
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3090/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3090/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2739, 'learning_rate': 5.3863909457962094e-05, 'epoch': 0.48}
{'loss': 0.2892, 'learning_rate': 5.3839807761251906e-05, 'epoch': 0.48}
{'loss': 0.2784, 'learning_rate': 5.381570516701e-05, 'epoch': 0.48}
{'loss': 0.8139, 'learning_rate': 5.379160168087021e-05, 'epoch': 0.48}
{'loss': 0.2807, 'learning_rate': 5.376749730846657e-05, 'epoch': 0.48}
 48%|████▊     | 3091/6500 [20:29:29<33:40:29, 35.56s/it]                                                          48%|████▊     | 3091/6500 [20:29:29<33:40:29, 35.56s/it] 48%|████▊     | 3092/6500 [20:29:47<28:40:47, 30.30s/it]                                                          48%|████▊     | 3092/6500 [20:29:47<28:40:47, 30.30s/it] 48%|████▊     | 3093/6500 [20:30:05<25:10:47, 26.61s/it]                                                          48%|████▊     | 3093/6500 [20:30:05<25:10:47, 26.61s/it] 48%|████▊     | 3094/6500 [20:30:23<22:44:04, 24.03s/it]                                                          48%|████▊     | 3094/6500 [20:30:23<22:44:04, 24.03s/it] 48%|████▊     | 3095/6500 [20:30:41<21:02:30, 22.25s/it]                                                          48%|████▊     | 3095/6500 [20:30:41<21:02:30, 22.25s/it] 48%|████▊     | 3096/6500 [20:30:59<1{'loss': 0.2859, 'learning_rate': 5.374339205543336e-05, 'epoch': 0.48}
{'loss': 0.2647, 'learning_rate': 5.371928592740503e-05, 'epoch': 0.48}
{'loss': 0.2833, 'learning_rate': 5.3695178930016196e-05, 'epoch': 0.48}
{'loss': 0.2773, 'learning_rate': 5.367107106890177e-05, 'epoch': 0.48}
{'loss': 0.246, 'learning_rate': 5.3646962349696806e-05, 'epoch': 0.48}
9:52:28, 21.02s/it]                                                          48%|████▊     | 3096/6500 [20:30:59<19:52:28, 21.02s/it] 48%|████▊     | 3097/6500 [20:31:17<19:02:41, 20.15s/it]                                                          48%|████▊     | 3097/6500 [20:31:17<19:02:41, 20.15s/it] 48%|████▊     | 3098/6500 [20:31:35<18:27:43, 19.54s/it]                                                          48%|████▊     | 3098/6500 [20:31:35<18:27:43, 19.54s/it] 48%|████▊     | 3099/6500 [20:31:53<18:03:30, 19.12s/it]                                                          48%|████▊     | 3099/6500 [20:31:53<18:03:30, 19.12s/it] 48%|████▊     | 3100/6500 [20:32:12<17:46:14, 18.82s/it]                                                          48%|████▊     | 3100/6500 [20:32:12<17:46:14, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8536329865455627, 'eval_runtime': 5.6204, 'eval_samples_per_second': 4.092, 'eval_steps_per_second': 1.068, 'epoch': 0.48}
                                                          48%|████▊     | 3100/6500 [20:32:17<17:46:14, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3100
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3100
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3100

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3100/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2697, 'learning_rate': 5.362285277803656e-05, 'epoch': 0.48}
{'loss': 0.2708, 'learning_rate': 5.3598742359556495e-05, 'epoch': 0.48}
{'loss': 0.2791, 'learning_rate': 5.35746310998923e-05, 'epoch': 0.48}
{'loss': 0.2856, 'learning_rate': 5.3550519004679823e-05, 'epoch': 0.48}
{'loss': 0.2825, 'learning_rate': 5.35264060795551e-05, 'epoch': 0.48}
 48%|████▊     | 3101/6500 [20:33:21<32:09:44, 34.06s/it]                                                          48%|████▊     | 3101/6500 [20:33:21<32:09:44, 34.06s/it] 48%|████▊     | 3102/6500 [20:33:39<27:36:39, 29.25s/it]                                                          48%|████▊     | 3102/6500 [20:33:39<27:36:39, 29.25s/it] 48%|████▊     | 3103/6500 [20:33:58<24:36:53, 26.09s/it]                                                          48%|████▊     | 3103/6500 [20:33:58<24:36:53, 26.09s/it] 48%|████▊     | 3104/6500 [20:34:16<22:20:11, 23.68s/it]                                                          48%|████▊     | 3104/6500 [20:34:16<22:20:11, 23.68s/it] 48%|████▊     | 3105/6500 [20:34:34<20:44:24, 21.99s/it]                                                          48%|████▊     | 3105/6500 [20:34:34<20:44:24, 21.99s/it] 48%|████▊     | 3106/6500 [20:34:52<1{'loss': 0.2846, 'learning_rate': 5.3502292330154404e-05, 'epoch': 0.48}
{'loss': 0.2885, 'learning_rate': 5.347817776211417e-05, 'epoch': 0.48}
{'loss': 0.3091, 'learning_rate': 5.3454062381071046e-05, 'epoch': 0.48}
{'loss': 0.2855, 'learning_rate': 5.342994619266182e-05, 'epoch': 0.48}
{'loss': 0.2924, 'learning_rate': 5.340582920252354e-05, 'epoch': 0.48}
9:37:35, 20.82s/it]                                                          48%|████▊     | 3106/6500 [20:34:52<19:37:35, 20.82s/it] 48%|████▊     | 3107/6500 [20:35:10<18:51:08, 20.00s/it]                                                          48%|████▊     | 3107/6500 [20:35:10<18:51:08, 20.00s/it] 48%|████▊     | 3108/6500 [20:35:28<18:18:46, 19.44s/it]                                                          48%|████▊     | 3108/6500 [20:35:28<18:18:46, 19.44s/it] 48%|████▊     | 3109/6500 [20:35:46<17:56:26, 19.05s/it]                                                          48%|████▊     | 3109/6500 [20:35:46<17:56:26, 19.05s/it] 48%|████▊     | 3110/6500 [20:36:05<17:40:40, 18.77s/it]                                                          48%|████▊     | 3110/6500 [20:36:05<17:40:40, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8375129103660583, 'eval_runtime': 5.3572, 'eval_samples_per_second': 4.293, 'eval_steps_per_second': 1.12, 'epoch': 0.48}
                                                          48%|████▊     | 3110/6500 [20:36:10<17:40:40, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3110
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3110
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3110/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3110/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3037, 'learning_rate': 5.338171141629338e-05, 'epoch': 0.48}
{'loss': 0.2736, 'learning_rate': 5.335759283960874e-05, 'epoch': 0.48}
{'loss': 0.282, 'learning_rate': 5.3333473478107184e-05, 'epoch': 0.48}
{'loss': 0.3041, 'learning_rate': 5.330935333742649e-05, 'epoch': 0.48}
{'loss': 0.2832, 'learning_rate': 5.328523242320456e-05, 'epoch': 0.48}
 48%|████▊     | 3111/6500 [20:37:02<28:37:07, 30.40s/it]                                                          48%|████▊     | 3111/6500 [20:37:02<28:37:07, 30.40s/it] 48%|████▊     | 3112/6500 [20:37:20<25:06:46, 26.68s/it]                                                          48%|████▊     | 3112/6500 [20:37:20<25:06:46, 26.68s/it] 48%|████▊     | 3113/6500 [20:37:38<22:39:15, 24.08s/it]                                                          48%|████▊     | 3113/6500 [20:37:38<22:39:15, 24.08s/it] 48%|████▊     | 3114/6500 [20:37:56<20:56:26, 22.26s/it]                                                          48%|████▊     | 3114/6500 [20:37:56<20:56:26, 22.26s/it] 48%|████▊     | 3115/6500 [20:38:14<19:44:32, 21.00s/it]                                                          48%|████▊     | 3115/6500 [20:38:14<19:44:32, 21.00s/it] 48%|████▊     | 3116/6500 [20:38:33<1{'loss': 0.2642, 'learning_rate': 5.3261110741079525e-05, 'epoch': 0.48}
{'loss': 0.2819, 'learning_rate': 5.323698829668968e-05, 'epoch': 0.48}
{'loss': 0.2795, 'learning_rate': 5.3212865095673514e-05, 'epoch': 0.48}
{'loss': 0.3584, 'learning_rate': 5.318874114366965e-05, 'epoch': 0.48}
{'loss': 0.2715, 'learning_rate': 5.316461644631694e-05, 'epoch': 0.48}
9:05:41, 20.31s/it]                                                          48%|████▊     | 3116/6500 [20:38:33<19:05:41, 20.31s/it] 48%|████▊     | 3117/6500 [20:38:51<18:27:29, 19.64s/it]                                                          48%|████▊     | 3117/6500 [20:38:51<18:27:29, 19.64s/it] 48%|████▊     | 3118/6500 [20:39:09<18:01:33, 19.19s/it]                                                          48%|████▊     | 3118/6500 [20:39:09<18:01:33, 19.19s/it] 48%|████▊     | 3119/6500 [20:39:28<17:47:47, 18.95s/it]                                                          48%|████▊     | 3119/6500 [20:39:28<17:47:47, 18.95s/it] 48%|████▊     | 3120/6500 [20:39:46<17:33:42, 18.70s/it]                                                          48%|████▊     | 3120/6500 [20:39:46<17:33:42, 18.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8435796499252319, 'eval_runtime': 5.3399, 'eval_samples_per_second': 4.307, 'eval_steps_per_second': 1.124, 'epoch': 0.48}
                                                          48%|████▊     | 3120/6500 [20:39:51<17:33:42, 18.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3120
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3120 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3120
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3120
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3120/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2749, 'learning_rate': 5.3140491009254376e-05, 'epoch': 0.48}
{'loss': 0.2934, 'learning_rate': 5.311636483812114e-05, 'epoch': 0.48}
{'loss': 0.7907, 'learning_rate': 5.309223793855655e-05, 'epoch': 0.48}
{'loss': 0.2818, 'learning_rate': 5.306811031620017e-05, 'epoch': 0.48}
{'loss': 0.2793, 'learning_rate': 5.3043981976691645e-05, 'epoch': 0.48}
 48%|████▊     | 3121/6500 [20:40:24<23:07:50, 24.64s/it]                                                          48%|████▊     | 3121/6500 [20:40:24<23:07:50, 24.64s/it] 48%|████▊     | 3122/6500 [20:40:42<21:15:47, 22.66s/it]                                                          48%|████▊     | 3122/6500 [20:40:42<21:15:47, 22.66s/it] 48%|████▊     | 3123/6500 [20:41:00<19:57:25, 21.28s/it]                                                          48%|████▊     | 3123/6500 [20:41:00<19:57:25, 21.28s/it] 48%|████▊     | 3124/6500 [20:41:18<19:03:22, 20.32s/it]                                                          48%|████▊     | 3124/6500 [20:41:18<19:03:22, 20.32s/it] 48%|████▊     | 3125/6500 [20:41:36<18:25:46, 19.66s/it]                                                          48%|████▊     | 3125/6500 [20:41:36<18:25:46, 19.66s/it] 48%|████▊     | 3126/6500 [20:41:55<1{'loss': 0.2895, 'learning_rate': 5.301985292567084e-05, 'epoch': 0.48}
{'loss': 0.2678, 'learning_rate': 5.299572316877778e-05, 'epoch': 0.48}
{'loss': 0.299, 'learning_rate': 5.297159271165264e-05, 'epoch': 0.48}
{'loss': 0.2607, 'learning_rate': 5.2947461559935786e-05, 'epoch': 0.48}
{'loss': 0.2652, 'learning_rate': 5.292332971926769e-05, 'epoch': 0.48}
7:59:35, 19.20s/it]                                                          48%|████▊     | 3126/6500 [20:41:55<17:59:35, 19.20s/it] 48%|████▊     | 3127/6500 [20:42:13<17:45:28, 18.95s/it]                                                          48%|████▊     | 3127/6500 [20:42:13<17:45:28, 18.95s/it] 48%|████▊     | 3128/6500 [20:42:31<17:31:29, 18.71s/it]                                                          48%|████▊     | 3128/6500 [20:42:31<17:31:29, 18.71s/it] 48%|████▊     | 3129/6500 [20:42:49<17:21:46, 18.54s/it]                                                          48%|████▊     | 3129/6500 [20:42:49<17:21:46, 18.54s/it] 48%|████▊     | 3130/6500 [20:43:07<17:15:18, 18.43s/it]                                                          48%|████▊     | 3130/6500 [20:43:07<17:15:18, 18.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8527871966362, 'eval_runtime': 5.3483, 'eval_samples_per_second': 4.3, 'eval_steps_per_second': 1.122, 'epoch': 0.48}
                                                          48%|████▊     | 3130/6500 [20:43:13<17:15:18, 18.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3130
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3130
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3130 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3130
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3130/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3130/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2735, 'learning_rate': 5.289919719528905e-05, 'epoch': 0.48}
{'loss': 0.2678, 'learning_rate': 5.2875063993640707e-05, 'epoch': 0.48}
{'loss': 0.2852, 'learning_rate': 5.285093011996362e-05, 'epoch': 0.48}
{'loss': 0.2742, 'learning_rate': 5.2826795579898956e-05, 'epoch': 0.48}
{'loss': 0.2856, 'learning_rate': 5.280266037908802e-05, 'epoch': 0.48}
 48%|████▊     | 3131/6500 [20:44:22<32:55:46, 35.19s/it]                                                          48%|████▊     | 3131/6500 [20:44:22<32:55:46, 35.19s/it] 48%|████▊     | 3132/6500 [20:44:40<28:06:03, 30.04s/it]                                                          48%|████▊     | 3132/6500 [20:44:40<28:06:03, 30.04s/it] 48%|████▊     | 3133/6500 [20:44:58<24:46:20, 26.49s/it]                                                          48%|████▊     | 3133/6500 [20:44:58<24:46:20, 26.49s/it] 48%|████▊     | 3134/6500 [20:45:16<22:27:16, 24.02s/it]                                                          48%|████▊     | 3134/6500 [20:45:16<22:27:16, 24.02s/it] 48%|████▊     | 3135/6500 [20:45:35<20:52:39, 22.34s/it]                                                          48%|████▊     | 3135/6500 [20:45:35<20:52:39, 22.34s/it] 48%|████▊     | 3136/6500 [20:45:53<1{'loss': 0.2654, 'learning_rate': 5.277852452317226e-05, 'epoch': 0.48}
{'loss': 0.2892, 'learning_rate': 5.2754388017793274e-05, 'epoch': 0.48}
{'loss': 0.2755, 'learning_rate': 5.2730250868592845e-05, 'epoch': 0.48}
{'loss': 0.2717, 'learning_rate': 5.270611308121287e-05, 'epoch': 0.48}
{'loss': 0.2873, 'learning_rate': 5.268197466129542e-05, 'epoch': 0.48}
9:50:06, 21.23s/it]                                                          48%|████▊     | 3136/6500 [20:45:53<19:50:06, 21.23s/it] 48%|████▊     | 3137/6500 [20:46:11<18:56:25, 20.28s/it]                                                          48%|████▊     | 3137/6500 [20:46:11<18:56:25, 20.28s/it] 48%|████▊     | 3138/6500 [20:46:29<18:20:13, 19.64s/it]                                                          48%|████▊     | 3138/6500 [20:46:29<18:20:13, 19.64s/it] 48%|████▊     | 3139/6500 [20:46:48<17:54:15, 19.18s/it]                                                          48%|████▊     | 3139/6500 [20:46:48<17:54:15, 19.18s/it] 48%|████▊     | 3140/6500 [20:47:06<17:36:05, 18.86s/it]                                                          48%|████▊     | 3140/6500 [20:47:06<17:36:05, 18.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8481982946395874, 'eval_runtime': 5.4841, 'eval_samples_per_second': 4.194, 'eval_steps_per_second': 1.094, 'epoch': 0.48}
                                                          48%|████▊     | 3140/6500 [20:47:11<17:36:05, 18.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3140
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3140
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3140/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3140/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2735, 'learning_rate': 5.2657835614482706e-05, 'epoch': 0.48}
{'loss': 0.285, 'learning_rate': 5.2633695946417075e-05, 'epoch': 0.48}
{'loss': 0.2688, 'learning_rate': 5.260955566274103e-05, 'epoch': 0.48}
{'loss': 0.3013, 'learning_rate': 5.2585414769097207e-05, 'epoch': 0.48}
{'loss': 0.2744, 'learning_rate': 5.2561273271128396e-05, 'epoch': 0.48}
 48%|████▊     | 3141/6500 [20:48:22<33:32:51, 35.95s/it]                                                          48%|████▊     | 3141/6500 [20:48:22<33:32:51, 35.95s/it] 48%|████▊     | 3142/6500 [20:48:40<28:30:45, 30.57s/it]                                                          48%|████▊     | 3142/6500 [20:48:40<28:30:45, 30.57s/it] 48%|████▊     | 3143/6500 [20:48:58<24:59:29, 26.80s/it]                                                          48%|████▊     | 3143/6500 [20:48:58<24:59:29, 26.80s/it] 48%|████▊     | 3144/6500 [20:49:16<22:32:13, 24.18s/it]                                                          48%|████▊     | 3144/6500 [20:49:16<22:32:13, 24.18s/it] 48%|████▊     | 3145/6500 [20:49:34<20:49:24, 22.34s/it]                                                          48%|████▊     | 3145/6500 [20:49:34<20:49:24, 22.34s/it] 48%|████▊     | 3146/6500 [20:49:52<1{'loss': 0.271, 'learning_rate': 5.253713117447755e-05, 'epoch': 0.48}
{'loss': 0.2677, 'learning_rate': 5.2512988484787704e-05, 'epoch': 0.48}
{'loss': 0.2936, 'learning_rate': 5.248884520770209e-05, 'epoch': 0.48}
{'loss': 0.3364, 'learning_rate': 5.246470134886403e-05, 'epoch': 0.48}
{'loss': 0.273, 'learning_rate': 5.2440556913917014e-05, 'epoch': 0.48}
9:38:04, 21.07s/it]                                                          48%|████▊     | 3146/6500 [20:49:52<19:38:04, 21.07s/it] 48%|████▊     | 3147/6500 [20:50:10<18:48:05, 20.19s/it]                                                          48%|████▊     | 3147/6500 [20:50:10<18:48:05, 20.19s/it] 48%|████▊     | 3148/6500 [20:50:28<18:13:22, 19.57s/it]                                                          48%|████▊     | 3148/6500 [20:50:28<18:13:22, 19.57s/it] 48%|████▊     | 3149/6500 [20:50:46<17:49:12, 19.14s/it]                                                          48%|████▊     | 3149/6500 [20:50:46<17:49:12, 19.14s/it] 48%|████▊     | 3150/6500 [20:51:04<17:32:32, 18.85s/it]                                                          48%|████▊     | 3150/6500 [20:51:04<17:32:32, 18.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8488549590110779, 'eval_runtime': 5.3455, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.122, 'epoch': 0.48}
                                                          48%|████▊     | 3150/6500 [20:51:10<17:32:32, 18.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3150I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3150/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2637, 'learning_rate': 5.241641190850466e-05, 'epoch': 0.48}
{'loss': 0.2938, 'learning_rate': 5.2392266338270736e-05, 'epoch': 0.48}
{'loss': 0.7953, 'learning_rate': 5.236812020885907e-05, 'epoch': 0.49}
{'loss': 0.2948, 'learning_rate': 5.2343973525913716e-05, 'epoch': 0.49}
{'loss': 0.2832, 'learning_rate': 5.23198262950788e-05, 'epoch': 0.49}
 48%|████▊     | 3151/6500 [20:52:29<35:58:15, 38.67s/it]                                                          48%|████▊     | 3151/6500 [20:52:29<35:58:15, 38.67s/it] 48%|████▊     | 3152/6500 [20:52:47<30:15:26, 32.53s/it]                                                          48%|████▊     | 3152/6500 [20:52:47<30:15:26, 32.53s/it] 49%|████▊     | 3153/6500 [20:53:05<26:11:04, 28.16s/it]                                                          49%|████▊     | 3153/6500 [20:53:05<26:11:04, 28.16s/it] 49%|████▊     | 3154/6500 [20:53:23<23:20:27, 25.11s/it]                                                          49%|████▊     | 3154/6500 [20:53:23<23:20:27, 25.11s/it] 49%|████▊     | 3155/6500 [20:53:41<21:21:25, 22.99s/it]                                                          49%|████▊     | 3155/6500 [20:53:41<21:21:25, 22.99s/it] 49%|████▊     | 3156/6500 [20:53:59<1{'loss': 0.2735, 'learning_rate': 5.229567852199859e-05, 'epoch': 0.49}
{'loss': 0.2669, 'learning_rate': 5.2271530212317487e-05, 'epoch': 0.49}
{'loss': 0.2893, 'learning_rate': 5.2247381371680014e-05, 'epoch': 0.49}
{'loss': 0.2655, 'learning_rate': 5.222323200573081e-05, 'epoch': 0.49}
{'loss': 0.2742, 'learning_rate': 5.219908212011463e-05, 'epoch': 0.49}
9:58:19, 21.50s/it]                                                          49%|████▊     | 3156/6500 [20:53:59<19:58:19, 21.50s/it] 49%|████▊     | 3157/6500 [20:54:18<19:00:35, 20.47s/it]                                                          49%|████▊     | 3157/6500 [20:54:18<19:00:35, 20.47s/it] 49%|████▊     | 3158/6500 [20:54:36<18:24:00, 19.82s/it]                                                          49%|████▊     | 3158/6500 [20:54:36<18:24:00, 19.82s/it] 49%|████▊     | 3159/6500 [20:54:54<17:55:07, 19.31s/it]                                                          49%|████▊     | 3159/6500 [20:54:54<17:55:07, 19.31s/it] 49%|████▊     | 3160/6500 [20:55:12<17:35:17, 18.96s/it]                                                          49%|████▊     | 3160/6500 [20:55:12<17:35:17, 18.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8565986752510071, 'eval_runtime': 6.3262, 'eval_samples_per_second': 3.636, 'eval_steps_per_second': 0.948, 'epoch': 0.49}
                                                          49%|████▊     | 3160/6500 [20:55:18<17:35:17, 18.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3160
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3160/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3160the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3160
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3160/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2806, 'learning_rate': 5.217493172047637e-05, 'epoch': 0.49}
{'loss': 0.2675, 'learning_rate': 5.2150780812461075e-05, 'epoch': 0.49}
{'loss': 0.2724, 'learning_rate': 5.2126629401713814e-05, 'epoch': 0.49}
{'loss': 0.2725, 'learning_rate': 5.210247749387986e-05, 'epoch': 0.49}
{'loss': 0.2835, 'learning_rate': 5.2078325094604596e-05, 'epoch': 0.49}
 49%|████▊     | 3161/6500 [20:55:51<23:02:27, 24.84s/it]                                                          49%|████▊     | 3161/6500 [20:55:51<23:02:27, 24.84s/it] 49%|████▊     | 3162/6500 [20:56:09<21:08:41, 22.80s/it]                                                          49%|████▊     | 3162/6500 [20:56:09<21:08:41, 22.80s/it] 49%|████▊     | 3163/6500 [20:56:27<19:49:00, 21.38s/it]                                                          49%|████▊     | 3163/6500 [20:56:27<19:49:00, 21.38s/it] 49%|████▊     | 3164/6500 [20:56:45<18:53:32, 20.39s/it]                                                          49%|████▊     | 3164/6500 [20:56:45<18:53:32, 20.39s/it] 49%|████▊     | 3165/6500 [20:57:03<18:14:56, 19.70s/it]                                                          49%|████▊     | 3165/6500 [20:57:03<18:14:56, 19.70s/it] 49%|████▊     | 3166/6500 [20:57:21<1{'loss': 0.2802, 'learning_rate': 5.205417220953346e-05, 'epoch': 0.49}
{'loss': 0.2889, 'learning_rate': 5.203001884431208e-05, 'epoch': 0.49}
{'loss': 0.2824, 'learning_rate': 5.200586500458612e-05, 'epoch': 0.49}
{'loss': 0.2855, 'learning_rate': 5.198171069600141e-05, 'epoch': 0.49}
{'loss': 0.2901, 'learning_rate': 5.195755592420387e-05, 'epoch': 0.49}
7:48:15, 19.22s/it]                                                          49%|████▊     | 3166/6500 [20:57:21<17:48:15, 19.22s/it] 49%|████▊     | 3167/6500 [20:57:39<17:29:40, 18.90s/it]                                                          49%|████▊     | 3167/6500 [20:57:39<17:29:40, 18.90s/it] 49%|████▊     | 3168/6500 [20:57:58<17:25:06, 18.82s/it]                                                          49%|████▊     | 3168/6500 [20:57:58<17:25:06, 18.82s/it] 49%|████▉     | 3169/6500 [20:58:16<17:13:32, 18.62s/it]                                                          49%|████▉     | 3169/6500 [20:58:16<17:13:32, 18.62s/it] 49%|████▉     | 3170/6500 [20:58:34<17:05:40, 18.48s/it]                                                          49%|████▉     | 3170/6500 [20:58:34<17:05:40, 18.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8440902829170227, 'eval_runtime': 5.328, 'eval_samples_per_second': 4.317, 'eval_steps_per_second': 1.126, 'epoch': 0.49}
                                                          49%|████▉     | 3170/6500 [20:58:39<17:05:40, 18.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3170
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3170/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3170


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3170/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3170/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2822, 'learning_rate': 5.193340069483955e-05, 'epoch': 0.49}
{'loss': 0.2842, 'learning_rate': 5.1909245013554564e-05, 'epoch': 0.49}
{'loss': 0.2841, 'learning_rate': 5.188508888599517e-05, 'epoch': 0.49}
{'loss': 0.2852, 'learning_rate': 5.186093231780771e-05, 'epoch': 0.49}
{'loss': 0.2679, 'learning_rate': 5.183677531463863e-05, 'epoch': 0.49}
 49%|████▉     | 3171/6500 [20:59:26<26:22:37, 28.52s/it]                                                          49%|████▉     | 3171/6500 [20:59:26<26:22:37, 28.52s/it] 49%|████▉     | 3172/6500 [20:59:44<23:31:15, 25.44s/it]                                                          49%|████▉     | 3172/6500 [20:59:44<23:31:15, 25.44s/it] 49%|████▉     | 3173/6500 [21:00:02<21:28:15, 23.23s/it]                                                          49%|████▉     | 3173/6500 [21:00:02<21:28:15, 23.23s/it] 49%|████▉     | 3174/6500 [21:00:20<20:01:43, 21.68s/it]                                                          49%|████▉     | 3174/6500 [21:00:20<20:01:43, 21.68s/it] 49%|████▉     | 3175/6500 [21:00:39<19:01:10, 20.59s/it]                                                          49%|████▉     | 3175/6500 [21:00:39<19:01:10, 20.59s/it] 49%|████▉     | 3176/6500 [21:00:57<1{'loss': 0.2733, 'learning_rate': 5.1812617882134486e-05, 'epoch': 0.49}
{'loss': 0.2567, 'learning_rate': 5.1788460025941934e-05, 'epoch': 0.49}
{'loss': 0.3054, 'learning_rate': 5.1764301751707735e-05, 'epoch': 0.49}
{'loss': 0.329, 'learning_rate': 5.174014306507873e-05, 'epoch': 0.49}
{'loss': 0.277, 'learning_rate': 5.171598397170184e-05, 'epoch': 0.49}
8:18:55, 19.84s/it]                                                          49%|████▉     | 3176/6500 [21:00:57<18:18:55, 19.84s/it] 49%|████▉     | 3177/6500 [21:01:15<17:49:46, 19.32s/it]                                                          49%|████▉     | 3177/6500 [21:01:15<17:49:46, 19.32s/it] 49%|████▉     | 3178/6500 [21:01:33<17:29:24, 18.95s/it]                                                          49%|████▉     | 3178/6500 [21:01:33<17:29:24, 18.95s/it] 49%|████▉     | 3179/6500 [21:01:51<17:15:28, 18.71s/it]                                                          49%|████▉     | 3179/6500 [21:01:51<17:15:28, 18.71s/it] 49%|████▉     | 3180/6500 [21:02:09<17:05:56, 18.54s/it]                                                          49%|████▉     | 3180/6500 [21:02:09<17:05:56, 18.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8514779806137085, 'eval_runtime': 5.353, 'eval_samples_per_second': 4.297, 'eval_steps_per_second': 1.121, 'epoch': 0.49}
                                                          49%|████▉     | 3180/6500 [21:02:14<17:05:56, 18.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3180
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3180
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3180

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3180/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2864, 'learning_rate': 5.169182447722415e-05, 'epoch': 0.49}
{'loss': 0.2845, 'learning_rate': 5.1667664587292776e-05, 'epoch': 0.49}
{'loss': 0.8121, 'learning_rate': 5.164350430755494e-05, 'epoch': 0.49}
{'loss': 0.2794, 'learning_rate': 5.161934364365796e-05, 'epoch': 0.49}
{'loss': 0.2808, 'learning_rate': 5.159518260124925e-05, 'epoch': 0.49}
 49%|████▉     | 3181/6500 [21:03:18<31:03:35, 33.69s/it]                                                          49%|████▉     | 3181/6500 [21:03:18<31:03:35, 33.69s/it] 49%|████▉     | 3182/6500 [21:03:36<26:45:31, 29.03s/it]                                                          49%|████▉     | 3182/6500 [21:03:36<26:45:31, 29.03s/it] 49%|████▉     | 3183/6500 [21:03:54<23:41:28, 25.71s/it]                                                          49%|████▉     | 3183/6500 [21:03:54<23:41:28, 25.71s/it] 49%|████▉     | 3184/6500 [21:04:13<21:46:28, 23.64s/it]                                                          49%|████▉     | 3184/6500 [21:04:13<21:46:28, 23.64s/it] 49%|████▉     | 3185/6500 [21:04:31<20:17:06, 22.03s/it]                                                          49%|████▉     | 3185/6500 [21:04:31<20:17:06, 22.03s/it] 49%|████▉     | 3186/6500 [21:04:49<1{'loss': 0.2564, 'learning_rate': 5.157102118597631e-05, 'epoch': 0.49}
{'loss': 0.2812, 'learning_rate': 5.154685940348671e-05, 'epoch': 0.49}
{'loss': 0.2736, 'learning_rate': 5.1522697259428146e-05, 'epoch': 0.49}
{'loss': 0.2471, 'learning_rate': 5.1498534759448346e-05, 'epoch': 0.49}
{'loss': 0.2788, 'learning_rate': 5.147437190919516e-05, 'epoch': 0.49}
9:11:04, 20.84s/it]                                                          49%|████▉     | 3186/6500 [21:04:49<19:11:04, 20.84s/it] 49%|████▉     | 3187/6500 [21:05:07<18:24:35, 20.00s/it]                                                          49%|████▉     | 3187/6500 [21:05:07<18:24:35, 20.00s/it] 49%|████▉     | 3188/6500 [21:05:26<17:52:43, 19.43s/it]                                                          49%|████▉     | 3188/6500 [21:05:26<17:52:43, 19.43s/it] 49%|████▉     | 3189/6500 [21:05:44<17:30:57, 19.04s/it]                                                          49%|████▉     | 3189/6500 [21:05:44<17:30:57, 19.04s/it] 49%|████▉     | 3190/6500 [21:06:02<17:15:42, 18.77s/it]                                                          49%|████▉     | 3190/6500 [21:06:02<17:15:42, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8525924682617188, 'eval_runtime': 5.5514, 'eval_samples_per_second': 4.143, 'eval_steps_per_second': 1.081, 'epoch': 0.49}
                                                          49%|████▉     | 3190/6500 [21:06:07<17:15:42, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3190
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3190
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3190
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3190
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3190/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3190/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2622, 'learning_rate': 5.1450208714316504e-05, 'epoch': 0.49}
{'loss': 0.2726, 'learning_rate': 5.142604518046038e-05, 'epoch': 0.49}
{'loss': 0.2776, 'learning_rate': 5.140188131327486e-05, 'epoch': 0.49}
{'loss': 0.2768, 'learning_rate': 5.1377717118408105e-05, 'epoch': 0.49}
{'loss': 0.2721, 'learning_rate': 5.1353552601508356e-05, 'epoch': 0.49}
 49%|████▉     | 3191/6500 [21:07:31<36:36:41, 39.83s/it]                                                          49%|████▉     | 3191/6500 [21:07:31<36:36:41, 39.83s/it] 49%|████▉     | 3192/6500 [21:07:49<30:35:17, 33.29s/it]                                                          49%|████▉     | 3192/6500 [21:07:49<30:35:17, 33.29s/it] 49%|████▉     | 3193/6500 [21:08:07<26:22:31, 28.71s/it]                                                          49%|████▉     | 3193/6500 [21:08:07<26:22:31, 28.71s/it] 49%|████▉     | 3194/6500 [21:08:25<23:26:18, 25.52s/it]                                                          49%|████▉     | 3194/6500 [21:08:25<23:26:18, 25.52s/it] 49%|████▉     | 3195/6500 [21:08:43<21:22:10, 23.28s/it]                                                          49%|████▉     | 3195/6500 [21:08:43<21:22:10, 23.28s/it] 49%|████▉     | 3196/6500 [21:09:01<1{'loss': 0.2788, 'learning_rate': 5.132938776822391e-05, 'epoch': 0.49}
{'loss': 0.2874, 'learning_rate': 5.130522262420316e-05, 'epoch': 0.49}
{'loss': 0.2652, 'learning_rate': 5.128105717509456e-05, 'epoch': 0.49}
{'loss': 0.288, 'learning_rate': 5.1256891426546625e-05, 'epoch': 0.49}
{'loss': 0.2732, 'learning_rate': 5.123272538420798e-05, 'epoch': 0.49}
9:55:52, 21.72s/it]                                                          49%|████▉     | 3196/6500 [21:09:01<19:55:52, 21.72s/it] 49%|████▉     | 3197/6500 [21:09:19<18:55:47, 20.63s/it]                                                          49%|████▉     | 3197/6500 [21:09:19<18:55:47, 20.63s/it] 49%|████▉     | 3198/6500 [21:09:37<18:14:15, 19.88s/it]                                                          49%|████▉     | 3198/6500 [21:09:37<18:14:15, 19.88s/it] 49%|████▉     | 3199/6500 [21:09:55<17:44:47, 19.35s/it]                                                          49%|████▉     | 3199/6500 [21:09:55<17:44:47, 19.35s/it] 49%|████▉     | 3200/6500 [21:10:14<17:28:45, 19.07s/it]                                                          49%|████▉     | 3200/6500 [21:10:14<17:28:45, 19.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8514358401298523, 'eval_runtime': 5.4807, 'eval_samples_per_second': 4.197, 'eval_steps_per_second': 1.095, 'epoch': 0.49}
                                                          49%|████▉     | 3200/6500 [21:10:19<17:28:45, 19.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3200
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3200
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3200/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2711, 'learning_rate': 5.1208559053727257e-05, 'epoch': 0.49}
{'loss': 0.28, 'learning_rate': 5.11843924407532e-05, 'epoch': 0.49}
{'loss': 0.286, 'learning_rate': 5.1160225550934624e-05, 'epoch': 0.49}
{'loss': 0.275, 'learning_rate': 5.1136058389920374e-05, 'epoch': 0.49}
{'loss': 0.2702, 'learning_rate': 5.111189096335939e-05, 'epoch': 0.49}
 49%|████▉     | 3201/6500 [21:11:29<32:49:52, 35.83s/it]                                                          49%|████▉     | 3201/6500 [21:11:29<32:49:52, 35.83s/it] 49%|████▉     | 3202/6500 [21:11:47<27:55:34, 30.48s/it]                                                          49%|████▉     | 3202/6500 [21:11:47<27:55:34, 30.48s/it] 49%|████▉     | 3203/6500 [21:12:05<24:29:35, 26.74s/it]                                                          49%|████▉     | 3203/6500 [21:12:05<24:29:35, 26.74s/it] 49%|████▉     | 3204/6500 [21:12:23<22:05:56, 24.14s/it]                                                          49%|████▉     | 3204/6500 [21:12:23<22:05:56, 24.14s/it] 49%|████▉     | 3205/6500 [21:12:41<20:25:51, 22.32s/it]                                                          49%|████▉     | 3205/6500 [21:12:41<20:25:51, 22.32s/it] 49%|████▉     | 3206/6500 [21:12:59<1{'loss': 0.2665, 'learning_rate': 5.1087723276900646e-05, 'epoch': 0.49}
{'loss': 0.2787, 'learning_rate': 5.106355533619319e-05, 'epoch': 0.49}
{'loss': 0.3401, 'learning_rate': 5.1039387146886154e-05, 'epoch': 0.49}
{'loss': 0.2689, 'learning_rate': 5.101521871462869e-05, 'epoch': 0.49}
{'loss': 0.2591, 'learning_rate': 5.0991050045070024e-05, 'epoch': 0.49}
9:17:20, 21.08s/it]                                                          49%|████▉     | 3206/6500 [21:12:59<19:17:20, 21.08s/it] 49%|████▉     | 3207/6500 [21:13:17<18:30:07, 20.23s/it]                                                          49%|████▉     | 3207/6500 [21:13:17<18:30:07, 20.23s/it] 49%|████▉     | 3208/6500 [21:13:35<17:55:25, 19.60s/it]                                                          49%|████▉     | 3208/6500 [21:13:35<17:55:25, 19.60s/it] 49%|████▉     | 3209/6500 [21:13:54<17:31:10, 19.16s/it]                                                          49%|████▉     | 3209/6500 [21:13:54<17:31:10, 19.16s/it] 49%|████▉     | 3210/6500 [21:14:12<17:16:03, 18.89s/it]                                                          49%|████▉     | 3210/6500 [21:14:12<17:16:03, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8489271998405457, 'eval_runtime': 5.345, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.123, 'epoch': 0.49}
                                                          49%|████▉     | 3210/6500 [21:14:17<17:16:03, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3210 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3210
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3210/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3210/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2885, 'learning_rate': 5.0966881143859435e-05, 'epoch': 0.49}
{'loss': 0.7934, 'learning_rate': 5.094271201664625e-05, 'epoch': 0.49}
{'loss': 0.2817, 'learning_rate': 5.091854266907987e-05, 'epoch': 0.49}
{'loss': 0.2695, 'learning_rate': 5.089437310680972e-05, 'epoch': 0.49}
{'loss': 0.2852, 'learning_rate': 5.08702033354853e-05, 'epoch': 0.49}
 49%|████▉     | 3211/6500 [21:15:38<35:43:56, 39.11s/it]                                                          49%|████▉     | 3211/6500 [21:15:38<35:43:56, 39.11s/it] 49%|████▉     | 3212/6500 [21:15:56<29:55:16, 32.76s/it]                                                          49%|████▉     | 3212/6500 [21:15:56<29:55:16, 32.76s/it] 49%|████▉     | 3213/6500 [21:16:14<25:51:52, 28.33s/it]                                                          49%|████▉     | 3213/6500 [21:16:14<25:51:52, 28.33s/it] 49%|████▉     | 3214/6500 [21:16:32<23:02:04, 25.24s/it]                                                          49%|████▉     | 3214/6500 [21:16:32<23:02:04, 25.24s/it] 49%|████▉     | 3215/6500 [21:16:50<21:03:45, 23.08s/it]                                                          49%|████▉     | 3215/6500 [21:16:50<21:03:45, 23.08s/it] 49%|████▉     | 3216/6500 [21:17:09<1{'loss': 0.2586, 'learning_rate': 5.0846033360756155e-05, 'epoch': 0.49}
{'loss': 0.2868, 'learning_rate': 5.082186318827184e-05, 'epoch': 0.49}
{'loss': 0.2599, 'learning_rate': 5.0797692823682e-05, 'epoch': 0.5}
{'loss': 0.2578, 'learning_rate': 5.077352227263632e-05, 'epoch': 0.5}
{'loss': 0.2748, 'learning_rate': 5.07493515407845e-05, 'epoch': 0.5}
9:51:45, 21.77s/it]                                                          49%|████▉     | 3216/6500 [21:17:09<19:51:45, 21.77s/it] 49%|████▉     | 3217/6500 [21:17:27<18:50:56, 20.67s/it]                                                          49%|████▉     | 3217/6500 [21:17:27<18:50:56, 20.67s/it] 50%|████▉     | 3218/6500 [21:17:46<18:19:32, 20.10s/it]                                                          50%|████▉     | 3218/6500 [21:17:46<18:19:32, 20.10s/it] 50%|████▉     | 3219/6500 [21:18:04<17:46:52, 19.51s/it]                                                          50%|████▉     | 3219/6500 [21:18:04<17:46:52, 19.51s/it] 50%|████▉     | 3220/6500 [21:18:22<17:24:03, 19.10s/it]                                                          50%|████▉     | 3220/6500 [21:18:22<17:24:03, 19.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8580039739608765, 'eval_runtime': 5.4832, 'eval_samples_per_second': 4.195, 'eval_steps_per_second': 1.094, 'epoch': 0.5}
                                                          50%|████▉     | 3220/6500 [21:18:28<17:24:03, 19.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3220
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3220
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3220/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2628, 'learning_rate': 5.0725180633776315e-05, 'epoch': 0.5}
{'loss': 0.2765, 'learning_rate': 5.070100955726159e-05, 'epoch': 0.5}
{'loss': 0.2661, 'learning_rate': 5.0676838316890116e-05, 'epoch': 0.5}
{'loss': 0.2833, 'learning_rate': 5.065266691831181e-05, 'epoch': 0.5}
{'loss': 0.2697, 'learning_rate': 5.0628495367176576e-05, 'epoch': 0.5}
 50%|████▉     | 3221/6500 [21:19:42<34:06:54, 37.45s/it]                                                          50%|████▉     | 3221/6500 [21:19:42<34:06:54, 37.45s/it] 50%|████▉     | 3222/6500 [21:20:00<28:48:04, 31.63s/it]                                                          50%|████▉     | 3222/6500 [21:20:00<28:48:04, 31.63s/it] 50%|████▉     | 3223/6500 [21:20:18<25:04:44, 27.55s/it]                                                          50%|████▉     | 3223/6500 [21:20:18<25:04:44, 27.55s/it] 50%|████▉     | 3224/6500 [21:20:36<22:28:51, 24.70s/it]                                                          50%|████▉     | 3224/6500 [21:20:36<22:28:51, 24.70s/it] 50%|████▉     | 3225/6500 [21:20:55<20:39:52, 22.72s/it]                                                          50%|████▉     | 3225/6500 [21:20:55<20:39:52, 22.72s/it] 50%|████▉     | 3226/6500 [21:21:13<1{'loss': 0.2844, 'learning_rate': 5.060432366913438e-05, 'epoch': 0.5}
{'loss': 0.2681, 'learning_rate': 5.058015182983519e-05, 'epoch': 0.5}
{'loss': 0.2727, 'learning_rate': 5.055597985492906e-05, 'epoch': 0.5}
{'loss': 0.2788, 'learning_rate': 5.053180775006599e-05, 'epoch': 0.5}
{'loss': 0.279, 'learning_rate': 5.050763552089611e-05, 'epoch': 0.5}
9:24:10, 21.33s/it]                                                          50%|████▉     | 3226/6500 [21:21:13<19:24:10, 21.33s/it] 50%|████▉     | 3227/6500 [21:21:31<18:31:28, 20.38s/it]                                                          50%|████▉     | 3227/6500 [21:21:31<18:31:28, 20.38s/it] 50%|████▉     | 3228/6500 [21:21:49<17:54:36, 19.71s/it]                                                          50%|████▉     | 3228/6500 [21:21:49<17:54:36, 19.71s/it] 50%|████▉     | 3229/6500 [21:22:07<17:29:18, 19.25s/it]                                                          50%|████▉     | 3229/6500 [21:22:07<17:29:18, 19.25s/it] 50%|████▉     | 3230/6500 [21:22:25<17:11:44, 18.93s/it]                                                          50%|████▉     | 3230/6500 [21:22:25<17:11:44, 18.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8550933599472046, 'eval_runtime': 5.3388, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.5}
                                                          50%|████▉     | 3230/6500 [21:22:31<17:11:44, 18.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3230/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3230/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2776, 'learning_rate': 5.04834631730695e-05, 'epoch': 0.5}
{'loss': 0.2835, 'learning_rate': 5.0459290712236326e-05, 'epoch': 0.5}
{'loss': 0.2821, 'learning_rate': 5.043511814404673e-05, 'epoch': 0.5}
{'loss': 0.262, 'learning_rate': 5.0410945474150916e-05, 'epoch': 0.5}
{'loss': 0.2619, 'learning_rate': 5.0386772708199104e-05, 'epoch': 0.5}
 50%|████▉     | 3231/6500 [21:23:35<31:02:41, 34.19s/it]                                                          50%|████▉     | 3231/6500 [21:23:35<31:02:41, 34.19s/it] 50%|████▉     | 3232/6500 [21:23:53<26:38:11, 29.34s/it]                                                          50%|████▉     | 3232/6500 [21:23:53<26:38:11, 29.34s/it] 50%|████▉     | 3233/6500 [21:24:12<23:39:49, 26.08s/it]                                                          50%|████▉     | 3233/6500 [21:24:12<23:39:49, 26.08s/it] 50%|████▉     | 3234/6500 [21:24:30<21:28:55, 23.68s/it]                                                          50%|████▉     | 3234/6500 [21:24:30<21:28:55, 23.68s/it] 50%|████▉     | 3235/6500 [21:24:48<19:57:32, 22.01s/it]                                                          50%|████▉     | 3235/6500 [21:24:48<19:57:32, 22.01s/it] 50%|████▉     | 3236/6500 [21:25:06<1{'loss': 0.251, 'learning_rate': 5.036259985184151e-05, 'epoch': 0.5}
{'loss': 0.296, 'learning_rate': 5.033842691072841e-05, 'epoch': 0.5}
{'loss': 0.3253, 'learning_rate': 5.031425389051009e-05, 'epoch': 0.5}
{'loss': 0.267, 'learning_rate': 5.0290080796836826e-05, 'epoch': 0.5}
{'loss': 0.2819, 'learning_rate': 5.0265907635358934e-05, 'epoch': 0.5}
8:52:44, 20.82s/it]                                                          50%|████▉     | 3236/6500 [21:25:06<18:52:44, 20.82s/it] 50%|████▉     | 3237/6500 [21:25:24<18:07:17, 19.99s/it]                                                          50%|████▉     | 3237/6500 [21:25:24<18:07:17, 19.99s/it] 50%|████▉     | 3238/6500 [21:25:42<17:36:13, 19.43s/it]                                                          50%|████▉     | 3238/6500 [21:25:42<17:36:13, 19.43s/it] 50%|████▉     | 3239/6500 [21:26:00<17:14:41, 19.04s/it]                                                          50%|████▉     | 3239/6500 [21:26:00<17:14:41, 19.04s/it] 50%|████▉     | 3240/6500 [21:26:18<16:59:46, 18.77s/it]                                                          50%|████▉     | 3240/6500 [21:26:18<16:59:46, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8534206748008728, 'eval_runtime': 5.3512, 'eval_samples_per_second': 4.298, 'eval_steps_per_second': 1.121, 'epoch': 0.5}
                                                          50%|████▉     | 3240/6500 [21:26:24<16:59:46, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3240
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3240
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3240
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3240
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3240/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2654, 'learning_rate': 5.024173441172675e-05, 'epoch': 0.5}
{'loss': 0.7895, 'learning_rate': 5.021756113159062e-05, 'epoch': 0.5}
{'loss': 0.2809, 'learning_rate': 5.01933878006009e-05, 'epoch': 0.5}
{'loss': 0.2764, 'learning_rate': 5.0169214424407965e-05, 'epoch': 0.5}
{'loss': 0.2573, 'learning_rate': 5.0145041008662166e-05, 'epoch': 0.5}
 50%|████▉     | 3241/6500 [21:28:04<40:41:49, 44.96s/it]                                                          50%|████▉     | 3241/6500 [21:28:04<40:41:49, 44.96s/it] 50%|████▉     | 3242/6500 [21:28:22<33:22:14, 36.87s/it]                                                          50%|████▉     | 3242/6500 [21:28:22<33:22:14, 36.87s/it] 50%|████▉     | 3243/6500 [21:28:40<28:13:35, 31.20s/it]                                                          50%|████▉     | 3243/6500 [21:28:40<28:13:35, 31.20s/it] 50%|████▉     | 3244/6500 [21:28:58<24:37:52, 27.23s/it]                                                          50%|████▉     | 3244/6500 [21:28:58<24:37:52, 27.23s/it] 50%|████▉     | 3245/6500 [21:29:16<22:06:43, 24.46s/it]                                                          50%|████▉     | 3245/6500 [21:29:16<22:06:43, 24.46s/it] 50%|████▉     | 3246/6500 [21:29:34<2{'loss': 0.2696, 'learning_rate': 5.012086755901393e-05, 'epoch': 0.5}
{'loss': 0.2876, 'learning_rate': 5.0096694081113625e-05, 'epoch': 0.5}
{'loss': 0.2487, 'learning_rate': 5.007252058061167e-05, 'epoch': 0.5}
{'loss': 0.2791, 'learning_rate': 5.0048347063158485e-05, 'epoch': 0.5}
{'loss': 0.2706, 'learning_rate': 5.002417353440445e-05, 'epoch': 0.5}
0:21:43, 22.53s/it]                                                          50%|████▉     | 3246/6500 [21:29:34<20:21:43, 22.53s/it] 50%|████▉     | 3247/6500 [21:29:52<19:08:09, 21.18s/it]                                                          50%|████▉     | 3247/6500 [21:29:52<19:08:09, 21.18s/it] 50%|████▉     | 3248/6500 [21:30:10<18:17:14, 20.24s/it]                                                          50%|████▉     | 3248/6500 [21:30:10<18:17:14, 20.24s/it] 50%|████▉     | 3249/6500 [21:30:29<17:49:13, 19.73s/it]                                                          50%|████▉     | 3249/6500 [21:30:29<17:49:13, 19.73s/it] 50%|█████     | 3250/6500 [21:30:47<17:22:35, 19.25s/it]                                                          50%|█████     | 3250/6500 [21:30:47<17:22:35, 19.25s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8575907349586487, 'eval_runtime': 5.6013, 'eval_samples_per_second': 4.106, 'eval_steps_per_second': 1.071, 'epoch': 0.5}
                                                          50%|█████     | 3250/6500 [21:30:53<17:22:35, 19.25s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3250/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3250

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3250/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3250/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2764, 'learning_rate': 5e-05, 'epoch': 0.5}
{'loss': 0.2776, 'learning_rate': 4.997582646559556e-05, 'epoch': 0.5}
{'loss': 0.2788, 'learning_rate': 4.9951652936841527e-05, 'epoch': 0.5}
{'loss': 0.2864, 'learning_rate': 4.992747941938834e-05, 'epoch': 0.5}
{'loss': 0.2758, 'learning_rate': 4.990330591888639e-05, 'epoch': 0.5}
 50%|█████     | 3251/6500 [21:31:49<28:54:14, 32.03s/it]                                                          50%|█████     | 3251/6500 [21:31:49<28:54:14, 32.03s/it] 50%|█████     | 3252/6500 [21:32:07<25:06:02, 27.82s/it]                                                          50%|█████     | 3252/6500 [21:32:07<25:06:02, 27.82s/it] 50%|█████     | 3253/6500 [21:32:25<22:26:43, 24.89s/it]                                                          50%|█████     | 3253/6500 [21:32:25<22:26:43, 24.89s/it] 50%|█████     | 3254/6500 [21:32:43<20:35:21, 22.83s/it]                                                          50%|█████     | 3254/6500 [21:32:43<20:35:21, 22.83s/it] 50%|█████     | 3255/6500 [21:33:01<19:17:27, 21.40s/it]                                                          50%|█████     | 3255/6500 [21:33:01<19:17:27, 21.40s/it] 50%|█████     | 3256/6500 [21:33:19<1{'loss': 0.2919, 'learning_rate': 4.987913244098609e-05, 'epoch': 0.5}
{'loss': 0.2682, 'learning_rate': 4.985495899133784e-05, 'epoch': 0.5}
{'loss': 0.2746, 'learning_rate': 4.9830785575592054e-05, 'epoch': 0.5}
{'loss': 0.2899, 'learning_rate': 4.98066121993991e-05, 'epoch': 0.5}
{'loss': 0.2782, 'learning_rate': 4.978243886840939e-05, 'epoch': 0.5}
8:23:37, 20.41s/it]                                                          50%|█████     | 3256/6500 [21:33:19<18:23:37, 20.41s/it] 50%|█████     | 3257/6500 [21:33:37<17:46:00, 19.72s/it]                                                          50%|█████     | 3257/6500 [21:33:37<17:46:00, 19.72s/it] 50%|█████     | 3258/6500 [21:33:55<17:19:50, 19.24s/it]                                                          50%|█████     | 3258/6500 [21:33:55<17:19:50, 19.24s/it] 50%|█████     | 3259/6500 [21:34:14<17:01:39, 18.91s/it]                                                          50%|█████     | 3259/6500 [21:34:14<17:01:39, 18.91s/it] 50%|█████     | 3260/6500 [21:34:32<16:48:51, 18.68s/it]                                                          50%|█████     | 3260/6500 [21:34:32<16:48:51, 18.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8528955578804016, 'eval_runtime': 5.3469, 'eval_samples_per_second': 4.302, 'eval_steps_per_second': 1.122, 'epoch': 0.5}
                                                          50%|█████     | 3260/6500 [21:34:37<16:48:51, 18.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3260/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3260
 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3260

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3260/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.285, 'learning_rate': 4.9758265588273264e-05, 'epoch': 0.5}
{'loss': 0.2743, 'learning_rate': 4.973409236464108e-05, 'epoch': 0.5}
{'loss': 0.2797, 'learning_rate': 4.9709919203163185e-05, 'epoch': 0.5}
{'loss': 0.2625, 'learning_rate': 4.968574610948992e-05, 'epoch': 0.5}
{'loss': 0.2697, 'learning_rate': 4.96615730892716e-05, 'epoch': 0.5}
 50%|█████     | 3261/6500 [21:35:43<30:58:34, 34.43s/it]                                                          50%|█████     | 3261/6500 [21:35:43<30:58:34, 34.43s/it] 50%|█████     | 3262/6500 [21:36:01<26:32:27, 29.51s/it]                                                          50%|█████     | 3262/6500 [21:36:01<26:32:27, 29.51s/it] 50%|█████     | 3263/6500 [21:36:19<23:25:56, 26.06s/it]                                                          50%|█████     | 3263/6500 [21:36:19<23:25:56, 26.06s/it] 50%|█████     | 3264/6500 [21:36:37<21:15:47, 23.66s/it]                                                          50%|█████     | 3264/6500 [21:36:37<21:15:47, 23.66s/it] 50%|█████     | 3265/6500 [21:36:55<19:48:42, 22.05s/it]                                                          50%|█████     | 3265/6500 [21:36:55<19:48:42, 22.05s/it] 50%|█████     | 3266/6500 [21:37:13<1{'loss': 0.2548, 'learning_rate': 4.9637400148158504e-05, 'epoch': 0.5}
{'loss': 0.3393, 'learning_rate': 4.9613227291800914e-05, 'epoch': 0.5}
{'loss': 0.2767, 'learning_rate': 4.9589054525849096e-05, 'epoch': 0.5}
{'loss': 0.2657, 'learning_rate': 4.956488185595328e-05, 'epoch': 0.5}
{'loss': 0.2802, 'learning_rate': 4.9540709287763685e-05, 'epoch': 0.5}
8:43:37, 20.85s/it]                                                          50%|█████     | 3266/6500 [21:37:13<18:43:37, 20.85s/it] 50%|█████     | 3267/6500 [21:37:31<17:58:24, 20.01s/it]                                                          50%|█████     | 3267/6500 [21:37:31<17:58:24, 20.01s/it] 50%|█████     | 3268/6500 [21:37:49<17:27:19, 19.44s/it]                                                          50%|█████     | 3268/6500 [21:37:49<17:27:19, 19.44s/it] 50%|█████     | 3269/6500 [21:38:08<17:05:47, 19.05s/it]                                                          50%|█████     | 3269/6500 [21:38:08<17:05:47, 19.05s/it] 50%|█████     | 3270/6500 [21:38:26<16:50:45, 18.78s/it]                                                          50%|█████     | 3270/6500 [21:38:26<16:50:45, 18.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8548725247383118, 'eval_runtime': 5.65, 'eval_samples_per_second': 4.071, 'eval_steps_per_second': 1.062, 'epoch': 0.5}
                                                          50%|█████     | 3270/6500 [21:38:31<16:50:45, 18.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3270

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3270the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3270/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3270/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7794, 'learning_rate': 4.9516536826930515e-05, 'epoch': 0.5}
{'loss': 0.2864, 'learning_rate': 4.9492364479103914e-05, 'epoch': 0.5}
{'loss': 0.2705, 'learning_rate': 4.9468192249934025e-05, 'epoch': 0.5}
{'loss': 0.2796, 'learning_rate': 4.944402014507097e-05, 'epoch': 0.5}
{'loss': 0.2571, 'learning_rate': 4.9419848170164815e-05, 'epoch': 0.5}
 50%|█████     | 3271/6500 [21:39:38<31:08:23, 34.72s/it]                                                          50%|█████     | 3271/6500 [21:39:38<31:08:23, 34.72s/it] 50%|█████     | 3272/6500 [21:39:56<26:37:34, 29.69s/it]                                                          50%|█████     | 3272/6500 [21:39:56<26:37:34, 29.69s/it] 50%|█████     | 3273/6500 [21:40:14<23:28:10, 26.18s/it]                                                          50%|█████     | 3273/6500 [21:40:14<23:28:10, 26.18s/it] 50%|█████     | 3274/6500 [21:40:32<21:16:14, 23.74s/it]                                                          50%|█████     | 3274/6500 [21:40:32<21:16:14, 23.74s/it] 50%|█████     | 3275/6500 [21:40:50<19:44:07, 22.03s/it]                                                          50%|█████     | 3275/6500 [21:40:50<19:44:07, 22.03s/it] 50%|█████     | 3276/6500 [21:41:08<1{'loss': 0.2757, 'learning_rate': 4.939567633086563e-05, 'epoch': 0.5}
{'loss': 0.2701, 'learning_rate': 4.937150463282344e-05, 'epoch': 0.5}
{'loss': 0.2398, 'learning_rate': 4.934733308168821e-05, 'epoch': 0.5}
{'loss': 0.2752, 'learning_rate': 4.93231616831099e-05, 'epoch': 0.5}
{'loss': 0.2503, 'learning_rate': 4.929899044273843e-05, 'epoch': 0.5}
8:39:54, 20.84s/it]                                                          50%|█████     | 3276/6500 [21:41:08<18:39:54, 20.84s/it] 50%|█████     | 3277/6500 [21:41:26<17:55:03, 20.01s/it]                                                          50%|█████     | 3277/6500 [21:41:26<17:55:03, 20.01s/it] 50%|█████     | 3278/6500 [21:41:44<17:30:26, 19.56s/it]                                                          50%|█████     | 3278/6500 [21:41:44<17:30:26, 19.56s/it] 50%|█████     | 3279/6500 [21:42:02<17:07:09, 19.13s/it]                                                          50%|█████     | 3279/6500 [21:42:02<17:07:09, 19.13s/it] 50%|█████     | 3280/6500 [21:42:21<16:51:14, 18.84s/it]                                                          50%|█████     | 3280/6500 [21:42:21<16:51:14, 18.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8628625273704529, 'eval_runtime': 5.3513, 'eval_samples_per_second': 4.298, 'eval_steps_per_second': 1.121, 'epoch': 0.5}
                                                          50%|█████     | 3280/6500 [21:42:26<16:51:14, 18.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3280
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3280
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3280
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3280/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3280/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2805, 'learning_rate': 4.927481936622369e-05, 'epoch': 0.5}
{'loss': 0.2692, 'learning_rate': 4.925064845921552e-05, 'epoch': 0.5}
{'loss': 0.2804, 'learning_rate': 4.922647772736371e-05, 'epoch': 0.51}
{'loss': 0.2669, 'learning_rate': 4.920230717631802e-05, 'epoch': 0.51}
{'loss': 0.2829, 'learning_rate': 4.917813681172818e-05, 'epoch': 0.51}
 50%|█████     | 3281/6500 [21:43:56<37:26:37, 41.88s/it]                                                          50%|█████     | 3281/6500 [21:43:56<37:26:37, 41.88s/it] 50%|█████     | 3282/6500 [21:44:14<31:01:10, 34.70s/it]                                                          50%|█████     | 3282/6500 [21:44:14<31:01:10, 34.70s/it] 51%|█████     | 3283/6500 [21:44:32<26:33:30, 29.72s/it]                                                          51%|█████     | 3283/6500 [21:44:32<26:33:30, 29.72s/it] 51%|█████     | 3284/6500 [21:44:50<23:24:12, 26.20s/it]                                                          51%|█████     | 3284/6500 [21:44:50<23:24:12, 26.20s/it] 51%|█████     | 3285/6500 [21:45:08<21:12:24, 23.75s/it]                                                          51%|█████     | 3285/6500 [21:45:08<21:12:24, 23.75s/it] 51%|█████     | 3286/6500 [21:45:26<1{'loss': 0.2962, 'learning_rate': 4.9153966639243864e-05, 'epoch': 0.51}
{'loss': 0.28, 'learning_rate': 4.91297966645147e-05, 'epoch': 0.51}
{'loss': 0.2859, 'learning_rate': 4.910562689319029e-05, 'epoch': 0.51}
{'loss': 0.2805, 'learning_rate': 4.908145733092013e-05, 'epoch': 0.51}
{'loss': 0.2745, 'learning_rate': 4.9057287983353745e-05, 'epoch': 0.51}
9:40:12, 22.03s/it]                                                          51%|█████     | 3286/6500 [21:45:26<19:40:12, 22.03s/it] 51%|█████     | 3287/6500 [21:45:44<18:35:52, 20.84s/it]                                                          51%|█████     | 3287/6500 [21:45:44<18:35:52, 20.84s/it] 51%|█████     | 3288/6500 [21:46:02<17:51:09, 20.01s/it]                                                          51%|█████     | 3288/6500 [21:46:02<17:51:09, 20.01s/it] 51%|█████     | 3289/6500 [21:46:21<17:20:23, 19.44s/it]                                                          51%|█████     | 3289/6500 [21:46:21<17:20:23, 19.44s/it] 51%|█████     | 3290/6500 [21:46:39<16:59:11, 19.05s/it]                                                          51%|█████     | 3290/6500 [21:46:39<16:59:11, 19.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8524175882339478, 'eval_runtime': 5.3323, 'eval_samples_per_second': 4.313, 'eval_steps_per_second': 1.125, 'epoch': 0.51}
                                                          51%|█████     | 3290/6500 [21:46:44<16:59:11, 19.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3290
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3290
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3290

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3290/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3290/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2755, 'learning_rate': 4.903311885614058e-05, 'epoch': 0.51}
{'loss': 0.2941, 'learning_rate': 4.900894995492998e-05, 'epoch': 0.51}
{'loss': 0.2704, 'learning_rate': 4.898478128537131e-05, 'epoch': 0.51}
{'loss': 0.2634, 'learning_rate': 4.8960612853113844e-05, 'epoch': 0.51}
{'loss': 0.2654, 'learning_rate': 4.89364446638068e-05, 'epoch': 0.51}
 51%|█████     | 3291/6500 [21:47:54<32:01:26, 35.93s/it]                                                          51%|█████     | 3291/6500 [21:47:54<32:01:26, 35.93s/it] 51%|█████     | 3292/6500 [21:48:12<27:12:50, 30.54s/it]                                                          51%|█████     | 3292/6500 [21:48:12<27:12:50, 30.54s/it] 51%|█████     | 3293/6500 [21:48:30<23:55:51, 26.86s/it]                                                          51%|█████     | 3293/6500 [21:48:30<23:55:51, 26.86s/it] 51%|█████     | 3294/6500 [21:48:48<21:33:49, 24.21s/it]                                                          51%|█████     | 3294/6500 [21:48:48<21:33:49, 24.21s/it] 51%|█████     | 3295/6500 [21:49:06<19:54:48, 22.37s/it]                                                          51%|█████     | 3295/6500 [21:49:06<19:54:48, 22.37s/it] 51%|█████     | 3296/6500 [21:49:24<1{'loss': 0.288, 'learning_rate': 4.891227672309935e-05, 'epoch': 0.51}
{'loss': 0.3334, 'learning_rate': 4.888810903664062e-05, 'epoch': 0.51}
{'loss': 0.2669, 'learning_rate': 4.886394161007963e-05, 'epoch': 0.51}
{'loss': 0.2715, 'learning_rate': 4.883977444906538e-05, 'epoch': 0.51}
{'loss': 0.2754, 'learning_rate': 4.881560755924679e-05, 'epoch': 0.51}
8:45:24, 21.08s/it]                                                          51%|█████     | 3296/6500 [21:49:24<18:45:24, 21.08s/it] 51%|█████     | 3297/6500 [21:49:43<18:01:46, 20.26s/it]                                                          51%|█████     | 3297/6500 [21:49:43<18:01:46, 20.26s/it] 51%|█████     | 3298/6500 [21:50:01<17:26:59, 19.62s/it]                                                          51%|█████     | 3298/6500 [21:50:01<17:26:59, 19.62s/it] 51%|█████     | 3299/6500 [21:50:19<17:02:52, 19.17s/it]                                                          51%|█████     | 3299/6500 [21:50:19<17:02:52, 19.17s/it] 51%|█████     | 3300/6500 [21:50:37<16:45:50, 18.86s/it]                                                          51%|█████     | 3300/6500 [21:50:37<16:45:50, 18.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8533260226249695, 'eval_runtime': 5.3443, 'eval_samples_per_second': 4.304, 'eval_steps_per_second': 1.123, 'epoch': 0.51}
                                                          51%|█████     | 3300/6500 [21:50:43<16:45:50, 18.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3300
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3300
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3300

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3300/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.776, 'learning_rate': 4.8791440946272735e-05, 'epoch': 0.51}
{'loss': 0.2785, 'learning_rate': 4.876727461579203e-05, 'epoch': 0.51}
{'loss': 0.2637, 'learning_rate': 4.874310857345337e-05, 'epoch': 0.51}
{'loss': 0.2748, 'learning_rate': 4.8718942824905445e-05, 'epoch': 0.51}
{'loss': 0.2684, 'learning_rate': 4.8694777375796844e-05, 'epoch': 0.51}
 51%|█████     | 3301/6500 [21:51:20<23:07:00, 26.01s/it]                                                          51%|█████     | 3301/6500 [21:51:20<23:07:00, 26.01s/it] 51%|█████     | 3302/6500 [21:51:38<20:59:22, 23.63s/it]                                                          51%|█████     | 3302/6500 [21:51:38<20:59:22, 23.63s/it] 51%|█████     | 3303/6500 [21:51:56<19:29:45, 21.95s/it]                                                          51%|█████     | 3303/6500 [21:51:56<19:29:45, 21.95s/it] 51%|█████     | 3304/6500 [21:52:14<18:26:59, 20.78s/it]                                                          51%|█████     | 3304/6500 [21:52:14<18:26:59, 20.78s/it] 51%|█████     | 3305/6500 [21:52:32<17:46:56, 20.04s/it]                                                          51%|█████     | 3305/6500 [21:52:32<17:46:56, 20.04s/it] 51%|█████     | 3306/6500 [21:52:50<1{'loss': 0.2856, 'learning_rate': 4.867061223177609e-05, 'epoch': 0.51}
{'loss': 0.2557, 'learning_rate': 4.864644739849165e-05, 'epoch': 0.51}
{'loss': 0.2635, 'learning_rate': 4.8622282881591906e-05, 'epoch': 0.51}
{'loss': 0.2644, 'learning_rate': 4.859811868672515e-05, 'epoch': 0.51}
{'loss': 0.2612, 'learning_rate': 4.8573954819539634e-05, 'epoch': 0.51}
7:15:39, 19.46s/it]                                                          51%|█████     | 3306/6500 [21:52:50<17:15:39, 19.46s/it] 51%|█████     | 3307/6500 [21:53:09<16:53:59, 19.05s/it]                                                          51%|█████     | 3307/6500 [21:53:09<16:53:59, 19.05s/it] 51%|█████     | 3308/6500 [21:53:27<16:38:53, 18.78s/it]                                                          51%|█████     | 3308/6500 [21:53:27<16:38:53, 18.78s/it] 51%|█████     | 3309/6500 [21:53:45<16:28:14, 18.58s/it]                                                          51%|█████     | 3309/6500 [21:53:45<16:28:14, 18.58s/it] 51%|█████     | 3310/6500 [21:54:03<16:21:06, 18.45s/it]                                                          51%|█████     | 3310/6500 [21:54:03<16:21:06, 18.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8611153364181519, 'eval_runtime': 5.3384, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.51}
                                                          51%|█████     | 3310/6500 [21:54:08<16:21:06, 18.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3310
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3310the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3310
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3310/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2765, 'learning_rate': 4.854979128568351e-05, 'epoch': 0.51}
{'loss': 0.2659, 'learning_rate': 4.852562809080485e-05, 'epoch': 0.51}
{'loss': 0.2805, 'learning_rate': 4.8501465240551666e-05, 'epoch': 0.51}
{'loss': 0.2715, 'learning_rate': 4.8477302740571866e-05, 'epoch': 0.51}
{'loss': 0.2757, 'learning_rate': 4.84531405965133e-05, 'epoch': 0.51}
 51%|█████     | 3311/6500 [21:55:18<31:22:13, 35.41s/it]                                                          51%|█████     | 3311/6500 [21:55:18<31:22:13, 35.41s/it] 51%|█████     | 3312/6500 [21:55:36<26:43:58, 30.19s/it]                                                          51%|█████     | 3312/6500 [21:55:36<26:43:58, 30.19s/it] 51%|█████     | 3313/6500 [21:55:54<23:35:50, 26.66s/it]                                                          51%|█████     | 3313/6500 [21:55:54<23:35:50, 26.66s/it] 51%|█████     | 3314/6500 [21:56:12<21:18:03, 24.07s/it]                                                          51%|█████     | 3314/6500 [21:56:12<21:18:03, 24.07s/it] 51%|█████     | 3315/6500 [21:56:30<19:41:33, 22.26s/it]                                                          51%|█████     | 3315/6500 [21:56:30<19:41:33, 22.26s/it] 51%|█████     | 3316/6500 [21:56:48<1{'loss': 0.2594, 'learning_rate': 4.84289788140237e-05, 'epoch': 0.51}
{'loss': 0.2614, 'learning_rate': 4.8404817398750756e-05, 'epoch': 0.51}
{'loss': 0.2733, 'learning_rate': 4.838065635634205e-05, 'epoch': 0.51}
{'loss': 0.2624, 'learning_rate': 4.835649569244508e-05, 'epoch': 0.51}
{'loss': 0.2699, 'learning_rate': 4.833233541270724e-05, 'epoch': 0.51}
8:34:14, 21.00s/it]                                                          51%|█████     | 3316/6500 [21:56:48<18:34:14, 21.00s/it] 51%|█████     | 3317/6500 [21:57:07<17:47:25, 20.12s/it]                                                          51%|█████     | 3317/6500 [21:57:07<17:47:25, 20.12s/it] 51%|█████     | 3318/6500 [21:57:25<17:14:48, 19.51s/it]                                                          51%|█████     | 3318/6500 [21:57:25<17:14:48, 19.51s/it] 51%|█████     | 3319/6500 [21:57:43<16:52:15, 19.09s/it]                                                          51%|█████     | 3319/6500 [21:57:43<16:52:15, 19.09s/it] 51%|█████     | 3320/6500 [21:58:01<16:36:46, 18.81s/it]                                                          51%|█████     | 3320/6500 [21:58:01<16:36:46, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8617786169052124, 'eval_runtime': 5.4862, 'eval_samples_per_second': 4.192, 'eval_steps_per_second': 1.094, 'epoch': 0.51}
                                                          51%|█████     | 3320/6500 [21:58:06<16:36:46, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3320/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3320


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3320

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3320/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2776, 'learning_rate': 4.8308175522775865e-05, 'epoch': 0.51}
{'loss': 0.2865, 'learning_rate': 4.828401602829816e-05, 'epoch': 0.51}
{'loss': 0.2574, 'learning_rate': 4.825985693492129e-05, 'epoch': 0.51}
{'loss': 0.2697, 'learning_rate': 4.823569824829227e-05, 'epoch': 0.51}
{'loss': 0.2489, 'learning_rate': 4.821153997405807e-05, 'epoch': 0.51}
 51%|█████     | 3321/6500 [21:59:12<30:35:05, 34.64s/it]                                                          51%|█████     | 3321/6500 [21:59:12<30:35:05, 34.64s/it] 51%|█████     | 3322/6500 [21:59:30<26:09:46, 29.64s/it]                                                          51%|█████     | 3322/6500 [21:59:30<26:09:46, 29.64s/it] 51%|█████     | 3323/6500 [21:59:48<23:04:07, 26.14s/it]                                                          51%|█████     | 3323/6500 [21:59:48<23:04:07, 26.14s/it] 51%|█████     | 3324/6500 [22:00:06<20:54:12, 23.69s/it]                                                          51%|█████     | 3324/6500 [22:00:06<20:54:12, 23.69s/it] 51%|█████     | 3325/6500 [22:00:24<19:23:51, 21.99s/it]                                                          51%|█████     | 3325/6500 [22:00:24<19:23:51, 21.99s/it] 51%|█████     | 3326/6500 [22:00:42<1{'loss': 0.2965, 'learning_rate': 4.8187382117865525e-05, 'epoch': 0.51}
{'loss': 0.3141, 'learning_rate': 4.816322468536139e-05, 'epoch': 0.51}
{'loss': 0.2591, 'learning_rate': 4.8139067682192303e-05, 'epoch': 0.51}
{'loss': 0.2723, 'learning_rate': 4.811491111400484e-05, 'epoch': 0.51}
{'loss': 0.2738, 'learning_rate': 4.8090754986445454e-05, 'epoch': 0.51}
8:21:03, 20.81s/it]                                                          51%|█████     | 3326/6500 [22:00:42<18:21:03, 20.81s/it] 51%|█████     | 3327/6500 [22:01:01<17:37:06, 19.99s/it]                                                          51%|█████     | 3327/6500 [22:01:01<17:37:06, 19.99s/it] 51%|█████     | 3328/6500 [22:01:19<17:06:41, 19.42s/it]                                                          51%|█████     | 3328/6500 [22:01:19<17:06:41, 19.42s/it] 51%|█████     | 3329/6500 [22:01:37<16:55:17, 19.21s/it]                                                          51%|█████     | 3329/6500 [22:01:37<16:55:17, 19.21s/it] 51%|█████     | 3330/6500 [22:01:55<16:38:01, 18.89s/it]                                                          51%|█████     | 3330/6500 [22:01:56<16:38:01, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.857681930065155, 'eval_runtime': 5.337, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.51}
                                                          51%|█████     | 3330/6500 [22:02:01<16:38:01, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3330 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3330
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3330/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7776, 'learning_rate': 4.806659930516047e-05, 'epoch': 0.51}
{'loss': 0.2787, 'learning_rate': 4.804244407579613e-05, 'epoch': 0.51}
{'loss': 0.2856, 'learning_rate': 4.8018289303998604e-05, 'epoch': 0.51}
{'loss': 0.2556, 'learning_rate': 4.79941349954139e-05, 'epoch': 0.51}
{'loss': 0.2787, 'learning_rate': 4.796998115568794e-05, 'epoch': 0.51}
 51%|█████     | 3331/6500 [22:02:53<26:41:53, 30.33s/it]                                                          51%|█████     | 3331/6500 [22:02:53<26:41:53, 30.33s/it] 51%|█████▏    | 3332/6500 [22:03:11<23:26:57, 26.65s/it]                                                          51%|█████▏    | 3332/6500 [22:03:11<23:26:57, 26.65s/it] 51%|█████▏    | 3333/6500 [22:03:29<21:09:32, 24.05s/it]                                                          51%|█████▏    | 3333/6500 [22:03:29<21:09:32, 24.05s/it] 51%|█████▏    | 3334/6500 [22:03:47<19:33:21, 22.24s/it]                                                          51%|█████▏    | 3334/6500 [22:03:47<19:33:21, 22.24s/it] 51%|█████▏    | 3335/6500 [22:04:05<18:26:14, 20.97s/it]                                                          51%|█████▏    | 3335/6500 [22:04:05<18:26:14, 20.97s/it] 51%|█████▏    | 333{'loss': 0.2662, 'learning_rate': 4.7945827790466554e-05, 'epoch': 0.51}
{'loss': 0.2498, 'learning_rate': 4.792167490539542e-05, 'epoch': 0.51}
{'loss': 0.2687, 'learning_rate': 4.789752250612014e-05, 'epoch': 0.51}
{'loss': 0.2688, 'learning_rate': 4.787337059828619e-05, 'epoch': 0.51}
{'loss': 0.2672, 'learning_rate': 4.7849219187538944e-05, 'epoch': 0.51}
6/6500 [22:04:23<17:43:22, 20.17s/it]                                                          51%|█████▏    | 3336/6500 [22:04:23<17:43:22, 20.17s/it] 51%|█████▏    | 3337/6500 [22:04:41<17:09:37, 19.53s/it]                                                          51%|█████▏    | 3337/6500 [22:04:41<17:09:37, 19.53s/it] 51%|█████▏    | 3338/6500 [22:04:59<16:46:21, 19.10s/it]                                                          51%|█████▏    | 3338/6500 [22:04:59<16:46:21, 19.10s/it] 51%|█████▏    | 3339/6500 [22:05:17<16:30:43, 18.81s/it]                                                          51%|█████▏    | 3339/6500 [22:05:17<16:30:43, 18.81s/it] 51%|█████▏    | 3340/6500 [22:05:35<16:19:47, 18.60s/it]                                                          51%|█████▏    | 3340/6500 [22:05:35<16:19:47, 18.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8641130924224854, 'eval_runtime': 5.3411, 'eval_samples_per_second': 4.306, 'eval_steps_per_second': 1.123, 'epoch': 0.51}
                                                          51%|█████▏    | 3340/6500 [22:05:41<16:19:47, 18.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3340
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3340 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3340/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3340/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2732, 'learning_rate': 4.782506827952364e-05, 'epoch': 0.51}
{'loss': 0.2674, 'learning_rate': 4.780091787988539e-05, 'epoch': 0.51}
{'loss': 0.266, 'learning_rate': 4.777676799426921e-05, 'epoch': 0.51}
{'loss': 0.2726, 'learning_rate': 4.775261862832e-05, 'epoch': 0.51}
{'loss': 0.2849, 'learning_rate': 4.772846978768252e-05, 'epoch': 0.51}
 51%|█████▏    | 3341/6500 [22:06:14<21:34:50, 24.59s/it]                                                          51%|█████▏    | 3341/6500 [22:06:14<21:34:50, 24.59s/it] 51%|█████▏    | 3342/6500 [22:06:32<19:51:22, 22.64s/it]                                                          51%|█████▏    | 3342/6500 [22:06:32<19:51:22, 22.64s/it] 51%|█████▏    | 3343/6500 [22:06:50<18:38:42, 21.26s/it]                                                          51%|█████▏    | 3343/6500 [22:06:50<18:38:42, 21.26s/it] 51%|█████▏    | 3344/6500 [22:07:08<17:48:08, 20.31s/it]                                                          51%|█████▏    | 3344/6500 [22:07:08<17:48:08, 20.31s/it] 51%|█████▏    | 3345/6500 [22:07:26<17:12:57, 19.64s/it]                                                          51%|█████▏    | 3345/6500 [22:07:26<17:12:57, 19.64s/it] 51%|█████▏    |{'loss': 0.2669, 'learning_rate': 4.7704321478001415e-05, 'epoch': 0.51}
{'loss': 0.2775, 'learning_rate': 4.768017370492121e-05, 'epoch': 0.51}
{'loss': 0.2884, 'learning_rate': 4.76560264740863e-05, 'epoch': 0.52}
{'loss': 0.261, 'learning_rate': 4.7631879791140946e-05, 'epoch': 0.52}
{'loss': 0.2781, 'learning_rate': 4.760773366172929e-05, 'epoch': 0.52}
 3346/6500 [22:07:45<17:06:40, 19.53s/it]                                                          51%|█████▏    | 3346/6500 [22:07:45<17:06:40, 19.53s/it] 51%|█████▏    | 3347/6500 [22:08:04<16:44:42, 19.12s/it]                                                          51%|█████▏    | 3347/6500 [22:08:04<16:44:42, 19.12s/it] 52%|█████▏    | 3348/6500 [22:08:22<16:28:41, 18.82s/it]                                                          52%|█████▏    | 3348/6500 [22:08:22<16:28:41, 18.82s/it] 52%|█████▏    | 3349/6500 [22:08:40<16:17:22, 18.61s/it]                                                          52%|█████▏    | 3349/6500 [22:08:40<16:17:22, 18.61s/it] 52%|█████▏    | 3350/6500 [22:08:58<16:13:07, 18.54s/it]                                                          52%|█████▏    | 3350/6500 [22:08:58<16:13:07, 18.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8595650792121887, 'eval_runtime': 5.9617, 'eval_samples_per_second': 3.858, 'eval_steps_per_second': 1.006, 'epoch': 0.52}
                                                          52%|█████▏    | 3350/6500 [22:09:04<16:13:07, 18.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3350 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3350
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3350/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.278, 'learning_rate': 4.7583588091495344e-05, 'epoch': 0.52}
{'loss': 0.2808, 'learning_rate': 4.7559443086083005e-05, 'epoch': 0.52}
{'loss': 0.254, 'learning_rate': 4.7535298651136e-05, 'epoch': 0.52}
{'loss': 0.2559, 'learning_rate': 4.751115479229794e-05, 'epoch': 0.52}
{'loss': 0.2549, 'learning_rate': 4.7487011515212315e-05, 'epoch': 0.52}
 52%|█████▏    | 3351/6500 [22:10:24<33:49:49, 38.68s/it]                                                          52%|█████▏    | 3351/6500 [22:10:24<33:49:49, 38.68s/it] 52%|█████▏    | 3352/6500 [22:10:42<28:23:42, 32.47s/it]                                                          52%|█████▏    | 3352/6500 [22:10:42<28:23:42, 32.47s/it] 52%|█████▏    | 3353/6500 [22:11:00<24:35:37, 28.13s/it]                                                          52%|█████▏    | 3353/6500 [22:11:00<24:35:37, 28.13s/it] 52%|█████▏    | 3354/6500 [22:11:18<21:57:20, 25.12s/it]                                                          52%|█████▏    | 3354/6500 [22:11:18<21:57:20, 25.12s/it] 52%|█████▏    | 3355/6500 [22:11:36<20:05:39, 23.00s/it]                                                          52%|█████▏    | 3355/6500 [22:11:36<20:05:39, 23.00s/it] 52%|█████▏    |{'loss': 0.3457, 'learning_rate': 4.7462868825522466e-05, 'epoch': 0.52}
{'loss': 0.2738, 'learning_rate': 4.7438726728871615e-05, 'epoch': 0.52}
{'loss': 0.2614, 'learning_rate': 4.741458523090282e-05, 'epoch': 0.52}
{'loss': 0.2905, 'learning_rate': 4.7390444337259e-05, 'epoch': 0.52}
{'loss': 0.7756, 'learning_rate': 4.7366304053582943e-05, 'epoch': 0.52}
 3356/6500 [22:11:54<18:48:41, 21.54s/it]                                                          52%|█████▏    | 3356/6500 [22:11:54<18:48:41, 21.54s/it] 52%|█████▏    | 3357/6500 [22:12:12<17:53:39, 20.50s/it]                                                          52%|█████▏    | 3357/6500 [22:12:12<17:53:39, 20.50s/it] 52%|█████▏    | 3358/6500 [22:12:32<17:41:09, 20.26s/it]                                                          52%|█████▏    | 3358/6500 [22:12:32<17:41:09, 20.26s/it] 52%|█████▏    | 3359/6500 [22:12:50<17:10:29, 19.68s/it]                                                          52%|█████▏    | 3359/6500 [22:12:50<17:10:29, 19.68s/it] 52%|█████▏    | 3360/6500 [22:13:08<16:45:05, 19.21s/it]                                                          52%|█████▏    | 3360/6500 [22:13:08<16:45:05, 19.21s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8572254180908203, 'eval_runtime': 6.783, 'eval_samples_per_second': 3.391, 'eval_steps_per_second': 0.885, 'epoch': 0.52}
                                                          52%|█████▏    | 3360/6500 [22:13:15<16:45:05, 19.21s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3360
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3360
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3360

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3360/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2901, 'learning_rate': 4.7342164385517305e-05, 'epoch': 0.52}
{'loss': 0.2666, 'learning_rate': 4.731802533870459e-05, 'epoch': 0.52}
{'loss': 0.2727, 'learning_rate': 4.729388691878714e-05, 'epoch': 0.52}
{'loss': 0.248, 'learning_rate': 4.726974913140717e-05, 'epoch': 0.52}
{'loss': 0.2778, 'learning_rate': 4.7245611982206724e-05, 'epoch': 0.52}
 52%|█████▏    | 3361/6500 [22:13:53<23:23:43, 26.83s/it]                                                          52%|█████▏    | 3361/6500 [22:13:53<23:23:43, 26.83s/it] 52%|█████▏    | 3362/6500 [22:14:12<21:14:38, 24.37s/it]                                                          52%|█████▏    | 3362/6500 [22:14:12<21:14:38, 24.37s/it] 52%|█████▏    | 3363/6500 [22:14:30<19:34:35, 22.47s/it]                                                          52%|█████▏    | 3363/6500 [22:14:30<19:34:35, 22.47s/it] 52%|█████▏    | 3364/6500 [22:14:48<18:24:55, 21.14s/it]                                                          52%|█████▏    | 3364/6500 [22:14:48<18:24:55, 21.14s/it] 52%|█████▏    | 3365/6500 [22:15:06<17:43:26, 20.35s/it]                                                          52%|█████▏    | 3365/6500 [22:15:06<17:43:26, 20.35s/it] 52%|█████▏    |{'loss': 0.2528, 'learning_rate': 4.7221475476827745e-05, 'epoch': 0.52}
{'loss': 0.2471, 'learning_rate': 4.719733962091198e-05, 'epoch': 0.52}
{'loss': 0.2663, 'learning_rate': 4.717320442010105e-05, 'epoch': 0.52}
{'loss': 0.2609, 'learning_rate': 4.714906988003638e-05, 'epoch': 0.52}
{'loss': 0.2673, 'learning_rate': 4.71249360063593e-05, 'epoch': 0.52}
 3366/6500 [22:15:24<17:07:32, 19.67s/it]                                                          52%|█████▏    | 3366/6500 [22:15:24<17:07:32, 19.67s/it] 52%|█████▏    | 3367/6500 [22:15:42<16:42:50, 19.21s/it]                                                          52%|█████▏    | 3367/6500 [22:15:42<16:42:50, 19.21s/it] 52%|█████▏    | 3368/6500 [22:16:01<16:25:48, 18.89s/it]                                                          52%|█████▏    | 3368/6500 [22:16:01<16:25:48, 18.89s/it] 52%|█████▏    | 3369/6500 [22:16:19<16:13:53, 18.66s/it]                                                          52%|█████▏    | 3369/6500 [22:16:19<16:13:53, 18.66s/it] 52%|█████▏    | 3370/6500 [22:16:37<16:05:38, 18.51s/it]                                                          52%|█████▏    | 3370/6500 [22:16:37<16:05:38, 18.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8703662753105164, 'eval_runtime': 5.3197, 'eval_samples_per_second': 4.324, 'eval_steps_per_second': 1.128, 'epoch': 0.52}
                                                          52%|█████▏    | 3370/6500 [22:16:42<16:05:38, 18.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3370
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3370


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3370 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3370/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3370/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3370/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2598, 'learning_rate': 4.7100802804710946e-05, 'epoch': 0.52}
{'loss': 0.2763, 'learning_rate': 4.707667028073232e-05, 'epoch': 0.52}
{'loss': 0.2534, 'learning_rate': 4.705253844006423e-05, 'epoch': 0.52}
{'loss': 0.2854, 'learning_rate': 4.702840728834736e-05, 'epoch': 0.52}
{'loss': 0.2618, 'learning_rate': 4.7004276831222224e-05, 'epoch': 0.52}
 52%|█████▏    | 3371/6500 [22:18:01<33:20:18, 38.36s/it]                                                          52%|█████▏    | 3371/6500 [22:18:01<33:20:18, 38.36s/it] 52%|█████▏    | 3372/6500 [22:18:19<28:01:01, 32.24s/it]                                                          52%|█████▏    | 3372/6500 [22:18:19<28:01:01, 32.24s/it] 52%|█████▏    | 3373/6500 [22:18:37<24:17:37, 27.97s/it]                                                          52%|█████▏    | 3373/6500 [22:18:37<24:17:37, 27.97s/it] 52%|█████▏    | 3374/6500 [22:18:55<21:41:52, 24.99s/it]                                                          52%|█████▏    | 3374/6500 [22:18:55<21:41:52, 24.99s/it] 52%|█████▏    | 3375/6500 [22:19:14<19:53:03, 22.91s/it]                                                          52%|█████▏    | 3375/6500 [22:19:14<19:53:03, 22.91s/it] 52%|█████▏    |{'loss': 0.2641, 'learning_rate': 4.698014707432916e-05, 'epoch': 0.52}
{'loss': 0.274, 'learning_rate': 4.695601802330835e-05, 'epoch': 0.52}
{'loss': 0.2612, 'learning_rate': 4.693188968379983e-05, 'epoch': 0.52}
{'loss': 0.276, 'learning_rate': 4.6907762061443446e-05, 'epoch': 0.52}
{'loss': 0.2558, 'learning_rate': 4.688363516187886e-05, 'epoch': 0.52}
 3376/6500 [22:19:32<18:37:03, 21.45s/it]                                                          52%|█████▏    | 3376/6500 [22:19:32<18:37:03, 21.45s/it] 52%|█████▏    | 3377/6500 [22:19:50<17:44:12, 20.45s/it]                                                          52%|█████▏    | 3377/6500 [22:19:50<17:44:12, 20.45s/it] 52%|█████▏    | 3378/6500 [22:20:08<17:11:26, 19.82s/it]                                                          52%|█████▏    | 3378/6500 [22:20:08<17:11:26, 19.82s/it] 52%|█████▏    | 3379/6500 [22:20:26<16:44:32, 19.31s/it]                                                          52%|█████▏    | 3379/6500 [22:20:26<16:44:32, 19.31s/it] 52%|█████▏    | 3380/6500 [22:20:44<16:25:52, 18.96s/it]                                                          52%|█████▏    | 3380/6500 [22:20:44<16:25:52, 18.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8612099289894104, 'eval_runtime': 5.4739, 'eval_samples_per_second': 4.202, 'eval_steps_per_second': 1.096, 'epoch': 0.52}
                                                          52%|█████▏    | 3380/6500 [22:20:50<16:25:52, 18.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3380/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3380

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3380

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3380/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2972, 'learning_rate': 4.685950899074562e-05, 'epoch': 0.52}
{'loss': 0.2541, 'learning_rate': 4.683538355368306e-05, 'epoch': 0.52}
{'loss': 0.2589, 'learning_rate': 4.681125885633035e-05, 'epoch': 0.52}
{'loss': 0.2539, 'learning_rate': 4.6787134904326504e-05, 'epoch': 0.52}
{'loss': 0.2822, 'learning_rate': 4.676301170331033e-05, 'epoch': 0.52}
 52%|█████▏    | 3381/6500 [22:21:50<28:32:24, 32.94s/it]                                                          52%|█████▏    | 3381/6500 [22:21:50<28:32:24, 32.94s/it] 52%|█████▏    | 3382/6500 [22:22:08<24:38:41, 28.45s/it]                                                          52%|█████▏    | 3382/6500 [22:22:08<24:38:41, 28.45s/it] 52%|█████▏    | 3383/6500 [22:22:26<21:55:24, 25.32s/it]                                                          52%|█████▏    | 3383/6500 [22:22:26<21:55:24, 25.32s/it] 52%|█████▏    | 3384/6500 [22:22:45<20:23:23, 23.56s/it]                                                          52%|█████▏    | 3384/6500 [22:22:45<20:23:23, 23.56s/it] 52%|█████▏    | 3385/6500 [22:23:03<18:57:41, 21.91s/it]                                                          52%|█████▏    | 3385/6500 [22:23:03<18:57:41, 21.91s/it] 52%|█████▏    |{'loss': 0.3258, 'learning_rate': 4.673888925892048e-05, 'epoch': 0.52}
{'loss': 0.2612, 'learning_rate': 4.6714767576795446e-05, 'epoch': 0.52}
{'loss': 0.2615, 'learning_rate': 4.669064666257352e-05, 'epoch': 0.52}
{'loss': 0.2737, 'learning_rate': 4.666652652189282e-05, 'epoch': 0.52}
{'loss': 0.7725, 'learning_rate': 4.664240716039127e-05, 'epoch': 0.52}
 3386/6500 [22:23:21<17:57:46, 20.77s/it]                                                          52%|█████▏    | 3386/6500 [22:23:21<17:57:46, 20.77s/it] 52%|█████▏    | 3387/6500 [22:23:40<17:16:16, 19.97s/it]                                                          52%|█████▏    | 3387/6500 [22:23:40<17:16:16, 19.97s/it] 52%|█████▏    | 3388/6500 [22:23:58<16:47:20, 19.42s/it]                                                          52%|█████▏    | 3388/6500 [22:23:58<16:47:20, 19.42s/it] 52%|█████▏    | 3389/6500 [22:24:16<16:27:32, 19.05s/it]                                                          52%|█████▏    | 3389/6500 [22:24:16<16:27:32, 19.05s/it] 52%|█████▏    | 3390/6500 [22:24:34<16:13:12, 18.78s/it]                                                          52%|█████▏    | 3390/6500 [22:24:34<16:13:12, 18.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8521015048027039, 'eval_runtime': 5.5508, 'eval_samples_per_second': 4.144, 'eval_steps_per_second': 1.081, 'epoch': 0.52}
                                                          52%|█████▏    | 3390/6500 [22:24:40<16:13:12, 18.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3390
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3390
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3390/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3390/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2758, 'learning_rate': 4.6618288583706634e-05, 'epoch': 0.52}
{'loss': 0.2588, 'learning_rate': 4.659417079747648e-05, 'epoch': 0.52}
{'loss': 0.2619, 'learning_rate': 4.6570053807338186e-05, 'epoch': 0.52}
{'loss': 0.2582, 'learning_rate': 4.654593761892897e-05, 'epoch': 0.52}
{'loss': 0.2767, 'learning_rate': 4.652182223788584e-05, 'epoch': 0.52}
 52%|█████▏    | 3391/6500 [22:25:36<27:16:45, 31.59s/it]                                                          52%|█████▏    | 3391/6500 [22:25:36<27:16:45, 31.59s/it] 52%|█████▏    | 3392/6500 [22:25:54<23:45:26, 27.52s/it]                                                          52%|█████▏    | 3392/6500 [22:25:54<23:45:26, 27.52s/it] 52%|█████▏    | 3393/6500 [22:26:12<21:18:12, 24.68s/it]                                                          52%|█████▏    | 3393/6500 [22:26:12<21:18:12, 24.68s/it] 52%|█████▏    | 3394/6500 [22:26:30<19:41:23, 22.82s/it]                                                          52%|█████▏    | 3394/6500 [22:26:30<19:41:23, 22.82s/it] 52%|█████▏    | 3395/6500 [22:26:48<18:26:58, 21.39s/it]                                                          52%|█████▏    | 3395/6500 [22:26:48<18:26:58, 21.39s/it] 52%|█████▏    |{'loss': 0.2526, 'learning_rate': 4.64977076698456e-05, 'epoch': 0.52}
{'loss': 0.2601, 'learning_rate': 4.647359392044491e-05, 'epoch': 0.52}
{'loss': 0.2624, 'learning_rate': 4.644948099532019e-05, 'epoch': 0.52}
{'loss': 0.2575, 'learning_rate': 4.64253689001077e-05, 'epoch': 0.52}
{'loss': 0.2657, 'learning_rate': 4.640125764044351e-05, 'epoch': 0.52}
 3396/6500 [22:27:06<17:35:17, 20.40s/it]                                                          52%|█████▏    | 3396/6500 [22:27:06<17:35:17, 20.40s/it] 52%|█████▏    | 3397/6500 [22:27:25<17:14:32, 20.00s/it]                                                          52%|█████▏    | 3397/6500 [22:27:25<17:14:32, 20.00s/it] 52%|█████▏    | 3398/6500 [22:27:43<16:44:37, 19.43s/it]                                                          52%|█████▏    | 3398/6500 [22:27:43<16:44:37, 19.43s/it] 52%|█████▏    | 3399/6500 [22:28:02<16:24:14, 19.04s/it]                                                          52%|█████▏    | 3399/6500 [22:28:02<16:24:14, 19.04s/it] 52%|█████▏    | 3400/6500 [22:28:20<16:09:58, 18.77s/it]                                                          52%|█████▏    | 3400/6500 [22:28:20<16:09:58, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8700180649757385, 'eval_runtime': 5.3466, 'eval_samples_per_second': 4.302, 'eval_steps_per_second': 1.122, 'epoch': 0.52}
                                                          52%|█████▏    | 3400/6500 [22:28:25<16:09:58, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3400
I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3400
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3400/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2556, 'learning_rate': 4.6377147221963455e-05, 'epoch': 0.52}
{'loss': 0.2769, 'learning_rate': 4.635303765030321e-05, 'epoch': 0.52}
{'loss': 0.2641, 'learning_rate': 4.6328928931098236e-05, 'epoch': 0.52}
{'loss': 0.2732, 'learning_rate': 4.630482106998381e-05, 'epoch': 0.52}
{'loss': 0.2653, 'learning_rate': 4.628071407259499e-05, 'epoch': 0.52}
 52%|█████▏    | 3401/6500 [22:28:58<21:15:32, 24.70s/it]                                                          52%|█████▏    | 3401/6500 [22:28:58<21:15:32, 24.70s/it] 52%|█████▏    | 3402/6500 [22:29:16<19:32:50, 22.71s/it]                                                          52%|█████▏    | 3402/6500 [22:29:16<19:32:50, 22.71s/it] 52%|█████▏    | 3403/6500 [22:29:34<18:19:52, 21.31s/it]                                                          52%|█████▏    | 3403/6500 [22:29:34<18:19:52, 21.31s/it] 52%|█████▏    | 3404/6500 [22:29:52<17:28:57, 20.33s/it]                                                          52%|█████▏    | 3404/6500 [22:29:52<17:28:57, 20.33s/it] 52%|█████▏    | 3405/6500 [22:30:10<16:53:47, 19.65s/it]                                                          52%|█████▏    | 3405/6500 [22:30:10<16:53:47, 19.65s/it] 52%|█████▏    |{'loss': 0.2611, 'learning_rate': 4.625660794456665e-05, 'epoch': 0.52}
{'loss': 0.2737, 'learning_rate': 4.623250269153343e-05, 'epoch': 0.52}
{'loss': 0.2659, 'learning_rate': 4.6208398319129804e-05, 'epoch': 0.52}
{'loss': 0.2769, 'learning_rate': 4.6184294832990016e-05, 'epoch': 0.52}
{'loss': 0.2678, 'learning_rate': 4.616019223874811e-05, 'epoch': 0.52}
 3406/6500 [22:30:29<16:29:06, 19.18s/it]                                                          52%|█████▏    | 3406/6500 [22:30:29<16:29:06, 19.18s/it] 52%|█████▏    | 3407/6500 [22:30:47<16:12:24, 18.86s/it]                                                          52%|█████▏    | 3407/6500 [22:30:47<16:12:24, 18.86s/it] 52%|█████▏    | 3408/6500 [22:31:05<16:01:08, 18.65s/it]                                                          52%|█████▏    | 3408/6500 [22:31:05<16:01:08, 18.65s/it] 52%|█████▏    | 3409/6500 [22:31:23<15:52:55, 18.50s/it]                                                          52%|█████▏    | 3409/6500 [22:31:23<15:52:55, 18.50s/it] 52%|█████▏    | 3410/6500 [22:31:41<15:51:05, 18.47s/it]                                                          52%|█████▏    | 3410/6500 [22:31:41<15:51:05, 18.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8623576164245605, 'eval_runtime': 5.487, 'eval_samples_per_second': 4.192, 'eval_steps_per_second': 1.094, 'epoch': 0.52}
                                                          52%|█████▏    | 3410/6500 [22:31:47<15:51:05, 18.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3410
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3410
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3410

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3410/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3410/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.278, 'learning_rate': 4.6136090542037924e-05, 'epoch': 0.52}
{'loss': 0.2493, 'learning_rate': 4.611198974849309e-05, 'epoch': 0.52}
{'loss': 0.2571, 'learning_rate': 4.6087889863747e-05, 'epoch': 0.53}
{'loss': 0.2401, 'learning_rate': 4.6063790893432874e-05, 'epoch': 0.53}
{'loss': 0.2955, 'learning_rate': 4.603969284318369e-05, 'epoch': 0.53}
 52%|█████▏    | 3411/6500 [22:32:33<24:28:05, 28.52s/it]                                                          52%|█████▏    | 3411/6500 [22:32:33<24:28:05, 28.52s/it] 52%|█████▏    | 3412/6500 [22:32:51<21:46:04, 25.38s/it]                                                          52%|█████▏    | 3412/6500 [22:32:51<21:46:04, 25.38s/it] 53%|█████▎    | 3413/6500 [22:33:09<19:52:33, 23.18s/it]                                                          53%|█████▎    | 3413/6500 [22:33:09<19:52:33, 23.18s/it] 53%|█████▎    | 3414/6500 [22:33:27<18:32:36, 21.63s/it]                                                          53%|█████▎    | 3414/6500 [22:33:27<18:32:36, 21.63s/it] 53%|█████▎    | 3415/6500 [22:33:45<17:36:56, 20.56s/it]                                                          53%|█████▎    | 3415/6500 [22:33:45<17:36:56, 20.56s/it] 53%|█████▎    |{'loss': 0.3073, 'learning_rate': 4.6015595718632226e-05, 'epoch': 0.53}
{'loss': 0.2547, 'learning_rate': 4.5991499525411046e-05, 'epoch': 0.53}
{'loss': 0.2743, 'learning_rate': 4.596740426915247e-05, 'epoch': 0.53}
{'loss': 0.2658, 'learning_rate': 4.594330995548863e-05, 'epoch': 0.53}
{'loss': 0.7874, 'learning_rate': 4.591921659005142e-05, 'epoch': 0.53}
 3416/6500 [22:34:04<16:58:37, 19.82s/it]                                                          53%|█████▎    | 3416/6500 [22:34:04<16:58:37, 19.82s/it] 53%|█████▎    | 3417/6500 [22:34:22<16:31:50, 19.30s/it]                                                          53%|█████▎    | 3417/6500 [22:34:22<16:31:50, 19.30s/it] 53%|█████▎    | 3418/6500 [22:34:40<16:13:14, 18.95s/it]                                                          53%|█████▎    | 3418/6500 [22:34:40<16:13:14, 18.95s/it] 53%|█████▎    | 3419/6500 [22:34:58<16:00:21, 18.70s/it]                                                          53%|█████▎    | 3419/6500 [22:34:58<16:00:21, 18.70s/it] 53%|█████▎    | 3420/6500 [22:35:16<15:51:43, 18.54s/it]                                                          53%|█████▎    | 3420/6500 [22:35:16<15:51:43, 18.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8611629605293274, 'eval_runtime': 5.3638, 'eval_samples_per_second': 4.288, 'eval_steps_per_second': 1.119, 'epoch': 0.53}
                                                          53%|█████▎    | 3420/6500 [22:35:21<15:51:43, 18.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3420


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3420
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3420

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3420/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3420/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2606, 'learning_rate': 4.589512417847252e-05, 'epoch': 0.53}
{'loss': 0.2756, 'learning_rate': 4.5871032726383386e-05, 'epoch': 0.53}
{'loss': 0.2524, 'learning_rate': 4.584694223941526e-05, 'epoch': 0.53}
{'loss': 0.2698, 'learning_rate': 4.582285272319913e-05, 'epoch': 0.53}
{'loss': 0.2634, 'learning_rate': 4.579876418336577e-05, 'epoch': 0.53}
 53%|█████▎    | 3421/6500 [22:36:50<35:06:26, 41.05s/it]                                                          53%|█████▎    | 3421/6500 [22:36:50<35:06:26, 41.05s/it] 53%|█████▎    | 3422/6500 [22:37:08<29:12:42, 34.17s/it]                                                          53%|█████▎    | 3422/6500 [22:37:08<29:12:42, 34.17s/it] 53%|█████▎    | 3423/6500 [22:37:26<25:02:58, 29.31s/it]                                                          53%|█████▎    | 3423/6500 [22:37:26<25:02:58, 29.31s/it] 53%|█████▎    | 3424/6500 [22:37:44<22:08:50, 25.92s/it]                                                          53%|█████▎    | 3424/6500 [22:37:44<22:08:50, 25.92s/it] 53%|█████▎    | 3425/6500 [22:38:02<20:08:43, 23.58s/it]                                                          53%|█████▎    | 3425/6500 [22:38:02<20:08:43, 23.58s/it] 53%|█████▎    |{'loss': 0.2342, 'learning_rate': 4.577467662554574e-05, 'epoch': 0.53}
{'loss': 0.2751, 'learning_rate': 4.575059005536935e-05, 'epoch': 0.53}
{'loss': 0.2571, 'learning_rate': 4.572650447846672e-05, 'epoch': 0.53}
{'loss': 0.2762, 'learning_rate': 4.570241990046767e-05, 'epoch': 0.53}
{'loss': 0.273, 'learning_rate': 4.5678336327001844e-05, 'epoch': 0.53}
 3426/6500 [22:38:20<18:49:32, 22.05s/it]                                                          53%|█████▎    | 3426/6500 [22:38:20<18:49:32, 22.05s/it] 53%|█████▎    | 3427/6500 [22:38:39<17:49:59, 20.89s/it]                                                          53%|█████▎    | 3427/6500 [22:38:39<17:49:59, 20.89s/it] 53%|█████▎    | 3428/6500 [22:38:57<17:07:04, 20.06s/it]                                                          53%|█████▎    | 3428/6500 [22:38:57<17:07:04, 20.06s/it] 53%|█████▎    | 3429/6500 [22:39:15<16:37:06, 19.48s/it]                                                          53%|█████▎    | 3429/6500 [22:39:15<16:37:06, 19.48s/it] 53%|█████▎    | 3430/6500 [22:39:33<16:16:23, 19.08s/it]                                                          53%|█████▎    | 3430/6500 [22:39:33<16:16:23, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8667356967926025, 'eval_runtime': 5.3505, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.121, 'epoch': 0.53}
                                                          53%|█████▎    | 3430/6500 [22:39:38<16:16:23, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3430
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3430/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3430


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2774, 'learning_rate': 4.565425376369862e-05, 'epoch': 0.53}
{'loss': 0.2632, 'learning_rate': 4.563017221618715e-05, 'epoch': 0.53}
{'loss': 0.2685, 'learning_rate': 4.560609169009636e-05, 'epoch': 0.53}
{'loss': 0.2861, 'learning_rate': 4.558201219105494e-05, 'epoch': 0.53}
{'loss': 0.2585, 'learning_rate': 4.555793372469129e-05, 'epoch': 0.53}
 53%|█████▎    | 3431/6500 [22:41:14<37:20:42, 43.81s/it]                                                          53%|█████▎    | 3431/6500 [22:41:14<37:20:42, 43.81s/it] 53%|█████▎    | 3432/6500 [22:41:32<30:44:20, 36.07s/it]                                                          53%|█████▎    | 3432/6500 [22:41:32<30:44:20, 36.07s/it] 53%|█████▎    | 3433/6500 [22:41:50<26:07:04, 30.66s/it]                                                          53%|█████▎    | 3433/6500 [22:41:50<26:07:04, 30.66s/it] 53%|█████▎    | 3434/6500 [22:42:08<22:52:32, 26.86s/it]                                                          53%|█████▎    | 3434/6500 [22:42:08<22:52:32, 26.86s/it] 53%|█████▎    | 3435/6500 [22:42:27<20:36:55, 24.21s/it]                                                          53%|█████▎    | 3435/6500 [22:42:27<20:36:55, 24.21s/it] 53%|█████▎    |{'loss': 0.2776, 'learning_rate': 4.553385629663363e-05, 'epoch': 0.53}
{'loss': 0.2802, 'learning_rate': 4.55097799125099e-05, 'epoch': 0.53}
{'loss': 0.2649, 'learning_rate': 4.548570457794782e-05, 'epoch': 0.53}
{'loss': 0.2692, 'learning_rate': 4.546163029857485e-05, 'epoch': 0.53}
{'loss': 0.2787, 'learning_rate': 4.5437557080018175e-05, 'epoch': 0.53}
 3436/6500 [22:42:45<19:02:27, 22.37s/it]                                                          53%|█████▎    | 3436/6500 [22:42:45<19:02:27, 22.37s/it] 53%|█████▎    | 3437/6500 [22:43:03<17:56:42, 21.09s/it]                                                          53%|█████▎    | 3437/6500 [22:43:03<17:56:42, 21.09s/it] 53%|█████▎    | 3438/6500 [22:43:21<17:10:40, 20.20s/it]                                                          53%|█████▎    | 3438/6500 [22:43:21<17:10:40, 20.20s/it] 53%|█████▎    | 3439/6500 [22:43:39<16:38:49, 19.58s/it]                                                          53%|█████▎    | 3439/6500 [22:43:39<16:38:49, 19.58s/it] 53%|█████▎    | 3440/6500 [22:43:57<16:16:50, 19.15s/it]                                                          53%|█████▎    | 3440/6500 [22:43:57<16:16:50, 19.15s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8544557094573975, 'eval_runtime': 5.3438, 'eval_samples_per_second': 4.304, 'eval_steps_per_second': 1.123, 'epoch': 0.53}
                                                          53%|█████▎    | 3440/6500 [22:44:02<16:16:50, 19.15s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3440
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3440/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2683, 'learning_rate': 4.541348492790482e-05, 'epoch': 0.53}
{'loss': 0.2616, 'learning_rate': 4.538941384786147e-05, 'epoch': 0.53}
{'loss': 0.2533, 'learning_rate': 4.536534384551462e-05, 'epoch': 0.53}
{'loss': 0.2651, 'learning_rate': 4.5341274926490446e-05, 'epoch': 0.53}
{'loss': 0.3231, 'learning_rate': 4.5317207096414934e-05, 'epoch': 0.53}
 53%|█████▎    | 3441/6500 [22:44:49<24:40:00, 29.03s/it]                                                          53%|█████▎    | 3441/6500 [22:44:49<24:40:00, 29.03s/it] 53%|█████▎    | 3442/6500 [22:45:07<21:52:22, 25.75s/it]                                                          53%|█████▎    | 3442/6500 [22:45:07<21:52:22, 25.75s/it] 53%|█████▎    | 3443/6500 [22:45:26<19:58:02, 23.51s/it]                                                          53%|█████▎    | 3443/6500 [22:45:26<19:58:02, 23.51s/it] 53%|█████▎    | 3444/6500 [22:45:44<18:36:25, 21.92s/it]                                                          53%|█████▎    | 3444/6500 [22:45:44<18:36:25, 21.92s/it] 53%|█████▎    | 3445/6500 [22:46:02<17:37:43, 20.77s/it]                                                          53%|█████▎    | 3445/6500 [22:46:02<17:37:43, 20.77s/it] 53%|█████▎    |{'loss': 0.2514, 'learning_rate': 4.529314036091379e-05, 'epoch': 0.53}
{'loss': 0.2549, 'learning_rate': 4.5269074725612474e-05, 'epoch': 0.53}
{'loss': 0.2817, 'learning_rate': 4.524501019613619e-05, 'epoch': 0.53}
{'loss': 0.7733, 'learning_rate': 4.522094677810985e-05, 'epoch': 0.53}
{'loss': 0.2715, 'learning_rate': 4.519688447715814e-05, 'epoch': 0.53}
 3446/6500 [22:46:20<16:56:39, 19.97s/it]                                                          53%|█████▎    | 3446/6500 [22:46:20<16:56:39, 19.97s/it] 53%|█████▎    | 3447/6500 [22:46:38<16:28:59, 19.44s/it]                                                          53%|█████▎    | 3447/6500 [22:46:38<16:28:59, 19.44s/it] 53%|█████▎    | 3448/6500 [22:46:56<16:09:25, 19.06s/it]                                                          53%|█████▎    | 3448/6500 [22:46:56<16:09:25, 19.06s/it] 53%|█████▎    | 3449/6500 [22:47:14<15:54:51, 18.78s/it]                                                          53%|█████▎    | 3449/6500 [22:47:14<15:54:51, 18.78s/it] 53%|█████▎    | 3450/6500 [22:47:33<15:45:28, 18.60s/it]                                                          53%|█████▎    | 3450/6500 [22:47:33<15:45:28, 18.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8543720245361328, 'eval_runtime': 5.3455, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.122, 'epoch': 0.53}
                                                          53%|█████▎    | 3450/6500 [22:47:38<15:45:28, 18.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3450
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3450
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3450

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3450/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3450/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2614, 'learning_rate': 4.517282329890548e-05, 'epoch': 0.53}
{'loss': 0.2675, 'learning_rate': 4.514876324897602e-05, 'epoch': 0.53}
{'loss': 0.2435, 'learning_rate': 4.512470433299366e-05, 'epoch': 0.53}
{'loss': 0.2701, 'learning_rate': 4.510064655658203e-05, 'epoch': 0.53}
{'loss': 0.2487, 'learning_rate': 4.5076589925364465e-05, 'epoch': 0.53}
 53%|█████▎    | 3451/6500 [22:48:27<24:43:51, 29.20s/it]                                                          53%|█████▎    | 3451/6500 [22:48:27<24:43:51, 29.20s/it] 53%|█████▎    | 3452/6500 [22:48:45<21:53:57, 25.87s/it]                                                          53%|█████▎    | 3452/6500 [22:48:45<21:53:57, 25.87s/it] 53%|█████▎    | 3453/6500 [22:49:03<19:55:38, 23.54s/it]                                                          53%|█████▎    | 3453/6500 [22:49:03<19:55:38, 23.54s/it] 53%|█████▎    | 3454/6500 [22:49:21<18:32:24, 21.91s/it]                                                          53%|█████▎    | 3454/6500 [22:49:21<18:32:24, 21.91s/it] 53%|█████▎    | 3455/6500 [22:49:39<17:34:06, 20.77s/it]                                                          53%|█████▎    | 3455/6500 [22:49:39<17:34:06, 20.77s/it] 53%|█████▎    |{'loss': 0.2428, 'learning_rate': 4.505253444496407e-05, 'epoch': 0.53}
{'loss': 0.2615, 'learning_rate': 4.502848012100367e-05, 'epoch': 0.53}
{'loss': 0.2487, 'learning_rate': 4.500442695910582e-05, 'epoch': 0.53}
{'loss': 0.267, 'learning_rate': 4.4980374964892794e-05, 'epoch': 0.53}
{'loss': 0.2601, 'learning_rate': 4.4956324143986596e-05, 'epoch': 0.53}
 3456/6500 [22:49:57<16:53:40, 19.98s/it]                                                          53%|█████▎    | 3456/6500 [22:49:57<16:53:40, 19.98s/it] 53%|█████▎    | 3457/6500 [22:50:15<16:25:35, 19.43s/it]                                                          53%|█████▎    | 3457/6500 [22:50:15<16:25:35, 19.43s/it] 53%|█████▎    | 3458/6500 [22:50:33<16:06:05, 19.06s/it]                                                          53%|█████▎    | 3458/6500 [22:50:33<16:06:05, 19.06s/it] 53%|█████▎    | 3459/6500 [22:50:52<16:00:09, 18.94s/it]                                                          53%|█████▎    | 3459/6500 [22:50:52<16:00:09, 18.94s/it] 53%|█████▎    | 3460/6500 [22:51:10<15:48:21, 18.72s/it]                                                          53%|█████▎    | 3460/6500 [22:51:10<15:48:21, 18.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8728855848312378, 'eval_runtime': 5.6818, 'eval_samples_per_second': 4.048, 'eval_steps_per_second': 1.056, 'epoch': 0.53}
                                                          53%|█████▎    | 3460/6500 [22:51:16<15:48:21, 18.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3460

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3460
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3460

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3460/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2806, 'learning_rate': 4.4932274502008956e-05, 'epoch': 0.53}
{'loss': 0.2629, 'learning_rate': 4.4908226044581346e-05, 'epoch': 0.53}
{'loss': 0.2908, 'learning_rate': 4.488417877732494e-05, 'epoch': 0.53}
{'loss': 0.2628, 'learning_rate': 4.4860132705860644e-05, 'epoch': 0.53}
{'loss': 0.2772, 'learning_rate': 4.4836087835809083e-05, 'epoch': 0.53}
 53%|█████▎    | 3461/6500 [22:52:12<26:37:10, 31.53s/it]                                                          53%|█████▎    | 3461/6500 [22:52:12<26:37:10, 31.53s/it] 53%|█████▎    | 3462/6500 [22:52:30<23:12:32, 27.50s/it]                                                          53%|█████▎    | 3462/6500 [22:52:30<23:12:32, 27.50s/it] 53%|█████▎    | 3463/6500 [22:52:48<20:48:40, 24.67s/it]                                                          53%|█████▎    | 3463/6500 [22:52:48<20:48:40, 24.67s/it] 53%|█████▎    | 3464/6500 [22:53:06<19:08:06, 22.69s/it]                                                          53%|█████▎    | 3464/6500 [22:53:06<19:08:06, 22.69s/it] 53%|█████▎    | 3465/6500 [22:53:24<17:58:03, 21.31s/it]                                                          53%|█████▎    | 3465/6500 [22:53:24<17:58:03, 21.31s/it] 53%|█████▎    |{'loss': 0.2705, 'learning_rate': 4.481204417279058e-05, 'epoch': 0.53}
{'loss': 0.2678, 'learning_rate': 4.478800172242521e-05, 'epoch': 0.53}
{'loss': 0.27, 'learning_rate': 4.476396049033275e-05, 'epoch': 0.53}
{'loss': 0.2791, 'learning_rate': 4.473992048213269e-05, 'epoch': 0.53}
{'loss': 0.2781, 'learning_rate': 4.471588170344423e-05, 'epoch': 0.53}
 3466/6500 [22:53:42<17:09:05, 20.35s/it]                                                          53%|█████▎    | 3466/6500 [22:53:42<17:09:05, 20.35s/it] 53%|█████▎    | 3467/6500 [22:54:00<16:35:06, 19.69s/it]                                                          53%|█████▎    | 3467/6500 [22:54:00<16:35:06, 19.69s/it] 53%|█████▎    | 3468/6500 [22:54:19<16:11:39, 19.23s/it]                                                          53%|█████▎    | 3468/6500 [22:54:19<16:11:39, 19.23s/it] 53%|█████▎    | 3469/6500 [22:54:37<15:55:17, 18.91s/it]                                                          53%|█████▎    | 3469/6500 [22:54:37<15:55:17, 18.91s/it] 53%|█████▎    | 3470/6500 [22:54:55<15:44:03, 18.69s/it]                                                          53%|█████▎    | 3470/6500 [22:54:55<15:44:03, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8583608865737915, 'eval_runtime': 5.3452, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.123, 'epoch': 0.53}
                                                          53%|█████▎    | 3470/6500 [22:55:00<15:44:03, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3470
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3470 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3470
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3470
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3470/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2548, 'learning_rate': 4.469184415988631e-05, 'epoch': 0.53}
{'loss': 0.2606, 'learning_rate': 4.466780785707752e-05, 'epoch': 0.53}
{'loss': 0.2518, 'learning_rate': 4.464377280063624e-05, 'epoch': 0.53}
{'loss': 0.291, 'learning_rate': 4.46197389961805e-05, 'epoch': 0.53}
{'loss': 0.323, 'learning_rate': 4.459570644932805e-05, 'epoch': 0.53}
 53%|█████▎    | 3471/6500 [22:56:14<31:03:58, 36.92s/it]                                                          53%|█████▎    | 3471/6500 [22:56:14<31:03:58, 36.92s/it] 53%|█████▎    | 3472/6500 [22:56:32<26:16:46, 31.24s/it]                                                          53%|█████▎    | 3472/6500 [22:56:32<26:16:46, 31.24s/it] 53%|█████▎    | 3473/6500 [22:56:50<22:56:11, 27.28s/it]                                                          53%|█████▎    | 3473/6500 [22:56:50<22:56:11, 27.28s/it] 53%|█████▎    | 3474/6500 [22:57:08<20:36:09, 24.51s/it]                                                          53%|█████▎    | 3474/6500 [22:57:08<20:36:09, 24.51s/it] 53%|█████▎    | 3475/6500 [22:57:27<19:04:05, 22.69s/it]                                                          53%|█████▎    | 3475/6500 [22:57:27<19:04:05, 22.69s/it] 53%|█████▎    |{'loss': 0.2573, 'learning_rate': 4.457167516569637e-05, 'epoch': 0.53}
{'loss': 0.2574, 'learning_rate': 4.454764515090261e-05, 'epoch': 0.53}
{'loss': 0.2733, 'learning_rate': 4.452361641056364e-05, 'epoch': 0.54}
{'loss': 0.7771, 'learning_rate': 4.449958895029604e-05, 'epoch': 0.54}
{'loss': 0.271, 'learning_rate': 4.447556277571608e-05, 'epoch': 0.54}
 3476/6500 [22:57:45<17:54:28, 21.32s/it]                                                          53%|█████▎    | 3476/6500 [22:57:45<17:54:28, 21.32s/it] 53%|█████▎    | 3477/6500 [22:58:03<17:05:48, 20.36s/it]                                                          53%|█████▎    | 3477/6500 [22:58:03<17:05:48, 20.36s/it] 54%|█████▎    | 3478/6500 [22:58:21<16:31:51, 19.69s/it]                                                          54%|█████▎    | 3478/6500 [22:58:21<16:31:51, 19.69s/it] 54%|█████▎    | 3479/6500 [22:58:39<16:08:00, 19.23s/it]                                                          54%|█████▎    | 3479/6500 [22:58:39<16:08:00, 19.23s/it] 54%|█████▎    | 3480/6500 [22:58:58<15:51:38, 18.91s/it]                                                          54%|█████▎    | 3480/6500 [22:58:58<15:51:38, 18.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8611753582954407, 'eval_runtime': 5.4716, 'eval_samples_per_second': 4.204, 'eval_steps_per_second': 1.097, 'epoch': 0.54}
                                                          54%|█████▎    | 3480/6500 [22:59:03<15:51:38, 18.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3480
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3480
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3480
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3480

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3480
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3480/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3480/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2692, 'learning_rate': 4.4451537892439735e-05, 'epoch': 0.54}
{'loss': 0.2443, 'learning_rate': 4.442751430608267e-05, 'epoch': 0.54}
{'loss': 0.2593, 'learning_rate': 4.440349202226026e-05, 'epoch': 0.54}
{'loss': 0.2761, 'learning_rate': 4.437947104658755e-05, 'epoch': 0.54}
{'loss': 0.2478, 'learning_rate': 4.4355451384679313e-05, 'epoch': 0.54}
 54%|█████▎    | 3481/6500 [23:00:09<28:59:19, 34.57s/it]                                                          54%|█████▎    | 3481/6500 [23:00:09<28:59:19, 34.57s/it] 54%|█████▎    | 3482/6500 [23:00:27<24:48:40, 29.60s/it]                                                          54%|█████▎    | 3482/6500 [23:00:27<24:48:40, 29.60s/it] 54%|█████▎    | 3483/6500 [23:00:45<21:53:28, 26.12s/it]                                                          54%|█████▎    | 3483/6500 [23:00:45<21:53:28, 26.12s/it] 54%|█████▎    | 3484/6500 [23:01:03<19:50:49, 23.69s/it]                                                          54%|█████▎    | 3484/6500 [23:01:03<19:50:49, 23.69s/it] 54%|█████▎    | 3485/6500 [23:01:21<18:25:14, 21.99s/it]                                                          54%|█████▎    | 3485/6500 [23:01:21<18:25:14, 21.99s/it] 54%|█████▎    |{'loss': 0.261, 'learning_rate': 4.4331433042150003e-05, 'epoch': 0.54}
{'loss': 0.2582, 'learning_rate': 4.430741602461376e-05, 'epoch': 0.54}
{'loss': 0.2608, 'learning_rate': 4.428340033768439e-05, 'epoch': 0.54}
{'loss': 0.2559, 'learning_rate': 4.4259385986975446e-05, 'epoch': 0.54}
{'loss': 0.2627, 'learning_rate': 4.423537297810012e-05, 'epoch': 0.54}
 3486/6500 [23:01:39<17:25:44, 20.82s/it]                                                          54%|█████▎    | 3486/6500 [23:01:39<17:25:44, 20.82s/it] 54%|█████▎    | 3487/6500 [23:01:57<16:44:10, 20.00s/it]                                                          54%|█████▎    | 3487/6500 [23:01:57<16:44:10, 20.00s/it] 54%|█████▎    | 3488/6500 [23:02:15<16:15:38, 19.44s/it]                                                          54%|█████▎    | 3488/6500 [23:02:15<16:15:38, 19.44s/it] 54%|█████▎    | 3489/6500 [23:02:33<15:55:43, 19.04s/it]                                                          54%|█████▎    | 3489/6500 [23:02:33<15:55:43, 19.04s/it] 54%|█████▎    | 3490/6500 [23:02:51<15:41:44, 18.77s/it]                                                          54%|█████▎    | 3490/6500 [23:02:51<15:41:44, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8716933131217957, 'eval_runtime': 5.3475, 'eval_samples_per_second': 4.301, 'eval_steps_per_second': 1.122, 'epoch': 0.54}
                                                          54%|█████▎    | 3490/6500 [23:02:57<15:41:44, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3490
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3490the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3490

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3490/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2657, 'learning_rate': 4.421136131667131e-05, 'epoch': 0.54}
{'loss': 0.2641, 'learning_rate': 4.4187351008301596e-05, 'epoch': 0.54}
{'loss': 0.2743, 'learning_rate': 4.4163342058603255e-05, 'epoch': 0.54}
{'loss': 0.2487, 'learning_rate': 4.413933447318821e-05, 'epoch': 0.54}
{'loss': 0.2531, 'learning_rate': 4.41153282576681e-05, 'epoch': 0.54}
 54%|█████▎    | 3491/6500 [23:03:49<25:22:07, 30.35s/it]                                                          54%|█████▎    | 3491/6500 [23:03:49<25:22:07, 30.35s/it] 54%|█████▎    | 3492/6500 [23:04:07<22:17:48, 26.68s/it]                                                          54%|█████▎    | 3492/6500 [23:04:07<22:17:48, 26.68s/it] 54%|█████▎    | 3493/6500 [23:04:25<20:09:25, 24.13s/it]                                                          54%|█████▎    | 3493/6500 [23:04:25<20:09:25, 24.13s/it] 54%|█████▍    | 3494/6500 [23:04:43<18:38:05, 22.32s/it]                                                          54%|█████▍    | 3494/6500 [23:04:43<18:38:05, 22.32s/it] 54%|█████▍    | 3495/6500 [23:05:01<17:34:40, 21.06s/it]                                                          54%|█████▍    | 3495/6500 [23:05:01<17:34:40, 21.06s/it] 54%|█████▍    |{'loss': 0.2687, 'learning_rate': 4.4091323417654225e-05, 'epoch': 0.54}
{'loss': 0.2554, 'learning_rate': 4.406731995875758e-05, 'epoch': 0.54}
{'loss': 0.2693, 'learning_rate': 4.404331788658882e-05, 'epoch': 0.54}
{'loss': 0.2562, 'learning_rate': 4.4019317206758297e-05, 'epoch': 0.54}
{'loss': 0.2749, 'learning_rate': 4.399531792487601e-05, 'epoch': 0.54}
 3496/6500 [23:05:19<16:51:40, 20.21s/it]                                                          54%|█████▍    | 3496/6500 [23:05:19<16:51:40, 20.21s/it] 54%|█████▍    | 3497/6500 [23:05:37<16:20:19, 19.59s/it]                                                          54%|█████▍    | 3497/6500 [23:05:37<16:20:19, 19.59s/it] 54%|█████▍    | 3498/6500 [23:05:56<15:58:24, 19.16s/it]                                                          54%|█████▍    | 3498/6500 [23:05:56<15:58:24, 19.16s/it] 54%|█████▍    | 3499/6500 [23:06:14<15:43:13, 18.86s/it]                                                          54%|█████▍    | 3499/6500 [23:06:14<15:43:13, 18.86s/it] 54%|█████▍    | 3500/6500 [23:06:32<15:32:49, 18.66s/it]                                                          54%|█████▍    | 3500/6500 [23:06:32<15:32:49, 18.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8645471334457397, 'eval_runtime': 5.3549, 'eval_samples_per_second': 4.295, 'eval_steps_per_second': 1.12, 'epoch': 0.54}
                                                          54%|█████▍    | 3500/6500 [23:06:37<15:32:49, 18.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3500I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3500
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3500/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3500/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2559, 'learning_rate': 4.397132004655165e-05, 'epoch': 0.54}
{'loss': 0.2539, 'learning_rate': 4.394732357739456e-05, 'epoch': 0.54}
{'loss': 0.2447, 'learning_rate': 4.392332852301379e-05, 'epoch': 0.54}
{'loss': 0.3295, 'learning_rate': 4.389933488901805e-05, 'epoch': 0.54}
{'loss': 0.2672, 'learning_rate': 4.387534268101566e-05, 'epoch': 0.54}
 54%|█████▍    | 3501/6500 [23:07:29<25:08:23, 30.18s/it]                                                          54%|█████▍    | 3501/6500 [23:07:29<25:08:23, 30.18s/it] 54%|█████▍    | 3502/6500 [23:07:47<22:06:23, 26.55s/it]                                                          54%|█████▍    | 3502/6500 [23:07:47<22:06:23, 26.55s/it] 54%|█████▍    | 3503/6500 [23:08:05<19:58:46, 24.00s/it]                                                          54%|█████▍    | 3503/6500 [23:08:05<19:58:46, 24.00s/it] 54%|█████▍    | 3504/6500 [23:08:23<18:29:46, 22.23s/it]                                                          54%|█████▍    | 3504/6500 [23:08:23<18:29:46, 22.23s/it] 54%|█████▍    | 3505/6500 [23:08:41<17:27:35, 20.99s/it]                                                          54%|█████▍    | 3505/6500 [23:08:41<17:27:35, 20.99s/it] 54%|█████▍    |{'loss': 0.2465, 'learning_rate': 4.38513519046147e-05, 'epoch': 0.54}
{'loss': 0.2709, 'learning_rate': 4.382736256542283e-05, 'epoch': 0.54}
{'loss': 0.6722, 'learning_rate': 4.3803374669047436e-05, 'epoch': 0.54}
{'loss': 0.3718, 'learning_rate': 4.377938822109554e-05, 'epoch': 0.54}
{'loss': 0.2687, 'learning_rate': 4.3755403227173836e-05, 'epoch': 0.54}
 3506/6500 [23:08:59<16:44:28, 20.13s/it]                                                          54%|█████▍    | 3506/6500 [23:08:59<16:44:28, 20.13s/it] 54%|█████▍    | 3507/6500 [23:09:18<16:18:15, 19.61s/it]                                                          54%|█████▍    | 3507/6500 [23:09:18<16:18:15, 19.61s/it] 54%|█████▍    | 3508/6500 [23:09:36<15:56:13, 19.18s/it]                                                          54%|█████▍    | 3508/6500 [23:09:36<15:56:13, 19.18s/it] 54%|█████▍    | 3509/6500 [23:09:54<15:40:46, 18.87s/it]                                                          54%|█████▍    | 3509/6500 [23:09:54<15:40:46, 18.87s/it] 54%|█████▍    | 3510/6500 [23:10:12<15:30:22, 18.67s/it]                                                          54%|█████▍    | 3510/6500 [23:10:12<15:30:22, 18.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8615349531173706, 'eval_runtime': 5.5009, 'eval_samples_per_second': 4.181, 'eval_steps_per_second': 1.091, 'epoch': 0.54}
                                                          54%|█████▍    | 3510/6500 [23:10:18<15:30:22, 18.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3510
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3510
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3510
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3510

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3510
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3510/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2723, 'learning_rate': 4.373141969288865e-05, 'epoch': 0.54}
{'loss': 0.2426, 'learning_rate': 4.3707437623845995e-05, 'epoch': 0.54}
{'loss': 0.2709, 'learning_rate': 4.368345702565153e-05, 'epoch': 0.54}
{'loss': 0.2611, 'learning_rate': 4.365947790391059e-05, 'epoch': 0.54}
{'loss': 0.2399, 'learning_rate': 4.3635500264228146e-05, 'epoch': 0.54}
 54%|█████▍    | 3511/6500 [23:11:20<27:41:06, 33.34s/it]                                                          54%|█████▍    | 3511/6500 [23:11:20<27:41:06, 33.34s/it] 54%|█████▍    | 3512/6500 [23:11:38<23:52:08, 28.76s/it]                                                          54%|█████▍    | 3512/6500 [23:11:38<23:52:08, 28.76s/it] 54%|█████▍    | 3513/6500 [23:11:56<21:11:25, 25.54s/it]                                                          54%|█████▍    | 3513/6500 [23:11:56<21:11:25, 25.54s/it] 54%|█████▍    | 3514/6500 [23:12:14<19:18:57, 23.29s/it]                                                          54%|█████▍    | 3514/6500 [23:12:14<19:18:57, 23.29s/it] 54%|█████▍    | 3515/6500 [23:12:32<18:00:29, 21.72s/it]                                                          54%|█████▍    | 3515/6500 [23:12:32<18:00:29, 21.72s/it] 54%|█████▍    |{'loss': 0.2745, 'learning_rate': 4.361152411220878e-05, 'epoch': 0.54}
{'loss': 0.2506, 'learning_rate': 4.358754945345684e-05, 'epoch': 0.54}
{'loss': 0.268, 'learning_rate': 4.356357629357624e-05, 'epoch': 0.54}
{'loss': 0.2601, 'learning_rate': 4.353960463817053e-05, 'epoch': 0.54}
{'loss': 0.2588, 'learning_rate': 4.3515634492842956e-05, 'epoch': 0.54}
 3516/6500 [23:12:50<17:05:59, 20.63s/it]                                                          54%|█████▍    | 3516/6500 [23:12:50<17:05:59, 20.63s/it] 54%|█████▍    | 3517/6500 [23:13:08<16:28:12, 19.88s/it]                                                          54%|█████▍    | 3517/6500 [23:13:08<16:28:12, 19.88s/it] 54%|█████▍    | 3518/6500 [23:13:27<16:02:06, 19.36s/it]                                                          54%|█████▍    | 3518/6500 [23:13:27<16:02:06, 19.36s/it] 54%|█████▍    | 3519/6500 [23:13:45<15:43:29, 18.99s/it]                                                          54%|█████▍    | 3519/6500 [23:13:45<15:43:29, 18.99s/it] 54%|█████▍    | 3520/6500 [23:14:03<15:30:59, 18.74s/it]                                                          54%|█████▍    | 3520/6500 [23:14:03<15:30:59, 18.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711104989051819, 'eval_runtime': 5.6191, 'eval_samples_per_second': 4.093, 'eval_steps_per_second': 1.068, 'epoch': 0.54}
                                                          54%|█████▍    | 3520/6500 [23:14:08<15:30:59, 18.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3520
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3520/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3520

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3520

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3520
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3520/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2549, 'learning_rate': 4.3491665863196396e-05, 'epoch': 0.54}
{'loss': 0.2677, 'learning_rate': 4.346769875483336e-05, 'epoch': 0.54}
{'loss': 0.2764, 'learning_rate': 4.3443733173356035e-05, 'epoch': 0.54}
{'loss': 0.2651, 'learning_rate': 4.3419769124366225e-05, 'epoch': 0.54}
{'loss': 0.2757, 'learning_rate': 4.339580661346536e-05, 'epoch': 0.54}
 54%|█████▍    | 3521/6500 [23:14:59<24:52:13, 30.05s/it]                                                          54%|█████▍    | 3521/6500 [23:14:59<24:52:13, 30.05s/it] 54%|█████▍    | 3522/6500 [23:15:17<21:53:27, 26.46s/it]                                                          54%|█████▍    | 3522/6500 [23:15:17<21:53:27, 26.46s/it] 54%|█████▍    | 3523/6500 [23:15:36<20:03:19, 24.25s/it]                                                          54%|█████▍    | 3523/6500 [23:15:36<20:03:19, 24.25s/it] 54%|█████▍    | 3524/6500 [23:15:55<18:31:09, 22.40s/it]                                                          54%|█████▍    | 3524/6500 [23:15:55<18:31:09, 22.40s/it] 54%|█████▍    | 3525/6500 [23:16:13<17:27:15, 21.12s/it]                                                          54%|█████▍    | 3525/6500 [23:16:13<17:27:15, 21.12s/it] 54%|█████▍    |{'loss': 0.2648, 'learning_rate': 4.337184564625455e-05, 'epoch': 0.54}
{'loss': 0.2641, 'learning_rate': 4.334788622833452e-05, 'epoch': 0.54}
{'loss': 0.2589, 'learning_rate': 4.3323928365305636e-05, 'epoch': 0.54}
{'loss': 0.2716, 'learning_rate': 4.3299972062767905e-05, 'epoch': 0.54}
{'loss': 0.2651, 'learning_rate': 4.3276017326320985e-05, 'epoch': 0.54}
 3526/6500 [23:16:31<16:42:41, 20.23s/it]                                                          54%|█████▍    | 3526/6500 [23:16:31<16:42:41, 20.23s/it] 54%|█████▍    | 3527/6500 [23:16:49<16:11:27, 19.61s/it]                                                          54%|█████▍    | 3527/6500 [23:16:49<16:11:27, 19.61s/it] 54%|█████▍    | 3528/6500 [23:17:07<15:49:57, 19.18s/it]                                                          54%|█████▍    | 3528/6500 [23:17:07<15:49:57, 19.18s/it] 54%|█████▍    | 3529/6500 [23:17:25<15:34:53, 18.88s/it]                                                          54%|█████▍    | 3529/6500 [23:17:25<15:34:53, 18.88s/it] 54%|█████▍    | 3530/6500 [23:17:44<15:24:38, 18.68s/it]                                                          54%|█████▍    | 3530/6500 [23:17:44<15:24:38, 18.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8613465428352356, 'eval_runtime': 5.3518, 'eval_samples_per_second': 4.298, 'eval_steps_per_second': 1.121, 'epoch': 0.54}
                                                          54%|█████▍    | 3530/6500 [23:17:49<15:24:38, 18.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3530
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3530
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3530
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3530

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3530
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3530/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2547, 'learning_rate': 4.3252064161564115e-05, 'epoch': 0.54}
{'loss': 0.2484, 'learning_rate': 4.322811257409622e-05, 'epoch': 0.54}
{'loss': 0.2705, 'learning_rate': 4.320416256951584e-05, 'epoch': 0.54}
{'loss': 0.3254, 'learning_rate': 4.3180214153421135e-05, 'epoch': 0.54}
{'loss': 0.2685, 'learning_rate': 4.315626733140992e-05, 'epoch': 0.54}
 54%|█████▍    | 3531/6500 [23:18:41<24:56:06, 30.23s/it]                                                          54%|█████▍    | 3531/6500 [23:18:41<24:56:06, 30.23s/it] 54%|█████▍    | 3532/6500 [23:18:59<21:55:02, 26.58s/it]                                                          54%|█████▍    | 3532/6500 [23:18:59<21:55:02, 26.58s/it] 54%|█████▍    | 3533/6500 [23:19:17<19:48:02, 24.02s/it]                                                          54%|█████▍    | 3533/6500 [23:19:17<19:48:02, 24.02s/it] 54%|█████▍    | 3534/6500 [23:19:35<18:19:07, 22.23s/it]                                                          54%|█████▍    | 3534/6500 [23:19:35<18:19:07, 22.23s/it] 54%|█████▍    | 3535/6500 [23:19:53<17:20:04, 21.05s/it]                                                          54%|█████▍    | 3535/6500 [23:19:53<17:20:04, 21.05s/it] 54%|█████▍    |{'loss': 0.2532, 'learning_rate': 4.3132322109079596e-05, 'epoch': 0.54}
{'loss': 0.2796, 'learning_rate': 4.3108378492027224e-05, 'epoch': 0.54}
{'loss': 0.7679, 'learning_rate': 4.3084436485849475e-05, 'epoch': 0.54}
{'loss': 0.2744, 'learning_rate': 4.306049609614265e-05, 'epoch': 0.54}
{'loss': 0.2486, 'learning_rate': 4.303655732850267e-05, 'epoch': 0.54}
 3536/6500 [23:20:11<16:37:38, 20.20s/it]                                                          54%|█████▍    | 3536/6500 [23:20:11<16:37:38, 20.20s/it] 54%|█████▍    | 3537/6500 [23:20:30<16:06:34, 19.57s/it]                                                          54%|█████▍    | 3537/6500 [23:20:30<16:06:34, 19.57s/it] 54%|█████▍    | 3538/6500 [23:20:48<15:44:41, 19.14s/it]                                                          54%|█████▍    | 3538/6500 [23:20:48<15:44:41, 19.14s/it] 54%|█████▍    | 3539/6500 [23:21:06<15:29:52, 18.84s/it]                                                          54%|█████▍    | 3539/6500 [23:21:06<15:29:52, 18.84s/it] 54%|█████▍    | 3540/6500 [23:21:25<15:37:56, 19.01s/it]                                                          54%|█████▍    | 3540/6500 [23:21:25<15:37:56, 19.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8620398044586182, 'eval_runtime': 6.0364, 'eval_samples_per_second': 3.81, 'eval_steps_per_second': 0.994, 'epoch': 0.54}
                                                          54%|█████▍    | 3540/6500 [23:21:31<15:37:56, 19.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3540
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3540
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3540

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3540
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3540/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2616, 'learning_rate': 4.301262018852509e-05, 'epoch': 0.54}
{'loss': 0.2426, 'learning_rate': 4.2988684681805036e-05, 'epoch': 0.54}
{'loss': 0.2725, 'learning_rate': 4.296475081393731e-05, 'epoch': 0.55}
{'loss': 0.2437, 'learning_rate': 4.294081859051632e-05, 'epoch': 0.55}
{'loss': 0.2453, 'learning_rate': 4.2916888017136055e-05, 'epoch': 0.55}
 54%|█████▍    | 3541/6500 [23:22:38<28:58:13, 35.25s/it]                                                          54%|█████▍    | 3541/6500 [23:22:38<28:58:13, 35.25s/it] 54%|█████▍    | 3542/6500 [23:22:56<24:42:41, 30.07s/it]                                                          54%|█████▍    | 3542/6500 [23:22:56<24:42:41, 30.07s/it] 55%|█████▍    | 3543/6500 [23:23:14<21:43:55, 26.46s/it]                                                          55%|█████▍    | 3543/6500 [23:23:14<21:43:55, 26.46s/it] 55%|█████▍    | 3544/6500 [23:23:32<19:38:55, 23.93s/it]                                                          55%|█████▍    | 3544/6500 [23:23:32<19:38:55, 23.93s/it] 55%|█████▍    | 3545/6500 [23:23:50<18:12:00, 22.17s/it]                                                          55%|█████▍    | 3545/6500 [23:23:50<18:12:00, 22.17s/it] 55%|█████▍    |{'loss': 0.2561, 'learning_rate': 4.289295909939016e-05, 'epoch': 0.55}
{'loss': 0.2558, 'learning_rate': 4.286903184287185e-05, 'epoch': 0.55}
{'loss': 0.263, 'learning_rate': 4.2845106253174e-05, 'epoch': 0.55}
{'loss': 0.2531, 'learning_rate': 4.282118233588905e-05, 'epoch': 0.55}
{'loss': 0.2735, 'learning_rate': 4.279726009660909e-05, 'epoch': 0.55}
 3546/6500 [23:24:09<17:11:24, 20.95s/it]                                                          55%|█████▍    | 3546/6500 [23:24:09<17:11:24, 20.95s/it] 55%|█████▍    | 3547/6500 [23:24:27<16:29:21, 20.10s/it]                                                          55%|█████▍    | 3547/6500 [23:24:27<16:29:21, 20.10s/it] 55%|█████▍    | 3548/6500 [23:24:45<16:00:19, 19.52s/it]                                                          55%|█████▍    | 3548/6500 [23:24:45<16:00:19, 19.52s/it] 55%|█████▍    | 3549/6500 [23:25:03<15:40:03, 19.11s/it]                                                          55%|█████▍    | 3549/6500 [23:25:03<15:40:03, 19.11s/it] 55%|█████▍    | 3550/6500 [23:25:21<15:26:08, 18.84s/it]                                                          55%|█████▍    | 3550/6500 [23:25:21<15:26:08, 18.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8791141510009766, 'eval_runtime': 5.3314, 'eval_samples_per_second': 4.314, 'eval_steps_per_second': 1.125, 'epoch': 0.55}
                                                          55%|█████▍    | 3550/6500 [23:25:27<15:26:08, 18.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3550
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3550
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3550
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3550

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3550
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3550/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3550/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2583, 'learning_rate': 4.277333954092579e-05, 'epoch': 0.55}
{'loss': 0.2642, 'learning_rate': 4.274942067443044e-05, 'epoch': 0.55}
{'loss': 0.2544, 'learning_rate': 4.272550350271391e-05, 'epoch': 0.55}
{'loss': 0.2557, 'learning_rate': 4.2701588031366706e-05, 'epoch': 0.55}
{'loss': 0.262, 'learning_rate': 4.267767426597893e-05, 'epoch': 0.55}
 55%|█████▍    | 3551/6500 [23:26:32<28:09:43, 34.38s/it]                                                          55%|█████▍    | 3551/6500 [23:26:32<28:09:43, 34.38s/it] 55%|█████▍    | 3552/6500 [23:26:50<24:08:11, 29.47s/it]                                                          55%|█████▍    | 3552/6500 [23:26:50<24:08:11, 29.47s/it] 55%|█████▍    | 3553/6500 [23:27:08<21:18:51, 26.04s/it]                                                          55%|█████▍    | 3553/6500 [23:27:08<21:18:51, 26.04s/it] 55%|█████▍    | 3554/6500 [23:27:26<19:20:31, 23.64s/it]                                                          55%|█████▍    | 3554/6500 [23:27:26<19:20:31, 23.64s/it] 55%|█████▍    | 3555/6500 [23:27:44<17:57:48, 21.96s/it]                                                          55%|█████▍    | 3555/6500 [23:27:44<17:57:48, 21.96s/it] 55%|█████▍    |{'loss': 0.2567, 'learning_rate': 4.2653762212140266e-05, 'epoch': 0.55}
{'loss': 0.2622, 'learning_rate': 4.262985187544003e-05, 'epoch': 0.55}
{'loss': 0.265, 'learning_rate': 4.2605943261467106e-05, 'epoch': 0.55}
{'loss': 0.283, 'learning_rate': 4.2582036375809984e-05, 'epoch': 0.55}
{'loss': 0.2429, 'learning_rate': 4.2558131224056755e-05, 'epoch': 0.55}
 3556/6500 [23:28:02<17:03:40, 20.86s/it]                                                          55%|█████▍    | 3556/6500 [23:28:02<17:03:40, 20.86s/it] 55%|█████▍    | 3557/6500 [23:28:20<16:22:10, 20.02s/it]                                                          55%|█████▍    | 3557/6500 [23:28:20<16:22:10, 20.02s/it] 55%|█████▍    | 3558/6500 [23:28:38<15:53:21, 19.44s/it]                                                          55%|█████▍    | 3558/6500 [23:28:38<15:53:21, 19.44s/it] 55%|█████▍    | 3559/6500 [23:28:57<15:33:33, 19.05s/it]                                                          55%|█████▍    | 3559/6500 [23:28:57<15:33:33, 19.05s/it] 55%|█████▍    | 3560/6500 [23:29:15<15:19:43, 18.77s/it]                                                          55%|█████▍    | 3560/6500 [23:29:15<15:19:43, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8625306487083435, 'eval_runtime': 5.4731, 'eval_samples_per_second': 4.202, 'eval_steps_per_second': 1.096, 'epoch': 0.55}
                                                          55%|█████▍    | 3560/6500 [23:29:20<15:19:43, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3560
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3560/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3560


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3560

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3560
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3560/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2566, 'learning_rate': 4.253422781179511e-05, 'epoch': 0.55}
{'loss': 0.2394, 'learning_rate': 4.251032614461232e-05, 'epoch': 0.55}
{'loss': 0.2822, 'learning_rate': 4.248642622809526e-05, 'epoch': 0.55}
{'loss': 0.3116, 'learning_rate': 4.246252806783038e-05, 'epoch': 0.55}
{'loss': 0.2508, 'learning_rate': 4.243863166940374e-05, 'epoch': 0.55}
 55%|█████▍    | 3561/6500 [23:30:51<34:13:28, 41.92s/it]                                                          55%|█████▍    | 3561/6500 [23:30:51<34:13:28, 41.92s/it] 55%|█████▍    | 3562/6500 [23:31:09<28:21:14, 34.74s/it]                                                          55%|█████▍    | 3562/6500 [23:31:09<28:21:14, 34.74s/it] 55%|█████▍    | 3563/6500 [23:31:27<24:14:42, 29.72s/it]                                                          55%|█████▍    | 3563/6500 [23:31:27<24:14:42, 29.72s/it] 55%|█████▍    | 3564/6500 [23:31:45<21:25:48, 26.28s/it]                                                          55%|█████▍    | 3564/6500 [23:31:45<21:25:48, 26.28s/it] 55%|█████▍    | 3565/6500 [23:32:03<19:25:01, 23.82s/it]                                                          55%|█████▍    | 3565/6500 [23:32:03<19:25:01, 23.82s/it] 55%|█████▍    |{'loss': 0.2682, 'learning_rate': 4.2414737038400964e-05, 'epoch': 0.55}
{'loss': 0.2491, 'learning_rate': 4.2390844180407285e-05, 'epoch': 0.55}
{'loss': 0.7637, 'learning_rate': 4.236695310100752e-05, 'epoch': 0.55}
{'loss': 0.2653, 'learning_rate': 4.234306380578607e-05, 'epoch': 0.55}
{'loss': 0.2632, 'learning_rate': 4.231917630032689e-05, 'epoch': 0.55}
 3566/6500 [23:32:22<18:12:28, 22.34s/it]                                                          55%|█████▍    | 3566/6500 [23:32:22<18:12:28, 22.34s/it] 55%|█████▍    | 3567/6500 [23:32:40<17:10:18, 21.08s/it]                                                          55%|█████▍    | 3567/6500 [23:32:40<17:10:18, 21.08s/it] 55%|█████▍    | 3568/6500 [23:32:58<16:26:21, 20.18s/it]                                                          55%|█████▍    | 3568/6500 [23:32:58<16:26:21, 20.18s/it] 55%|█████▍    | 3569/6500 [23:33:17<16:04:14, 19.74s/it]                                                          55%|█████▍    | 3569/6500 [23:33:17<16:04:14, 19.74s/it] 55%|█████▍    | 3570/6500 [23:33:35<15:40:48, 19.27s/it]                                                          55%|█████▍    | 3570/6500 [23:33:35<15:40:48, 19.27s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8636406660079956, 'eval_runtime': 5.4398, 'eval_samples_per_second': 4.228, 'eval_steps_per_second': 1.103, 'epoch': 0.55}
                                                          55%|█████▍    | 3570/6500 [23:33:40<15:40:48, 19.27s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3570
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3570
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3570 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3570

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3570
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3570/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3570/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2437, 'learning_rate': 4.229529059021354e-05, 'epoch': 0.55}
{'loss': 0.2667, 'learning_rate': 4.227140668102918e-05, 'epoch': 0.55}
{'loss': 0.2543, 'learning_rate': 4.224752457835652e-05, 'epoch': 0.55}
{'loss': 0.2411, 'learning_rate': 4.222364428777786e-05, 'epoch': 0.55}
{'loss': 0.2554, 'learning_rate': 4.219976581487505e-05, 'epoch': 0.55}
 55%|█████▍    | 3571/6500 [23:34:32<24:56:05, 30.65s/it]                                                          55%|█████▍    | 3571/6500 [23:34:32<24:56:05, 30.65s/it] 55%|█████▍    | 3572/6500 [23:34:51<21:58:11, 27.01s/it]                                                          55%|█████▍    | 3572/6500 [23:34:51<21:58:11, 27.01s/it] 55%|█████▍    | 3573/6500 [23:35:09<19:46:42, 24.33s/it]                                                          55%|█████▍    | 3573/6500 [23:35:09<19:46:42, 24.33s/it] 55%|█████▍    | 3574/6500 [23:35:27<18:14:45, 22.45s/it]                                                          55%|█████▍    | 3574/6500 [23:35:27<18:14:45, 22.45s/it] 55%|█████▌    | 3575/6500 [23:35:45<17:12:56, 21.19s/it]                                                          55%|█████▌    | 3575/6500 [23:35:45<17:12:56, 21.19s/it] 55%|█████▌    |{'loss': 0.2509, 'learning_rate': 4.217588916522956e-05, 'epoch': 0.55}
{'loss': 0.253, 'learning_rate': 4.215201434442241e-05, 'epoch': 0.55}
{'loss': 0.262, 'learning_rate': 4.2128141358034186e-05, 'epoch': 0.55}
{'loss': 0.2544, 'learning_rate': 4.210427021164506e-05, 'epoch': 0.55}
{'loss': 0.258, 'learning_rate': 4.2080400910834773e-05, 'epoch': 0.55}
 3576/6500 [23:36:03<16:28:24, 20.28s/it]                                                          55%|█████▌    | 3576/6500 [23:36:03<16:28:24, 20.28s/it] 55%|█████▌    | 3577/6500 [23:36:21<15:57:08, 19.65s/it]                                                          55%|█████▌    | 3577/6500 [23:36:21<15:57:08, 19.65s/it] 55%|█████▌    | 3578/6500 [23:36:40<15:35:12, 19.20s/it]                                                          55%|█████▌    | 3578/6500 [23:36:40<15:35:12, 19.20s/it] 55%|█████▌    | 3579/6500 [23:36:58<15:20:11, 18.90s/it]                                                          55%|█████▌    | 3579/6500 [23:36:58<15:20:11, 18.90s/it] 55%|█████▌    | 3580/6500 [23:37:16<15:09:34, 18.69s/it]                                                          55%|█████▌    | 3580/6500 [23:37:16<15:09:34, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8793239593505859, 'eval_runtime': 5.5621, 'eval_samples_per_second': 4.135, 'eval_steps_per_second': 1.079, 'epoch': 0.55}
                                                          55%|█████▌    | 3580/6500 [23:37:21<15:09:34, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3580
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3580

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3580
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3580

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3580
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3580/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3580/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2534, 'learning_rate': 4.205653346118261e-05, 'epoch': 0.55}
{'loss': 0.2749, 'learning_rate': 4.203266786826745e-05, 'epoch': 0.55}
{'loss': 0.2471, 'learning_rate': 4.2008804137667744e-05, 'epoch': 0.55}
{'loss': 0.2654, 'learning_rate': 4.198494227496148e-05, 'epoch': 0.55}
{'loss': 0.2784, 'learning_rate': 4.1961082285726234e-05, 'epoch': 0.55}
 55%|█████▌    | 3581/6500 [23:38:57<35:17:12, 43.52s/it]                                                          55%|█████▌    | 3581/6500 [23:38:57<35:17:12, 43.52s/it] 55%|█████▌    | 3582/6500 [23:39:16<29:06:50, 35.92s/it]                                                          55%|█████▌    | 3582/6500 [23:39:16<29:06:50, 35.92s/it] 55%|█████▌    | 3583/6500 [23:39:34<24:45:11, 30.55s/it]                                                          55%|█████▌    | 3583/6500 [23:39:34<24:45:11, 30.55s/it] 55%|█████▌    | 3584/6500 [23:39:52<21:42:30, 26.80s/it]                                                          55%|█████▌    | 3584/6500 [23:39:52<21:42:30, 26.80s/it] 55%|█████▌    | 3585/6500 [23:40:10<19:35:16, 24.19s/it]                                                          55%|█████▌    | 3585/6500 [23:40:10<19:35:16, 24.19s/it] 55%|█████▌    |{'loss': 0.2505, 'learning_rate': 4.1937224175539116e-05, 'epoch': 0.55}
{'loss': 0.2677, 'learning_rate': 4.1913367949976826e-05, 'epoch': 0.55}
{'loss': 0.2659, 'learning_rate': 4.188951361461561e-05, 'epoch': 0.55}
{'loss': 0.2632, 'learning_rate': 4.1865661175031276e-05, 'epoch': 0.55}
{'loss': 0.244, 'learning_rate': 4.184181063679918e-05, 'epoch': 0.55}
 3586/6500 [23:40:28<18:06:19, 22.37s/it]                                                          55%|█████▌    | 3586/6500 [23:40:28<18:06:19, 22.37s/it] 55%|█████▌    | 3587/6500 [23:40:46<17:04:27, 21.10s/it]                                                          55%|█████▌    | 3587/6500 [23:40:46<17:04:27, 21.10s/it] 55%|█████▌    | 3588/6500 [23:41:05<16:28:50, 20.37s/it]                                                          55%|█████▌    | 3588/6500 [23:41:05<16:28:50, 20.37s/it] 55%|█████▌    | 3589/6500 [23:41:23<15:56:34, 19.72s/it]                                                          55%|█████▌    | 3589/6500 [23:41:23<15:56:34, 19.72s/it] 55%|█████▌    | 3590/6500 [23:41:41<15:33:39, 19.25s/it]                                                          55%|█████▌    | 3590/6500 [23:41:41<15:33:39, 19.25s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8619295954704285, 'eval_runtime': 6.2339, 'eval_samples_per_second': 3.689, 'eval_steps_per_second': 0.962, 'epoch': 0.55}
                                                          55%|█████▌    | 3590/6500 [23:41:47<15:33:39, 19.25s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3590
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3590
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3590

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3590
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3590/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3590/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2477, 'learning_rate': 4.181796200549426e-05, 'epoch': 0.55}
{'loss': 0.2409, 'learning_rate': 4.179411528669094e-05, 'epoch': 0.55}
{'loss': 0.3348, 'learning_rate': 4.17702704859633e-05, 'epoch': 0.55}
{'loss': 0.2504, 'learning_rate': 4.174642760888491e-05, 'epoch': 0.55}
{'loss': 0.2452, 'learning_rate': 4.172258666102887e-05, 'epoch': 0.55}
 55%|█████▌    | 3591/6500 [23:44:15<48:12:26, 59.66s/it]                                                          55%|█████▌    | 3591/6500 [23:44:15<48:12:26, 59.66s/it] 55%|█████▌    | 3592/6500 [23:44:33<38:05:28, 47.16s/it]                                                          55%|█████▌    | 3592/6500 [23:44:33<38:05:28, 47.16s/it] 55%|█████▌    | 3593/6500 [23:44:51<31:00:26, 38.40s/it]                                                          55%|█████▌    | 3593/6500 [23:44:51<31:00:26, 38.40s/it] 55%|█████▌    | 3594/6500 [23:45:09<26:03:06, 32.27s/it]                                                          55%|█████▌    | 3594/6500 [23:45:09<26:03:06, 32.27s/it] 55%|█████▌    | 3595/6500 [23:45:27<22:35:32, 28.00s/it]                                                          55%|█████▌    | 3595/6500 [23:45:27<22:35:32, 28.00s/it] 55%|█████▌    |{'loss': 0.2721, 'learning_rate': 4.169874764796787e-05, 'epoch': 0.55}
{'loss': 0.7608, 'learning_rate': 4.167491057527413e-05, 'epoch': 0.55}
{'loss': 0.2765, 'learning_rate': 4.165107544851944e-05, 'epoch': 0.55}
{'loss': 0.258, 'learning_rate': 4.162724227327509e-05, 'epoch': 0.55}
{'loss': 0.2623, 'learning_rate': 4.160341105511196e-05, 'epoch': 0.55}
 3596/6500 [23:45:45<20:10:37, 25.01s/it]                                                          55%|█████▌    | 3596/6500 [23:45:45<20:10:37, 25.01s/it] 55%|█████▌    | 3597/6500 [23:46:03<18:29:17, 22.93s/it]                                                          55%|█████▌    | 3597/6500 [23:46:03<18:29:17, 22.93s/it] 55%|█████▌    | 3598/6500 [23:46:21<17:18:52, 21.48s/it]                                                          55%|█████▌    | 3598/6500 [23:46:21<17:18:52, 21.48s/it] 55%|█████▌    | 3599/6500 [23:46:39<16:29:58, 20.48s/it]                                                          55%|█████▌    | 3599/6500 [23:46:39<16:29:58, 20.48s/it] 55%|█████▌    | 3600/6500 [23:46:57<15:55:53, 19.78s/it]                                                          55%|█████▌    | 3600/6500 [23:46:57<15:55:53, 19.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8717530965805054, 'eval_runtime': 5.3609, 'eval_samples_per_second': 4.29, 'eval_steps_per_second': 1.119, 'epoch': 0.55}
                                                          55%|█████▌    | 3600/6500 [23:47:03<15:55:53, 19.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3600
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3600/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3600
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3600/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3600/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2493, 'learning_rate': 4.157958179960044e-05, 'epoch': 0.55}
{'loss': 0.269, 'learning_rate': 4.155575451231048e-05, 'epoch': 0.55}
{'loss': 0.2458, 'learning_rate': 4.1531929198811556e-05, 'epoch': 0.55}
{'loss': 0.236, 'learning_rate': 4.15081058646727e-05, 'epoch': 0.55}
{'loss': 0.2689, 'learning_rate': 4.148428451546247e-05, 'epoch': 0.55}
 55%|█████▌    | 3601/6500 [23:47:40<21:27:13, 26.64s/it]                                                          55%|█████▌    | 3601/6500 [23:47:40<21:27:13, 26.64s/it] 55%|█████▌    | 3602/6500 [23:47:58<19:22:54, 24.08s/it]                                                          55%|█████▌    | 3602/6500 [23:47:58<19:22:54, 24.08s/it] 55%|█████▌    | 3603/6500 [23:48:16<17:55:48, 22.28s/it]                                                          55%|█████▌    | 3603/6500 [23:48:16<17:55:48, 22.28s/it] 55%|█████▌    | 3604/6500 [23:48:35<17:02:01, 21.17s/it]                                                          55%|█████▌    | 3604/6500 [23:48:35<17:02:01, 21.17s/it] 55%|█████▌    | 3605/6500 [23:48:53<16:17:25, 20.26s/it]                                                          55%|█████▌    | 3605/6500 [23:48:53<16:17:25, 20.26s/it] 55%|█████▌    |{'loss': 0.2569, 'learning_rate': 4.1460465156748954e-05, 'epoch': 0.55}
{'loss': 0.2715, 'learning_rate': 4.143664779409978e-05, 'epoch': 0.55}
{'loss': 0.2644, 'learning_rate': 4.1412832433082124e-05, 'epoch': 0.56}
{'loss': 0.271, 'learning_rate': 4.138901907926267e-05, 'epoch': 0.56}
{'loss': 0.2525, 'learning_rate': 4.136520773820765e-05, 'epoch': 0.56}
 3606/6500 [23:49:11<15:46:20, 19.62s/it]                                                          55%|█████▌    | 3606/6500 [23:49:11<15:46:20, 19.62s/it] 55%|█████▌    | 3607/6500 [23:49:29<15:25:07, 19.19s/it]                                                          55%|█████▌    | 3607/6500 [23:49:29<15:25:07, 19.19s/it] 56%|█████▌    | 3608/6500 [23:49:47<15:10:17, 18.89s/it]                                                          56%|█████▌    | 3608/6500 [23:49:47<15:10:17, 18.89s/it] 56%|█████▌    | 3609/6500 [23:50:06<15:00:03, 18.68s/it]                                                          56%|█████▌    | 3609/6500 [23:50:06<15:00:03, 18.68s/it] 56%|█████▌    | 3610/6500 [23:50:24<14:52:41, 18.53s/it]                                                          56%|█████▌    | 3610/6500 [23:50:24<14:52:41, 18.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718429207801819, 'eval_runtime': 5.3518, 'eval_samples_per_second': 4.298, 'eval_steps_per_second': 1.121, 'epoch': 0.56}
                                                          56%|█████▌    | 3610/6500 [23:50:29<14:52:41, 18.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3610
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3610

the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3610
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3610

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3610
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3610/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3610/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2799, 'learning_rate': 4.134139841548283e-05, 'epoch': 0.56}
{'loss': 0.2657, 'learning_rate': 4.131759111665349e-05, 'epoch': 0.56}
{'loss': 0.2594, 'learning_rate': 4.129378584728442e-05, 'epoch': 0.56}
{'loss': 0.2708, 'learning_rate': 4.1269982612939983e-05, 'epoch': 0.56}
{'loss': 0.266, 'learning_rate': 4.124618141918403e-05, 'epoch': 0.56}
 56%|█████▌    | 3611/6500 [23:52:13<36:42:58, 45.75s/it]                                                          56%|█████▌    | 3611/6500 [23:52:13<36:42:58, 45.75s/it] 56%|█████▌    | 3612/6500 [23:52:31<30:01:31, 37.43s/it]                                                          56%|█████▌    | 3612/6500 [23:52:31<30:01:31, 37.43s/it] 56%|█████▌    | 3613/6500 [23:52:49<25:20:20, 31.60s/it]                                                          56%|█████▌    | 3613/6500 [23:52:49<25:20:20, 31.60s/it] 56%|█████▌    | 3614/6500 [23:53:07<22:04:23, 27.53s/it]                                                          56%|█████▌    | 3614/6500 [23:53:07<22:04:23, 27.53s/it] 56%|█████▌    | 3615/6500 [23:53:25<19:47:34, 24.70s/it]                                                          56%|█████▌    | 3615/6500 [23:53:25<19:47:34, 24.70s/it] 56%|█████▌    |{'loss': 0.2667, 'learning_rate': 4.122238227157994e-05, 'epoch': 0.56}
{'loss': 0.2518, 'learning_rate': 4.119858517569064e-05, 'epoch': 0.56}
{'loss': 0.2812, 'learning_rate': 4.117479013707854e-05, 'epoch': 0.56}
{'loss': 0.2528, 'learning_rate': 4.115099716130557e-05, 'epoch': 0.56}
{'loss': 0.2533, 'learning_rate': 4.112720625393322e-05, 'epoch': 0.56}
 3616/6500 [23:53:43<18:11:52, 22.72s/it]                                                          56%|█████▌    | 3616/6500 [23:53:43<18:11:52, 22.72s/it] 56%|█████▌    | 3617/6500 [23:54:01<17:05:07, 21.33s/it]                                                          56%|█████▌    | 3617/6500 [23:54:01<17:05:07, 21.33s/it] 56%|█████▌    | 3618/6500 [23:54:20<16:18:38, 20.37s/it]                                                          56%|█████▌    | 3618/6500 [23:54:20<16:18:38, 20.37s/it] 56%|█████▌    | 3619/6500 [23:54:38<15:46:28, 19.71s/it]                                                          56%|█████▌    | 3619/6500 [23:54:38<15:46:28, 19.71s/it] 56%|█████▌    | 3620/6500 [23:54:56<15:27:16, 19.32s/it]                                                          56%|█████▌    | 3620/6500 [23:54:56<15:27:16, 19.32s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.85732501745224, 'eval_runtime': 5.3549, 'eval_samples_per_second': 4.295, 'eval_steps_per_second': 1.12, 'epoch': 0.56}
                                                          56%|█████▌    | 3620/6500 [23:55:01<15:27:16, 19.32s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3620
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3620
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3620I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3620

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3620
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3620/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3620/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2478, 'learning_rate': 4.110341742052245e-05, 'epoch': 0.56}
{'loss': 0.2653, 'learning_rate': 4.1079630666633796e-05, 'epoch': 0.56}
{'loss': 0.3156, 'learning_rate': 4.105584599782721e-05, 'epoch': 0.56}
{'loss': 0.2471, 'learning_rate': 4.1032063419662245e-05, 'epoch': 0.56}
{'loss': 0.2488, 'learning_rate': 4.100828293769794e-05, 'epoch': 0.56}
 56%|█████▌    | 3621/6500 [23:56:08<28:07:02, 35.16s/it]                                                          56%|█████▌    | 3621/6500 [23:56:08<28:07:02, 35.16s/it] 56%|█████▌    | 3622/6500 [23:56:28<24:24:46, 30.54s/it]                                                          56%|█████▌    | 3622/6500 [23:56:28<24:24:46, 30.54s/it] 56%|█████▌    | 3623/6500 [23:56:46<21:24:30, 26.79s/it]                                                          56%|█████▌    | 3623/6500 [23:56:46<21:24:30, 26.79s/it] 56%|█████▌    | 3624/6500 [23:57:04<19:18:25, 24.17s/it]                                                          56%|█████▌    | 3624/6500 [23:57:04<19:18:25, 24.17s/it] 56%|█████▌    | 3625/6500 [23:57:22<17:50:37, 22.34s/it]                                                          56%|█████▌    | 3625/6500 [23:57:22<17:50:37, 22.34s/it] 56%|█████▌    |{'loss': 0.2645, 'learning_rate': 4.098450455749281e-05, 'epoch': 0.56}
{'loss': 0.7614, 'learning_rate': 4.096072828460494e-05, 'epoch': 0.56}
{'loss': 0.2681, 'learning_rate': 4.093695412459188e-05, 'epoch': 0.56}
{'loss': 0.2557, 'learning_rate': 4.0913182083010676e-05, 'epoch': 0.56}
{'loss': 0.2503, 'learning_rate': 4.088941216541791e-05, 'epoch': 0.56}
 3626/6500 [23:57:40<16:49:15, 21.07s/it]                                                          56%|█████▌    | 3626/6500 [23:57:40<16:49:15, 21.07s/it] 56%|█████▌    | 3627/6500 [23:57:58<16:06:21, 20.18s/it]                                                          56%|█████▌    | 3627/6500 [23:57:58<16:06:21, 20.18s/it] 56%|█████▌    | 3628/6500 [23:58:17<15:36:50, 19.57s/it]                                                          56%|█████▌    | 3628/6500 [23:58:17<15:36:50, 19.57s/it] 56%|█████▌    | 3629/6500 [23:58:35<15:16:14, 19.15s/it]                                                          56%|█████▌    | 3629/6500 [23:58:35<15:16:14, 19.15s/it] 56%|█████▌    | 3630/6500 [23:58:53<15:01:40, 18.85s/it]                                                          56%|█████▌    | 3630/6500 [23:58:53<15:01:40, 18.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8617281913757324, 'eval_runtime': 5.5116, 'eval_samples_per_second': 4.173, 'eval_steps_per_second': 1.089, 'epoch': 0.56}
                                                          56%|█████▌    | 3630/6500 [23:58:58<15:01:40, 18.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3630
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3630
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3630
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3630/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2408, 'learning_rate': 4.086564437736966e-05, 'epoch': 0.56}
{'loss': 0.2619, 'learning_rate': 4.08418787244215e-05, 'epoch': 0.56}
{'loss': 0.2423, 'learning_rate': 4.081811521212851e-05, 'epoch': 0.56}
{'loss': 0.247, 'learning_rate': 4.079435384604526e-05, 'epoch': 0.56}
{'loss': 0.249, 'learning_rate': 4.077059463172582e-05, 'epoch': 0.56}
 56%|█████▌    | 3631/6500 [24:00:22<31:47:05, 39.88s/it]                                                          56%|█████▌    | 3631/6500 [24:00:22<31:47:05, 39.88s/it] 56%|█████▌    | 3632/6500 [24:00:40<26:32:49, 33.32s/it]                                                          56%|█████▌    | 3632/6500 [24:00:40<26:32:49, 33.32s/it] 56%|█████▌    | 3633/6500 [24:00:58<22:53:18, 28.74s/it]                                                          56%|█████▌    | 3633/6500 [24:00:58<22:53:18, 28.74s/it] 56%|█████▌    | 3634/6500 [24:01:16<20:20:05, 25.54s/it]                                                          56%|█████▌    | 3634/6500 [24:01:16<20:20:05, 25.54s/it] 56%|█████▌    | 3635/6500 [24:01:34<18:32:54, 23.31s/it]                                                          56%|█████▌    | 3635/6500 [24:01:34<18:32:54, 23.31s/it] 56%|█████▌    |{'loss': 0.2491, 'learning_rate': 4.0746837574723776e-05, 'epoch': 0.56}
{'loss': 0.2575, 'learning_rate': 4.072308268059219e-05, 'epoch': 0.56}
{'loss': 0.254, 'learning_rate': 4.069932995488361e-05, 'epoch': 0.56}
{'loss': 0.2717, 'learning_rate': 4.0675579403150125e-05, 'epoch': 0.56}
{'loss': 0.262, 'learning_rate': 4.0651831030943246e-05, 'epoch': 0.56}
 3636/6500 [24:01:53<17:26:17, 21.92s/it]                                                          56%|█████▌    | 3636/6500 [24:01:53<17:26:17, 21.92s/it] 56%|█████▌    | 3637/6500 [24:02:11<16:40:18, 20.96s/it]                                                          56%|█████▌    | 3637/6500 [24:02:11<16:40:18, 20.96s/it] 56%|█████▌    | 3638/6500 [24:02:30<15:59:52, 20.12s/it]                                                          56%|█████▌    | 3638/6500 [24:02:30<15:59:52, 20.12s/it] 56%|█████▌    | 3639/6500 [24:02:48<15:32:37, 19.56s/it]                                                          56%|█████▌    | 3639/6500 [24:02:48<15:32:37, 19.56s/it] 56%|█████▌    | 3640/6500 [24:03:06<15:12:07, 19.14s/it]                                                          56%|█████▌    | 3640/6500 [24:03:06<15:12:07, 19.14s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8805959224700928, 'eval_runtime': 5.3681, 'eval_samples_per_second': 4.285, 'eval_steps_per_second': 1.118, 'epoch': 0.56}
                                                          56%|█████▌    | 3640/6500 [24:03:11<15:12:07, 19.14s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3640
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3640
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3640
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3640

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3640
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.
  warnings.warn("Repo card metadata block was not found. Setting CardData to empty.")
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3640/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3640/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3640/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2765, 'learning_rate': 4.062808484381403e-05, 'epoch': 0.56}
{'loss': 0.2649, 'learning_rate': 4.0604340847313e-05, 'epoch': 0.56}
{'loss': 0.2653, 'learning_rate': 4.058059904699017e-05, 'epoch': 0.56}
{'loss': 0.266, 'learning_rate': 4.0556859448395037e-05, 'epoch': 0.56}
{'loss': 0.2631, 'learning_rate': 4.0533122057076604e-05, 'epoch': 0.56}
 56%|█████▌    | 3641/6500 [24:04:37<32:22:41, 40.77s/it]                                                          56%|█████▌    | 3641/6500 [24:04:37<32:22:41, 40.77s/it] 56%|█████▌    | 3642/6500 [24:04:55<26:56:14, 33.93s/it]                                                          56%|█████▌    | 3642/6500 [24:04:55<26:56:14, 33.93s/it] 56%|█████▌    | 3643/6500 [24:05:13<23:08:17, 29.16s/it]                                                          56%|█████▌    | 3643/6500 [24:05:13<23:08:17, 29.16s/it] 56%|█████▌    | 3644/6500 [24:05:31<20:28:41, 25.81s/it]                                                          56%|█████▌    | 3644/6500 [24:05:31<20:28:41, 25.81s/it] 56%|█████▌    | 3645/6500 [24:05:49<18:37:26, 23.48s/it]                                                          56%|█████▌    | 3645/6500 [24:05:49<18:37:26, 23.48s/it] 56%|█████▌    |{'loss': 0.2655, 'learning_rate': 4.050938687858333e-05, 'epoch': 0.56}
{'loss': 0.2664, 'learning_rate': 4.048565391846316e-05, 'epoch': 0.56}
{'loss': 0.2752, 'learning_rate': 4.046192318226354e-05, 'epoch': 0.56}
{'loss': 0.2399, 'learning_rate': 4.043819467553138e-05, 'epoch': 0.56}
{'loss': 0.2537, 'learning_rate': 4.0414468403813095e-05, 'epoch': 0.56}
 3646/6500 [24:06:07<17:19:46, 21.86s/it]                                                          56%|█████▌    | 3646/6500 [24:06:07<17:19:46, 21.86s/it] 56%|█████▌    | 3647/6500 [24:06:25<16:26:04, 20.74s/it]                                                          56%|█████▌    | 3647/6500 [24:06:25<16:26:04, 20.74s/it] 56%|█████▌    | 3648/6500 [24:06:44<15:48:41, 19.96s/it]                                                          56%|█████▌    | 3648/6500 [24:06:44<15:48:41, 19.96s/it] 56%|█████▌    | 3649/6500 [24:07:02<15:22:40, 19.42s/it]                                                          56%|█████▌    | 3649/6500 [24:07:02<15:22:40, 19.42s/it] 56%|█████▌    | 3650/6500 [24:07:20<15:04:24, 19.04s/it]                                                          56%|█████▌    | 3650/6500 [24:07:20<15:04:24, 19.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8602684736251831, 'eval_runtime': 5.3467, 'eval_samples_per_second': 4.302, 'eval_steps_per_second': 1.122, 'epoch': 0.56}
                                                          56%|█████▌    | 3650/6500 [24:07:25<15:04:24, 19.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3650
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3650
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3650
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3650
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3650/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3650/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2484, 'learning_rate': 4.039074437265452e-05, 'epoch': 0.56}
{'loss': 0.285, 'learning_rate': 4.0367022587601035e-05, 'epoch': 0.56}
{'loss': 0.3063, 'learning_rate': 4.034330305419744e-05, 'epoch': 0.56}
{'loss': 0.2462, 'learning_rate': 4.031958577798805e-05, 'epoch': 0.56}
{'loss': 0.2669, 'learning_rate': 4.029587076451662e-05, 'epoch': 0.56}
 56%|█████▌    | 3651/6500 [24:08:37<28:53:50, 36.51s/it]                                                          56%|█████▌    | 3651/6500 [24:08:37<28:53:50, 36.51s/it] 56%|█████▌    | 3652/6500 [24:08:55<24:33:04, 31.03s/it]                                                          56%|█████▌    | 3652/6500 [24:08:56<24:33:04, 31.03s/it] 56%|█████▌    | 3653/6500 [24:09:14<21:32:17, 27.23s/it]                                                          56%|█████▌    | 3653/6500 [24:09:14<21:32:17, 27.23s/it] 56%|█████▌    | 3654/6500 [24:09:32<19:21:09, 24.48s/it]                                                          56%|█████▌    | 3654/6500 [24:09:32<19:21:09, 24.48s/it] 56%|█████▌    | 3655/6500 [24:09:50<17:49:28, 22.55s/it]                                                          56%|█████▌    | 3655/6500 [24:09:50<17:49:28, 22.55s/it] 56%|█████▌    |{'loss': 0.2553, 'learning_rate': 4.0272158019326414e-05, 'epoch': 0.56}
{'loss': 0.7697, 'learning_rate': 4.024844754796011e-05, 'epoch': 0.56}
{'loss': 0.2522, 'learning_rate': 4.0224739355959905e-05, 'epoch': 0.56}
{'loss': 0.2665, 'learning_rate': 4.020103344886744e-05, 'epoch': 0.56}
{'loss': 0.2413, 'learning_rate': 4.017732983222382e-05, 'epoch': 0.56}
 3656/6500 [24:10:08<16:49:44, 21.30s/it]                                                          56%|█████▌    | 3656/6500 [24:10:08<16:49:44, 21.30s/it] 56%|█████▋    | 3657/6500 [24:10:26<16:03:50, 20.34s/it]                                                          56%|█████▋    | 3657/6500 [24:10:26<16:03:50, 20.34s/it] 56%|█████▋    | 3658/6500 [24:10:45<15:31:49, 19.67s/it]                                                          56%|█████▋    | 3658/6500 [24:10:45<15:31:49, 19.67s/it] 56%|█████▋    | 3659/6500 [24:11:03<15:09:47, 19.21s/it]                                                          56%|█████▋    | 3659/6500 [24:11:03<15:09:47, 19.21s/it] 56%|█████▋    | 3660/6500 [24:11:21<14:54:15, 18.89s/it]                                                          56%|█████▋    | 3660/6500 [24:11:21<14:54:15, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8717837333679199, 'eval_runtime': 5.3761, 'eval_samples_per_second': 4.278, 'eval_steps_per_second': 1.116, 'epoch': 0.56}
                                                          56%|█████▋    | 3660/6500 [24:11:26<14:54:15, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3660
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3660
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3660
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3660

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3660/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3660/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2672, 'learning_rate': 4.0153628511569646e-05, 'epoch': 0.56}
{'loss': 0.2539, 'learning_rate': 4.012992949244493e-05, 'epoch': 0.56}
{'loss': 0.2323, 'learning_rate': 4.010623278038919e-05, 'epoch': 0.56}
{'loss': 0.2556, 'learning_rate': 4.008253838094137e-05, 'epoch': 0.56}
{'loss': 0.2507, 'learning_rate': 4.005884629963991e-05, 'epoch': 0.56}
 56%|█████▋    | 3661/6500 [24:12:22<24:52:34, 31.54s/it]                                                          56%|█████▋    | 3661/6500 [24:12:22<24:52:34, 31.54s/it] 56%|█████▋    | 3662/6500 [24:12:40<21:40:20, 27.49s/it]                                                          56%|█████▋    | 3662/6500 [24:12:40<21:40:20, 27.49s/it] 56%|█████▋    | 3663/6500 [24:12:58<19:25:49, 24.66s/it]                                                          56%|█████▋    | 3663/6500 [24:12:58<19:25:49, 24.66s/it] 56%|█████▋    | 3664/6500 [24:13:16<17:52:06, 22.68s/it]                                                          56%|█████▋    | 3664/6500 [24:13:16<17:52:06, 22.68s/it] 56%|█████▋    | 3665/6500 [24:13:34<16:46:43, 21.31s/it]                                                          56%|█████▋    | 3665/6500 [24:13:34<16:46:43, 21.31s/it] 56%|█████▋    |{'loss': 0.2591, 'learning_rate': 4.0035156542022684e-05, 'epoch': 0.56}
{'loss': 0.259, 'learning_rate': 4.001146911362702e-05, 'epoch': 0.56}
{'loss': 0.2579, 'learning_rate': 3.998778401998973e-05, 'epoch': 0.56}
{'loss': 0.2454, 'learning_rate': 3.9964101266647044e-05, 'epoch': 0.56}
{'loss': 0.2584, 'learning_rate': 3.9940420859134686e-05, 'epoch': 0.56}
 3666/6500 [24:13:52<16:01:24, 20.35s/it]                                                          56%|█████▋    | 3666/6500 [24:13:52<16:01:24, 20.35s/it] 56%|█████▋    | 3667/6500 [24:14:10<15:29:59, 19.70s/it]                                                          56%|█████▋    | 3667/6500 [24:14:10<15:29:59, 19.70s/it] 56%|█████▋    | 3668/6500 [24:14:29<15:07:45, 19.23s/it]                                                          56%|█████▋    | 3668/6500 [24:14:29<15:07:45, 19.23s/it] 56%|█████▋    | 3669/6500 [24:14:47<15:00:48, 19.09s/it]                                                          56%|█████▋    | 3669/6500 [24:14:47<15:00:48, 19.09s/it] 56%|█████▋    | 3670/6500 [24:15:06<14:47:33, 18.82s/it]                                                          56%|█████▋    | 3670/6500 [24:15:06<14:47:33, 18.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.877697765827179, 'eval_runtime': 5.3374, 'eval_samples_per_second': 4.309, 'eval_steps_per_second': 1.124, 'epoch': 0.56}
                                                          56%|█████▋    | 3670/6500 [24:15:11<14:47:33, 18.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3670
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3670/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3670

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3670

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3670
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3670/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3670/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2626, 'learning_rate': 3.9916742802987774e-05, 'epoch': 0.56}
{'loss': 0.2406, 'learning_rate': 3.9893067103740925e-05, 'epoch': 0.56}
{'loss': 0.2526, 'learning_rate': 3.986939376692819e-05, 'epoch': 0.57}
{'loss': 0.2701, 'learning_rate': 3.9845722798083066e-05, 'epoch': 0.57}
{'loss': 0.2492, 'learning_rate': 3.98220542027385e-05, 'epoch': 0.57}
 56%|█████▋    | 3671/6500 [24:16:49<34:40:06, 44.12s/it]                                                          56%|█████▋    | 3671/6500 [24:16:49<34:40:06, 44.12s/it] 56%|█████▋    | 3672/6500 [24:17:07<28:30:00, 36.28s/it]                                                          56%|█████▋    | 3672/6500 [24:17:07<28:30:00, 36.28s/it] 57%|█████▋    | 3673/6500 [24:17:25<24:11:40, 30.81s/it]                                                          57%|█████▋    | 3673/6500 [24:17:25<24:11:40, 30.81s/it] 57%|█████▋    | 3674/6500 [24:17:43<21:11:10, 26.99s/it]                                                          57%|█████▋    | 3674/6500 [24:17:43<21:11:10, 26.99s/it] 57%|█████▋    | 3675/6500 [24:18:01<19:05:04, 24.32s/it]                                                          57%|█████▋    | 3675/6500 [24:18:01<19:05:04, 24.32s/it] 57%|█████▋    |{'loss': 0.2549, 'learning_rate': 3.97983879864269e-05, 'epoch': 0.57}
{'loss': 0.269, 'learning_rate': 3.977472415468006e-05, 'epoch': 0.57}
{'loss': 0.2503, 'learning_rate': 3.975106271302928e-05, 'epoch': 0.57}
{'loss': 0.2502, 'learning_rate': 3.972740366700528e-05, 'epoch': 0.57}
{'loss': 0.2455, 'learning_rate': 3.970374702213821e-05, 'epoch': 0.57}
 3676/6500 [24:18:19<17:36:56, 22.46s/it]                                                          57%|█████▋    | 3676/6500 [24:18:19<17:36:56, 22.46s/it] 57%|█████▋    | 3677/6500 [24:18:37<16:35:40, 21.16s/it]                                                          57%|█████▋    | 3677/6500 [24:18:37<16:35:40, 21.16s/it] 57%|█████▋    | 3678/6500 [24:18:55<15:52:58, 20.26s/it]                                                          57%|█████▋    | 3678/6500 [24:18:55<15:52:58, 20.26s/it] 57%|█████▋    | 3679/6500 [24:19:13<15:23:10, 19.63s/it]                                                          57%|█████▋    | 3679/6500 [24:19:13<15:23:10, 19.63s/it] 57%|█████▋    | 3680/6500 [24:19:32<15:02:21, 19.20s/it]                                                          57%|█████▋    | 3680/6500 [24:19:32<15:02:21, 19.20s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8693621158599854, 'eval_runtime': 5.3463, 'eval_samples_per_second': 4.302, 'eval_steps_per_second': 1.122, 'epoch': 0.57}
                                                          57%|█████▋    | 3680/6500 [24:19:37<15:02:21, 19.20s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3680
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3680
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3680
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3680

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3680
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3680/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3680/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3680/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2588, 'learning_rate': 3.968009278395768e-05, 'epoch': 0.57}
{'loss': 0.3159, 'learning_rate': 3.9656440957992716e-05, 'epoch': 0.57}
{'loss': 0.2451, 'learning_rate': 3.9632791549771776e-05, 'epoch': 0.57}
{'loss': 0.2377, 'learning_rate': 3.960914456482278e-05, 'epoch': 0.57}
{'loss': 0.2726, 'learning_rate': 3.958550000867307e-05, 'epoch': 0.57}
 57%|█████▋    | 3681/6500 [24:20:46<27:56:26, 35.68s/it]                                                          57%|█████▋    | 3681/6500 [24:20:46<27:56:26, 35.68s/it] 57%|█████▋    | 3682/6500 [24:21:04<23:47:24, 30.39s/it]                                                          57%|█████▋    | 3682/6500 [24:21:04<23:47:24, 30.39s/it] 57%|█████▋    | 3683/6500 [24:21:22<20:52:57, 26.69s/it]                                                          57%|█████▋    | 3683/6500 [24:21:22<20:52:57, 26.69s/it] 57%|█████▋    | 3684/6500 [24:21:40<18:51:18, 24.10s/it]                                                          57%|█████▋    | 3684/6500 [24:21:40<18:51:18, 24.10s/it] 57%|█████▋    | 3685/6500 [24:21:59<17:37:43, 22.54s/it]                                                          57%|█████▋    | 3685/6500 [24:21:59<17:37:43, 22.54s/it] 57%|█████▋    |{'loss': 0.759, 'learning_rate': 3.9561857886849405e-05, 'epoch': 0.57}
{'loss': 0.2655, 'learning_rate': 3.9538218204878015e-05, 'epoch': 0.57}
{'loss': 0.2643, 'learning_rate': 3.951458096828449e-05, 'epoch': 0.57}
{'loss': 0.2636, 'learning_rate': 3.9490946182593914e-05, 'epoch': 0.57}
{'loss': 0.2408, 'learning_rate': 3.9467313853330776e-05, 'epoch': 0.57}
 3686/6500 [24:22:17<16:34:50, 21.21s/it]                                                          57%|█████▋    | 3686/6500 [24:22:17<16:34:50, 21.21s/it] 57%|█████▋    | 3687/6500 [24:22:35<15:51:12, 20.29s/it]                                                          57%|█████▋    | 3687/6500 [24:22:35<15:51:12, 20.29s/it] 57%|█████▋    | 3688/6500 [24:22:53<15:20:30, 19.64s/it]                                                          57%|█████▋    | 3688/6500 [24:22:53<15:20:30, 19.64s/it] 57%|█████▋    | 3689/6500 [24:23:11<14:59:10, 19.19s/it]                                                          57%|█████▋    | 3689/6500 [24:23:11<14:59:10, 19.19s/it] 57%|█████▋    | 3690/6500 [24:23:30<14:44:19, 18.88s/it]                                                          57%|█████▋    | 3690/6500 [24:23:30<14:44:19, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.86963951587677, 'eval_runtime': 5.7469, 'eval_samples_per_second': 4.002, 'eval_steps_per_second': 1.044, 'epoch': 0.57}
                                                          57%|█████▋    | 3690/6500 [24:23:35<14:44:19, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3690
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3690the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3690

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3690
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3690/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2656, 'learning_rate': 3.944368398601898e-05, 'epoch': 0.57}
{'loss': 0.2484, 'learning_rate': 3.942005658618188e-05, 'epoch': 0.57}
{'loss': 0.2443, 'learning_rate': 3.9396431659342216e-05, 'epoch': 0.57}
{'loss': 0.2609, 'learning_rate': 3.937280921102218e-05, 'epoch': 0.57}
{'loss': 0.2457, 'learning_rate': 3.934918924674338e-05, 'epoch': 0.57}
 57%|█████▋    | 3691/6500 [24:24:44<27:41:21, 35.49s/it]                                                          57%|█████▋    | 3691/6500 [24:24:44<27:41:21, 35.49s/it] 57%|█████▋    | 3692/6500 [24:25:02<23:35:26, 30.24s/it]                                                          57%|█████▋    | 3692/6500 [24:25:02<23:35:26, 30.24s/it] 57%|█████▋    | 3693/6500 [24:25:20<20:43:24, 26.58s/it]                                                          57%|█████▋    | 3693/6500 [24:25:20<20:43:24, 26.58s/it] 57%|█████▋    | 3694/6500 [24:25:38<18:43:08, 24.02s/it]                                                          57%|█████▋    | 3694/6500 [24:25:38<18:43:08, 24.02s/it] 57%|█████▋    | 3695/6500 [24:25:56<17:19:46, 22.24s/it]                                                          57%|█████▋    | 3695/6500 [24:25:56<17:19:46, 22.24s/it] 57%|█████▋    |{'loss': 0.257, 'learning_rate': 3.9325571772026834e-05, 'epoch': 0.57}
{'loss': 0.2528, 'learning_rate': 3.930195679239298e-05, 'epoch': 0.57}
{'loss': 0.2651, 'learning_rate': 3.9278344313361696e-05, 'epoch': 0.57}
{'loss': 0.2492, 'learning_rate': 3.925473434045223e-05, 'epoch': 0.57}
{'loss': 0.27, 'learning_rate': 3.923112687918328e-05, 'epoch': 0.57}
 3696/6500 [24:26:14<16:21:37, 21.00s/it]                                                          57%|█████▋    | 3696/6500 [24:26:14<16:21:37, 21.00s/it] 57%|█████▋    | 3697/6500 [24:26:32<15:44:16, 20.21s/it]                                                          57%|█████▋    | 3697/6500 [24:26:32<15:44:16, 20.21s/it] 57%|█████▋    | 3698/6500 [24:26:51<15:14:49, 19.59s/it]                                                          57%|█████▋    | 3698/6500 [24:26:51<15:14:49, 19.59s/it] 57%|█████▋    | 3699/6500 [24:27:09<14:54:24, 19.16s/it]                                                          57%|█████▋    | 3699/6500 [24:27:09<14:54:24, 19.16s/it] 57%|█████▋    | 3700/6500 [24:27:27<14:40:16, 18.86s/it]                                                          57%|█████▋    | 3700/6500 [24:27:27<14:40:16, 18.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8809371590614319, 'eval_runtime': 5.3634, 'eval_samples_per_second': 4.288, 'eval_steps_per_second': 1.119, 'epoch': 0.57}
                                                          57%|█████▋    | 3700/6500 [24:27:32<14:40:16, 18.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3700
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3700
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3700
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3700

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3700
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3700/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3700/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2558, 'learning_rate': 3.9207521935072946e-05, 'epoch': 0.57}
{'loss': 0.2613, 'learning_rate': 3.9183919513638734e-05, 'epoch': 0.57}
{'loss': 0.2645, 'learning_rate': 3.916031962039758e-05, 'epoch': 0.57}
{'loss': 0.2567, 'learning_rate': 3.913672226086581e-05, 'epoch': 0.57}
{'loss': 0.2608, 'learning_rate': 3.911312744055917e-05, 'epoch': 0.57}
 57%|█████▋    | 3701/6500 [24:28:57<31:12:55, 40.15s/it]                                                          57%|█████▋    | 3701/6500 [24:28:57<31:12:55, 40.15s/it] 57%|█████▋    | 3702/6500 [24:29:15<26:02:26, 33.50s/it]                                                          57%|█████▋    | 3702/6500 [24:29:15<26:02:26, 33.50s/it] 57%|█████▋    | 3703/6500 [24:29:33<22:25:26, 28.86s/it]                                                          57%|█████▋    | 3703/6500 [24:29:33<22:25:26, 28.86s/it] 57%|█████▋    | 3704/6500 [24:29:51<19:53:54, 25.62s/it]                                                          57%|█████▋    | 3704/6500 [24:29:51<19:53:54, 25.62s/it] 57%|█████▋    | 3705/6500 [24:30:09<18:08:02, 23.36s/it]                                                          57%|█████▋    | 3705/6500 [24:30:09<18:08:02, 23.36s/it] 57%|█████▋    |{'loss': 0.2636, 'learning_rate': 3.908953516499278e-05, 'epoch': 0.57}
{'loss': 0.2617, 'learning_rate': 3.9065945439681214e-05, 'epoch': 0.57}
{'loss': 0.2496, 'learning_rate': 3.904235827013843e-05, 'epoch': 0.57}
{'loss': 0.2429, 'learning_rate': 3.901877366187777e-05, 'epoch': 0.57}
{'loss': 0.2393, 'learning_rate': 3.8995191620412e-05, 'epoch': 0.57}
 3706/6500 [24:30:27<16:54:13, 21.78s/it]                                                          57%|█████▋    | 3706/6500 [24:30:27<16:54:13, 21.78s/it] 57%|█████▋    | 3707/6500 [24:30:45<16:03:37, 20.70s/it]                                                          57%|█████▋    | 3707/6500 [24:30:45<16:03:37, 20.70s/it] 57%|█████▋    | 3708/6500 [24:31:03<15:28:38, 19.96s/it]                                                          57%|█████▋    | 3708/6500 [24:31:03<15:28:38, 19.96s/it] 57%|█████▋    | 3709/6500 [24:31:22<15:03:06, 19.41s/it]                                                          57%|█████▋    | 3709/6500 [24:31:22<15:03:06, 19.41s/it] 57%|█████▋    | 3710/6500 [24:31:40<14:45:27, 19.04s/it]                                                          57%|█████▋    | 3710/6500 [24:31:40<14:45:27, 19.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8638142347335815, 'eval_runtime': 5.3497, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.122, 'epoch': 0.57}
                                                          57%|█████▋    | 3710/6500 [24:31:45<14:45:27, 19.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3710
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3710
I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3710
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3710

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3710
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3710/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3710/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.273, 'learning_rate': 3.8971612151253275e-05, 'epoch': 0.57}
{'loss': 0.314, 'learning_rate': 3.8948035259913154e-05, 'epoch': 0.57}
{'loss': 0.2598, 'learning_rate': 3.89244609519026e-05, 'epoch': 0.57}
{'loss': 0.2507, 'learning_rate': 3.8900889232731954e-05, 'epoch': 0.57}
{'loss': 0.2674, 'learning_rate': 3.887732010791098e-05, 'epoch': 0.57}
 57%|█████▋    | 3711/6500 [24:33:16<32:42:25, 42.22s/it]                                                          57%|█████▋    | 3711/6500 [24:33:16<32:42:25, 42.22s/it] 57%|█████▋    | 3712/6500 [24:33:34<27:03:34, 34.94s/it]                                                          57%|█████▋    | 3712/6500 [24:33:34<27:03:34, 34.94s/it] 57%|█████▋    | 3713/6500 [24:33:52<23:06:27, 29.85s/it]                                                          57%|█████▋    | 3713/6500 [24:33:52<23:06:27, 29.85s/it] 57%|█████▋    | 3714/6500 [24:34:10<20:20:52, 26.29s/it]                                                          57%|█████▋    | 3714/6500 [24:34:10<20:20:52, 26.29s/it] 57%|█████▋    | 3715/6500 [24:34:28<18:25:21, 23.81s/it]                                                          57%|█████▋    | 3715/6500 [24:34:28<18:25:21, 23.81s/it] 57%|█████▋    |{'loss': 0.7659, 'learning_rate': 3.8853753582948785e-05, 'epoch': 0.57}
{'loss': 0.2685, 'learning_rate': 3.883018966335393e-05, 'epoch': 0.57}
{'loss': 0.2563, 'learning_rate': 3.880662835463432e-05, 'epoch': 0.57}
{'loss': 0.2385, 'learning_rate': 3.878306966229728e-05, 'epoch': 0.57}
{'loss': 0.2437, 'learning_rate': 3.875951359184951e-05, 'epoch': 0.57}
 3716/6500 [24:34:46<17:04:34, 22.08s/it]                                                          57%|█████▋    | 3716/6500 [24:34:46<17:04:34, 22.08s/it] 57%|█████▋    | 3717/6500 [24:35:04<16:14:16, 21.00s/it]                                                          57%|█████▋    | 3717/6500 [24:35:04<16:14:16, 21.00s/it] 57%|█████▋    | 3718/6500 [24:35:23<15:33:43, 20.14s/it]                                                          57%|█████▋    | 3718/6500 [24:35:23<15:33:43, 20.14s/it] 57%|█████▋    | 3719/6500 [24:35:41<15:05:24, 19.53s/it]                                                          57%|█████▋    | 3719/6500 [24:35:41<15:05:24, 19.53s/it] 57%|█████▋    | 3720/6500 [24:35:59<14:45:49, 19.12s/it]                                                          57%|█████▋    | 3720/6500 [24:35:59<14:45:49, 19.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8670833706855774, 'eval_runtime': 5.4899, 'eval_samples_per_second': 4.19, 'eval_steps_per_second': 1.093, 'epoch': 0.57}
                                                          57%|█████▋    | 3720/6500 [24:36:04<14:45:49, 19.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3720
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3720
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3720
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3720

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3720
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3720/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2615, 'learning_rate': 3.873596014879709e-05, 'epoch': 0.57}
{'loss': 0.2386, 'learning_rate': 3.87124093386455e-05, 'epoch': 0.57}
{'loss': 0.2471, 'learning_rate': 3.868886116689959e-05, 'epoch': 0.57}
{'loss': 0.2446, 'learning_rate': 3.866531563906362e-05, 'epoch': 0.57}
{'loss': 0.2505, 'learning_rate': 3.86417727606412e-05, 'epoch': 0.57}
 57%|█████▋    | 3721/6500 [24:36:56<23:37:46, 30.61s/it]                                                          57%|█████▋    | 3721/6500 [24:36:56<23:37:46, 30.61s/it] 57%|█████▋    | 3722/6500 [24:37:14<20:42:08, 26.83s/it]                                                          57%|█████▋    | 3722/6500 [24:37:14<20:42:08, 26.83s/it] 57%|█████▋    | 3723/6500 [24:37:32<18:39:28, 24.19s/it]                                                          57%|█████▋    | 3723/6500 [24:37:32<18:39:28, 24.19s/it] 57%|█████▋    | 3724/6500 [24:37:50<17:13:40, 22.34s/it]                                                          57%|█████▋    | 3724/6500 [24:37:50<17:13:40, 22.34s/it] 57%|█████▋    | 3725/6500 [24:38:08<16:13:38, 21.05s/it]                                                          57%|█████▋    | 3725/6500 [24:38:08<16:13:38, 21.05s/it] 57%|█████▋    |{'loss': 0.2503, 'learning_rate': 3.861823253713535e-05, 'epoch': 0.57}
{'loss': 0.2514, 'learning_rate': 3.8594694974048426e-05, 'epoch': 0.57}
{'loss': 0.2649, 'learning_rate': 3.8571160076882204e-05, 'epoch': 0.57}
{'loss': 0.2549, 'learning_rate': 3.8547627851137836e-05, 'epoch': 0.57}
{'loss': 0.2615, 'learning_rate': 3.852409830231582e-05, 'epoch': 0.57}
 3726/6500 [24:38:26<15:31:57, 20.16s/it]                                                          57%|█████▋    | 3726/6500 [24:38:26<15:31:57, 20.16s/it] 57%|█████▋    | 3727/6500 [24:38:45<15:03:03, 19.54s/it]                                                          57%|█████▋    | 3727/6500 [24:38:45<15:03:03, 19.54s/it] 57%|█████▋    | 3728/6500 [24:39:03<14:42:48, 19.11s/it]                                                          57%|█████▋    | 3728/6500 [24:39:03<14:42:48, 19.11s/it] 57%|█████▋    | 3729/6500 [24:39:21<14:28:59, 18.82s/it]                                                          57%|█████▋    | 3729/6500 [24:39:21<14:28:59, 18.82s/it] 57%|█████▋    | 3730/6500 [24:39:39<14:19:31, 18.62s/it]                                                          57%|█████▋    | 3730/6500 [24:39:39<14:19:31, 18.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8840349316596985, 'eval_runtime': 6.991, 'eval_samples_per_second': 3.29, 'eval_steps_per_second': 0.858, 'epoch': 0.57}
                                                          57%|█████▋    | 3730/6500 [24:39:46<14:19:31, 18.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3730
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3730
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3730

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3730
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3730/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.248, 'learning_rate': 3.850057143591605e-05, 'epoch': 0.57}
{'loss': 0.2489, 'learning_rate': 3.84770472574378e-05, 'epoch': 0.57}
{'loss': 0.2587, 'learning_rate': 3.845352577237969e-05, 'epoch': 0.57}
{'loss': 0.2496, 'learning_rate': 3.843000698623972e-05, 'epoch': 0.57}
{'loss': 0.2656, 'learning_rate': 3.840649090451527e-05, 'epoch': 0.57}
 57%|█████▋    | 3731/6500 [24:41:04<29:43:26, 38.64s/it]                                                          57%|█████▋    | 3731/6500 [24:41:04<29:43:26, 38.64s/it] 57%|█████▋    | 3732/6500 [24:41:22<24:56:45, 32.44s/it]                                                          57%|█████▋    | 3732/6500 [24:41:22<24:56:45, 32.44s/it] 57%|█████▋    | 3733/6500 [24:41:40<21:35:55, 28.10s/it]                                                          57%|█████▋    | 3733/6500 [24:41:40<21:35:55, 28.10s/it] 57%|█████▋    | 3734/6500 [24:41:59<19:19:19, 25.15s/it]                                                          57%|█████▋    | 3734/6500 [24:41:59<19:19:19, 25.15s/it] 57%|█████▋    | 3735/6500 [24:42:17<17:40:35, 23.01s/it]                                                          57%|█████▋    | 3735/6500 [24:42:17<17:40:35, 23.01s/it] 57%|█████▋    |{'loss': 0.2524, 'learning_rate': 3.838297753270308e-05, 'epoch': 0.57}
{'loss': 0.2671, 'learning_rate': 3.835946687629927e-05, 'epoch': 0.57}
{'loss': 0.2414, 'learning_rate': 3.83359589407993e-05, 'epoch': 0.58}
{'loss': 0.2505, 'learning_rate': 3.8312453731698e-05, 'epoch': 0.58}
{'loss': 0.2427, 'learning_rate': 3.8288951254489583e-05, 'epoch': 0.58}
 3736/6500 [24:42:35<16:31:25, 21.52s/it]                                                          57%|█████▋    | 3736/6500 [24:42:35<16:31:25, 21.52s/it] 57%|█████▋    | 3737/6500 [24:42:53<15:43:04, 20.48s/it]                                                          57%|█████▋    | 3737/6500 [24:42:53<15:43:04, 20.48s/it] 58%|█████▊    | 3738/6500 [24:43:11<15:09:39, 19.76s/it]                                                          58%|█████▊    | 3738/6500 [24:43:11<15:09:39, 19.76s/it] 58%|█████▊    | 3739/6500 [24:43:29<14:46:40, 19.27s/it]                                                          58%|█████▊    | 3739/6500 [24:43:29<14:46:40, 19.27s/it] 58%|█████▊    | 3740/6500 [24:43:47<14:30:38, 18.93s/it]                                                          58%|█████▊    | 3740/6500 [24:43:47<14:30:38, 18.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8653894662857056, 'eval_runtime': 6.0112, 'eval_samples_per_second': 3.826, 'eval_steps_per_second': 0.998, 'epoch': 0.58}
                                                          58%|█████▊    | 3740/6500 [24:43:53<14:30:38, 18.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3740
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3740
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3740
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3740

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3740
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3740/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3740/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3171, 'learning_rate': 3.82654515146676e-05, 'epoch': 0.58}
{'loss': 0.2547, 'learning_rate': 3.824195451772499e-05, 'epoch': 0.58}
{'loss': 0.2435, 'learning_rate': 3.8218460269154e-05, 'epoch': 0.58}
{'loss': 0.26, 'learning_rate': 3.81949687744463e-05, 'epoch': 0.58}
{'loss': 0.5743, 'learning_rate': 3.817148003909288e-05, 'epoch': 0.58}
 58%|█████▊    | 3741/6500 [24:44:58<26:34:51, 34.68s/it]                                                          58%|█████▊    | 3741/6500 [24:44:58<26:34:51, 34.68s/it] 58%|█████▊    | 3742/6500 [24:45:16<22:44:06, 29.68s/it]                                                          58%|█████▊    | 3742/6500 [24:45:16<22:44:06, 29.68s/it] 58%|█████▊    | 3743/6500 [24:45:34<20:02:53, 26.18s/it]                                                          58%|█████▊    | 3743/6500 [24:45:34<20:02:53, 26.18s/it] 58%|█████▊    | 3744/6500 [24:45:52<18:09:24, 23.72s/it]                                                          58%|█████▊    | 3744/6500 [24:45:52<18:09:24, 23.72s/it] 58%|█████▊    | 3745/6500 [24:46:10<16:49:50, 21.99s/it]                                                          58%|█████▊    | 3745/6500 [24:46:10<16:49:50, 21.99s/it] 58%|█████▊    |{'loss': 0.4348, 'learning_rate': 3.8147994068584087e-05, 'epoch': 0.58}
{'loss': 0.2525, 'learning_rate': 3.812451086840961e-05, 'epoch': 0.58}
{'loss': 0.26, 'learning_rate': 3.8101030444058515e-05, 'epoch': 0.58}
{'loss': 0.2426, 'learning_rate': 3.807755280101921e-05, 'epoch': 0.58}
{'loss': 0.2544, 'learning_rate': 3.8054077944779434e-05, 'epoch': 0.58}
 3746/6500 [24:46:28<15:54:54, 20.80s/it]                                                          58%|█████▊    | 3746/6500 [24:46:28<15:54:54, 20.80s/it] 58%|█████▊    | 3747/6500 [24:46:46<15:16:29, 19.97s/it]                                                          58%|█████▊    | 3747/6500 [24:46:46<15:16:29, 19.97s/it] 58%|█████▊    | 3748/6500 [24:47:05<14:49:56, 19.40s/it]                                                          58%|█████▊    | 3748/6500 [24:47:05<14:49:56, 19.40s/it] 58%|█████▊    | 3749/6500 [24:47:23<14:31:43, 19.01s/it]                                                          58%|█████▊    | 3749/6500 [24:47:23<14:31:43, 19.01s/it] 58%|█████▊    | 3750/6500 [24:47:42<14:36:28, 19.12s/it]                                                          58%|█████▊    | 3750/6500 [24:47:42<14:36:28, 19.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8822335600852966, 'eval_runtime': 5.3428, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.58}
                                                          58%|█████▊    | 3750/6500 [24:47:47<14:36:28, 19.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3750
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL the checkpoint model will be saved in 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3750
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3750
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3750

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3750
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3750/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2499, 'learning_rate': 3.803060588082633e-05, 'epoch': 0.58}
{'loss': 0.2247, 'learning_rate': 3.800713661464631e-05, 'epoch': 0.58}
{'loss': 0.2618, 'learning_rate': 3.7983670151725195e-05, 'epoch': 0.58}
{'loss': 0.2314, 'learning_rate': 3.7960206497548115e-05, 'epoch': 0.58}
{'loss': 0.2578, 'learning_rate': 3.793674565759957e-05, 'epoch': 0.58}
 58%|█████▊    | 3751/6500 [24:48:57<27:25:51, 35.92s/it]                                                          58%|█████▊    | 3751/6500 [24:48:57<27:25:51, 35.92s/it] 58%|█████▊    | 3752/6500 [24:49:15<23:18:46, 30.54s/it]                                                          58%|█████▊    | 3752/6500 [24:49:15<23:18:46, 30.54s/it] 58%|█████▊    | 3753/6500 [24:49:33<20:26:17, 26.78s/it]                                                          58%|█████▊    | 3753/6500 [24:49:33<20:26:17, 26.78s/it] 58%|█████▊    | 3754/6500 [24:49:51<18:25:51, 24.16s/it]                                                          58%|█████▊    | 3754/6500 [24:49:51<18:25:51, 24.16s/it] 58%|█████▊    | 3755/6500 [24:50:09<17:01:54, 22.34s/it]                                                          58%|█████▊    | 3755/6500 [24:50:09<17:01:54, 22.34s/it] 58%|█████▊    |{'loss': 0.2527, 'learning_rate': 3.791328763736337e-05, 'epoch': 0.58}
{'loss': 0.2534, 'learning_rate': 3.788983244232272e-05, 'epoch': 0.58}
{'loss': 0.2474, 'learning_rate': 3.7866380077960085e-05, 'epoch': 0.58}
{'loss': 0.2497, 'learning_rate': 3.784293054975734e-05, 'epoch': 0.58}
{'loss': 0.2637, 'learning_rate': 3.781948386319566e-05, 'epoch': 0.58}
 3756/6500 [24:50:27<16:03:15, 21.06s/it]                                                          58%|█████▊    | 3756/6500 [24:50:27<16:03:15, 21.06s/it] 58%|█████▊    | 3757/6500 [24:50:45<15:22:34, 20.18s/it]                                                          58%|█████▊    | 3757/6500 [24:50:45<15:22:34, 20.18s/it] 58%|█████▊    | 3758/6500 [24:51:04<14:54:06, 19.56s/it]                                                          58%|█████▊    | 3758/6500 [24:51:04<14:54:06, 19.56s/it] 58%|█████▊    | 3759/6500 [24:51:22<14:37:08, 19.20s/it]                                                          58%|█████▊    | 3759/6500 [24:51:22<14:37:08, 19.20s/it] 58%|█████▊    | 3760/6500 [24:51:40<14:22:42, 18.89s/it]                                                          58%|█████▊    | 3760/6500 [24:51:40<14:22:42, 18.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8834606409072876, 'eval_runtime': 5.3311, 'eval_samples_per_second': 4.314, 'eval_steps_per_second': 1.125, 'epoch': 0.58}
                                                          58%|█████▊    | 3760/6500 [24:51:45<14:22:42, 18.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3760
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3760
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3760
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3760

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3760
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3760/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2465, 'learning_rate': 3.7796040023755566e-05, 'epoch': 0.58}
{'loss': 0.2606, 'learning_rate': 3.777259903691692e-05, 'epoch': 0.58}
{'loss': 0.2605, 'learning_rate': 3.774916090815891e-05, 'epoch': 0.58}
{'loss': 0.2543, 'learning_rate': 3.772572564296005e-05, 'epoch': 0.58}
{'loss': 0.2532, 'learning_rate': 3.770229324679818e-05, 'epoch': 0.58}
 58%|█████▊    | 3761/6500 [24:52:37<23:03:52, 30.32s/it]                                                          58%|█████▊    | 3761/6500 [24:52:37<23:03:52, 30.32s/it] 58%|█████▊    | 3762/6500 [24:52:55<20:15:24, 26.63s/it]                                                          58%|█████▊    | 3762/6500 [24:52:55<20:15:24, 26.63s/it] 58%|█████▊    | 3763/6500 [24:53:13<18:17:23, 24.06s/it]                                                          58%|█████▊    | 3763/6500 [24:53:13<18:17:23, 24.06s/it] 58%|█████▊    | 3764/6500 [24:53:31<16:55:04, 22.26s/it]                                                          58%|█████▊    | 3764/6500 [24:53:31<16:55:04, 22.26s/it] 58%|█████▊    | 3765/6500 [24:53:49<15:57:32, 21.01s/it]                                                          58%|█████▊    | 3765/6500 [24:53:49<15:57:32, 21.01s/it] 58%|█████▊    |{'loss': 0.2644, 'learning_rate': 3.7678863725150505e-05, 'epoch': 0.58}
{'loss': 0.2489, 'learning_rate': 3.765543708349351e-05, 'epoch': 0.58}
{'loss': 0.2423, 'learning_rate': 3.7632013327303055e-05, 'epoch': 0.58}
{'loss': 0.2367, 'learning_rate': 3.760859246205427e-05, 'epoch': 0.58}
{'loss': 0.2547, 'learning_rate': 3.7585174493221664e-05, 'epoch': 0.58}
 3766/6500 [24:54:08<15:21:17, 20.22s/it]                                                          58%|█████▊    | 3766/6500 [24:54:08<15:21:17, 20.22s/it] 58%|█████▊    | 3767/6500 [24:54:26<14:52:32, 19.59s/it]                                                          58%|█████▊    | 3767/6500 [24:54:26<14:52:32, 19.59s/it] 58%|█████▊    | 3768/6500 [24:54:44<14:32:32, 19.16s/it]                                                          58%|█████▊    | 3768/6500 [24:54:44<14:32:32, 19.16s/it] 58%|█████▊    | 3769/6500 [24:55:02<14:18:38, 18.86s/it]                                                          58%|█████▊    | 3769/6500 [24:55:02<14:18:38, 18.86s/it] 58%|█████▊    | 3770/6500 [24:55:20<14:08:51, 18.66s/it]                                                          58%|█████▊    | 3770/6500 [24:55:20<14:08:51, 18.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8681598901748657, 'eval_runtime': 5.3569, 'eval_samples_per_second': 4.294, 'eval_steps_per_second': 1.12, 'epoch': 0.58}
                                                          58%|█████▊    | 3770/6500 [24:55:26<14:08:51, 18.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3770
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3770
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3770
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3770

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3770
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3770/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3134, 'learning_rate': 3.756175942627904e-05, 'epoch': 0.58}
{'loss': 0.2489, 'learning_rate': 3.753834726669951e-05, 'epoch': 0.58}
{'loss': 0.2337, 'learning_rate': 3.7514938019955554e-05, 'epoch': 0.58}
{'loss': 0.2708, 'learning_rate': 3.7491531691518935e-05, 'epoch': 0.58}
{'loss': 0.7495, 'learning_rate': 3.74681282868607e-05, 'epoch': 0.58}
 58%|█████▊    | 3771/6500 [24:56:44<28:55:23, 38.15s/it]                                                          58%|█████▊    | 3771/6500 [24:56:44<28:55:23, 38.15s/it] 58%|█████▊    | 3772/6500 [24:57:02<24:21:38, 32.15s/it]                                                          58%|█████▊    | 3772/6500 [24:57:02<24:21:38, 32.15s/it] 58%|█████▊    | 3773/6500 [24:57:20<21:08:06, 27.90s/it]                                                          58%|█████▊    | 3773/6500 [24:57:20<21:08:06, 27.90s/it] 58%|█████▊    | 3774/6500 [24:57:38<18:52:44, 24.93s/it]                                                          58%|█████▊    | 3774/6500 [24:57:38<18:52:44, 24.93s/it] 58%|█████▊    | 3775/6500 [24:57:56<17:18:10, 22.86s/it]                                                          58%|█████▊    | 3775/6500 [24:57:56<17:18:10, 22.86s/it] 58%|█████▊    |{'loss': 0.2606, 'learning_rate': 3.744472781145131e-05, 'epoch': 0.58}
{'loss': 0.2489, 'learning_rate': 3.742133027076043e-05, 'epoch': 0.58}
{'loss': 0.2606, 'learning_rate': 3.739793567025714e-05, 'epoch': 0.58}
{'loss': 0.2372, 'learning_rate': 3.737454401540977e-05, 'epoch': 0.58}
{'loss': 0.2668, 'learning_rate': 3.735115531168596e-05, 'epoch': 0.58}
 3776/6500 [24:58:14<16:12:35, 21.42s/it]                                                          58%|█████▊    | 3776/6500 [24:58:14<16:12:35, 21.42s/it] 58%|█████▊    | 3777/6500 [24:58:32<15:26:39, 20.42s/it]                                                          58%|█████▊    | 3777/6500 [24:58:32<15:26:39, 20.42s/it] 58%|█████▊    | 3778/6500 [24:58:51<14:56:10, 19.75s/it]                                                          58%|█████▊    | 3778/6500 [24:58:51<14:56:10, 19.75s/it] 58%|█████▊    | 3779/6500 [24:59:09<14:33:29, 19.26s/it]                                                          58%|█████▊    | 3779/6500 [24:59:09<14:33:29, 19.26s/it] 58%|█████▊    | 3780/6500 [24:59:27<14:19:06, 18.95s/it]                                                          58%|█████▊    | 3780/6500 [24:59:27<14:19:06, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8713374733924866, 'eval_runtime': 5.3394, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.58}
                                                          58%|█████▊    | 3780/6500 [24:59:32<14:19:06, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3780
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3780
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3780
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3780

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3780
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3780/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3780/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2318, 'learning_rate': 3.73277695645527e-05, 'epoch': 0.58}
{'loss': 0.2408, 'learning_rate': 3.730438677947624e-05, 'epoch': 0.58}
{'loss': 0.2613, 'learning_rate': 3.728100696192218e-05, 'epoch': 0.58}
{'loss': 0.2567, 'learning_rate': 3.725763011735542e-05, 'epoch': 0.58}
{'loss': 0.2602, 'learning_rate': 3.723425625124015e-05, 'epoch': 0.58}
 58%|█████▊    | 3781/6500 [25:00:38<26:04:06, 34.51s/it]                                                          58%|█████▊    | 3781/6500 [25:00:38<26:04:06, 34.51s/it] 58%|█████▊    | 3782/6500 [25:00:56<22:24:10, 29.67s/it]                                                          58%|█████▊    | 3782/6500 [25:00:56<22:24:10, 29.67s/it] 58%|█████▊    | 3783/6500 [25:01:14<19:46:36, 26.20s/it]                                                          58%|█████▊    | 3783/6500 [25:01:14<19:46:36, 26.20s/it] 58%|█████▊    | 3784/6500 [25:01:32<17:55:15, 23.75s/it]                                                          58%|█████▊    | 3784/6500 [25:01:32<17:55:15, 23.75s/it] 58%|█████▊    | 3785/6500 [25:01:50<16:37:43, 22.05s/it]                                                          58%|█████▊    | 3785/6500 [25:01:50<16:37:43, 22.05s/it] 58%|█████▊    |{'loss': 0.2554, 'learning_rate': 3.721088536903986e-05, 'epoch': 0.58}
{'loss': 0.2708, 'learning_rate': 3.718751747621735e-05, 'epoch': 0.58}
{'loss': 0.2526, 'learning_rate': 3.7164152578234734e-05, 'epoch': 0.58}
{'loss': 0.276, 'learning_rate': 3.714079068055341e-05, 'epoch': 0.58}
{'loss': 0.2544, 'learning_rate': 3.711743178863407e-05, 'epoch': 0.58}
 3786/6500 [25:02:08<15:43:21, 20.86s/it]                                                          58%|█████▊    | 3786/6500 [25:02:08<15:43:21, 20.86s/it] 58%|█████▊    | 3787/6500 [25:02:27<15:07:27, 20.07s/it]                                                          58%|█████▊    | 3787/6500 [25:02:27<15:07:27, 20.07s/it] 58%|█████▊    | 3788/6500 [25:02:45<14:40:40, 19.48s/it]                                                          58%|█████▊    | 3788/6500 [25:02:45<14:40:40, 19.48s/it] 58%|█████▊    | 3789/6500 [25:03:03<14:22:03, 19.08s/it]                                                          58%|█████▊    | 3789/6500 [25:03:03<14:22:03, 19.08s/it] 58%|█████▊    | 3790/6500 [25:03:21<14:08:59, 18.80s/it]                                                          58%|█████▊    | 3790/6500 [25:03:21<14:08:59, 18.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807402849197388, 'eval_runtime': 5.3357, 'eval_samples_per_second': 4.311, 'eval_steps_per_second': 1.125, 'epoch': 0.58}
                                                          58%|█████▊    | 3790/6500 [25:03:26<14:08:59, 18.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3790
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3790

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3790
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3790

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3790
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3790/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2492, 'learning_rate': 3.709407590793673e-05, 'epoch': 0.58}
{'loss': 0.2616, 'learning_rate': 3.707072304392068e-05, 'epoch': 0.58}
{'loss': 0.2623, 'learning_rate': 3.70473732020445e-05, 'epoch': 0.58}
{'loss': 0.2554, 'learning_rate': 3.702402638776609e-05, 'epoch': 0.58}
{'loss': 0.2574, 'learning_rate': 3.7000682606542605e-05, 'epoch': 0.58}
 58%|█████▊    | 3791/6500 [25:04:52<30:25:48, 40.44s/it]                                                          58%|█████▊    | 3791/6500 [25:04:52<30:25:48, 40.44s/it] 58%|█████▊    | 3792/6500 [25:05:10<25:20:59, 33.70s/it]                                                          58%|█████▊    | 3792/6500 [25:05:10<25:20:59, 33.70s/it] 58%|█████▊    | 3793/6500 [25:05:28<21:47:27, 28.98s/it]                                                          58%|█████▊    | 3793/6500 [25:05:28<21:47:27, 28.98s/it] 58%|█████▊    | 3794/6500 [25:05:46<19:17:52, 25.67s/it]                                                          58%|█████▊    | 3794/6500 [25:05:46<19:17:52, 25.67s/it] 58%|█████▊    | 3795/6500 [25:06:04<17:33:44, 23.37s/it]                                                          58%|█████▊    | 3795/6500 [25:06:04<17:33:44, 23.37s/it] 58%|█████▊    |{'loss': 0.2677, 'learning_rate': 3.6977341863830534e-05, 'epoch': 0.58}
{'loss': 0.2409, 'learning_rate': 3.695400416508562e-05, 'epoch': 0.58}
{'loss': 0.2518, 'learning_rate': 3.6930669515762906e-05, 'epoch': 0.58}
{'loss': 0.2352, 'learning_rate': 3.690733792131673e-05, 'epoch': 0.58}
{'loss': 0.2689, 'learning_rate': 3.6884009387200714e-05, 'epoch': 0.58}
 3796/6500 [25:06:22<16:21:08, 21.77s/it]                                                          58%|█████▊    | 3796/6500 [25:06:22<16:21:08, 21.77s/it] 58%|█████▊    | 3797/6500 [25:06:40<15:30:39, 20.66s/it]                                                          58%|█████▊    | 3797/6500 [25:06:40<15:30:39, 20.66s/it] 58%|█████▊    | 3798/6500 [25:06:58<15:01:55, 20.03s/it]                                                          58%|█████▊    | 3798/6500 [25:06:58<15:01:55, 20.03s/it] 58%|█████▊    | 3799/6500 [25:07:17<14:35:28, 19.45s/it]                                                          58%|█████▊    | 3799/6500 [25:07:17<14:35:28, 19.45s/it] 58%|█████▊    | 3800/6500 [25:07:35<14:17:08, 19.05s/it]                                                          58%|█████▊    | 3800/6500 [25:07:35<14:17:08, 19.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8693439364433289, 'eval_runtime': 5.4078, 'eval_samples_per_second': 4.253, 'eval_steps_per_second': 1.11, 'epoch': 0.58}
                                                          58%|█████▊    | 3800/6500 [25:07:40<14:17:08, 19.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3800
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3800
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3800
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3800

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3800/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3800/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3800/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2989, 'learning_rate': 3.6860683918867756e-05, 'epoch': 0.58}
{'loss': 0.2438, 'learning_rate': 3.683736152177005e-05, 'epoch': 0.58}
{'loss': 0.2622, 'learning_rate': 3.6814042201359056e-05, 'epoch': 0.59}
{'loss': 0.2481, 'learning_rate': 3.6790725963085516e-05, 'epoch': 0.59}
{'loss': 0.7527, 'learning_rate': 3.6767412812399473e-05, 'epoch': 0.59}
 58%|█████▊    | 3801/6500 [25:08:55<28:06:14, 37.49s/it]                                                          58%|█████▊    | 3801/6500 [25:08:55<28:06:14, 37.49s/it] 58%|█████▊    | 3802/6500 [25:09:17<24:32:10, 32.74s/it]                                                          58%|█████▊    | 3802/6500 [25:09:17<24:32:10, 32.74s/it] 59%|█████▊    | 3803/6500 [25:09:35<21:13:37, 28.33s/it]                                                          59%|█████▊    | 3803/6500 [25:09:35<21:13:37, 28.33s/it] 59%|█████▊    | 3804/6500 [25:09:53<18:53:42, 25.23s/it]                                                          59%|█████▊    | 3804/6500 [25:09:53<18:53:42, 25.23s/it] 59%|█████▊    | 3805/6500 [25:10:11<17:16:06, 23.07s/it]                                                          59%|█████▊    | 3805/6500 [25:10:11<17:16:06, 23.07s/it] 59%|█████▊    |{'loss': 0.2629, 'learning_rate': 3.674410275475023e-05, 'epoch': 0.59}
{'loss': 0.2588, 'learning_rate': 3.6720795795586384e-05, 'epoch': 0.59}
{'loss': 0.2349, 'learning_rate': 3.6697491940355765e-05, 'epoch': 0.59}
{'loss': 0.2423, 'learning_rate': 3.667419119450553e-05, 'epoch': 0.59}
{'loss': 0.2546, 'learning_rate': 3.665089356348208e-05, 'epoch': 0.59}
 3806/6500 [25:10:29<16:08:18, 21.57s/it]                                                          59%|█████▊    | 3806/6500 [25:10:29<16:08:18, 21.57s/it] 59%|█████▊    | 3807/6500 [25:10:47<15:21:00, 20.52s/it]                                                          59%|█████▊    | 3807/6500 [25:10:47<15:21:00, 20.52s/it] 59%|█████▊    | 3808/6500 [25:11:05<14:48:20, 19.80s/it]                                                          59%|█████▊    | 3808/6500 [25:11:05<14:48:20, 19.80s/it] 59%|█████▊    | 3809/6500 [25:11:23<14:25:24, 19.30s/it]                                                          59%|█████▊    | 3809/6500 [25:11:23<14:25:24, 19.30s/it] 59%|█████▊    | 3810/6500 [25:11:41<14:09:34, 18.95s/it]                                                          59%|█████▊    | 3810/6500 [25:11:41<14:09:34, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8827194571495056, 'eval_runtime': 5.5877, 'eval_samples_per_second': 4.116, 'eval_steps_per_second': 1.074, 'epoch': 0.59}
                                                          59%|█████▊    | 3810/6500 [25:11:47<14:09:34, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3810
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3810the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3810

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3810

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3810
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3810/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2214, 'learning_rate': 3.662759905273109e-05, 'epoch': 0.59}
{'loss': 0.2435, 'learning_rate': 3.660430766769752e-05, 'epoch': 0.59}
{'loss': 0.2416, 'learning_rate': 3.65810194138256e-05, 'epoch': 0.59}
{'loss': 0.2473, 'learning_rate': 3.655773429655879e-05, 'epoch': 0.59}
{'loss': 0.2569, 'learning_rate': 3.6534452321339854e-05, 'epoch': 0.59}
 59%|█████▊    | 3811/6500 [25:13:09<29:31:38, 39.53s/it]                                                          59%|█████▊    | 3811/6500 [25:13:09<29:31:38, 39.53s/it] 59%|█████▊    | 3812/6500 [25:13:27<24:40:57, 33.06s/it]                                                          59%|█████▊    | 3812/6500 [25:13:27<24:40:57, 33.06s/it] 59%|█████▊    | 3813/6500 [25:13:45<21:17:47, 28.53s/it]                                                          59%|█████▊    | 3813/6500 [25:13:45<21:17:47, 28.53s/it] 59%|█████▊    | 3814/6500 [25:14:04<19:16:55, 25.84s/it]                                                          59%|█████▊    | 3814/6500 [25:14:05<19:16:55, 25.84s/it] 59%|█████▊    | 3815/6500 [25:14:23<17:32:27, 23.52s/it]                                                          59%|█████▊    | 3815/6500 [25:14:23<17:32:27, 23.52s/it] 59%|█████▊    |{'loss': 0.2521, 'learning_rate': 3.6511173493610825e-05, 'epoch': 0.59}
{'loss': 0.2603, 'learning_rate': 3.648789781881297e-05, 'epoch': 0.59}
{'loss': 0.2573, 'learning_rate': 3.646462530238684e-05, 'epoch': 0.59}
{'loss': 0.2791, 'learning_rate': 3.6441355949772253e-05, 'epoch': 0.59}
{'loss': 0.2504, 'learning_rate': 3.641808976640828e-05, 'epoch': 0.59}
 3816/6500 [25:14:41<16:18:43, 21.88s/it]                                                          59%|█████▊    | 3816/6500 [25:14:41<16:18:43, 21.88s/it] 59%|█████▊    | 3817/6500 [25:14:59<15:27:08, 20.73s/it]                                                          59%|█████▊    | 3817/6500 [25:14:59<15:27:08, 20.73s/it] 59%|█████▊    | 3818/6500 [25:15:17<14:56:29, 20.06s/it]                                                          59%|█████▊    | 3818/6500 [25:15:17<14:56:29, 20.06s/it] 59%|█████▉    | 3819/6500 [25:15:35<14:30:13, 19.48s/it]                                                          59%|█████▉    | 3819/6500 [25:15:35<14:30:13, 19.48s/it] 59%|█████▉    | 3820/6500 [25:15:53<14:11:51, 19.07s/it]                                                          59%|█████▉    | 3820/6500 [25:15:53<14:11:51, 19.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807095885276794, 'eval_runtime': 5.3735, 'eval_samples_per_second': 4.28, 'eval_steps_per_second': 1.117, 'epoch': 0.59}
                                                          59%|█████▉    | 3820/6500 [25:15:59<14:11:51, 19.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3820
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3820

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3820
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3820

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3820
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3820/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2595, 'learning_rate': 3.639482675773324e-05, 'epoch': 0.59}
{'loss': 0.2712, 'learning_rate': 3.6371566929184744e-05, 'epoch': 0.59}
{'loss': 0.2459, 'learning_rate': 3.634831028619959e-05, 'epoch': 0.59}
{'loss': 0.2635, 'learning_rate': 3.632505683421392e-05, 'epoch': 0.59}
{'loss': 0.2631, 'learning_rate': 3.630180657866306e-05, 'epoch': 0.59}
 59%|█████▉    | 3821/6500 [25:16:36<19:32:35, 26.26s/it]                                                          59%|█████▉    | 3821/6500 [25:16:36<19:32:35, 26.26s/it] 59%|█████▉    | 3822/6500 [25:16:55<17:42:44, 23.81s/it]                                                          59%|█████▉    | 3822/6500 [25:16:55<17:42:44, 23.81s/it] 59%|█████▉    | 3823/6500 [25:17:13<16:25:39, 22.09s/it]                                                          59%|█████▉    | 3823/6500 [25:17:13<16:25:39, 22.09s/it] 59%|█████▉    | 3824/6500 [25:17:31<15:31:33, 20.89s/it]                                                          59%|█████▉    | 3824/6500 [25:17:31<15:31:33, 20.89s/it] 59%|█████▉    | 3825/6500 [25:17:49<14:53:56, 20.05s/it]                                                          59%|█████▉    | 3825/6500 [25:17:49<14:53:56, 20.05s/it] 59%|█████▉    |{'loss': 0.26, 'learning_rate': 3.627855952498163e-05, 'epoch': 0.59}
{'loss': 0.2349, 'learning_rate': 3.6255315678603494e-05, 'epoch': 0.59}
{'loss': 0.2488, 'learning_rate': 3.6232075044961735e-05, 'epoch': 0.59}
{'loss': 0.2496, 'learning_rate': 3.620883762948873e-05, 'epoch': 0.59}
{'loss': 0.3233, 'learning_rate': 3.6185603437616065e-05, 'epoch': 0.59}
 3826/6500 [25:18:07<14:27:35, 19.47s/it]                                                          59%|█████▉    | 3826/6500 [25:18:07<14:27:35, 19.47s/it] 59%|█████▉    | 3827/6500 [25:18:25<14:09:16, 19.06s/it]                                                          59%|█████▉    | 3827/6500 [25:18:25<14:09:16, 19.06s/it] 59%|█████▉    | 3828/6500 [25:18:43<13:56:26, 18.78s/it]                                                          59%|█████▉    | 3828/6500 [25:18:43<13:56:26, 18.78s/it] 59%|█████▉    | 3829/6500 [25:19:01<13:47:44, 18.59s/it]                                                          59%|█████▉    | 3829/6500 [25:19:01<13:47:44, 18.59s/it] 59%|█████▉    | 3830/6500 [25:19:20<13:44:51, 18.54s/it]                                                          59%|█████▉    | 3830/6500 [25:19:20<13:44:51, 18.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8655617833137512, 'eval_runtime': 5.3585, 'eval_samples_per_second': 4.292, 'eval_steps_per_second': 1.12, 'epoch': 0.59}
                                                          59%|█████▉    | 3830/6500 [25:19:25<13:44:51, 18.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3830
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3830

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3830/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3830


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3830
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3830/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2444, 'learning_rate': 3.616237247477462e-05, 'epoch': 0.59}
{'loss': 0.2461, 'learning_rate': 3.6139144746394464e-05, 'epoch': 0.59}
{'loss': 0.2623, 'learning_rate': 3.6115920257904963e-05, 'epoch': 0.59}
{'loss': 0.7383, 'learning_rate': 3.609269901473467e-05, 'epoch': 0.59}
{'loss': 0.2617, 'learning_rate': 3.606948102231143e-05, 'epoch': 0.59}
 59%|█████▉    | 3831/6500 [25:20:48<29:12:59, 39.41s/it]                                                          59%|█████▉    | 3831/6500 [25:20:48<29:12:59, 39.41s/it] 59%|█████▉    | 3832/6500 [25:21:06<24:28:02, 33.01s/it]                                                          59%|█████▉    | 3832/6500 [25:21:06<24:28:02, 33.01s/it] 59%|█████▉    | 3833/6500 [25:21:24<21:06:46, 28.50s/it]                                                          59%|█████▉    | 3833/6500 [25:21:24<21:06:46, 28.50s/it] 59%|█████▉    | 3834/6500 [25:21:42<18:45:52, 25.34s/it]                                                          59%|█████▉    | 3834/6500 [25:21:42<18:45:52, 25.34s/it] 59%|█████▉    | 3835/6500 [25:22:00<17:07:55, 23.14s/it]                                                          59%|█████▉    | 3835/6500 [25:22:00<17:07:55, 23.14s/it] 59%|█████▉    |{'loss': 0.2481, 'learning_rate': 3.60462662860623e-05, 'epoch': 0.59}
{'loss': 0.2608, 'learning_rate': 3.6023054811413584e-05, 'epoch': 0.59}
{'loss': 0.2429, 'learning_rate': 3.599984660379084e-05, 'epoch': 0.59}
{'loss': 0.259, 'learning_rate': 3.5976641668618816e-05, 'epoch': 0.59}
{'loss': 0.2467, 'learning_rate': 3.595344001132154e-05, 'epoch': 0.59}
 3836/6500 [25:22:18<15:59:31, 21.61s/it]                                                          59%|█████▉    | 3836/6500 [25:22:18<15:59:31, 21.61s/it] 59%|█████▉    | 3837/6500 [25:22:36<15:11:58, 20.55s/it]                                                          59%|█████▉    | 3837/6500 [25:22:36<15:11:58, 20.55s/it] 59%|█████▉    | 3838/6500 [25:22:54<14:38:47, 19.81s/it]                                                          59%|█████▉    | 3838/6500 [25:22:54<14:38:47, 19.81s/it] 59%|█████▉    | 3839/6500 [25:23:12<14:16:04, 19.30s/it]                                                          59%|█████▉    | 3839/6500 [25:23:12<14:16:04, 19.30s/it] 59%|█████▉    | 3840/6500 [25:23:30<14:00:14, 18.95s/it]                                                          59%|█████▉    | 3840/6500 [25:23:30<14:00:14, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8769702911376953, 'eval_runtime': 5.3447, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.123, 'epoch': 0.59}
                                                          59%|█████▉    | 3840/6500 [25:23:36<14:00:14, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3840
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3840 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3840the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3840
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3840/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3840/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2263, 'learning_rate': 3.593024163732225e-05, 'epoch': 0.59}
{'loss': 0.2582, 'learning_rate': 3.5907046552043436e-05, 'epoch': 0.59}
{'loss': 0.2397, 'learning_rate': 3.588385476090681e-05, 'epoch': 0.59}
{'loss': 0.2606, 'learning_rate': 3.586066626933331e-05, 'epoch': 0.59}
{'loss': 0.2466, 'learning_rate': 3.583748108274309e-05, 'epoch': 0.59}
 59%|█████▉    | 3841/6500 [25:24:36<24:21:23, 32.98s/it]                                                          59%|█████▉    | 3841/6500 [25:24:36<24:21:23, 32.98s/it] 59%|█████▉    | 3842/6500 [25:24:54<21:02:37, 28.50s/it]                                                          59%|█████▉    | 3842/6500 [25:24:54<21:02:37, 28.50s/it] 59%|█████▉    | 3843/6500 [25:25:12<18:43:11, 25.36s/it]                                                          59%|█████▉    | 3843/6500 [25:25:12<18:43:11, 25.36s/it] 59%|█████▉    | 3844/6500 [25:25:30<17:05:50, 23.17s/it]                                                          59%|█████▉    | 3844/6500 [25:25:30<17:05:50, 23.17s/it] 59%|█████▉    | 3845/6500 [25:25:48<15:57:38, 21.64s/it]                                                          59%|█████▉    | 3845/6500 [25:25:48<15:57:38, 21.64s/it] 59%|█████▉    |{'loss': 0.2549, 'learning_rate': 3.5814299206555555e-05, 'epoch': 0.59}
{'loss': 0.2382, 'learning_rate': 3.579112064618934e-05, 'epoch': 0.59}
{'loss': 0.2636, 'learning_rate': 3.576794540706227e-05, 'epoch': 0.59}
{'loss': 0.2419, 'learning_rate': 3.5744773494591445e-05, 'epoch': 0.59}
{'loss': 0.2428, 'learning_rate': 3.5721604914193144e-05, 'epoch': 0.59}
 3846/6500 [25:26:06<15:10:17, 20.58s/it]                                                          59%|█████▉    | 3846/6500 [25:26:06<15:10:17, 20.58s/it] 59%|█████▉    | 3847/6500 [25:26:25<14:44:37, 20.01s/it]                                                          59%|█████▉    | 3847/6500 [25:26:25<14:44:37, 20.01s/it] 59%|█████▉    | 3848/6500 [25:26:43<14:19:31, 19.45s/it]                                                          59%|█████▉    | 3848/6500 [25:26:43<14:19:31, 19.45s/it] 59%|█████▉    | 3849/6500 [25:27:01<14:01:55, 19.06s/it]                                                          59%|█████▉    | 3849/6500 [25:27:01<14:01:55, 19.06s/it] 59%|█████▉    | 3850/6500 [25:27:20<13:50:57, 18.81s/it]                                                          59%|█████▉    | 3850/6500 [25:27:20<13:50:57, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8835396766662598, 'eval_runtime': 5.3275, 'eval_samples_per_second': 4.317, 'eval_steps_per_second': 1.126, 'epoch': 0.59}
                                                          59%|█████▉    | 3850/6500 [25:27:25<13:50:57, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3850
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3850the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3850
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3850

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3850
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3850/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3850/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2568, 'learning_rate': 3.569843967128287e-05, 'epoch': 0.59}
{'loss': 0.2448, 'learning_rate': 3.567527777127536e-05, 'epoch': 0.59}
{'loss': 0.2567, 'learning_rate': 3.5652119219584586e-05, 'epoch': 0.59}
{'loss': 0.2431, 'learning_rate': 3.56289640216237e-05, 'epoch': 0.59}
{'loss': 0.265, 'learning_rate': 3.5605812182805116e-05, 'epoch': 0.59}
 59%|█████▉    | 3851/6500 [25:28:44<28:20:47, 38.52s/it]                                                          59%|█████▉    | 3851/6500 [25:28:44<28:20:47, 38.52s/it] 59%|█████▉    | 3852/6500 [25:29:02<23:53:11, 32.47s/it]                                                          59%|█████▉    | 3852/6500 [25:29:02<23:53:11, 32.47s/it] 59%|█████▉    | 3853/6500 [25:29:20<20:41:18, 28.14s/it]                                                          59%|█████▉    | 3853/6500 [25:29:20<20:41:18, 28.14s/it] 59%|█████▉    | 3854/6500 [25:29:38<18:27:23, 25.11s/it]                                                          59%|█████▉    | 3854/6500 [25:29:38<18:27:23, 25.11s/it] 59%|█████▉    | 3855/6500 [25:29:57<16:53:35, 22.99s/it]                                                          59%|█████▉    | 3855/6500 [25:29:57<16:53:35, 22.99s/it] 59%|█████▉    |{'loss': 0.2482, 'learning_rate': 3.55826637085404e-05, 'epoch': 0.59}
{'loss': 0.2449, 'learning_rate': 3.5559518604240385e-05, 'epoch': 0.59}
{'loss': 0.2411, 'learning_rate': 3.5536376875315095e-05, 'epoch': 0.59}
{'loss': 0.2525, 'learning_rate': 3.551323852717378e-05, 'epoch': 0.59}
{'loss': 0.3, 'learning_rate': 3.5490103565224865e-05, 'epoch': 0.59}
 3856/6500 [25:30:15<15:48:13, 21.52s/it]                                                          59%|█████▉    | 3856/6500 [25:30:15<15:48:13, 21.52s/it] 59%|█████▉    | 3857/6500 [25:30:33<15:02:37, 20.49s/it]                                                          59%|█████▉    | 3857/6500 [25:30:33<15:02:37, 20.49s/it] 59%|█████▉    | 3858/6500 [25:30:51<14:30:52, 19.78s/it]                                                          59%|█████▉    | 3858/6500 [25:30:51<14:30:52, 19.78s/it] 59%|█████▉    | 3859/6500 [25:31:09<14:09:03, 19.29s/it]                                                          59%|█████▉    | 3859/6500 [25:31:09<14:09:03, 19.29s/it] 59%|█████▉    | 3860/6500 [25:31:27<13:53:50, 18.95s/it]                                                          59%|█████▉    | 3860/6500 [25:31:27<13:53:50, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8712893128395081, 'eval_runtime': 5.5516, 'eval_samples_per_second': 4.143, 'eval_steps_per_second': 1.081, 'epoch': 0.59}
                                                          59%|█████▉    | 3860/6500 [25:31:33<13:53:50, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3860
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3860the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3860
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3860
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3860/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2449, 'learning_rate': 3.546697199487603e-05, 'epoch': 0.59}
{'loss': 0.2381, 'learning_rate': 3.544384382153413e-05, 'epoch': 0.59}
{'loss': 0.264, 'learning_rate': 3.542071905060522e-05, 'epoch': 0.59}
{'loss': 0.7457, 'learning_rate': 3.5397597687494596e-05, 'epoch': 0.59}
{'loss': 0.261, 'learning_rate': 3.5374479737606733e-05, 'epoch': 0.59}
 59%|█████▉    | 3861/6500 [25:32:20<21:17:15, 29.04s/it]                                                          59%|█████▉    | 3861/6500 [25:32:20<21:17:15, 29.04s/it] 59%|█████▉    | 3862/6500 [25:32:38<18:51:34, 25.74s/it]                                                          59%|█████▉    | 3862/6500 [25:32:38<18:51:34, 25.74s/it] 59%|█████▉    | 3863/6500 [25:32:56<17:13:54, 23.52s/it]                                                          59%|█████▉    | 3863/6500 [25:32:56<17:13:54, 23.52s/it] 59%|█████▉    | 3864/6500 [25:33:14<16:01:56, 21.90s/it]                                                          59%|█████▉    | 3864/6500 [25:33:14<16:01:56, 21.90s/it] 59%|█████▉    | 3865/6500 [25:33:32<15:11:28, 20.75s/it]                                                          59%|█████▉    | 3865/6500 [25:33:32<15:11:28, 20.75s/it] 59%|█████▉    |{'loss': 0.2575, 'learning_rate': 3.535136520634531e-05, 'epoch': 0.59}
{'loss': 0.2487, 'learning_rate': 3.53282540991132e-05, 'epoch': 0.59}
{'loss': 0.2399, 'learning_rate': 3.530514642131249e-05, 'epoch': 0.6}
{'loss': 0.2556, 'learning_rate': 3.528204217834444e-05, 'epoch': 0.6}
{'loss': 0.2429, 'learning_rate': 3.5258941375609565e-05, 'epoch': 0.6}
 3866/6500 [25:33:50<14:36:16, 19.96s/it]                                                          59%|█████▉    | 3866/6500 [25:33:50<14:36:16, 19.96s/it] 59%|█████▉    | 3867/6500 [25:34:09<14:12:10, 19.42s/it]                                                          59%|█████▉    | 3867/6500 [25:34:09<14:12:10, 19.42s/it] 60%|█████▉    | 3868/6500 [25:34:27<13:54:51, 19.03s/it]                                                          60%|█████▉    | 3868/6500 [25:34:27<13:54:51, 19.03s/it] 60%|█████▉    | 3869/6500 [25:34:45<13:42:48, 18.76s/it]                                                          60%|█████▉    | 3869/6500 [25:34:45<13:42:48, 18.76s/it] 60%|█████▉    | 3870/6500 [25:35:03<13:38:43, 18.68s/it]                                                          60%|█████▉    | 3870/6500 [25:35:03<13:38:43, 18.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8819496035575867, 'eval_runtime': 5.3764, 'eval_samples_per_second': 4.278, 'eval_steps_per_second': 1.116, 'epoch': 0.6}
                                                          60%|█████▉    | 3870/6500 [25:35:09<13:38:43, 18.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3870
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3870
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3870/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3870


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3870
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3870/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2419, 'learning_rate': 3.523584401850751e-05, 'epoch': 0.6}
{'loss': 0.2559, 'learning_rate': 3.521275011243715e-05, 'epoch': 0.6}
{'loss': 0.2453, 'learning_rate': 3.5189659662796546e-05, 'epoch': 0.6}
{'loss': 0.2449, 'learning_rate': 3.5166572674982944e-05, 'epoch': 0.6}
{'loss': 0.2442, 'learning_rate': 3.5143489154392785e-05, 'epoch': 0.6}
 60%|█████▉    | 3871/6500 [25:36:00<22:02:39, 30.19s/it]                                                          60%|█████▉    | 3871/6500 [25:36:00<22:02:39, 30.19s/it] 60%|█████▉    | 3872/6500 [25:36:18<19:22:27, 26.54s/it]                                                          60%|█████▉    | 3872/6500 [25:36:18<19:22:27, 26.54s/it] 60%|█████▉    | 3873/6500 [25:36:36<17:30:08, 23.99s/it]                                                          60%|█████▉    | 3873/6500 [25:36:36<17:30:08, 23.99s/it] 60%|█████▉    | 3874/6500 [25:36:54<16:11:33, 22.20s/it]                                                          60%|█████▉    | 3874/6500 [25:36:54<16:11:33, 22.20s/it] 60%|█████▉    | 3875/6500 [25:37:12<15:16:48, 20.96s/it]                                                          60%|█████▉    | 3875/6500 [25:37:12<15:16:48, 20.96s/it] 60%|█████▉    |{'loss': 0.2612, 'learning_rate': 3.5120409106421716e-05, 'epoch': 0.6}
{'loss': 0.2501, 'learning_rate': 3.509733253646454e-05, 'epoch': 0.6}
{'loss': 0.2612, 'learning_rate': 3.5074259449915284e-05, 'epoch': 0.6}
{'loss': 0.2523, 'learning_rate': 3.505118985216713e-05, 'epoch': 0.6}
{'loss': 0.2547, 'learning_rate': 3.502812374861245e-05, 'epoch': 0.6}
 3876/6500 [25:37:31<14:38:48, 20.09s/it]                                                          60%|█████▉    | 3876/6500 [25:37:31<14:38:48, 20.09s/it] 60%|█████▉    | 3877/6500 [25:37:49<14:12:18, 19.50s/it]                                                          60%|█████▉    | 3877/6500 [25:37:49<14:12:18, 19.50s/it] 60%|█████▉    | 3878/6500 [25:38:07<13:53:39, 19.08s/it]                                                          60%|█████▉    | 3878/6500 [25:38:07<13:53:39, 19.08s/it] 60%|█████▉    | 3879/6500 [25:38:25<13:46:47, 18.93s/it]                                                          60%|█████▉    | 3879/6500 [25:38:25<13:46:47, 18.93s/it] 60%|█████▉    | 3880/6500 [25:38:43<13:35:59, 18.69s/it]                                                          60%|█████▉    | 3880/6500 [25:38:43<13:35:59, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.882910430431366, 'eval_runtime': 5.49, 'eval_samples_per_second': 4.189, 'eval_steps_per_second': 1.093, 'epoch': 0.6}
                                                          60%|█████▉    | 3880/6500 [25:38:49<13:35:59, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3880
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3880
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3880

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3880
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3880/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2562, 'learning_rate': 3.500506114464282e-05, 'epoch': 0.6}
{'loss': 0.2588, 'learning_rate': 3.498200204564897e-05, 'epoch': 0.6}
{'loss': 0.2566, 'learning_rate': 3.495894645702084e-05, 'epoch': 0.6}
{'loss': 0.2559, 'learning_rate': 3.4935894384147516e-05, 'epoch': 0.6}
{'loss': 0.2607, 'learning_rate': 3.49128458324173e-05, 'epoch': 0.6}
 60%|█████▉    | 3881/6500 [25:40:11<28:40:57, 39.43s/it]                                                          60%|█████▉    | 3881/6500 [25:40:11<28:40:57, 39.43s/it] 60%|█████▉    | 3882/6500 [25:40:29<23:59:13, 32.98s/it]                                                          60%|█████▉    | 3882/6500 [25:40:29<23:59:13, 32.98s/it] 60%|█████▉    | 3883/6500 [25:40:47<20:41:59, 28.48s/it]                                                          60%|█████▉    | 3883/6500 [25:40:47<20:41:59, 28.48s/it] 60%|█████▉    | 3884/6500 [25:41:05<18:24:30, 25.33s/it]                                                          60%|█████▉    | 3884/6500 [25:41:05<18:24:30, 25.33s/it] 60%|█████▉    | 3885/6500 [25:41:23<16:48:27, 23.14s/it]                                                          60%|█████▉    | 3885/6500 [25:41:23<16:48:27, 23.14s/it] 60%|█████▉    |{'loss': 0.2361, 'learning_rate': 3.488980080721762e-05, 'epoch': 0.6}
{'loss': 0.2433, 'learning_rate': 3.486675931393514e-05, 'epoch': 0.6}
{'loss': 0.2325, 'learning_rate': 3.484372135795566e-05, 'epoch': 0.6}
{'loss': 0.2719, 'learning_rate': 3.482068694466417e-05, 'epoch': 0.6}
{'loss': 0.3029, 'learning_rate': 3.4797656079444806e-05, 'epoch': 0.6}
 3886/6500 [25:41:41<15:41:27, 21.61s/it]                                                          60%|█████▉    | 3886/6500 [25:41:41<15:41:27, 21.61s/it] 60%|█████▉    | 3887/6500 [25:41:59<14:54:49, 20.55s/it]                                                          60%|█████▉    | 3887/6500 [25:41:59<14:54:49, 20.55s/it] 60%|█████▉    | 3888/6500 [25:42:17<14:22:20, 19.81s/it]                                                          60%|█████▉    | 3888/6500 [25:42:17<14:22:20, 19.81s/it] 60%|█████▉    | 3889/6500 [25:42:36<13:59:49, 19.30s/it]                                                          60%|█████▉    | 3889/6500 [25:42:36<13:59:49, 19.30s/it] 60%|█████▉    | 3890/6500 [25:42:54<13:44:11, 18.95s/it]                                                          60%|█████▉    | 3890/6500 [25:42:54<13:44:11, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.872920572757721, 'eval_runtime': 5.4461, 'eval_samples_per_second': 4.223, 'eval_steps_per_second': 1.102, 'epoch': 0.6}
                                                          60%|█████▉    | 3890/6500 [25:42:59<13:44:11, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3890
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3890
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3890
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3890/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3890/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2535, 'learning_rate': 3.47746287676809e-05, 'epoch': 0.6}
{'loss': 0.2616, 'learning_rate': 3.475160501475495e-05, 'epoch': 0.6}
{'loss': 0.2579, 'learning_rate': 3.472858482604861e-05, 'epoch': 0.6}
{'loss': 0.7625, 'learning_rate': 3.4705568206942706e-05, 'epoch': 0.6}
{'loss': 0.2491, 'learning_rate': 3.468255516281725e-05, 'epoch': 0.6}
 60%|█████▉    | 3891/6500 [25:44:04<24:57:37, 34.44s/it]                                                          60%|█████▉    | 3891/6500 [25:44:04<24:57:37, 34.44s/it] 60%|█████▉    | 3892/6500 [25:44:22<21:22:35, 29.51s/it]                                                          60%|█████▉    | 3892/6500 [25:44:22<21:22:35, 29.51s/it] 60%|█████▉    | 3893/6500 [25:44:40<18:51:55, 26.05s/it]                                                          60%|█████▉    | 3893/6500 [25:44:40<18:51:55, 26.05s/it] 60%|█████▉    | 3894/6500 [25:44:58<17:06:46, 23.64s/it]                                                          60%|█████▉    | 3894/6500 [25:44:58<17:06:46, 23.64s/it] 60%|█████▉    | 3895/6500 [25:45:17<16:00:41, 22.13s/it]                                                          60%|█████▉    | 3895/6500 [25:45:17<16:00:41, 22.13s/it] 60%|█████▉    |{'loss': 0.2559, 'learning_rate': 3.465954569905141e-05, 'epoch': 0.6}
{'loss': 0.2311, 'learning_rate': 3.463653982102347e-05, 'epoch': 0.6}
{'loss': 0.2522, 'learning_rate': 3.461353753411096e-05, 'epoch': 0.6}
{'loss': 0.2453, 'learning_rate': 3.4590538843690485e-05, 'epoch': 0.6}
{'loss': 0.2232, 'learning_rate': 3.456754375513786e-05, 'epoch': 0.6}
 3896/6500 [25:45:35<15:07:07, 20.90s/it]                                                          60%|█████▉    | 3896/6500 [25:45:35<15:07:07, 20.90s/it] 60%|█████▉    | 3897/6500 [25:45:53<14:29:58, 20.05s/it]                                                          60%|█████▉    | 3897/6500 [25:45:53<14:29:58, 20.05s/it] 60%|█████▉    | 3898/6500 [25:46:11<14:03:52, 19.46s/it]                                                          60%|█████▉    | 3898/6500 [25:46:11<14:03:52, 19.46s/it] 60%|█████▉    | 3899/6500 [25:46:29<13:45:23, 19.04s/it]                                                          60%|█████▉    | 3899/6500 [25:46:29<13:45:23, 19.04s/it] 60%|██████    | 3900/6500 [25:46:47<13:32:34, 18.75s/it]                                                          60%|██████    | 3900/6500 [25:46:47<13:32:34, 18.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8767615556716919, 'eval_runtime': 5.3313, 'eval_samples_per_second': 4.314, 'eval_steps_per_second': 1.125, 'epoch': 0.6}
                                                          60%|██████    | 3900/6500 [25:46:52<13:32:34, 18.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3900
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3900


the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3900
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3900
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3900/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3900/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2536, 'learning_rate': 3.4544552273828056e-05, 'epoch': 0.6}
{'loss': 0.2369, 'learning_rate': 3.452156440513519e-05, 'epoch': 0.6}
{'loss': 0.2485, 'learning_rate': 3.449858015443254e-05, 'epoch': 0.6}
{'loss': 0.2509, 'learning_rate': 3.447559952709252e-05, 'epoch': 0.6}
{'loss': 0.2512, 'learning_rate': 3.445262252848672e-05, 'epoch': 0.6}
 60%|██████    | 3901/6500 [25:48:06<26:34:48, 36.82s/it]                                                          60%|██████    | 3901/6500 [25:48:06<26:34:48, 36.82s/it] 60%|██████    | 3902/6500 [25:48:24<22:29:39, 31.17s/it]                                                          60%|██████    | 3902/6500 [25:48:24<22:29:39, 31.17s/it] 60%|██████    | 3903/6500 [25:48:42<19:37:50, 27.21s/it]                                                          60%|██████    | 3903/6500 [25:48:42<19:37:50, 27.21s/it] 60%|██████    | 3904/6500 [25:49:01<17:48:47, 24.70s/it]                                                          60%|██████    | 3904/6500 [25:49:01<17:48:47, 24.70s/it] 60%|██████    | 3905/6500 [25:49:19<16:22:09, 22.71s/it]                                                          60%|██████    | 3905/6500 [25:49:19<16:22:09, 22.71s/it] 60%|██████    |{'loss': 0.2446, 'learning_rate': 3.442964916398588e-05, 'epoch': 0.6}
{'loss': 0.2488, 'learning_rate': 3.440667943895986e-05, 'epoch': 0.6}
{'loss': 0.2602, 'learning_rate': 3.4383713358777735e-05, 'epoch': 0.6}
{'loss': 0.2412, 'learning_rate': 3.4360750928807664e-05, 'epoch': 0.6}
{'loss': 0.246, 'learning_rate': 3.4337792154416966e-05, 'epoch': 0.6}
 3906/6500 [25:49:37<15:20:59, 21.30s/it]                                                          60%|██████    | 3906/6500 [25:49:37<15:20:59, 21.30s/it] 60%|██████    | 3907/6500 [25:49:55<14:38:30, 20.33s/it]                                                          60%|██████    | 3907/6500 [25:49:55<14:38:30, 20.33s/it] 60%|██████    | 3908/6500 [25:50:13<14:09:16, 19.66s/it]                                                          60%|██████    | 3908/6500 [25:50:13<14:09:16, 19.66s/it] 60%|██████    | 3909/6500 [25:50:31<13:48:39, 19.19s/it]                                                          60%|██████    | 3909/6500 [25:50:31<13:48:39, 19.19s/it] 60%|██████    | 3910/6500 [25:50:49<13:34:13, 18.86s/it]                                                          60%|██████    | 3910/6500 [25:50:49<13:34:13, 18.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8856194615364075, 'eval_runtime': 5.4789, 'eval_samples_per_second': 4.198, 'eval_steps_per_second': 1.095, 'epoch': 0.6}
                                                          60%|██████    | 3910/6500 [25:50:55<13:34:13, 18.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3910
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3910
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3910

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3910
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3910/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2596, 'learning_rate': 3.431483704097212e-05, 'epoch': 0.6}
{'loss': 0.2443, 'learning_rate': 3.4291885593838755e-05, 'epoch': 0.6}
{'loss': 0.2512, 'learning_rate': 3.426893781838162e-05, 'epoch': 0.6}
{'loss': 0.2646, 'learning_rate': 3.4245993719964634e-05, 'epoch': 0.6}
{'loss': 0.2464, 'learning_rate': 3.4223053303950827e-05, 'epoch': 0.6}
 60%|██████    | 3911/6500 [25:52:14<27:46:34, 38.62s/it]                                                          60%|██████    | 3911/6500 [25:52:14<27:46:34, 38.62s/it] 60%|██████    | 3912/6500 [25:52:32<23:19:16, 32.44s/it]                                                          60%|██████    | 3912/6500 [25:52:32<23:19:16, 32.44s/it] 60%|██████    | 3913/6500 [25:52:50<20:12:20, 28.12s/it]                                                          60%|██████    | 3913/6500 [25:52:50<20:12:20, 28.12s/it] 60%|██████    | 3914/6500 [25:53:08<18:01:56, 25.10s/it]                                                          60%|██████    | 3914/6500 [25:53:08<18:01:56, 25.10s/it] 60%|██████    | 3915/6500 [25:53:26<16:30:51, 23.00s/it]                                                          60%|██████    | 3915/6500 [25:53:26<16:30:51, 23.00s/it] 60%|██████    |{'loss': 0.2406, 'learning_rate': 3.420011657570238e-05, 'epoch': 0.6}
{'loss': 0.2391, 'learning_rate': 3.417718354058062e-05, 'epoch': 0.6}
{'loss': 0.2479, 'learning_rate': 3.4154254203946e-05, 'epoch': 0.6}
{'loss': 0.3189, 'learning_rate': 3.413132857115812e-05, 'epoch': 0.6}
{'loss': 0.2348, 'learning_rate': 3.4108406647575706e-05, 'epoch': 0.6}
 3916/6500 [25:53:44<15:27:04, 21.53s/it]                                                          60%|██████    | 3916/6500 [25:53:44<15:27:04, 21.53s/it] 60%|██████    | 3917/6500 [25:54:03<14:42:47, 20.51s/it]                                                          60%|██████    | 3917/6500 [25:54:03<14:42:47, 20.51s/it] 60%|██████    | 3918/6500 [25:54:21<14:12:05, 19.80s/it]                                                          60%|██████    | 3918/6500 [25:54:21<14:12:05, 19.80s/it] 60%|██████    | 3919/6500 [25:54:39<13:50:41, 19.31s/it]                                                          60%|██████    | 3919/6500 [25:54:39<13:50:41, 19.31s/it] 60%|██████    | 3920/6500 [25:54:58<13:45:33, 19.20s/it]                                                          60%|██████    | 3920/6500 [25:54:58<13:45:33, 19.20s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.874692976474762, 'eval_runtime': 6.1936, 'eval_samples_per_second': 3.714, 'eval_steps_per_second': 0.969, 'epoch': 0.6}
                                                          60%|██████    | 3920/6500 [25:55:04<13:45:33, 19.20s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3920
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3920
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3920
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3920
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3920/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3920/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2366, 'learning_rate': 3.408548843855661e-05, 'epoch': 0.6}
{'loss': 0.2627, 'learning_rate': 3.406257394945783e-05, 'epoch': 0.6}
{'loss': 0.7506, 'learning_rate': 3.403966318563549e-05, 'epoch': 0.6}
{'loss': 0.2515, 'learning_rate': 3.401675615244483e-05, 'epoch': 0.6}
{'loss': 0.2421, 'learning_rate': 3.399385285524026e-05, 'epoch': 0.6}
 60%|██████    | 3921/6500 [25:56:15<26:11:50, 36.57s/it]                                                          60%|██████    | 3921/6500 [25:56:15<26:11:50, 36.57s/it] 60%|██████    | 3922/6500 [25:56:33<22:12:22, 31.01s/it]                                                          60%|██████    | 3922/6500 [25:56:33<22:12:22, 31.01s/it] 60%|██████    | 3923/6500 [25:56:51<19:24:51, 27.12s/it]                                                          60%|██████    | 3923/6500 [25:56:51<19:24:51, 27.12s/it] 60%|██████    | 3924/6500 [25:57:09<17:29:42, 24.45s/it]                                                          60%|██████    | 3924/6500 [25:57:09<17:29:42, 24.45s/it] 60%|██████    | 3925/6500 [25:57:27<16:08:02, 22.56s/it]                                                          60%|██████    | 3925/6500 [25:57:27<16:08:02, 22.56s/it] 60%|██████    |{'loss': 0.2486, 'learning_rate': 3.397095329937526e-05, 'epoch': 0.6}
{'loss': 0.2363, 'learning_rate': 3.394805749020246e-05, 'epoch': 0.6}
{'loss': 0.2519, 'learning_rate': 3.3925165433073624e-05, 'epoch': 0.6}
{'loss': 0.2318, 'learning_rate': 3.3902277133339635e-05, 'epoch': 0.6}
{'loss': 0.2294, 'learning_rate': 3.387939259635049e-05, 'epoch': 0.6}
 3926/6500 [25:57:45<15:10:04, 21.21s/it]                                                          60%|██████    | 3926/6500 [25:57:45<15:10:04, 21.21s/it] 60%|██████    | 3927/6500 [25:58:04<14:35:14, 20.41s/it]                                                          60%|██████    | 3927/6500 [25:58:04<14:35:14, 20.41s/it] 60%|██████    | 3928/6500 [25:58:22<14:05:20, 19.72s/it]                                                          60%|██████    | 3928/6500 [25:58:22<14:05:20, 19.72s/it] 60%|██████    | 3929/6500 [25:58:40<13:44:38, 19.24s/it]                                                          60%|██████    | 3929/6500 [25:58:40<13:44:38, 19.24s/it] 60%|██████    | 3930/6500 [25:58:58<13:30:17, 18.92s/it]                                                          60%|██████    | 3930/6500 [25:58:58<13:30:17, 18.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8886585235595703, 'eval_runtime': 5.8282, 'eval_samples_per_second': 3.946, 'eval_steps_per_second': 1.029, 'epoch': 0.6}
                                                          60%|██████    | 3930/6500 [25:59:04<13:30:17, 18.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3930
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3930

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3930
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3930

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3930
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3930/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3930/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2514, 'learning_rate': 3.385651182745532e-05, 'epoch': 0.6}
{'loss': 0.2355, 'learning_rate': 3.383363483200235e-05, 'epoch': 0.6}
{'loss': 0.2521, 'learning_rate': 3.3810761615338934e-05, 'epoch': 0.61}
{'loss': 0.2424, 'learning_rate': 3.3787892182811564e-05, 'epoch': 0.61}
{'loss': 0.2553, 'learning_rate': 3.3765026539765834e-05, 'epoch': 0.61}
 60%|██████    | 3931/6500 [26:00:15<25:52:44, 36.26s/it]                                                          60%|██████    | 3931/6500 [26:00:15<25:52:44, 36.26s/it] 60%|██████    | 3932/6500 [26:00:33<21:57:28, 30.78s/it]                                                          60%|██████    | 3932/6500 [26:00:33<21:57:28, 30.78s/it] 61%|██████    | 3933/6500 [26:00:51<19:14:08, 26.98s/it]                                                          61%|██████    | 3933/6500 [26:00:51<19:14:08, 26.98s/it] 61%|██████    | 3934/6500 [26:01:09<17:18:46, 24.29s/it]                                                          61%|██████    | 3934/6500 [26:01:09<17:18:46, 24.29s/it] 61%|██████    | 3935/6500 [26:01:27<15:58:21, 22.42s/it]                                                          61%|██████    | 3935/6500 [26:01:27<15:58:21, 22.42s/it] 61%|██████    |{'loss': 0.2443, 'learning_rate': 3.374216469154643e-05, 'epoch': 0.61}
{'loss': 0.2524, 'learning_rate': 3.371930664349719e-05, 'epoch': 0.61}
{'loss': 0.2395, 'learning_rate': 3.3696452400961023e-05, 'epoch': 0.61}
{'loss': 0.2527, 'learning_rate': 3.3673601969279986e-05, 'epoch': 0.61}
{'loss': 0.2493, 'learning_rate': 3.3650755353795216e-05, 'epoch': 0.61}
 3936/6500 [26:01:45<15:02:10, 21.11s/it]                                                          61%|██████    | 3936/6500 [26:01:45<15:02:10, 21.11s/it] 61%|██████    | 3937/6500 [26:02:03<14:23:09, 20.21s/it]                                                          61%|██████    | 3937/6500 [26:02:03<14:23:09, 20.21s/it] 61%|██████    | 3938/6500 [26:02:22<13:56:04, 19.58s/it]                                                          61%|██████    | 3938/6500 [26:02:22<13:56:04, 19.58s/it] 61%|██████    | 3939/6500 [26:02:40<13:37:16, 19.15s/it]                                                          61%|██████    | 3939/6500 [26:02:40<13:37:16, 19.15s/it] 61%|██████    | 3940/6500 [26:02:58<13:24:02, 18.84s/it]                                                          61%|██████    | 3940/6500 [26:02:58<13:24:02, 18.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8852406144142151, 'eval_runtime': 5.3446, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.123, 'epoch': 0.61}
                                                          61%|██████    | 3940/6500 [26:03:03<13:24:02, 18.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3940
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3940 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3940
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3940
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3940/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3940/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2527, 'learning_rate': 3.3627912559846985e-05, 'epoch': 0.61}
{'loss': 0.2557, 'learning_rate': 3.360507359277466e-05, 'epoch': 0.61}
{'loss': 0.2563, 'learning_rate': 3.358223845791668e-05, 'epoch': 0.61}
{'loss': 0.259, 'learning_rate': 3.355940716061064e-05, 'epoch': 0.61}
{'loss': 0.2381, 'learning_rate': 3.3536579706193224e-05, 'epoch': 0.61}
 61%|██████    | 3941/6500 [26:04:21<27:09:58, 38.22s/it]                                                          61%|██████    | 3941/6500 [26:04:21<27:09:58, 38.22s/it] 61%|██████    | 3942/6500 [26:04:39<22:50:18, 32.14s/it]                                                          61%|██████    | 3942/6500 [26:04:39<22:50:18, 32.14s/it] 61%|██████    | 3943/6500 [26:04:57<19:48:34, 27.89s/it]                                                          61%|██████    | 3943/6500 [26:04:57<19:48:34, 27.89s/it] 61%|██████    | 3944/6500 [26:05:15<17:45:17, 25.01s/it]                                                          61%|██████    | 3944/6500 [26:05:15<17:45:17, 25.01s/it] 61%|██████    | 3945/6500 [26:05:33<16:15:55, 22.92s/it]                                                          61%|██████    | 3945/6500 [26:05:33<16:15:55, 22.92s/it] 61%|██████    |{'loss': 0.2318, 'learning_rate': 3.351375610000019e-05, 'epoch': 0.61}
{'loss': 0.2298, 'learning_rate': 3.349093634736644e-05, 'epoch': 0.61}
{'loss': 0.2596, 'learning_rate': 3.346812045362595e-05, 'epoch': 0.61}
{'loss': 0.3019, 'learning_rate': 3.344530842411178e-05, 'epoch': 0.61}
{'loss': 0.2401, 'learning_rate': 3.342250026415611e-05, 'epoch': 0.61}
 3946/6500 [26:05:52<15:13:19, 21.46s/it]                                                          61%|██████    | 3946/6500 [26:05:52<15:13:19, 21.46s/it] 61%|██████    | 3947/6500 [26:06:10<14:29:34, 20.44s/it]                                                          61%|██████    | 3947/6500 [26:06:10<14:29:34, 20.44s/it] 61%|██████    | 3948/6500 [26:06:28<13:59:16, 19.73s/it]                                                          61%|██████    | 3948/6500 [26:06:28<13:59:16, 19.73s/it] 61%|██████    | 3949/6500 [26:06:46<13:38:29, 19.25s/it]                                                          61%|██████    | 3949/6500 [26:06:46<13:38:29, 19.25s/it] 61%|██████    | 3950/6500 [26:07:04<13:24:02, 18.92s/it]                                                          61%|██████    | 3950/6500 [26:07:04<13:24:02, 18.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8739980459213257, 'eval_runtime': 5.5043, 'eval_samples_per_second': 4.179, 'eval_steps_per_second': 1.09, 'epoch': 0.61}
                                                          61%|██████    | 3950/6500 [26:07:09<13:24:02, 18.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3950
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3950/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3950/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3950



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3950
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3950/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3950/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.237, 'learning_rate': 3.339969597909021e-05, 'epoch': 0.61}
{'loss': 0.2556, 'learning_rate': 3.337689557424445e-05, 'epoch': 0.61}
{'loss': 0.7411, 'learning_rate': 3.335409905494828e-05, 'epoch': 0.61}
{'loss': 0.2593, 'learning_rate': 3.333130642653024e-05, 'epoch': 0.61}
{'loss': 0.2444, 'learning_rate': 3.330851769431798e-05, 'epoch': 0.61}
 61%|██████    | 3951/6500 [26:08:00<21:19:28, 30.12s/it]                                                          61%|██████    | 3951/6500 [26:08:00<21:19:28, 30.12s/it] 61%|██████    | 3952/6500 [26:08:18<18:45:28, 26.50s/it]                                                          61%|██████    | 3952/6500 [26:08:18<18:45:28, 26.50s/it] 61%|██████    | 3953/6500 [26:08:36<16:57:01, 23.96s/it]                                                          61%|██████    | 3953/6500 [26:08:36<16:57:01, 23.96s/it] 61%|██████    | 3954/6500 [26:08:54<15:41:44, 22.19s/it]                                                          61%|██████    | 3954/6500 [26:08:54<15:41:44, 22.19s/it] 61%|██████    | 3955/6500 [26:09:12<14:48:58, 20.96s/it]                                                          61%|██████    | 3955/6500 [26:09:12<14:48:58, 20.96s/it] 61%|██████    |{'loss': 0.2376, 'learning_rate': 3.3285732863638215e-05, 'epoch': 0.61}
{'loss': 0.239, 'learning_rate': 3.326295193981677e-05, 'epoch': 0.61}
{'loss': 0.2571, 'learning_rate': 3.3240174928178544e-05, 'epoch': 0.61}
{'loss': 0.2291, 'learning_rate': 3.321740183404755e-05, 'epoch': 0.61}
{'loss': 0.25, 'learning_rate': 3.319463266274682e-05, 'epoch': 0.61}
 3956/6500 [26:09:30<14:11:47, 20.09s/it]                                                          61%|██████    | 3956/6500 [26:09:30<14:11:47, 20.09s/it] 61%|██████    | 3957/6500 [26:09:49<13:46:03, 19.49s/it]                                                          61%|██████    | 3957/6500 [26:09:49<13:46:03, 19.49s/it] 61%|██████    | 3958/6500 [26:10:07<13:28:13, 19.08s/it]                                                          61%|██████    | 3958/6500 [26:10:07<13:28:13, 19.08s/it] 61%|██████    | 3959/6500 [26:10:25<13:16:01, 18.80s/it]                                                          61%|██████    | 3959/6500 [26:10:25<13:16:01, 18.80s/it] 61%|██████    | 3960/6500 [26:10:43<13:12:52, 18.73s/it]                                                          61%|██████    | 3960/6500 [26:10:43<13:12:52, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8826684355735779, 'eval_runtime': 5.3346, 'eval_samples_per_second': 4.312, 'eval_steps_per_second': 1.125, 'epoch': 0.61}
                                                          61%|██████    | 3960/6500 [26:10:49<13:12:52, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3960
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3960

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3960
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3960

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3960
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3960/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3960/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2459, 'learning_rate': 3.317186741959852e-05, 'epoch': 0.61}
{'loss': 0.253, 'learning_rate': 3.31491061099239e-05, 'epoch': 0.61}
{'loss': 0.2565, 'learning_rate': 3.312634873904327e-05, 'epoch': 0.61}
{'loss': 0.2525, 'learning_rate': 3.3103595312276035e-05, 'epoch': 0.61}
{'loss': 0.2589, 'learning_rate': 3.3080845834940664e-05, 'epoch': 0.61}
 61%|██████    | 3961/6500 [26:11:55<24:22:14, 34.55s/it]                                                          61%|██████    | 3961/6500 [26:11:55<24:22:14, 34.55s/it] 61%|██████    | 3962/6500 [26:12:13<20:51:37, 29.59s/it]                                                          61%|██████    | 3962/6500 [26:12:13<20:51:37, 29.59s/it] 61%|██████    | 3963/6500 [26:12:31<18:26:11, 26.16s/it]                                                          61%|██████    | 3963/6500 [26:12:31<18:26:11, 26.16s/it] 61%|██████    | 3964/6500 [26:12:49<16:42:44, 23.72s/it]                                                          61%|██████    | 3964/6500 [26:12:49<16:42:44, 23.72s/it] 61%|██████    | 3965/6500 [26:13:07<15:30:38, 22.03s/it]                                                          61%|██████    | 3965/6500 [26:13:07<15:30:38, 22.03s/it] 61%|██████    |{'loss': 0.2524, 'learning_rate': 3.305810031235471e-05, 'epoch': 0.61}
{'loss': 0.2595, 'learning_rate': 3.303535874983479e-05, 'epoch': 0.61}
{'loss': 0.2452, 'learning_rate': 3.301262115269662e-05, 'epoch': 0.61}
{'loss': 0.2477, 'learning_rate': 3.298988752625496e-05, 'epoch': 0.61}
{'loss': 0.2605, 'learning_rate': 3.296715787582367e-05, 'epoch': 0.61}
 3966/6500 [26:13:25<14:40:21, 20.85s/it]                                                          61%|██████    | 3966/6500 [26:13:25<14:40:21, 20.85s/it] 61%|██████    | 3967/6500 [26:13:43<14:05:37, 20.03s/it]                                                          61%|██████    | 3967/6500 [26:13:43<14:05:37, 20.03s/it] 61%|██████    | 3968/6500 [26:14:04<14:13:47, 20.23s/it]                                                          61%|██████    | 3968/6500 [26:14:04<14:13:47, 20.23s/it] 61%|██████    | 3969/6500 [26:14:22<13:47:18, 19.61s/it]                                                          61%|██████    | 3969/6500 [26:14:22<13:47:18, 19.61s/it] 61%|██████    | 3970/6500 [26:14:40<13:28:04, 19.16s/it]                                                          61%|██████    | 3970/6500 [26:14:40<13:28:04, 19.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8824381828308105, 'eval_runtime': 5.4002, 'eval_samples_per_second': 4.259, 'eval_steps_per_second': 1.111, 'epoch': 0.61}
                                                          61%|██████    | 3970/6500 [26:14:46<13:28:04, 19.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3970
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3970/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3970/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3970



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3970
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3970/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3970/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2501, 'learning_rate': 3.2944432206715684e-05, 'epoch': 0.61}
{'loss': 0.2554, 'learning_rate': 3.2921710524242956e-05, 'epoch': 0.61}
{'loss': 0.2471, 'learning_rate': 3.289899283371657e-05, 'epoch': 0.61}
{'loss': 0.2611, 'learning_rate': 3.287627914044662e-05, 'epoch': 0.61}
{'loss': 0.2379, 'learning_rate': 3.28535694497423e-05, 'epoch': 0.61}
 61%|██████    | 3971/6500 [26:15:36<21:14:12, 30.23s/it]                                                          61%|██████    | 3971/6500 [26:15:36<21:14:12, 30.23s/it] 61%|██████    | 3972/6500 [26:15:54<18:39:02, 26.56s/it]                                                          61%|██████    | 3972/6500 [26:15:54<18:39:02, 26.56s/it] 61%|██████    | 3973/6500 [26:16:12<16:50:15, 23.99s/it]                                                          61%|██████    | 3973/6500 [26:16:12<16:50:15, 23.99s/it] 61%|██████    | 3974/6500 [26:16:30<15:34:08, 22.19s/it]                                                          61%|██████    | 3974/6500 [26:16:30<15:34:08, 22.19s/it] 61%|██████    | 3975/6500 [26:16:48<14:41:21, 20.94s/it]                                                          61%|██████    | 3975/6500 [26:16:48<14:41:21, 20.94s/it] 61%|██████    |{'loss': 0.2427, 'learning_rate': 3.283086376691188e-05, 'epoch': 0.61}
{'loss': 0.2272, 'learning_rate': 3.2808162097262664e-05, 'epoch': 0.61}
{'loss': 0.2674, 'learning_rate': 3.278546444610103e-05, 'epoch': 0.61}
{'loss': 0.2843, 'learning_rate': 3.276277081873243e-05, 'epoch': 0.61}
{'loss': 0.2356, 'learning_rate': 3.274008122046132e-05, 'epoch': 0.61}
 3976/6500 [26:17:07<14:07:42, 20.15s/it]                                                          61%|██████    | 3976/6500 [26:17:07<14:07:42, 20.15s/it] 61%|██████    | 3977/6500 [26:17:25<13:40:54, 19.52s/it]                                                          61%|██████    | 3977/6500 [26:17:25<13:40:54, 19.52s/it] 61%|██████    | 3978/6500 [26:17:43<13:22:25, 19.09s/it]                                                          61%|██████    | 3978/6500 [26:17:43<13:22:25, 19.09s/it] 61%|██████    | 3979/6500 [26:18:01<13:09:46, 18.80s/it]                                                          61%|██████    | 3979/6500 [26:18:01<13:09:46, 18.80s/it] 61%|██████    | 3980/6500 [26:18:19<13:01:06, 18.60s/it]                                                          61%|██████    | 3980/6500 [26:18:19<13:01:06, 18.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8757579922676086, 'eval_runtime': 5.3449, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.123, 'epoch': 0.61}
                                                          61%|██████    | 3980/6500 [26:18:24<13:01:06, 18.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3980
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3980
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3980 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3980

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3980
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3980/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2539, 'learning_rate': 3.271739565659129e-05, 'epoch': 0.61}
{'loss': 0.4178, 'learning_rate': 3.269471413242495e-05, 'epoch': 0.61}
{'loss': 0.5792, 'learning_rate': 3.267203665326396e-05, 'epoch': 0.61}
{'loss': 0.2428, 'learning_rate': 3.264936322440905e-05, 'epoch': 0.61}
{'loss': 0.2499, 'learning_rate': 3.262669385116001e-05, 'epoch': 0.61}
 61%|██████    | 3981/6500 [26:19:15<20:56:34, 29.93s/it]                                                          61%|██████    | 3981/6500 [26:19:15<20:56:34, 29.93s/it] 61%|██████▏   | 3982/6500 [26:19:33<18:26:00, 26.35s/it]                                                          61%|██████▏   | 3982/6500 [26:19:33<18:26:00, 26.35s/it] 61%|██████▏   | 3983/6500 [26:19:51<16:40:38, 23.85s/it]                                                          61%|██████▏   | 3983/6500 [26:19:51<16:40:38, 23.85s/it] 61%|██████▏   | 3984/6500 [26:20:10<15:27:53, 22.13s/it]                                                          61%|██████▏   | 3984/6500 [26:20:10<15:27:53, 22.13s/it] 61%|██████▏   | 3985/6500 [26:20:28<14:41:03, 21.02s/it]                                                          61%|██████▏   | 3985/6500 [26:20:28<14:41:03, 21.02s/it] 61%|██{'loss': 0.2326, 'learning_rate': 3.260402853881562e-05, 'epoch': 0.61}
{'loss': 0.2439, 'learning_rate': 3.2581367292673806e-05, 'epoch': 0.61}
{'loss': 0.2416, 'learning_rate': 3.255871011803148e-05, 'epoch': 0.61}
{'loss': 0.214, 'learning_rate': 3.253605702018461e-05, 'epoch': 0.61}
{'loss': 0.2553, 'learning_rate': 3.251340800442825e-05, 'epoch': 0.61}
███▏   | 3986/6500 [26:20:46<14:04:46, 20.16s/it]                                                          61%|██████▏   | 3986/6500 [26:20:46<14:04:46, 20.16s/it] 61%|██████▏   | 3987/6500 [26:21:04<13:38:19, 19.54s/it]                                                          61%|██████▏   | 3987/6500 [26:21:04<13:38:19, 19.54s/it] 61%|██████▏   | 3988/6500 [26:21:22<13:20:14, 19.11s/it]                                                          61%|██████▏   | 3988/6500 [26:21:22<13:20:14, 19.11s/it] 61%|██████▏   | 3989/6500 [26:21:41<13:07:34, 18.82s/it]                                                          61%|██████▏   | 3989/6500 [26:21:41<13:07:34, 18.82s/it] 61%|██████▏   | 3990/6500 [26:21:59<12:58:46, 18.62s/it]                                                          61%|██████▏   | 3990/6500 [26:21:59<12:58:46, 18.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8884784579277039, 'eval_runtime': 5.913, 'eval_samples_per_second': 3.89, 'eval_steps_per_second': 1.015, 'epoch': 0.61}
                                                          61%|██████▏   | 3990/6500 [26:22:05<12:58:46, 18.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-3990
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3990

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3990
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3990

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3990
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3990/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-3990/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2245, 'learning_rate': 3.249076307605643e-05, 'epoch': 0.61}
{'loss': 0.2485, 'learning_rate': 3.2468122240362284e-05, 'epoch': 0.61}
{'loss': 0.2495, 'learning_rate': 3.2445485502637976e-05, 'epoch': 0.61}
{'loss': 0.2543, 'learning_rate': 3.242285286817469e-05, 'epoch': 0.61}
{'loss': 0.2436, 'learning_rate': 3.240022434226268e-05, 'epoch': 0.61}
 61%|██████▏   | 3991/6500 [26:23:31<28:23:06, 40.73s/it]                                                          61%|██████▏   | 3991/6500 [26:23:31<28:23:06, 40.73s/it] 61%|██████▏   | 3992/6500 [26:23:50<23:46:12, 34.12s/it]                                                          61%|██████▏   | 3992/6500 [26:23:50<23:46:12, 34.12s/it] 61%|██████▏   | 3993/6500 [26:24:08<20:23:20, 29.28s/it]                                                          61%|██████▏   | 3993/6500 [26:24:08<20:23:20, 29.28s/it] 61%|██████▏   | 3994/6500 [26:24:26<18:01:59, 25.91s/it]                                                          61%|██████▏   | 3994/6500 [26:24:26<18:01:59, 25.91s/it] 61%|██████▏   | 3995/6500 [26:24:44<16:26:16, 23.62s/it]                                                          61%|██████▏   | 3995/6500 [26:24:44<16:26:16, 23.62s/it] 61%|█{'loss': 0.2591, 'learning_rate': 3.2377599930191224e-05, 'epoch': 0.61}
{'loss': 0.2655, 'learning_rate': 3.2354979637248636e-05, 'epoch': 0.61}
{'loss': 0.2548, 'learning_rate': 3.233236346872227e-05, 'epoch': 0.62}
{'loss': 0.2609, 'learning_rate': 3.230975142989853e-05, 'epoch': 0.62}
{'loss': 0.251, 'learning_rate': 3.2287143526062825e-05, 'epoch': 0.62}
█████▏   | 3996/6500 [26:25:02<15:16:30, 21.96s/it]                                                          61%|██████▏   | 3996/6500 [26:25:02<15:16:30, 21.96s/it] 61%|██████▏   | 3997/6500 [26:25:20<14:28:57, 20.83s/it]                                                          61%|██████▏   | 3997/6500 [26:25:20<14:28:57, 20.83s/it] 62%|██████▏   | 3998/6500 [26:25:39<13:56:16, 20.05s/it]                                                          62%|██████▏   | 3998/6500 [26:25:39<13:56:16, 20.05s/it] 62%|██████▏   | 3999/6500 [26:25:57<13:32:15, 19.49s/it]                                                          62%|██████▏   | 3999/6500 [26:25:57<13:32:15, 19.49s/it] 62%|██████▏   | 4000/6500 [26:26:15<13:15:51, 19.10s/it]                                                          62%|██████▏   | 4000/6500 [26:26:15<13:15:51, 19.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.882719099521637, 'eval_runtime': 5.3787, 'eval_samples_per_second': 4.276, 'eval_steps_per_second': 1.116, 'epoch': 0.62}
                                                          62%|██████▏   | 4000/6500 [26:26:20<13:15:51, 19.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4000
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4000 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4000
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4000

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4000
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4000/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4000/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2458, 'learning_rate': 3.2264539762499644e-05, 'epoch': 0.62}
{'loss': 0.2486, 'learning_rate': 3.224194014449245e-05, 'epoch': 0.62}
{'loss': 0.2669, 'learning_rate': 3.221934467732377e-05, 'epoch': 0.62}
{'loss': 0.2434, 'learning_rate': 3.219675336627516e-05, 'epoch': 0.62}
{'loss': 0.2366, 'learning_rate': 3.2174166216627214e-05, 'epoch': 0.62}
 62%|██████▏   | 4001/6500 [26:27:07<20:10:57, 29.07s/it]                                                          62%|██████▏   | 4001/6500 [26:27:07<20:10:57, 29.07s/it] 62%|██████▏   | 4002/6500 [26:27:25<17:53:11, 25.78s/it]                                                          62%|██████▏   | 4002/6500 [26:27:25<17:53:11, 25.78s/it] 62%|██████▏   | 4003/6500 [26:27:43<16:16:23, 23.46s/it]                                                          62%|██████▏   | 4003/6500 [26:27:43<16:16:23, 23.46s/it] 62%|██████▏   | 4004/6500 [26:28:01<15:08:26, 21.84s/it]                                                          62%|██████▏   | 4004/6500 [26:28:01<15:08:26, 21.84s/it] 62%|██████▏   | 4005/6500 [26:28:19<14:21:04, 20.71s/it]                                                          62%|██████▏   | 4005/6500 [26:28:19<14:21:04, 20.71s/it] 62%|█{'loss': 0.24, 'learning_rate': 3.2151583233659526e-05, 'epoch': 0.62}
{'loss': 0.2545, 'learning_rate': 3.212900442265075e-05, 'epoch': 0.62}
{'loss': 0.3147, 'learning_rate': 3.2106429788878525e-05, 'epoch': 0.62}
{'loss': 0.239, 'learning_rate': 3.2083859337619534e-05, 'epoch': 0.62}
{'loss': 0.2362, 'learning_rate': 3.20612930741495e-05, 'epoch': 0.62}
█████▏   | 4006/6500 [26:28:38<13:47:51, 19.92s/it]                                                          62%|██████▏   | 4006/6500 [26:28:38<13:47:51, 19.92s/it] 62%|██████▏   | 4007/6500 [26:28:56<13:24:56, 19.37s/it]                                                          62%|██████▏   | 4007/6500 [26:28:56<13:24:56, 19.37s/it] 62%|██████▏   | 4008/6500 [26:29:14<13:13:16, 19.10s/it]                                                          62%|██████▏   | 4008/6500 [26:29:14<13:13:16, 19.10s/it] 62%|██████▏   | 4009/6500 [26:29:32<13:00:54, 18.81s/it]                                                          62%|██████▏   | 4009/6500 [26:29:32<13:00:54, 18.81s/it] 62%|██████▏   | 4010/6500 [26:29:50<12:52:25, 18.61s/it]                                                          62%|██████▏   | 4010/6500 [26:29:50<12:52:25, 18.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8726518154144287, 'eval_runtime': 5.3368, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.62}
                                                          62%|██████▏   | 4010/6500 [26:29:56<12:52:25, 18.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4010
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4010
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4010
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4010

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4010
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4010/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2588, 'learning_rate': 3.203873100374314e-05, 'epoch': 0.62}
{'loss': 0.73, 'learning_rate': 3.201617313167421e-05, 'epoch': 0.62}
{'loss': 0.2495, 'learning_rate': 3.1993619463215454e-05, 'epoch': 0.62}
{'loss': 0.2407, 'learning_rate': 3.197107000363867e-05, 'epoch': 0.62}
{'loss': 0.2567, 'learning_rate': 3.194852475821465e-05, 'epoch': 0.62}
 62%|██████▏   | 4011/6500 [26:31:23<28:16:33, 40.90s/it]                                                          62%|██████▏   | 4011/6500 [26:31:23<28:16:33, 40.90s/it] 62%|██████▏   | 4012/6500 [26:31:41<23:30:29, 34.01s/it]                                                          62%|██████▏   | 4012/6500 [26:31:41<23:30:29, 34.01s/it] 62%|██████▏   | 4013/6500 [26:31:59<20:10:36, 29.21s/it]                                                          62%|██████▏   | 4013/6500 [26:31:59<20:10:36, 29.21s/it] 62%|██████▏   | 4014/6500 [26:32:17<17:51:10, 25.85s/it]                                                          62%|██████▏   | 4014/6500 [26:32:17<17:51:10, 25.85s/it] 62%|██████▏   | 4015/6500 [26:32:35<16:13:45, 23.51s/it]                                                          62%|██████▏   | 4015/6500 [26:32:35<16:13:45, 23.51s/it] 62%|█{'loss': 0.233, 'learning_rate': 3.192598373221322e-05, 'epoch': 0.62}
{'loss': 0.2574, 'learning_rate': 3.1903446930903205e-05, 'epoch': 0.62}
{'loss': 0.2288, 'learning_rate': 3.188091435955244e-05, 'epoch': 0.62}
{'loss': 0.2271, 'learning_rate': 3.1858386023427774e-05, 'epoch': 0.62}
{'loss': 0.2488, 'learning_rate': 3.183586192779507e-05, 'epoch': 0.62}
█████▏   | 4016/6500 [26:32:53<15:05:34, 21.87s/it]                                                          62%|██████▏   | 4016/6500 [26:32:53<15:05:34, 21.87s/it] 62%|██████▏   | 4017/6500 [26:33:11<14:18:14, 20.74s/it]                                                          62%|██████▏   | 4017/6500 [26:33:11<14:18:14, 20.74s/it] 62%|██████▏   | 4018/6500 [26:33:30<13:45:16, 19.95s/it]                                                          62%|██████▏   | 4018/6500 [26:33:30<13:45:16, 19.95s/it] 62%|██████▏   | 4019/6500 [26:33:48<13:22:18, 19.40s/it]                                                          62%|██████▏   | 4019/6500 [26:33:48<13:22:18, 19.40s/it] 62%|██████▏   | 4020/6500 [26:34:06<13:06:19, 19.02s/it]                                                          62%|██████▏   | 4020/6500 [26:34:06<13:06:19, 19.02s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8875865340232849, 'eval_runtime': 5.3389, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.62}
                                                          62%|██████▏   | 4020/6500 [26:34:11<13:06:19, 19.02s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4020
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4020/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4020

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4020

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4020
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4020/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4020/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4020/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.235, 'learning_rate': 3.18133420779192e-05, 'epoch': 0.62}
{'loss': 0.2547, 'learning_rate': 3.1790826479064046e-05, 'epoch': 0.62}
{'loss': 0.2411, 'learning_rate': 3.176831513649249e-05, 'epoch': 0.62}
{'loss': 0.249, 'learning_rate': 3.174580805546642e-05, 'epoch': 0.62}
{'loss': 0.2382, 'learning_rate': 3.172330524124673e-05, 'epoch': 0.62}
 62%|██████▏   | 4021/6500 [26:35:21<24:47:09, 35.99s/it]                                                          62%|██████▏   | 4021/6500 [26:35:21<24:47:09, 35.99s/it] 62%|██████▏   | 4022/6500 [26:35:39<21:03:27, 30.59s/it]                                                          62%|██████▏   | 4022/6500 [26:35:39<21:03:27, 30.59s/it] 62%|██████▏   | 4023/6500 [26:35:57<18:26:59, 26.81s/it]                                                          62%|██████▏   | 4023/6500 [26:35:57<18:26:59, 26.81s/it] 62%|██████▏   | 4024/6500 [26:36:16<16:42:59, 24.31s/it]                                                          62%|██████▏   | 4024/6500 [26:36:16<16:42:59, 24.31s/it] 62%|██████▏   | 4025/6500 [26:36:34<15:24:59, 22.42s/it]                                                          62%|██████▏   | 4025/6500 [26:36:34<15:24:59, 22.42s/it] 62%|█{'loss': 0.2557, 'learning_rate': 3.170080669909331e-05, 'epoch': 0.62}
{'loss': 0.2384, 'learning_rate': 3.167831243426507e-05, 'epoch': 0.62}
{'loss': 0.2358, 'learning_rate': 3.165582245201989e-05, 'epoch': 0.62}
{'loss': 0.2448, 'learning_rate': 3.1633336757614694e-05, 'epoch': 0.62}
{'loss': 0.2413, 'learning_rate': 3.1610855356305354e-05, 'epoch': 0.62}
█████▏   | 4026/6500 [26:36:52<14:30:34, 21.11s/it]                                                          62%|██████▏   | 4026/6500 [26:36:52<14:30:34, 21.11s/it] 62%|██████▏   | 4027/6500 [26:37:10<13:52:49, 20.21s/it]                                                          62%|██████▏   | 4027/6500 [26:37:10<13:52:49, 20.21s/it] 62%|██████▏   | 4028/6500 [26:37:28<13:26:31, 19.58s/it]                                                          62%|██████▏   | 4028/6500 [26:37:28<13:26:31, 19.58s/it] 62%|██████▏   | 4029/6500 [26:37:46<13:08:28, 19.15s/it]                                                          62%|██████▏   | 4029/6500 [26:37:46<13:08:28, 19.15s/it] 62%|██████▏   | 4030/6500 [26:38:05<13:01:09, 18.98s/it]                                                          62%|██████▏   | 4030/6500 [26:38:05<13:01:09, 18.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.887911319732666, 'eval_runtime': 5.4695, 'eval_samples_per_second': 4.205, 'eval_steps_per_second': 1.097, 'epoch': 0.62}
                                                          62%|██████▏   | 4030/6500 [26:38:10<13:01:09, 18.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4030
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4030

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4030
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4030

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4030
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4030/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4030/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.241, 'learning_rate': 3.158837825334676e-05, 'epoch': 0.62}
{'loss': 0.2499, 'learning_rate': 3.15659054539928e-05, 'epoch': 0.62}
{'loss': 0.2598, 'learning_rate': 3.154343696349638e-05, 'epoch': 0.62}
{'loss': 0.2322, 'learning_rate': 3.152097278710933e-05, 'epoch': 0.62}
{'loss': 0.2436, 'learning_rate': 3.149851293008256e-05, 'epoch': 0.62}
 62%|██████▏   | 4031/6500 [26:39:15<23:36:09, 34.41s/it]                                                          62%|██████▏   | 4031/6500 [26:39:15<23:36:09, 34.41s/it] 62%|██████▏   | 4032/6500 [26:39:33<20:13:18, 29.50s/it]                                                          62%|██████▏   | 4032/6500 [26:39:33<20:13:18, 29.50s/it] 62%|██████▏   | 4033/6500 [26:39:52<17:54:06, 26.12s/it]                                                          62%|██████▏   | 4033/6500 [26:39:52<17:54:06, 26.12s/it] 62%|██████▏   | 4034/6500 [26:40:10<16:13:53, 23.70s/it]                                                          62%|██████▏   | 4034/6500 [26:40:10<16:13:53, 23.70s/it] 62%|██████▏   | 4035/6500 [26:40:28<15:03:51, 22.00s/it]                                                          62%|██████▏   | 4035/6500 [26:40:28<15:03:51, 22.00s/it] 62%|█{'loss': 0.2285, 'learning_rate': 3.147605739766588e-05, 'epoch': 0.62}
{'loss': 0.2654, 'learning_rate': 3.145360619510817e-05, 'epoch': 0.62}
{'loss': 0.2858, 'learning_rate': 3.143115932765723e-05, 'epoch': 0.62}
{'loss': 0.2363, 'learning_rate': 3.140871680055991e-05, 'epoch': 0.62}
{'loss': 0.2467, 'learning_rate': 3.1386278619062006e-05, 'epoch': 0.62}
█████▏   | 4036/6500 [26:40:46<14:14:59, 20.82s/it]                                                          62%|██████▏   | 4036/6500 [26:40:46<14:14:59, 20.82s/it] 62%|██████▏   | 4037/6500 [26:41:04<13:41:06, 20.00s/it]                                                          62%|██████▏   | 4037/6500 [26:41:04<13:41:06, 20.00s/it] 62%|██████▏   | 4038/6500 [26:41:22<13:17:29, 19.44s/it]                                                          62%|██████▏   | 4038/6500 [26:41:22<13:17:29, 19.44s/it] 62%|██████▏   | 4039/6500 [26:41:40<13:00:58, 19.04s/it]                                                          62%|██████▏   | 4039/6500 [26:41:40<13:00:58, 19.04s/it] 62%|██████▏   | 4040/6500 [26:41:58<12:49:34, 18.77s/it]                                                          62%|██████▏   | 4040/6500 [26:41:58<12:49:34, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8767998814582825, 'eval_runtime': 5.9993, 'eval_samples_per_second': 3.834, 'eval_steps_per_second': 1.0, 'epoch': 0.62}
                                                          62%|██████▏   | 4040/6500 [26:42:04<12:49:34, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4040
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4040
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4040/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4040/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2429, 'learning_rate': 3.13638447884083e-05, 'epoch': 0.62}
{'loss': 0.7348, 'learning_rate': 3.134141531384256e-05, 'epoch': 0.62}
{'loss': 0.2526, 'learning_rate': 3.131899020060754e-05, 'epoch': 0.62}
{'loss': 0.2614, 'learning_rate': 3.1296569453944977e-05, 'epoch': 0.62}
{'loss': 0.2321, 'learning_rate': 3.127415307909558e-05, 'epoch': 0.62}
 62%|██████▏   | 4041/6500 [26:43:21<25:54:31, 37.93s/it]                                                          62%|██████▏   | 4041/6500 [26:43:21<25:54:31, 37.93s/it] 62%|██████▏   | 4042/6500 [26:43:39<21:48:03, 31.93s/it]                                                          62%|██████▏   | 4042/6500 [26:43:39<21:48:03, 31.93s/it] 62%|██████▏   | 4043/6500 [26:43:57<18:56:08, 27.74s/it]                                                          62%|██████▏   | 4043/6500 [26:43:57<18:56:08, 27.74s/it] 62%|██████▏   | 4044/6500 [26:44:15<16:56:14, 24.83s/it]                                                          62%|██████▏   | 4044/6500 [26:44:15<16:56:14, 24.83s/it] 62%|██████▏   | 4045/6500 [26:44:33<15:32:14, 22.78s/it]                                                          62%|██████▏   | 4045/6500 [26:44:33<15:32:14, 22.78s/it] 62%|█{'loss': 0.2366, 'learning_rate': 3.125174108129906e-05, 'epoch': 0.62}
{'loss': 0.2467, 'learning_rate': 3.122933346579406e-05, 'epoch': 0.62}
{'loss': 0.2312, 'learning_rate': 3.1206930237818245e-05, 'epoch': 0.62}
{'loss': 0.245, 'learning_rate': 3.118453140260823e-05, 'epoch': 0.62}
{'loss': 0.2503, 'learning_rate': 3.116213696539959e-05, 'epoch': 0.62}
█████▏   | 4046/6500 [26:44:51<14:33:51, 21.37s/it]                                                          62%|██████▏   | 4046/6500 [26:44:51<14:33:51, 21.37s/it] 62%|██████▏   | 4047/6500 [26:45:09<13:53:21, 20.38s/it]                                                          62%|██████▏   | 4047/6500 [26:45:09<13:53:21, 20.38s/it] 62%|██████▏   | 4048/6500 [26:45:27<13:24:57, 19.70s/it]                                                          62%|██████▏   | 4048/6500 [26:45:27<13:24:57, 19.70s/it] 62%|██████▏   | 4049/6500 [26:45:45<13:08:21, 19.30s/it]                                                          62%|██████▏   | 4049/6500 [26:45:45<13:08:21, 19.30s/it] 62%|██████▏   | 4050/6500 [26:46:04<12:53:57, 18.95s/it]                                                          62%|██████▏   | 4050/6500 [26:46:04<12:53:57, 18.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8849052786827087, 'eval_runtime': 5.336, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.62}
                                                          62%|██████▏   | 4050/6500 [26:46:09<12:53:57, 18.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4050
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4050I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4050
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4050/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4050/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2399, 'learning_rate': 3.1139746931426894e-05, 'epoch': 0.62}
{'loss': 0.2488, 'learning_rate': 3.1117361305923684e-05, 'epoch': 0.62}
{'loss': 0.2328, 'learning_rate': 3.109498009412246e-05, 'epoch': 0.62}
{'loss': 0.2495, 'learning_rate': 3.10726033012547e-05, 'epoch': 0.62}
{'loss': 0.2422, 'learning_rate': 3.105023093255084e-05, 'epoch': 0.62}
 62%|██████▏   | 4051/6500 [26:47:00<20:38:35, 30.35s/it]                                                          62%|██████▏   | 4051/6500 [26:47:00<20:38:35, 30.35s/it] 62%|██████▏   | 4052/6500 [26:47:18<18:07:17, 26.65s/it]                                                          62%|██████▏   | 4052/6500 [26:47:18<18:07:17, 26.65s/it] 62%|██████▏   | 4053/6500 [26:47:37<16:21:18, 24.06s/it]                                                          62%|██████▏   | 4053/6500 [26:47:37<16:21:18, 24.06s/it] 62%|██████▏   | 4054/6500 [26:47:55<15:07:05, 22.25s/it]                                                          62%|██████▏   | 4054/6500 [26:47:55<15:07:05, 22.25s/it] 62%|██████▏   | 4055/6500 [26:48:13<14:15:20, 20.99s/it]                                                          62%|██████▏   | 4055/6500 [26:48:13<14:15:20, 20.99s/it] 62%|█{'loss': 0.26, 'learning_rate': 3.102786299324028e-05, 'epoch': 0.62}
{'loss': 0.2379, 'learning_rate': 3.100549948855138e-05, 'epoch': 0.62}
{'loss': 0.2548, 'learning_rate': 3.0983140423711495e-05, 'epoch': 0.62}
{'loss': 0.2615, 'learning_rate': 3.096078580394691e-05, 'epoch': 0.62}
{'loss': 0.2385, 'learning_rate': 3.09384356344829e-05, 'epoch': 0.62}
█████▏   | 4056/6500 [26:48:31<13:39:19, 20.11s/it]                                                          62%|██████▏   | 4056/6500 [26:48:31<13:39:19, 20.11s/it] 62%|██████▏   | 4057/6500 [26:48:49<13:19:58, 19.65s/it]                                                          62%|██████▏   | 4057/6500 [26:48:49<13:19:58, 19.65s/it] 62%|██████▏   | 4058/6500 [26:49:07<13:02:49, 19.23s/it]                                                          62%|██████▏   | 4058/6500 [26:49:07<13:02:49, 19.23s/it] 62%|██████▏   | 4059/6500 [26:49:26<12:49:03, 18.90s/it]                                                          62%|██████▏   | 4059/6500 [26:49:26<12:49:03, 18.90s/it] 62%|██████▏   | 4060/6500 [26:49:44<12:39:28, 18.68s/it]                                                          62%|██████▏   | 4060/6500 [26:49:44<12:39:28, 18.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8880312442779541, 'eval_runtime': 5.3525, 'eval_samples_per_second': 4.297, 'eval_steps_per_second': 1.121, 'epoch': 0.62}
                                                          62%|██████▏   | 4060/6500 [26:49:49<12:39:28, 18.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4060
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4060/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4060


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4060

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4060
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4060/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2558, 'learning_rate': 3.091608992054365e-05, 'epoch': 0.62}
{'loss': 0.2458, 'learning_rate': 3.089374866735234e-05, 'epoch': 0.62}
{'loss': 0.2537, 'learning_rate': 3.0871411880131126e-05, 'epoch': 0.63}
{'loss': 0.2317, 'learning_rate': 3.084907956410107e-05, 'epoch': 0.63}
{'loss': 0.231, 'learning_rate': 3.082675172448223e-05, 'epoch': 0.63}
 62%|██████▏   | 4061/6500 [26:50:41<20:27:30, 30.20s/it]                                                          62%|██████▏   | 4061/6500 [26:50:41<20:27:30, 30.20s/it] 62%|██████▏   | 4062/6500 [26:50:59<17:58:58, 26.55s/it]                                                          62%|██████▏   | 4062/6500 [26:50:59<17:58:58, 26.55s/it] 63%|██████▎   | 4063/6500 [26:51:17<16:15:58, 24.03s/it]                                                          63%|██████▎   | 4063/6500 [26:51:17<16:15:58, 24.03s/it] 63%|██████▎   | 4064/6500 [26:51:35<15:02:43, 22.23s/it]                                                          63%|██████▎   | 4064/6500 [26:51:35<15:02:43, 22.23s/it] 63%|██████▎   | 4065/6500 [26:51:53<14:11:34, 20.98s/it]                                                          63%|██████▎   | 4065/6500 [26:51:53<14:11:34, 20.98s/it] 63%|█{'loss': 0.2321, 'learning_rate': 3.080442836649361e-05, 'epoch': 0.63}
{'loss': 0.3069, 'learning_rate': 3.078210949535314e-05, 'epoch': 0.63}
{'loss': 0.2578, 'learning_rate': 3.0759795116277725e-05, 'epoch': 0.63}
{'loss': 0.2422, 'learning_rate': 3.0737485234483223e-05, 'epoch': 0.63}
{'loss': 0.2569, 'learning_rate': 3.071517985518442e-05, 'epoch': 0.63}
█████▎   | 4066/6500 [26:52:11<13:35:58, 20.11s/it]                                                          63%|██████▎   | 4066/6500 [26:52:11<13:35:58, 20.11s/it] 63%|██████▎   | 4067/6500 [26:52:29<13:12:39, 19.55s/it]                                                          63%|██████▎   | 4067/6500 [26:52:29<13:12:39, 19.55s/it] 63%|██████▎   | 4068/6500 [26:52:48<12:56:15, 19.15s/it]                                                          63%|██████▎   | 4068/6500 [26:52:48<12:56:15, 19.15s/it] 63%|██████▎   | 4069/6500 [26:53:06<12:43:46, 18.85s/it]                                                          63%|██████▎   | 4069/6500 [26:53:06<12:43:46, 18.85s/it] 63%|██████▎   | 4070/6500 [26:53:24<12:35:08, 18.65s/it]                                                          63%|██████▎   | 4070/6500 [26:53:24<12:35:08, 18.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8754798173904419, 'eval_runtime': 5.3344, 'eval_samples_per_second': 4.312, 'eval_steps_per_second': 1.125, 'epoch': 0.63}
                                                          63%|██████▎   | 4070/6500 [26:53:29<12:35:08, 18.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4070
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4070
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4070

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4070

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4070
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4070/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7377, 'learning_rate': 3.069287898359509e-05, 'epoch': 0.63}
{'loss': 0.2665, 'learning_rate': 3.06705826249279e-05, 'epoch': 0.63}
{'loss': 0.2377, 'learning_rate': 3.06482907843945e-05, 'epoch': 0.63}
{'loss': 0.2466, 'learning_rate': 3.062600346720545e-05, 'epoch': 0.63}
{'loss': 0.2307, 'learning_rate': 3.060372067857031e-05, 'epoch': 0.63}
 63%|██████▎   | 4071/6500 [26:54:43<24:45:01, 36.68s/it]                                                          63%|██████▎   | 4071/6500 [26:54:43<24:45:01, 36.68s/it] 63%|██████▎   | 4072/6500 [26:55:01<20:57:21, 31.07s/it]                                                          63%|██████▎   | 4072/6500 [26:55:01<20:57:21, 31.07s/it] 63%|██████▎   | 4073/6500 [26:55:19<18:25:15, 27.32s/it]                                                          63%|██████▎   | 4073/6500 [26:55:19<18:25:15, 27.32s/it] 63%|██████▎   | 4074/6500 [26:55:37<16:32:01, 24.53s/it]                                                          63%|██████▎   | 4074/6500 [26:55:37<16:32:01, 24.53s/it] 63%|██████▎   | 4075/6500 [26:55:55<15:13:05, 22.59s/it]                                                          63%|██████▎   | 4075/6500 [26:55:55<15:13:05, 22.59s/it] 63%|█{'loss': 0.2484, 'learning_rate': 3.058144242369753e-05, 'epoch': 0.63}
{'loss': 0.236, 'learning_rate': 3.055916870779453e-05, 'epoch': 0.63}
{'loss': 0.2148, 'learning_rate': 3.053689953606762e-05, 'epoch': 0.63}
{'loss': 0.2526, 'learning_rate': 3.051463491372211e-05, 'epoch': 0.63}
{'loss': 0.2307, 'learning_rate': 3.0492374845962225e-05, 'epoch': 0.63}
█████▎   | 4076/6500 [26:56:13<14:17:48, 21.23s/it]                                                          63%|██████▎   | 4076/6500 [26:56:13<14:17:48, 21.23s/it] 63%|██████▎   | 4077/6500 [26:56:32<13:39:33, 20.29s/it]                                                          63%|██████▎   | 4077/6500 [26:56:32<13:39:33, 20.29s/it] 63%|██████▎   | 4078/6500 [26:56:50<13:12:54, 19.64s/it]                                                          63%|██████▎   | 4078/6500 [26:56:50<13:12:54, 19.64s/it] 63%|██████▎   | 4079/6500 [26:57:08<12:54:17, 19.19s/it]                                                          63%|██████▎   | 4079/6500 [26:57:08<12:54:17, 19.19s/it] 63%|██████▎   | 4080/6500 [26:57:26<12:41:29, 18.88s/it]                                                          63%|██████▎   | 4080/6500 [26:57:26<12:41:29, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8897359371185303, 'eval_runtime': 5.3484, 'eval_samples_per_second': 4.3, 'eval_steps_per_second': 1.122, 'epoch': 0.63}
                                                          63%|██████▎   | 4080/6500 [26:57:31<12:41:29, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4080
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4080
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4080
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4080

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4080
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4080/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4080/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2451, 'learning_rate': 3.04701193379911e-05, 'epoch': 0.63}
{'loss': 0.2383, 'learning_rate': 3.0447868395010836e-05, 'epoch': 0.63}
{'loss': 0.2478, 'learning_rate': 3.0425622022222478e-05, 'epoch': 0.63}
{'loss': 0.2337, 'learning_rate': 3.040338022482594e-05, 'epoch': 0.63}
{'loss': 0.2556, 'learning_rate': 3.038114300802012e-05, 'epoch': 0.63}
 63%|██████▎   | 4081/6500 [26:58:59<27:33:11, 41.01s/it]                                                          63%|██████▎   | 4081/6500 [26:58:59<27:33:11, 41.01s/it] 63%|██████▎   | 4082/6500 [26:59:17<22:53:47, 34.09s/it]                                                          63%|██████▎   | 4082/6500 [26:59:17<22:53:47, 34.09s/it] 63%|██████▎   | 4083/6500 [26:59:35<19:38:15, 29.25s/it]                                                          63%|██████▎   | 4083/6500 [26:59:35<19:38:15, 29.25s/it] 63%|██████▎   | 4084/6500 [26:59:53<17:21:36, 25.87s/it]                                                          63%|██████▎   | 4084/6500 [26:59:53<17:21:36, 25.87s/it] 63%|██████▎   | 4085/6500 [27:00:11<15:46:45, 23.52s/it]                                                          63%|██████▎   | 4085/6500 [27:00:11<15:46:45, 23.52s/it] 63%|█{'loss': 0.2375, 'learning_rate': 3.0358910377002848e-05, 'epoch': 0.63}
{'loss': 0.2388, 'learning_rate': 3.0336682336970846e-05, 'epoch': 0.63}
{'loss': 0.2512, 'learning_rate': 3.0314458893119808e-05, 'epoch': 0.63}
{'loss': 0.2383, 'learning_rate': 3.0292240050644304e-05, 'epoch': 0.63}
{'loss': 0.2511, 'learning_rate': 3.0270025814737856e-05, 'epoch': 0.63}
█████▎   | 4086/6500 [27:00:29<14:40:10, 21.88s/it]                                                          63%|██████▎   | 4086/6500 [27:00:29<14:40:10, 21.88s/it] 63%|██████▎   | 4087/6500 [27:00:47<13:53:41, 20.73s/it]                                                          63%|██████▎   | 4087/6500 [27:00:47<13:53:41, 20.73s/it] 63%|██████▎   | 4088/6500 [27:01:05<13:21:33, 19.94s/it]                                                          63%|██████▎   | 4088/6500 [27:01:05<13:21:33, 19.94s/it] 63%|██████▎   | 4089/6500 [27:01:23<13:02:49, 19.48s/it]                                                          63%|██████▎   | 4089/6500 [27:01:23<13:02:49, 19.48s/it] 63%|██████▎   | 4090/6500 [27:01:41<12:46:13, 19.08s/it]                                                          63%|██████▎   | 4090/6500 [27:01:41<12:46:13, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8897139430046082, 'eval_runtime': 5.4166, 'eval_samples_per_second': 4.246, 'eval_steps_per_second': 1.108, 'epoch': 0.63}
                                                          63%|██████▎   | 4090/6500 [27:01:47<12:46:13, 19.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4090
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4090
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4090
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4090

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4090
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4090/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4090/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2318, 'learning_rate': 3.024781619059292e-05, 'epoch': 0.63}
{'loss': 0.2638, 'learning_rate': 3.0225611183400855e-05, 'epoch': 0.63}
{'loss': 0.2354, 'learning_rate': 3.0203410798351945e-05, 'epoch': 0.63}
{'loss': 0.2396, 'learning_rate': 3.0181215040635402e-05, 'epoch': 0.63}
{'loss': 0.2345, 'learning_rate': 3.0159023915439338e-05, 'epoch': 0.63}
 63%|██████▎   | 4091/6500 [27:02:47<22:11:28, 33.16s/it]                                                          63%|██████▎   | 4091/6500 [27:02:47<22:11:28, 33.16s/it] 63%|██████▎   | 4092/6500 [27:03:05<19:08:31, 28.62s/it]                                                          63%|██████▎   | 4092/6500 [27:03:05<19:08:31, 28.62s/it] 63%|██████▎   | 4093/6500 [27:03:23<17:00:15, 25.43s/it]                                                          63%|██████▎   | 4093/6500 [27:03:23<17:00:15, 25.43s/it] 63%|██████▎   | 4094/6500 [27:03:41<15:30:32, 23.21s/it]                                                          63%|██████▎   | 4094/6500 [27:03:41<15:30:32, 23.21s/it] 63%|██████▎   | 4095/6500 [27:04:00<14:33:57, 21.80s/it]                                                          63%|██████▎   | 4095/6500 [27:04:00<14:33:57, 21.80s/it] 63%|█{'loss': 0.2532, 'learning_rate': 3.0136837427950793e-05, 'epoch': 0.63}
{'loss': 0.2988, 'learning_rate': 3.0114655583355733e-05, 'epoch': 0.63}
{'loss': 0.2373, 'learning_rate': 3.009247838683903e-05, 'epoch': 0.63}
{'loss': 0.2337, 'learning_rate': 3.007030584358447e-05, 'epoch': 0.63}
{'loss': 0.2486, 'learning_rate': 3.004813795877473e-05, 'epoch': 0.63}
█████▎   | 4096/6500 [27:04:18<13:48:20, 20.67s/it]                                                          63%|██████▎   | 4096/6500 [27:04:18<13:48:20, 20.67s/it] 63%|██████▎   | 4097/6500 [27:04:36<13:16:32, 19.89s/it]                                                          63%|██████▎   | 4097/6500 [27:04:36<13:16:32, 19.89s/it] 63%|██████▎   | 4098/6500 [27:04:54<12:54:38, 19.35s/it]                                                          63%|██████▎   | 4098/6500 [27:04:54<12:54:38, 19.35s/it] 63%|██████▎   | 4099/6500 [27:05:12<12:39:29, 18.98s/it]                                                          63%|██████▎   | 4099/6500 [27:05:12<12:39:29, 18.98s/it] 63%|██████▎   | 4100/6500 [27:05:30<12:29:02, 18.73s/it]                                                          63%|██████▎   | 4100/6500 [27:05:30<12:29:02, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8768686056137085, 'eval_runtime': 5.345, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.123, 'epoch': 0.63}
                                                          63%|██████▎   | 4100/6500 [27:05:36<12:29:02, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4100
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4100
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4100

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4100
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4100/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7257, 'learning_rate': 3.0025974737591434e-05, 'epoch': 0.63}
{'loss': 0.2522, 'learning_rate': 3.0003816185215107e-05, 'epoch': 0.63}
{'loss': 0.2408, 'learning_rate': 2.9981662306825163e-05, 'epoch': 0.63}
{'loss': 0.2382, 'learning_rate': 2.995951310759994e-05, 'epoch': 0.63}
{'loss': 0.2336, 'learning_rate': 2.993736859271669e-05, 'epoch': 0.63}
 63%|██████▎   | 4101/6500 [27:06:09<16:33:29, 24.85s/it]                                                          63%|██████▎   | 4101/6500 [27:06:09<16:33:29, 24.85s/it] 63%|██████▎   | 4102/6500 [27:06:27<15:11:32, 22.81s/it]                                                          63%|██████▎   | 4102/6500 [27:06:27<15:11:32, 22.81s/it] 63%|██████▎   | 4103/6500 [27:06:46<14:14:09, 21.38s/it]                                                          63%|██████▎   | 4103/6500 [27:06:46<14:14:09, 21.38s/it] 63%|██████▎   | 4104/6500 [27:07:04<13:33:57, 20.38s/it]                                                          63%|██████▎   | 4104/6500 [27:07:04<13:33:57, 20.38s/it] 63%|██████▎   | 4105/6500 [27:07:22<13:12:24, 19.85s/it]                                                          63%|██████▎   | 4105/6500 [27:07:22<13:12:24, 19.85s/it] 63%|█{'loss': 0.2474, 'learning_rate': 2.991522876735154e-05, 'epoch': 0.63}
{'loss': 0.23, 'learning_rate': 2.9893093636679546e-05, 'epoch': 0.63}
{'loss': 0.2336, 'learning_rate': 2.987096320587467e-05, 'epoch': 0.63}
{'loss': 0.2382, 'learning_rate': 2.984883748010975e-05, 'epoch': 0.63}
{'loss': 0.2348, 'learning_rate': 2.982671646455655e-05, 'epoch': 0.63}
█████▎   | 4106/6500 [27:07:40<12:50:56, 19.32s/it]                                                          63%|██████▎   | 4106/6500 [27:07:40<12:50:56, 19.32s/it] 63%|██████▎   | 4107/6500 [27:07:58<12:36:16, 18.96s/it]                                                          63%|██████▎   | 4107/6500 [27:07:58<12:36:16, 18.96s/it] 63%|██████▎   | 4108/6500 [27:08:17<12:26:06, 18.72s/it]                                                          63%|██████▎   | 4108/6500 [27:08:17<12:26:06, 18.72s/it] 63%|██████▎   | 4109/6500 [27:08:35<12:18:57, 18.54s/it]                                                          63%|██████▎   | 4109/6500 [27:08:35<12:18:57, 18.54s/it] 63%|██████▎   | 4110/6500 [27:08:53<12:17:05, 18.50s/it]                                                          63%|██████▎   | 4110/6500 [27:08:53<12:17:05, 18.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8904507756233215, 'eval_runtime': 5.3403, 'eval_samples_per_second': 4.307, 'eval_steps_per_second': 1.124, 'epoch': 0.63}
                                                          63%|██████▎   | 4110/6500 [27:08:58<12:17:05, 18.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4110
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4110
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4110
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4110/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4110/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2404, 'learning_rate': 2.9804600164385733e-05, 'epoch': 0.63}
{'loss': 0.2353, 'learning_rate': 2.9782488584766822e-05, 'epoch': 0.63}
{'loss': 0.2498, 'learning_rate': 2.976038173086828e-05, 'epoch': 0.63}
{'loss': 0.2423, 'learning_rate': 2.9738279607857455e-05, 'epoch': 0.63}
{'loss': 0.2486, 'learning_rate': 2.9716182220900578e-05, 'epoch': 0.63}
 63%|██████▎   | 4111/6500 [27:10:12<24:16:22, 36.58s/it]                                                          63%|██████▎   | 4111/6500 [27:10:12<24:16:22, 36.58s/it] 63%|██████▎   | 4112/6500 [27:10:30<20:33:53, 31.00s/it]                                                          63%|██████▎   | 4112/6500 [27:10:30<20:33:53, 31.00s/it] 63%|██████▎   | 4113/6500 [27:10:48<17:57:59, 27.10s/it]                                                          63%|██████▎   | 4113/6500 [27:10:48<17:57:59, 27.10s/it] 63%|██████▎   | 4114/6500 [27:11:06<16:08:55, 24.37s/it]                                                          63%|██████▎   | 4114/6500 [27:11:06<16:08:55, 24.37s/it] 63%|██████▎   | 4115/6500 [27:11:24<14:52:45, 22.46s/it]                                                          63%|██████▎   | 4115/6500 [27:11:24<14:52:45, 22.46s/it] 63%|█{'loss': 0.2368, 'learning_rate': 2.9694089575162785e-05, 'epoch': 0.63}
{'loss': 0.2367, 'learning_rate': 2.9672001675808086e-05, 'epoch': 0.63}
{'loss': 0.2472, 'learning_rate': 2.96499185279994e-05, 'epoch': 0.63}
{'loss': 0.2428, 'learning_rate': 2.9627840136898523e-05, 'epoch': 0.63}
{'loss': 0.2491, 'learning_rate': 2.9605766507666145e-05, 'epoch': 0.63}
█████▎   | 4116/6500 [27:11:42<13:59:31, 21.13s/it]                                                          63%|██████▎   | 4116/6500 [27:11:42<13:59:31, 21.13s/it] 63%|██████▎   | 4117/6500 [27:12:00<13:22:35, 20.21s/it]                                                          63%|██████▎   | 4117/6500 [27:12:00<13:22:35, 20.21s/it] 63%|██████▎   | 4118/6500 [27:12:18<12:57:00, 19.57s/it]                                                          63%|██████▎   | 4118/6500 [27:12:18<12:57:00, 19.57s/it] 63%|██████▎   | 4119/6500 [27:12:36<12:39:19, 19.13s/it]                                                          63%|██████▎   | 4119/6500 [27:12:36<12:39:19, 19.13s/it] 63%|██████▎   | 4120/6500 [27:12:54<12:26:59, 18.83s/it]                                                          63%|██████▎   | 4120/6500 [27:12:54<12:26:59, 18.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8879366517066956, 'eval_runtime': 5.3369, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.63}
                                                          63%|██████▎   | 4120/6500 [27:13:00<12:26:59, 18.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4120
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4120
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4120
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4120/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4120/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2429, 'learning_rate': 2.9583697645461848e-05, 'epoch': 0.63}
{'loss': 0.2551, 'learning_rate': 2.95616335554441e-05, 'epoch': 0.63}
{'loss': 0.2249, 'learning_rate': 2.953957424277024e-05, 'epoch': 0.63}
{'loss': 0.2312, 'learning_rate': 2.9517519712596498e-05, 'epoch': 0.63}
{'loss': 0.2178, 'learning_rate': 2.9495469970078e-05, 'epoch': 0.63}
 63%|██████▎   | 4121/6500 [27:14:06<22:52:09, 34.61s/it]                                                          63%|██████▎   | 4121/6500 [27:14:06<22:52:09, 34.61s/it] 63%|██████▎   | 4122/6500 [27:14:24<19:34:10, 29.63s/it]                                                          63%|██████▎   | 4122/6500 [27:14:24<19:34:10, 29.63s/it] 63%|██████▎   | 4123/6500 [27:14:42<17:15:43, 26.14s/it]                                                          63%|██████▎   | 4123/6500 [27:14:42<17:15:43, 26.14s/it] 63%|██████▎   | 4124/6500 [27:15:00<15:38:59, 23.71s/it]                                                          63%|██████▎   | 4124/6500 [27:15:00<15:38:59, 23.71s/it] 63%|██████▎   | 4125/6500 [27:15:18<14:31:23, 22.01s/it]                                                          63%|██████▎   | 4125/6500 [27:15:18<14:31:23, 22.01s/it] 63%|█{'loss': 0.2651, 'learning_rate': 2.9473425020368716e-05, 'epoch': 0.63}
{'loss': 0.2846, 'learning_rate': 2.9451384868621523e-05, 'epoch': 0.63}
{'loss': 0.2291, 'learning_rate': 2.942934951998819e-05, 'epoch': 0.64}
{'loss': 0.2504, 'learning_rate': 2.940731897961933e-05, 'epoch': 0.64}
{'loss': 0.2419, 'learning_rate': 2.9385293252664452e-05, 'epoch': 0.64}
█████▎   | 4126/6500 [27:15:36<13:45:53, 20.87s/it]                                                          63%|██████▎   | 4126/6500 [27:15:36<13:45:53, 20.87s/it] 63%|██████▎   | 4127/6500 [27:15:54<13:13:07, 20.05s/it]                                                          63%|██████▎   | 4127/6500 [27:15:54<13:13:07, 20.05s/it] 64%|██████▎   | 4128/6500 [27:16:12<12:50:20, 19.49s/it]                                                          64%|██████▎   | 4128/6500 [27:16:12<12:50:20, 19.49s/it] 64%|██████▎   | 4129/6500 [27:16:30<12:34:18, 19.09s/it]                                                          64%|██████▎   | 4129/6500 [27:16:30<12:34:18, 19.09s/it] 64%|██████▎   | 4130/6500 [27:16:49<12:22:54, 18.81s/it]                                                          64%|██████▎   | 4130/6500 [27:16:49<12:22:54, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8819548487663269, 'eval_runtime': 5.3497, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.122, 'epoch': 0.64}
                                                          64%|██████▎   | 4130/6500 [27:16:54<12:22:54, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4130
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4130/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4130
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4130/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4130/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7413, 'learning_rate': 2.936327234427195e-05, 'epoch': 0.64}
{'loss': 0.2349, 'learning_rate': 2.9341256259589052e-05, 'epoch': 0.64}
{'loss': 0.2503, 'learning_rate': 2.9319245003761895e-05, 'epoch': 0.64}
{'loss': 0.2337, 'learning_rate': 2.9297238581935482e-05, 'epoch': 0.64}
{'loss': 0.2437, 'learning_rate': 2.9275236999253674e-05, 'epoch': 0.64}
 64%|██████▎   | 4131/6500 [27:18:13<25:13:50, 38.34s/it]                                                          64%|██████▎   | 4131/6500 [27:18:13<25:13:50, 38.34s/it] 64%|██████▎   | 4132/6500 [27:18:31<21:12:28, 32.24s/it]                                                          64%|██████▎   | 4132/6500 [27:18:31<21:12:28, 32.24s/it] 64%|██████▎   | 4133/6500 [27:18:49<18:23:34, 27.97s/it]                                                          64%|██████▎   | 4133/6500 [27:18:49<18:23:34, 27.97s/it] 64%|██████▎   | 4134/6500 [27:19:07<16:25:14, 24.98s/it]                                                          64%|██████▎   | 4134/6500 [27:19:07<16:25:14, 24.98s/it] 64%|██████▎   | 4135/6500 [27:19:25<15:04:08, 22.94s/it]                                                          64%|██████▎   | 4135/6500 [27:19:25<15:04:08, 22.94s/it] 64%|█{'loss': 0.2394, 'learning_rate': 2.9253240260859215e-05, 'epoch': 0.64}
{'loss': 0.2144, 'learning_rate': 2.9231248371893695e-05, 'epoch': 0.64}
{'loss': 0.2499, 'learning_rate': 2.920926133749759e-05, 'epoch': 0.64}
{'loss': 0.2348, 'learning_rate': 2.9187279162810243e-05, 'epoch': 0.64}
{'loss': 0.2591, 'learning_rate': 2.916530185296984e-05, 'epoch': 0.64}
█████▎   | 4136/6500 [27:19:43<14:06:11, 21.48s/it]                                                          64%|██████▎   | 4136/6500 [27:19:43<14:06:11, 21.48s/it] 64%|██████▎   | 4137/6500 [27:20:01<13:27:26, 20.50s/it]                                                          64%|██████▎   | 4137/6500 [27:20:01<13:27:26, 20.50s/it] 64%|██████▎   | 4138/6500 [27:20:20<13:03:31, 19.90s/it]                                                          64%|██████▎   | 4138/6500 [27:20:20<13:03:31, 19.90s/it] 64%|██████▎   | 4139/6500 [27:20:38<12:43:08, 19.39s/it]                                                          64%|██████▎   | 4139/6500 [27:20:38<12:43:08, 19.39s/it] 64%|██████▎   | 4140/6500 [27:20:56<12:29:35, 19.06s/it]                                                          64%|██████▎   | 4140/6500 [27:20:56<12:29:35, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8896486163139343, 'eval_runtime': 5.3393, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.64}
                                                          64%|██████▎   | 4140/6500 [27:21:01<12:29:35, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4140/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4140
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4140/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2556, 'learning_rate': 2.9143329413113464e-05, 'epoch': 0.64}
{'loss': 0.2551, 'learning_rate': 2.9121361848377014e-05, 'epoch': 0.64}
{'loss': 0.2371, 'learning_rate': 2.909939916389529e-05, 'epoch': 0.64}
{'loss': 0.2459, 'learning_rate': 2.9077441364801938e-05, 'epoch': 0.64}
{'loss': 0.2644, 'learning_rate': 2.9055488456229473e-05, 'epoch': 0.64}
 64%|██████▎   | 4141/6500 [27:21:34<16:10:58, 24.70s/it]                                                          64%|██████▎   | 4141/6500 [27:21:34<16:10:58, 24.70s/it] 64%|██████▎   | 4142/6500 [27:21:52<14:52:52, 22.72s/it]                                                          64%|██████▎   | 4142/6500 [27:21:52<14:52:52, 22.72s/it] 64%|██████▎   | 4143/6500 [27:22:10<13:57:41, 21.32s/it]                                                          64%|██████▎   | 4143/6500 [27:22:10<13:57:41, 21.32s/it] 64%|██████▍   | 4144/6500 [27:22:28<13:18:49, 20.34s/it]                                                          64%|██████▍   | 4144/6500 [27:22:28<13:18:49, 20.34s/it] 64%|██████▍   | 4145/6500 [27:22:46<12:51:53, 19.67s/it]                                                          64%|██████▍   | 4145/6500 [27:22:46<12:51:53, 19.67s/it] 64%|█{'loss': 0.2343, 'learning_rate': 2.9033540443309227e-05, 'epoch': 0.64}
{'loss': 0.2534, 'learning_rate': 2.9011597331171414e-05, 'epoch': 0.64}
{'loss': 0.2568, 'learning_rate': 2.898965912494511e-05, 'epoch': 0.64}
{'loss': 0.2399, 'learning_rate': 2.8967725829758248e-05, 'epoch': 0.64}
{'loss': 0.2473, 'learning_rate': 2.8945797450737587e-05, 'epoch': 0.64}
█████▍   | 4146/6500 [27:23:04<12:33:14, 19.20s/it]                                                          64%|██████▍   | 4146/6500 [27:23:04<12:33:14, 19.20s/it] 64%|██████▍   | 4147/6500 [27:23:22<12:20:26, 18.88s/it]                                                          64%|██████▍   | 4147/6500 [27:23:22<12:20:26, 18.88s/it] 64%|██████▍   | 4148/6500 [27:23:41<12:11:33, 18.66s/it]                                                          64%|██████▍   | 4148/6500 [27:23:41<12:11:33, 18.66s/it] 64%|██████▍   | 4149/6500 [27:23:59<12:05:04, 18.50s/it]                                                          64%|██████▍   | 4149/6500 [27:23:59<12:05:04, 18.50s/it] 64%|██████▍   | 4150/6500 [27:24:17<12:03:07, 18.46s/it]                                                          64%|██████▍   | 4150/6500 [27:24:17<12:03:07, 18.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8855995535850525, 'eval_runtime': 5.4464, 'eval_samples_per_second': 4.223, 'eval_steps_per_second': 1.102, 'epoch': 0.64}
                                                          64%|██████▍   | 4150/6500 [27:24:23<12:03:07, 18.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4150 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4150
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4150/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4150/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.255, 'learning_rate': 2.892387399300877e-05, 'epoch': 0.64}
{'loss': 0.244, 'learning_rate': 2.8901955461696262e-05, 'epoch': 0.64}
{'loss': 0.2358, 'learning_rate': 2.888004186192338e-05, 'epoch': 0.64}
{'loss': 0.2343, 'learning_rate': 2.8858133198812305e-05, 'epoch': 0.64}
{'loss': 0.2243, 'learning_rate': 2.8836229477484046e-05, 'epoch': 0.64}
 64%|██████▍   | 4151/6500 [27:24:55<15:51:54, 24.31s/it]                                                          64%|██████▍   | 4151/6500 [27:24:55<15:51:54, 24.31s/it] 64%|██████▍   | 4152/6500 [27:25:13<14:38:37, 22.45s/it]                                                          64%|██████▍   | 4152/6500 [27:25:13<14:38:37, 22.45s/it] 64%|██████▍   | 4153/6500 [27:25:31<13:47:12, 21.15s/it]                                                          64%|██████▍   | 4153/6500 [27:25:31<13:47:12, 21.15s/it] 64%|██████▍   | 4154/6500 [27:25:50<13:15:32, 20.35s/it]                                                          64%|██████▍   | 4154/6500 [27:25:50<13:15:32, 20.35s/it] 64%|██████▍   | 4155/6500 [27:26:08<12:49:04, 19.68s/it]                                                          64%|██████▍   | 4155/6500 [27:26:08<12:49:04, 19.68s/it] 64%|█{'loss': 0.3173, 'learning_rate': 2.881433070305849e-05, 'epoch': 0.64}
{'loss': 0.2234, 'learning_rate': 2.8792436880654305e-05, 'epoch': 0.64}
{'loss': 0.2324, 'learning_rate': 2.8770548015389054e-05, 'epoch': 0.64}
{'loss': 0.2551, 'learning_rate': 2.8748664112379127e-05, 'epoch': 0.64}
{'loss': 0.7337, 'learning_rate': 2.872678517673975e-05, 'epoch': 0.64}
█████▍   | 4156/6500 [27:26:26<12:30:46, 19.22s/it]                                                          64%|██████▍   | 4156/6500 [27:26:26<12:30:46, 19.22s/it] 64%|██████▍   | 4157/6500 [27:26:44<12:18:04, 18.90s/it]                                                          64%|██████▍   | 4157/6500 [27:26:44<12:18:04, 18.90s/it] 64%|██████▍   | 4158/6500 [27:27:02<12:09:20, 18.69s/it]                                                          64%|██████▍   | 4158/6500 [27:27:02<12:09:20, 18.69s/it] 64%|██████▍   | 4159/6500 [27:27:21<12:03:25, 18.54s/it]                                                          64%|██████▍   | 4159/6500 [27:27:21<12:03:25, 18.54s/it] 64%|██████▍   | 4160/6500 [27:27:39<12:01:22, 18.50s/it]                                                          64%|██████▍   | 4160/6500 [27:27:39<12:01:22, 18.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8800987601280212, 'eval_runtime': 5.3367, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.64}
                                                          64%|██████▍   | 4160/6500 [27:27:44<12:01:22, 18.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4160
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4160
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4160
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4160
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4160/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4160/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.25, 'learning_rate': 2.8704911213584996e-05, 'epoch': 0.64}
{'loss': 0.2371, 'learning_rate': 2.8683042228027772e-05, 'epoch': 0.64}
{'loss': 0.2479, 'learning_rate': 2.866117822517982e-05, 'epoch': 0.64}
{'loss': 0.2234, 'learning_rate': 2.863931921015171e-05, 'epoch': 0.64}
{'loss': 0.2425, 'learning_rate': 2.861746518805286e-05, 'epoch': 0.64}
 64%|██████▍   | 4161/6500 [27:29:58<35:29:48, 54.63s/it]                                                          64%|██████▍   | 4161/6500 [27:29:58<35:29:48, 54.63s/it] 64%|██████▍   | 4162/6500 [27:30:16<28:19:58, 43.63s/it]                                                          64%|██████▍   | 4162/6500 [27:30:16<28:19:58, 43.63s/it] 64%|██████▍   | 4163/6500 [27:30:34<23:18:53, 35.92s/it]                                                          64%|██████▍   | 4163/6500 [27:30:34<23:18:53, 35.92s/it] 64%|██████▍   | 4164/6500 [27:30:52<19:48:26, 30.52s/it]                                                          64%|██████▍   | 4164/6500 [27:30:52<19:48:26, 30.52s/it] 64%|██████▍   | 4165/6500 [27:31:10<17:21:30, 26.76s/it]                                                          64%|██████▍   | 4165/6500 [27:31:10<17:21:30, 26.76s/it] 64%|█{'loss': 0.2264, 'learning_rate': 2.8595616163991523e-05, 'epoch': 0.64}
{'loss': 0.2202, 'learning_rate': 2.857377214307478e-05, 'epoch': 0.64}
{'loss': 0.2373, 'learning_rate': 2.8551933130408505e-05, 'epoch': 0.64}
{'loss': 0.2313, 'learning_rate': 2.8530099131097455e-05, 'epoch': 0.64}
{'loss': 0.2495, 'learning_rate': 2.850827015024519e-05, 'epoch': 0.64}
█████▍   | 4166/6500 [27:31:28<15:39:08, 24.14s/it]                                                          64%|██████▍   | 4166/6500 [27:31:28<15:39:08, 24.14s/it] 64%|██████▍   | 4167/6500 [27:31:46<14:27:42, 22.32s/it]                                                          64%|██████▍   | 4167/6500 [27:31:46<14:27:42, 22.32s/it] 64%|██████▍   | 4168/6500 [27:32:04<13:37:51, 21.04s/it]                                                          64%|██████▍   | 4168/6500 [27:32:04<13:37:51, 21.04s/it] 64%|██████▍   | 4169/6500 [27:32:22<13:03:20, 20.16s/it]                                                          64%|██████▍   | 4169/6500 [27:32:22<13:03:20, 20.16s/it] 64%|██████▍   | 4170/6500 [27:32:41<12:53:36, 19.92s/it]                                                          64%|██████▍   | 4170/6500 [27:32:41<12:53:36, 19.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8913965821266174, 'eval_runtime': 5.6373, 'eval_samples_per_second': 4.08, 'eval_steps_per_second': 1.064, 'epoch': 0.64}
                                                          64%|██████▍   | 4170/6500 [27:32:47<12:53:36, 19.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4170
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4170
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4170
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4170/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4170/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2384, 'learning_rate': 2.8486446192954107e-05, 'epoch': 0.64}
{'loss': 0.2599, 'learning_rate': 2.8464627264325426e-05, 'epoch': 0.64}
{'loss': 0.2421, 'learning_rate': 2.8442813369459187e-05, 'epoch': 0.64}
{'loss': 0.2573, 'learning_rate': 2.842100451345424e-05, 'epoch': 0.64}
{'loss': 0.2482, 'learning_rate': 2.8399200701408303e-05, 'epoch': 0.64}
 64%|██████▍   | 4171/6500 [27:34:14<26:55:59, 41.63s/it]                                                          64%|██████▍   | 4171/6500 [27:34:14<26:55:59, 41.63s/it] 64%|██████▍   | 4172/6500 [27:34:32<22:20:19, 34.54s/it]                                                          64%|██████▍   | 4172/6500 [27:34:32<22:20:19, 34.54s/it] 64%|██████▍   | 4173/6500 [27:34:50<19:07:43, 29.59s/it]                                                          64%|██████▍   | 4173/6500 [27:34:50<19:07:43, 29.59s/it] 64%|██████▍   | 4174/6500 [27:35:08<16:53:11, 26.14s/it]                                                          64%|██████▍   | 4174/6500 [27:35:08<16:53:11, 26.14s/it] 64%|██████▍   | 4175/6500 [27:35:26<15:19:22, 23.73s/it]                                                          64%|██████▍   | 4175/6500 [27:35:26<15:19:22, 23.73s/it] 64%|█{'loss': 0.2495, 'learning_rate': 2.8377401938417858e-05, 'epoch': 0.64}
{'loss': 0.2569, 'learning_rate': 2.835560822957824e-05, 'epoch': 0.64}
{'loss': 0.251, 'learning_rate': 2.8333819579983623e-05, 'epoch': 0.64}
{'loss': 0.2459, 'learning_rate': 2.8312035994726926e-05, 'epoch': 0.64}
{'loss': 0.2423, 'learning_rate': 2.8290257478899945e-05, 'epoch': 0.64}
█████▍   | 4176/6500 [27:35:44<14:14:01, 22.05s/it]                                                          64%|██████▍   | 4176/6500 [27:35:44<14:14:01, 22.05s/it] 64%|██████▍   | 4177/6500 [27:36:02<13:28:19, 20.88s/it]                                                          64%|██████▍   | 4177/6500 [27:36:02<13:28:19, 20.88s/it] 64%|██████▍   | 4178/6500 [27:36:20<12:56:38, 20.07s/it]                                                          64%|██████▍   | 4178/6500 [27:36:20<12:56:38, 20.07s/it] 64%|██████▍   | 4179/6500 [27:36:38<12:34:29, 19.50s/it]                                                          64%|██████▍   | 4179/6500 [27:36:38<12:34:29, 19.50s/it] 64%|██████▍   | 4180/6500 [27:36:57<12:19:01, 19.11s/it]                                                          64%|██████▍   | 4180/6500 [27:36:57<12:19:01, 19.11s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8837593793869019, 'eval_runtime': 5.3783, 'eval_samples_per_second': 4.276, 'eval_steps_per_second': 1.116, 'epoch': 0.64}
                                                          64%|██████▍   | 4180/6500 [27:37:02<12:19:01, 19.11s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4180
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4180 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4180


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4180
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4180/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2624, 'learning_rate': 2.8268484037593286e-05, 'epoch': 0.64}
{'loss': 0.2334, 'learning_rate': 2.824671567589635e-05, 'epoch': 0.64}
{'loss': 0.236, 'learning_rate': 2.8224952398897363e-05, 'epoch': 0.64}
{'loss': 0.2314, 'learning_rate': 2.820319421168336e-05, 'epoch': 0.64}
{'loss': 0.2637, 'learning_rate': 2.818144111934019e-05, 'epoch': 0.64}
 64%|██████▍   | 4181/6500 [27:38:15<23:43:40, 36.83s/it]                                                          64%|██████▍   | 4181/6500 [27:38:15<23:43:40, 36.83s/it] 64%|██████▍   | 4182/6500 [27:38:33<20:05:12, 31.20s/it]                                                          64%|██████▍   | 4182/6500 [27:38:33<20:05:12, 31.20s/it] 64%|██████▍   | 4183/6500 [27:38:51<17:32:15, 27.25s/it]                                                          64%|██████▍   | 4183/6500 [27:38:51<17:32:15, 27.25s/it] 64%|██████▍   | 4184/6500 [27:39:09<15:45:52, 24.50s/it]                                                          64%|██████▍   | 4184/6500 [27:39:09<15:45:52, 24.50s/it] 64%|██████▍   | 4185/6500 [27:39:27<14:31:37, 22.59s/it]                                                          64%|██████▍   | 4185/6500 [27:39:27<14:31:37, 22.59s/it] 64%|█{'loss': 0.3012, 'learning_rate': 2.815969312695249e-05, 'epoch': 0.64}
{'loss': 0.2325, 'learning_rate': 2.8137950239603734e-05, 'epoch': 0.64}
{'loss': 0.2375, 'learning_rate': 2.8116212462376183e-05, 'epoch': 0.64}
{'loss': 0.2477, 'learning_rate': 2.8094479800350938e-05, 'epoch': 0.64}
{'loss': 0.7376, 'learning_rate': 2.8072752258607828e-05, 'epoch': 0.64}
█████▍   | 4186/6500 [27:39:46<13:42:19, 21.32s/it]                                                          64%|██████▍   | 4186/6500 [27:39:46<13:42:19, 21.32s/it] 64%|██████▍   | 4187/6500 [27:40:04<13:05:00, 20.36s/it]                                                          64%|██████▍   | 4187/6500 [27:40:04<13:05:00, 20.36s/it] 64%|██████▍   | 4188/6500 [27:40:22<12:39:16, 19.70s/it]                                                          64%|██████▍   | 4188/6500 [27:40:22<12:39:16, 19.70s/it] 64%|██████▍   | 4189/6500 [27:40:40<12:21:24, 19.25s/it]                                                          64%|██████▍   | 4189/6500 [27:40:40<12:21:24, 19.25s/it] 64%|██████▍   | 4190/6500 [27:40:58<12:09:59, 18.96s/it]                                                          64%|██████▍   | 4190/6500 [27:40:58<12:09:59, 18.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8815320134162903, 'eval_runtime': 5.3432, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.64}
                                                          64%|██████▍   | 4190/6500 [27:41:04<12:09:59, 18.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4190
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4190
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4190
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4190
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4190/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4190/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2463, 'learning_rate': 2.8051029842225556e-05, 'epoch': 0.64}
{'loss': 0.2386, 'learning_rate': 2.8029312556281613e-05, 'epoch': 0.64}
{'loss': 0.2383, 'learning_rate': 2.8007600405852275e-05, 'epoch': 0.65}
{'loss': 0.235, 'learning_rate': 2.798589339601262e-05, 'epoch': 0.65}
{'loss': 0.2483, 'learning_rate': 2.7964191531836535e-05, 'epoch': 0.65}
 64%|██████▍   | 4191/6500 [27:41:59<20:13:15, 31.53s/it]                                                          64%|██████▍   | 4191/6500 [27:41:59<20:13:15, 31.53s/it] 64%|██████▍   | 4192/6500 [27:42:17<17:37:29, 27.49s/it]                                                          64%|██████▍   | 4192/6500 [27:42:17<17:37:29, 27.49s/it] 65%|██████▍   | 4193/6500 [27:42:35<15:48:00, 24.66s/it]                                                          65%|██████▍   | 4193/6500 [27:42:35<15:48:00, 24.66s/it] 65%|██████▍   | 4194/6500 [27:42:53<14:31:48, 22.68s/it]                                                          65%|██████▍   | 4194/6500 [27:42:53<14:31:48, 22.68s/it] 65%|██████▍   | 4195/6500 [27:43:11<13:38:42, 21.31s/it]                                                          65%|██████▍   | 4195/6500 [27:43:11<13:38:42, 21.31s/it] 65%|█{'loss': 0.2289, 'learning_rate': 2.794249481839669e-05, 'epoch': 0.65}
{'loss': 0.2332, 'learning_rate': 2.7920803260764582e-05, 'epoch': 0.65}
{'loss': 0.2406, 'learning_rate': 2.789911686401049e-05, 'epoch': 0.65}
{'loss': 0.2331, 'learning_rate': 2.787743563320343e-05, 'epoch': 0.65}
{'loss': 0.2433, 'learning_rate': 2.785575957341129e-05, 'epoch': 0.65}
█████▍   | 4196/6500 [27:43:30<13:05:41, 20.46s/it]                                                          65%|██████▍   | 4196/6500 [27:43:30<13:05:41, 20.46s/it] 65%|██████▍   | 4197/6500 [27:43:48<12:38:45, 19.77s/it]                                                          65%|██████▍   | 4197/6500 [27:43:48<12:38:45, 19.77s/it] 65%|██████▍   | 4198/6500 [27:44:06<12:19:57, 19.29s/it]                                                          65%|██████▍   | 4198/6500 [27:44:06<12:19:57, 19.29s/it] 65%|██████▍   | 4199/6500 [27:44:24<12:07:03, 18.96s/it]                                                          65%|██████▍   | 4199/6500 [27:44:24<12:07:03, 18.96s/it] 65%|██████▍   | 4200/6500 [27:44:43<11:57:54, 18.73s/it]                                                          65%|██████▍   | 4200/6500 [27:44:43<11:57:54, 18.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8931092023849487, 'eval_runtime': 5.6579, 'eval_samples_per_second': 4.065, 'eval_steps_per_second': 1.06, 'epoch': 0.65}
                                                          65%|██████▍   | 4200/6500 [27:44:48<11:57:54, 18.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4200the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4200
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4200
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4200/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4200/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2363, 'learning_rate': 2.783408868970071e-05, 'epoch': 0.65}
{'loss': 0.2432, 'learning_rate': 2.7812422987137128e-05, 'epoch': 0.65}
{'loss': 0.2426, 'learning_rate': 2.779076247078477e-05, 'epoch': 0.65}
{'loss': 0.2399, 'learning_rate': 2.7769107145706645e-05, 'epoch': 0.65}
{'loss': 0.2314, 'learning_rate': 2.7747457016964563e-05, 'epoch': 0.65}
 65%|██████▍   | 4201/6500 [27:46:51<32:57:47, 51.62s/it]                                                          65%|██████▍   | 4201/6500 [27:46:51<32:57:47, 51.62s/it] 65%|██████▍   | 4202/6500 [27:47:10<26:42:50, 41.85s/it]                                                          65%|██████▍   | 4202/6500 [27:47:10<26:42:50, 41.85s/it] 65%|██████▍   | 4203/6500 [27:47:28<22:07:58, 34.69s/it]                                                          65%|██████▍   | 4203/6500 [27:47:28<22:07:58, 34.69s/it] 65%|██████▍   | 4204/6500 [27:47:46<18:55:49, 29.68s/it]                                                          65%|██████▍   | 4204/6500 [27:47:46<18:55:49, 29.68s/it] 65%|██████▍   | 4205/6500 [27:48:04<16:41:31, 26.18s/it]                                                          65%|██████▍   | 4205/6500 [27:48:04<16:41:31, 26.18s/it] 65%|█{'loss': 0.2329, 'learning_rate': 2.772581208961911e-05, 'epoch': 0.65}
{'loss': 0.2419, 'learning_rate': 2.7704172368729642e-05, 'epoch': 0.65}
{'loss': 0.2372, 'learning_rate': 2.7682537859354328e-05, 'epoch': 0.65}
{'loss': 0.2419, 'learning_rate': 2.7660908566550113e-05, 'epoch': 0.65}
{'loss': 0.2388, 'learning_rate': 2.7639284495372682e-05, 'epoch': 0.65}
█████▍   | 4206/6500 [27:48:22<15:08:00, 23.75s/it]                                                          65%|██████▍   | 4206/6500 [27:48:22<15:08:00, 23.75s/it] 65%|██████▍   | 4207/6500 [27:48:40<14:03:49, 22.08s/it]                                                          65%|██████▍   | 4207/6500 [27:48:40<14:03:49, 22.08s/it] 65%|██████▍   | 4208/6500 [27:48:58<13:17:36, 20.88s/it]                                                          65%|██████▍   | 4208/6500 [27:48:58<13:17:36, 20.88s/it] 65%|██████▍   | 4209/6500 [27:49:16<12:45:22, 20.04s/it]                                                          65%|██████▍   | 4209/6500 [27:49:16<12:45:22, 20.04s/it] 65%|██████▍   | 4210/6500 [27:49:35<12:23:06, 19.47s/it]                                                          65%|██████▍   | 4210/6500 [27:49:35<12:23:06, 19.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8921712636947632, 'eval_runtime': 5.3457, 'eval_samples_per_second': 4.303, 'eval_steps_per_second': 1.122, 'epoch': 0.65}
                                                          65%|██████▍   | 4210/6500 [27:49:40<12:23:06, 19.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4210 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4210/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4210


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4210
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4210/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2496, 'learning_rate': 2.7617665650876538e-05, 'epoch': 0.65}
{'loss': 0.2337, 'learning_rate': 2.759605203811497e-05, 'epoch': 0.65}
{'loss': 0.2348, 'learning_rate': 2.7574443662140016e-05, 'epoch': 0.65}
{'loss': 0.2222, 'learning_rate': 2.7552840528002498e-05, 'epoch': 0.65}
{'loss': 0.2605, 'learning_rate': 2.7531242640752035e-05, 'epoch': 0.65}
 65%|██████▍   | 4211/6500 [27:50:32<19:36:28, 30.84s/it]                                                          65%|██████▍   | 4211/6500 [27:50:32<19:36:28, 30.84s/it] 65%|██████▍   | 4212/6500 [27:50:53<17:40:49, 27.82s/it]                                                          65%|██████▍   | 4212/6500 [27:50:53<17:40:49, 27.82s/it] 65%|██████▍   | 4213/6500 [27:51:11<15:50:01, 24.92s/it]                                                          65%|██████▍   | 4213/6500 [27:51:11<15:50:01, 24.92s/it] 65%|██████▍   | 4214/6500 [27:51:29<14:31:26, 22.87s/it]                                                          65%|██████▍   | 4214/6500 [27:51:29<14:31:26, 22.87s/it] 65%|██████▍   | 4215/6500 [27:51:47<13:36:39, 21.44s/it]                                                          65%|██████▍   | 4215/6500 [27:51:47<13:36:39, 21.44s/it] 65%|█{'loss': 0.2801, 'learning_rate': 2.7509650005436994e-05, 'epoch': 0.65}
{'loss': 0.2238, 'learning_rate': 2.7488062627104517e-05, 'epoch': 0.65}
{'loss': 0.2439, 'learning_rate': 2.7466480510800523e-05, 'epoch': 0.65}
{'loss': 0.3227, 'learning_rate': 2.744490366156971e-05, 'epoch': 0.65}
{'loss': 0.6564, 'learning_rate': 2.7423332084455544e-05, 'epoch': 0.65}
█████▍   | 4216/6500 [27:52:05<12:58:25, 20.45s/it]                                                          65%|██████▍   | 4216/6500 [27:52:05<12:58:25, 20.45s/it] 65%|██████▍   | 4217/6500 [27:52:23<12:31:50, 19.76s/it]                                                          65%|██████▍   | 4217/6500 [27:52:23<12:31:50, 19.76s/it] 65%|██████▍   | 4218/6500 [27:52:43<12:24:47, 19.58s/it]                                                          65%|██████▍   | 4218/6500 [27:52:43<12:24:47, 19.58s/it] 65%|██████▍   | 4219/6500 [27:53:01<12:08:15, 19.16s/it]                                                          65%|██████▍   | 4219/6500 [27:53:01<12:08:15, 19.16s/it] 65%|██████▍   | 4220/6500 [27:53:19<11:56:53, 18.87s/it]                                                          65%|██████▍   | 4220/6500 [27:53:19<11:56:53, 18.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8825762271881104, 'eval_runtime': 5.8282, 'eval_samples_per_second': 3.946, 'eval_steps_per_second': 1.029, 'epoch': 0.65}
                                                          65%|██████▍   | 4220/6500 [27:53:25<11:56:53, 18.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4220/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4220
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4220/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4220/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2502, 'learning_rate': 2.7401765784500215e-05, 'epoch': 0.65}
{'loss': 0.2473, 'learning_rate': 2.7380204766744737e-05, 'epoch': 0.65}
{'loss': 0.2243, 'learning_rate': 2.7358649036228866e-05, 'epoch': 0.65}
{'loss': 0.2431, 'learning_rate': 2.7337098597991116e-05, 'epoch': 0.65}
{'loss': 0.2378, 'learning_rate': 2.7315553457068778e-05, 'epoch': 0.65}
 65%|██████▍   | 4221/6500 [27:54:34<22:39:27, 35.79s/it]                                                          65%|██████▍   | 4221/6500 [27:54:34<22:39:27, 35.79s/it] 65%|██████▍   | 4222/6500 [27:54:52<19:16:42, 30.47s/it]                                                          65%|██████▍   | 4222/6500 [27:54:52<19:16:42, 30.47s/it] 65%|██████▍   | 4223/6500 [27:55:10<16:54:17, 26.73s/it]                                                          65%|██████▍   | 4223/6500 [27:55:10<16:54:17, 26.73s/it] 65%|██████▍   | 4224/6500 [27:55:28<15:14:46, 24.12s/it]                                                          65%|██████▍   | 4224/6500 [27:55:28<15:14:46, 24.12s/it] 65%|██████▌   | 4225/6500 [27:55:46<14:05:12, 22.29s/it]                                                          65%|██████▌   | 4225/6500 [27:55:46<14:05:12, 22.29s/it] 65%|█{'loss': 0.2171, 'learning_rate': 2.7294013618497894e-05, 'epoch': 0.65}
{'loss': 0.2509, 'learning_rate': 2.7272479087313274e-05, 'epoch': 0.65}
{'loss': 0.2342, 'learning_rate': 2.725094986854848e-05, 'epoch': 0.65}
{'loss': 0.2379, 'learning_rate': 2.7229425967235846e-05, 'epoch': 0.65}
{'loss': 0.2414, 'learning_rate': 2.720790738840644e-05, 'epoch': 0.65}
█████▌   | 4226/6500 [27:56:04<13:16:51, 21.03s/it]                                                          65%|██████▌   | 4226/6500 [27:56:04<13:16:51, 21.03s/it] 65%|██████▌   | 4227/6500 [27:56:22<12:43:21, 20.15s/it]                                                          65%|██████▌   | 4227/6500 [27:56:22<12:43:21, 20.15s/it] 65%|██████▌   | 4228/6500 [27:56:41<12:19:57, 19.54s/it]                                                          65%|██████▌   | 4228/6500 [27:56:41<12:19:57, 19.54s/it] 65%|██████▌   | 4229/6500 [27:56:59<12:03:52, 19.12s/it]                                                          65%|██████▌   | 4229/6500 [27:56:59<12:03:52, 19.12s/it] 65%|██████▌   | 4230/6500 [27:57:17<11:52:36, 18.84s/it]                                                          65%|██████▌   | 4230/6500 [27:57:17<11:52:36, 18.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8905285596847534, 'eval_runtime': 5.3402, 'eval_samples_per_second': 4.307, 'eval_steps_per_second': 1.124, 'epoch': 0.65}
                                                          65%|██████▌   | 4230/6500 [27:57:22<11:52:36, 18.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4230I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4230
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4230/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2404, 'learning_rate': 2.71863941370901e-05, 'epoch': 0.65}
{'loss': 0.2307, 'learning_rate': 2.7164886218315444e-05, 'epoch': 0.65}
{'loss': 0.2453, 'learning_rate': 2.7143383637109772e-05, 'epoch': 0.65}
{'loss': 0.2533, 'learning_rate': 2.7121886398499207e-05, 'epoch': 0.65}
{'loss': 0.2437, 'learning_rate': 2.71003945075086e-05, 'epoch': 0.65}
 65%|██████▌   | 4231/6500 [27:58:12<18:47:48, 29.82s/it]                                                          65%|██████▌   | 4231/6500 [27:58:12<18:47:48, 29.82s/it] 65%|██████▌   | 4232/6500 [27:58:30<16:33:52, 26.29s/it]                                                          65%|██████▌   | 4232/6500 [27:58:30<16:33:52, 26.29s/it] 65%|██████▌   | 4233/6500 [27:58:48<15:00:17, 23.83s/it]                                                          65%|██████▌   | 4233/6500 [27:58:48<15:00:17, 23.83s/it] 65%|██████▌   | 4234/6500 [27:59:07<13:54:55, 22.11s/it]                                                          65%|██████▌   | 4234/6500 [27:59:07<13:54:55, 22.11s/it] 65%|██████▌   | 4235/6500 [27:59:25<13:17:17, 21.12s/it]                                                          65%|██████▌   | 4235/6500 [27:59:25<13:17:17, 21.12s/it] 65%|█{'loss': 0.2525, 'learning_rate': 2.707890796916153e-05, 'epoch': 0.65}
{'loss': 0.2458, 'learning_rate': 2.7057426788480372e-05, 'epoch': 0.65}
{'loss': 0.2441, 'learning_rate': 2.7035950970486207e-05, 'epoch': 0.65}
{'loss': 0.24, 'learning_rate': 2.701448052019888e-05, 'epoch': 0.65}
{'loss': 0.245, 'learning_rate': 2.699301544263697e-05, 'epoch': 0.65}
█████▌   | 4236/6500 [27:59:44<12:43:11, 20.23s/it]                                                          65%|██████▌   | 4236/6500 [27:59:44<12:43:11, 20.23s/it] 65%|██████▌   | 4237/6500 [28:00:02<12:19:49, 19.62s/it]                                                          65%|██████▌   | 4237/6500 [28:00:02<12:19:49, 19.62s/it] 65%|██████▌   | 4238/6500 [28:00:20<12:03:21, 19.19s/it]                                                          65%|██████▌   | 4238/6500 [28:00:20<12:03:21, 19.19s/it] 65%|██████▌   | 4239/6500 [28:00:38<11:51:50, 18.89s/it]                                                          65%|██████▌   | 4239/6500 [28:00:38<11:51:50, 18.89s/it] 65%|██████▌   | 4240/6500 [28:00:56<11:44:03, 18.69s/it]                                                          65%|██████▌   | 4240/6500 [28:00:56<11:44:03, 18.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8903042078018188, 'eval_runtime': 5.5307, 'eval_samples_per_second': 4.159, 'eval_steps_per_second': 1.085, 'epoch': 0.65}
                                                          65%|██████▌   | 4240/6500 [28:01:02<11:44:03, 18.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4240
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4240
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4240
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4240/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4240/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4240/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2412, 'learning_rate': 2.6971555742817823e-05, 'epoch': 0.65}
{'loss': 0.2325, 'learning_rate': 2.6950101425757507e-05, 'epoch': 0.65}
{'loss': 0.2227, 'learning_rate': 2.6928652496470853e-05, 'epoch': 0.65}
{'loss': 0.2357, 'learning_rate': 2.690720895997138e-05, 'epoch': 0.65}
{'loss': 0.3051, 'learning_rate': 2.688577082127141e-05, 'epoch': 0.65}
 65%|██████▌   | 4241/6500 [28:02:08<21:44:18, 34.64s/it]                                                          65%|██████▌   | 4241/6500 [28:02:08<21:44:18, 34.64s/it] 65%|██████▌   | 4242/6500 [28:02:26<18:35:57, 29.65s/it]                                                          65%|██████▌   | 4242/6500 [28:02:26<18:35:57, 29.65s/it] 65%|██████▌   | 4243/6500 [28:02:44<16:23:54, 26.16s/it]                                                          65%|██████▌   | 4243/6500 [28:02:44<16:23:54, 26.16s/it] 65%|██████▌   | 4244/6500 [28:03:02<14:51:41, 23.72s/it]                                                          65%|██████▌   | 4244/6500 [28:03:02<14:51:41, 23.72s/it] 65%|██████▌   | 4245/6500 [28:03:20<13:47:29, 22.02s/it]                                                          65%|██████▌   | 4245/6500 [28:03:20<13:47:29, 22.02s/it] 65%|█{'loss': 0.2471, 'learning_rate': 2.6864338085381975e-05, 'epoch': 0.65}
{'loss': 0.2293, 'learning_rate': 2.6842910757312843e-05, 'epoch': 0.65}
{'loss': 0.2577, 'learning_rate': 2.6821488842072524e-05, 'epoch': 0.65}
{'loss': 0.734, 'learning_rate': 2.6800072344668258e-05, 'epoch': 0.65}
{'loss': 0.251, 'learning_rate': 2.6778661270106025e-05, 'epoch': 0.65}
█████▌   | 4246/6500 [28:03:38<13:02:22, 20.83s/it]                                                          65%|██████▌   | 4246/6500 [28:03:38<13:02:22, 20.83s/it] 65%|██████▌   | 4247/6500 [28:03:56<12:31:17, 20.01s/it]                                                          65%|██████▌   | 4247/6500 [28:03:56<12:31:17, 20.01s/it] 65%|██████▌   | 4248/6500 [28:04:15<12:09:47, 19.44s/it]                                                          65%|██████▌   | 4248/6500 [28:04:15<12:09:47, 19.44s/it] 65%|██████▌   | 4249/6500 [28:04:33<11:54:33, 19.05s/it]                                                          65%|██████▌   | 4249/6500 [28:04:33<11:54:33, 19.05s/it] 65%|██████▌   | 4250/6500 [28:04:51<11:44:30, 18.79s/it]                                                          65%|██████▌   | 4250/6500 [28:04:51<11:44:30, 18.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8855298757553101, 'eval_runtime': 5.3386, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.65}
                                                          65%|██████▌   | 4250/6500 [28:04:56<11:44:30, 18.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4250
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4250
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4250/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2321, 'learning_rate': 2.6757255623390532e-05, 'epoch': 0.65}
{'loss': 0.2495, 'learning_rate': 2.6735855409525224e-05, 'epoch': 0.65}
{'loss': 0.2167, 'learning_rate': 2.6714460633512262e-05, 'epoch': 0.65}
{'loss': 0.2434, 'learning_rate': 2.6693071300352568e-05, 'epoch': 0.65}
{'loss': 0.2242, 'learning_rate': 2.6671687415045733e-05, 'epoch': 0.65}
 65%|██████▌   | 4251/6500 [28:06:07<22:24:19, 35.86s/it]                                                          65%|██████▌   | 4251/6500 [28:06:07<22:24:19, 35.86s/it] 65%|██████▌   | 4252/6500 [28:06:25<19:02:50, 30.50s/it]                                                          65%|██████▌   | 4252/6500 [28:06:25<19:02:50, 30.50s/it] 65%|██████▌   | 4253/6500 [28:06:43<16:41:54, 26.75s/it]                                                          65%|██████▌   | 4253/6500 [28:06:43<16:41:54, 26.75s/it] 65%|██████▌   | 4254/6500 [28:07:01<15:03:32, 24.14s/it]                                                          65%|██████▌   | 4254/6500 [28:07:01<15:03:32, 24.14s/it] 65%|██████▌   | 4255/6500 [28:07:19<13:54:52, 22.31s/it]                                                          65%|██████▌   | 4255/6500 [28:07:19<13:54:52, 22.31s/it] 65%|█{'loss': 0.2183, 'learning_rate': 2.665030898259012e-05, 'epoch': 0.65}
{'loss': 0.2419, 'learning_rate': 2.6628936007982815e-05, 'epoch': 0.65}
{'loss': 0.233, 'learning_rate': 2.660756849621962e-05, 'epoch': 0.66}
{'loss': 0.2437, 'learning_rate': 2.6586206452295058e-05, 'epoch': 0.66}
{'loss': 0.2291, 'learning_rate': 2.6564849881202393e-05, 'epoch': 0.66}
█████▌   | 4256/6500 [28:07:37<13:07:06, 21.05s/it]                                                          65%|██████▌   | 4256/6500 [28:07:37<13:07:06, 21.05s/it] 65%|██████▌   | 4257/6500 [28:07:55<12:33:28, 20.16s/it]                                                          65%|██████▌   | 4257/6500 [28:07:55<12:33:28, 20.16s/it] 66%|██████▌   | 4258/6500 [28:08:13<12:10:13, 19.54s/it]                                                          66%|██████▌   | 4258/6500 [28:08:13<12:10:13, 19.54s/it] 66%|██████▌   | 4259/6500 [28:08:31<11:54:16, 19.12s/it]                                                          66%|██████▌   | 4259/6500 [28:08:31<11:54:16, 19.12s/it] 66%|██████▌   | 4260/6500 [28:08:49<11:42:56, 18.83s/it]                                                          66%|██████▌   | 4260/6500 [28:08:49<11:42:56, 18.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8927764892578125, 'eval_runtime': 5.4385, 'eval_samples_per_second': 4.229, 'eval_steps_per_second': 1.103, 'epoch': 0.66}
                                                          66%|██████▌   | 4260/6500 [28:08:55<11:42:56, 18.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4260/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4260

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4260

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4260
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4260/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4260/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2475, 'learning_rate': 2.6543498787933563e-05, 'epoch': 0.66}
{'loss': 0.2308, 'learning_rate': 2.652215317747928e-05, 'epoch': 0.66}
{'loss': 0.2493, 'learning_rate': 2.650081305482895e-05, 'epoch': 0.66}
{'loss': 0.2326, 'learning_rate': 2.6479478424970683e-05, 'epoch': 0.66}
{'loss': 0.2295, 'learning_rate': 2.6458149292891353e-05, 'epoch': 0.66}
 66%|██████▌   | 4261/6500 [28:10:23<25:47:13, 41.46s/it]                                                          66%|██████▌   | 4261/6500 [28:10:24<25:47:13, 41.46s/it] 66%|██████▌   | 4262/6500 [28:10:42<21:24:47, 34.44s/it]                                                          66%|██████▌   | 4262/6500 [28:10:42<21:24:47, 34.44s/it] 66%|██████▌   | 4263/6500 [28:11:00<18:19:53, 29.50s/it]                                                          66%|██████▌   | 4263/6500 [28:11:00<18:19:53, 29.50s/it] 66%|██████▌   | 4264/6500 [28:11:18<16:10:32, 26.04s/it]                                                          66%|██████▌   | 4264/6500 [28:11:18<16:10:32, 26.04s/it] 66%|██████▌   | 4265/6500 [28:11:36<14:40:21, 23.63s/it]                                                          66%|██████▌   | 4265/6500 [28:11:36<14:40:21, 23.63s/it] 66%|█{'loss': 0.2398, 'learning_rate': 2.6436825663576466e-05, 'epoch': 0.66}
{'loss': 0.2339, 'learning_rate': 2.6415507542010315e-05, 'epoch': 0.66}
{'loss': 0.2404, 'learning_rate': 2.6394194933175875e-05, 'epoch': 0.66}
{'loss': 0.2472, 'learning_rate': 2.637288784205485e-05, 'epoch': 0.66}
{'loss': 0.2542, 'learning_rate': 2.6351586273627636e-05, 'epoch': 0.66}
█████▌   | 4266/6500 [28:11:54<13:37:32, 21.96s/it]                                                          66%|██████▌   | 4266/6500 [28:11:54<13:37:32, 21.96s/it] 66%|██████▌   | 4267/6500 [28:12:12<12:58:51, 20.93s/it]                                                          66%|██████▌   | 4267/6500 [28:12:12<12:58:51, 20.93s/it] 66%|██████▌   | 4268/6500 [28:12:30<12:26:42, 20.07s/it]                                                          66%|██████▌   | 4268/6500 [28:12:30<12:26:42, 20.07s/it] 66%|██████▌   | 4269/6500 [28:12:48<12:04:16, 19.48s/it]                                                          66%|██████▌   | 4269/6500 [28:12:48<12:04:16, 19.48s/it] 66%|██████▌   | 4270/6500 [28:13:06<11:48:43, 19.07s/it]                                                          66%|██████▌   | 4270/6500 [28:13:06<11:48:43, 19.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8904525637626648, 'eval_runtime': 6.1786, 'eval_samples_per_second': 3.722, 'eval_steps_per_second': 0.971, 'epoch': 0.66}
                                                          66%|██████▌   | 4270/6500 [28:13:13<11:48:43, 19.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4270/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4270
 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4270
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4270/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4270/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2215, 'learning_rate': 2.6330290232873345e-05, 'epoch': 0.66}
{'loss': 0.236, 'learning_rate': 2.6308999724769778e-05, 'epoch': 0.66}
{'loss': 0.2208, 'learning_rate': 2.6287714754293503e-05, 'epoch': 0.66}
{'loss': 0.2584, 'learning_rate': 2.6266435326419747e-05, 'epoch': 0.66}
{'loss': 0.2916, 'learning_rate': 2.624516144612241e-05, 'epoch': 0.66}
 66%|██████▌   | 4271/6500 [28:14:27<23:11:08, 37.45s/it]                                                          66%|██████▌   | 4271/6500 [28:14:27<23:11:08, 37.45s/it] 66%|██████▌   | 4272/6500 [28:14:45<19:34:17, 31.62s/it]                                                          66%|██████▌   | 4272/6500 [28:14:45<19:34:17, 31.62s/it] 66%|██████▌   | 4273/6500 [28:15:03<17:02:18, 27.54s/it]                                                          66%|██████▌   | 4273/6500 [28:15:03<17:02:18, 27.54s/it] 66%|██████▌   | 4274/6500 [28:15:21<15:16:03, 24.69s/it]                                                          66%|██████▌   | 4274/6500 [28:15:21<15:16:03, 24.69s/it] 66%|██████▌   | 4275/6500 [28:15:39<14:02:06, 22.71s/it]                                                          66%|██████▌   | 4275/6500 [28:15:39<14:02:06, 22.71s/it] 66%|█{'loss': 0.228, 'learning_rate': 2.6223893118374154e-05, 'epoch': 0.66}
{'loss': 0.2472, 'learning_rate': 2.6202630348146324e-05, 'epoch': 0.66}
{'loss': 0.2254, 'learning_rate': 2.618137314040896e-05, 'epoch': 0.66}
{'loss': 0.7189, 'learning_rate': 2.61601215001308e-05, 'epoch': 0.66}
{'loss': 0.244, 'learning_rate': 2.6138875432279297e-05, 'epoch': 0.66}
█████▌   | 4276/6500 [28:15:57<13:10:30, 21.33s/it]                                                          66%|██████▌   | 4276/6500 [28:15:57<13:10:30, 21.33s/it] 66%|██████▌   | 4277/6500 [28:16:15<12:34:06, 20.35s/it]                                                          66%|██████▌   | 4277/6500 [28:16:15<12:34:06, 20.35s/it] 66%|██████▌   | 4278/6500 [28:16:33<12:08:49, 19.68s/it]                                                          66%|██████▌   | 4278/6500 [28:16:33<12:08:49, 19.68s/it] 66%|██████▌   | 4279/6500 [28:16:51<11:52:46, 19.26s/it]                                                          66%|██████▌   | 4279/6500 [28:16:51<11:52:46, 19.26s/it] 66%|██████▌   | 4280/6500 [28:17:10<11:40:46, 18.94s/it]                                                          66%|██████▌   | 4280/6500 [28:17:10<11:40:46, 18.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8837787508964539, 'eval_runtime': 5.5668, 'eval_samples_per_second': 4.132, 'eval_steps_per_second': 1.078, 'epoch': 0.66}
                                                          66%|██████▌   | 4280/6500 [28:17:15<11:40:46, 18.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4280
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4280 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4280the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4280/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4280/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4280/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2422, 'learning_rate': 2.6117634941820578e-05, 'epoch': 0.66}
{'loss': 0.2269, 'learning_rate': 2.6096400033719487e-05, 'epoch': 0.66}
{'loss': 0.2298, 'learning_rate': 2.607517071293955e-05, 'epoch': 0.66}
{'loss': 0.2361, 'learning_rate': 2.6053946984443e-05, 'epoch': 0.66}
{'loss': 0.2227, 'learning_rate': 2.6032728853190757e-05, 'epoch': 0.66}
 66%|██████▌   | 4281/6500 [28:18:58<28:14:33, 45.82s/it]                                                          66%|██████▌   | 4281/6500 [28:18:58<28:14:33, 45.82s/it] 66%|██████▌   | 4282/6500 [28:19:16<23:05:32, 37.48s/it]                                                          66%|██████▌   | 4282/6500 [28:19:16<23:05:32, 37.48s/it] 66%|██████▌   | 4283/6500 [28:19:35<19:35:07, 31.80s/it]                                                          66%|██████▌   | 4283/6500 [28:19:35<19:35:07, 31.80s/it] 66%|██████▌   | 4284/6500 [28:19:53<17:01:37, 27.66s/it]                                                          66%|██████▌   | 4284/6500 [28:19:53<17:01:37, 27.66s/it] 66%|██████▌   | 4285/6500 [28:20:11<15:14:25, 24.77s/it]                                                          66%|██████▌   | 4285/6500 [28:20:11<15:14:25, 24.77s/it] 66%|█{'loss': 0.2336, 'learning_rate': 2.601151632414241e-05, 'epoch': 0.66}
{'loss': 0.232, 'learning_rate': 2.5990309402256264e-05, 'epoch': 0.66}
{'loss': 0.2329, 'learning_rate': 2.596910809248932e-05, 'epoch': 0.66}
{'loss': 0.246, 'learning_rate': 2.5947912399797246e-05, 'epoch': 0.66}
{'loss': 0.2265, 'learning_rate': 2.5926722329134412e-05, 'epoch': 0.66}
█████▌   | 4286/6500 [28:20:29<13:59:42, 22.76s/it]                                                          66%|██████▌   | 4286/6500 [28:20:29<13:59:42, 22.76s/it] 66%|██████▌   | 4287/6500 [28:20:47<13:07:29, 21.35s/it]                                                          66%|██████▌   | 4287/6500 [28:20:47<13:07:29, 21.35s/it] 66%|██████▌   | 4288/6500 [28:21:05<12:31:03, 20.37s/it]                                                          66%|██████▌   | 4288/6500 [28:21:05<12:31:03, 20.37s/it] 66%|██████▌   | 4289/6500 [28:21:23<12:05:43, 19.69s/it]                                                          66%|██████▌   | 4289/6500 [28:21:23<12:05:43, 19.69s/it] 66%|██████▌   | 4290/6500 [28:21:41<11:48:03, 19.22s/it]                                                          66%|██████▌   | 4290/6500 [28:21:41<11:48:03, 19.22s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8966537117958069, 'eval_runtime': 5.3364, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.124, 'epoch': 0.66}
                                                          66%|██████▌   | 4290/6500 [28:21:47<11:48:03, 19.22s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4290
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4290

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4290
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4290/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4290/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2443, 'learning_rate': 2.5905537885453856e-05, 'epoch': 0.66}
{'loss': 0.2343, 'learning_rate': 2.588435907370733e-05, 'epoch': 0.66}
{'loss': 0.2518, 'learning_rate': 2.5863185898845243e-05, 'epoch': 0.66}
{'loss': 0.2235, 'learning_rate': 2.58420183658167e-05, 'epoch': 0.66}
{'loss': 0.2396, 'learning_rate': 2.5820856479569487e-05, 'epoch': 0.66}
 66%|██████▌   | 4291/6500 [28:22:43<19:35:04, 31.92s/it]                                                          66%|██████▌   | 4291/6500 [28:22:43<19:35:04, 31.92s/it] 66%|██████▌   | 4292/6500 [28:23:01<17:01:03, 27.75s/it]                                                          66%|██████▌   | 4292/6500 [28:23:01<17:01:03, 27.75s/it] 66%|██████▌   | 4293/6500 [28:23:19<15:13:18, 24.83s/it]                                                          66%|██████▌   | 4293/6500 [28:23:19<15:13:18, 24.83s/it] 66%|██████▌   | 4294/6500 [28:23:37<13:57:53, 22.79s/it]                                                          66%|██████▌   | 4294/6500 [28:23:37<13:57:53, 22.79s/it] 66%|██████▌   | 4295/6500 [28:23:55<13:05:12, 21.37s/it]                                                          66%|██████▌   | 4295/6500 [28:23:55<13:05:12, 21.37s/it] 66%|█{'loss': 0.2563, 'learning_rate': 2.5799700245050074e-05, 'epoch': 0.66}
{'loss': 0.2341, 'learning_rate': 2.5778549667203568e-05, 'epoch': 0.66}
{'loss': 0.2495, 'learning_rate': 2.5757404750973806e-05, 'epoch': 0.66}
{'loss': 0.2384, 'learning_rate': 2.573626550130329e-05, 'epoch': 0.66}
{'loss': 0.2432, 'learning_rate': 2.5715131923133184e-05, 'epoch': 0.66}
█████▌   | 4296/6500 [28:24:13<12:28:23, 20.37s/it]                                                          66%|██████▌   | 4296/6500 [28:24:13<12:28:23, 20.37s/it] 66%|██████▌   | 4297/6500 [28:24:31<12:03:11, 19.70s/it]                                                          66%|██████▌   | 4297/6500 [28:24:31<12:03:11, 19.70s/it] 66%|██████▌   | 4298/6500 [28:24:49<11:45:27, 19.22s/it]                                                          66%|██████▌   | 4298/6500 [28:24:49<11:45:27, 19.22s/it] 66%|██████▌   | 4299/6500 [28:25:08<11:35:39, 18.96s/it]                                                          66%|██████▌   | 4299/6500 [28:25:08<11:35:39, 18.96s/it] 66%|██████▌   | 4300/6500 [28:25:26<11:26:07, 18.71s/it]                                                          66%|██████▌   | 4300/6500 [28:25:26<11:26:07, 18.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8907667994499207, 'eval_runtime': 5.3329, 'eval_samples_per_second': 4.313, 'eval_steps_per_second': 1.125, 'epoch': 0.66}
                                                          66%|██████▌   | 4300/6500 [28:25:31<11:26:07, 18.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4300
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4300
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4300

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4300
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4300/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4300/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2211, 'learning_rate': 2.569400402140334e-05, 'epoch': 0.66}
{'loss': 0.2276, 'learning_rate': 2.5672881801052273e-05, 'epoch': 0.66}
{'loss': 0.219, 'learning_rate': 2.565176526701717e-05, 'epoch': 0.66}
{'loss': 0.2993, 'learning_rate': 2.5630654424233903e-05, 'epoch': 0.66}
{'loss': 0.2407, 'learning_rate': 2.560954927763699e-05, 'epoch': 0.66}
 66%|██████▌   | 4301/6500 [28:26:45<22:37:55, 37.05s/it]                                                          66%|██████▌   | 4301/6500 [28:26:45<22:37:55, 37.05s/it] 66%|██████▌   | 4302/6500 [28:27:03<19:07:26, 31.32s/it]                                                          66%|██████▌   | 4302/6500 [28:27:03<19:07:26, 31.32s/it] 66%|██████▌   | 4303/6500 [28:27:21<16:40:21, 27.32s/it]                                                          66%|██████▌   | 4303/6500 [28:27:21<16:40:21, 27.32s/it] 66%|██████▌   | 4304/6500 [28:27:39<14:57:55, 24.53s/it]                                                          66%|██████▌   | 4304/6500 [28:27:39<14:57:55, 24.53s/it] 66%|██████▌   | 4305/6500 [28:27:58<13:46:17, 22.59s/it]                                                          66%|██████▌   | 4305/6500 [28:27:58<13:46:17, 22.59s/it] 66%|█{'loss': 0.2241, 'learning_rate': 2.5588449832159633e-05, 'epoch': 0.66}
{'loss': 0.245, 'learning_rate': 2.556735609273373e-05, 'epoch': 0.66}
{'loss': 0.7188, 'learning_rate': 2.554626806428977e-05, 'epoch': 0.66}
{'loss': 0.2492, 'learning_rate': 2.5525185751756963e-05, 'epoch': 0.66}
{'loss': 0.2276, 'learning_rate': 2.5504109160063182e-05, 'epoch': 0.66}
█████▌   | 4306/6500 [28:28:16<12:56:11, 21.23s/it]                                                          66%|██████▌   | 4306/6500 [28:28:16<12:56:11, 21.23s/it] 66%|██████▋   | 4307/6500 [28:28:34<12:21:31, 20.29s/it]                                                          66%|██████▋   | 4307/6500 [28:28:34<12:21:31, 20.29s/it] 66%|██████▋   | 4308/6500 [28:28:52<11:57:09, 19.63s/it]                                                          66%|██████▋   | 4308/6500 [28:28:52<11:57:09, 19.63s/it] 66%|██████▋   | 4309/6500 [28:29:10<11:40:17, 19.18s/it]                                                          66%|██████▋   | 4309/6500 [28:29:10<11:40:17, 19.18s/it] 66%|██████▋   | 4310/6500 [28:29:28<11:28:42, 18.87s/it]                                                          66%|██████▋   | 4310/6500 [28:29:28<11:28:42, 18.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8898993730545044, 'eval_runtime': 5.3497, 'eval_samples_per_second': 4.299, 'eval_steps_per_second': 1.122, 'epoch': 0.66}
                                                          66%|██████▋   | 4310/6500 [28:29:33<11:28:42, 18.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4310
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4310
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4310
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4310/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4310/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4310/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2517, 'learning_rate': 2.5483038294134943e-05, 'epoch': 0.66}
{'loss': 0.228, 'learning_rate': 2.5461973158897435e-05, 'epoch': 0.66}
{'loss': 0.2387, 'learning_rate': 2.5440913759274514e-05, 'epoch': 0.66}
{'loss': 0.23, 'learning_rate': 2.5419860100188674e-05, 'epoch': 0.66}
{'loss': 0.2132, 'learning_rate': 2.5398812186561095e-05, 'epoch': 0.66}
 66%|██████▋   | 4311/6500 [28:30:49<22:49:31, 37.54s/it]                                                          66%|██████▋   | 4311/6500 [28:30:49<22:49:31, 37.54s/it] 66%|██████▋   | 4312/6500 [28:31:07<19:14:56, 31.67s/it]                                                          66%|██████▋   | 4312/6500 [28:31:07<19:14:56, 31.67s/it] 66%|██████▋   | 4313/6500 [28:31:25<16:44:38, 27.56s/it]                                                          66%|██████▋   | 4313/6500 [28:31:25<16:44:38, 27.56s/it] 66%|██████▋   | 4314/6500 [28:31:43<14:59:56, 24.70s/it]                                                          66%|██████▋   | 4314/6500 [28:31:43<14:59:56, 24.70s/it] 66%|██████▋   | 4315/6500 [28:32:02<13:50:42, 22.81s/it]                                                          66%|██████▋   | 4315/6500 [28:32:02<13:50:42, 22.81s/it] 66%|█{'loss': 0.2502, 'learning_rate': 2.537777002331158e-05, 'epoch': 0.66}
{'loss': 0.2322, 'learning_rate': 2.535673361535862e-05, 'epoch': 0.66}
{'loss': 0.2532, 'learning_rate': 2.5335702967619347e-05, 'epoch': 0.66}
{'loss': 0.2444, 'learning_rate': 2.5314678085009558e-05, 'epoch': 0.66}
{'loss': 0.2481, 'learning_rate': 2.5293658972443663e-05, 'epoch': 0.66}
█████▋   | 4316/6500 [28:32:20<12:58:31, 21.39s/it]                                                          66%|██████▋   | 4316/6500 [28:32:20<12:58:31, 21.39s/it] 66%|██████▋   | 4317/6500 [28:32:38<12:27:33, 20.55s/it]                                                          66%|██████▋   | 4317/6500 [28:32:38<12:27:33, 20.55s/it] 66%|██████▋   | 4318/6500 [28:32:56<12:00:57, 19.82s/it]                                                          66%|██████▋   | 4318/6500 [28:32:56<12:00:57, 19.82s/it] 66%|██████▋   | 4319/6500 [28:33:14<11:42:03, 19.31s/it]                                                          66%|██████▋   | 4319/6500 [28:33:14<11:42:03, 19.31s/it] 66%|██████▋   | 4320/6500 [28:33:33<11:29:06, 18.97s/it]                                                          66%|██████▋   | 4320/6500 [28:33:33<11:29:06, 18.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8931580185890198, 'eval_runtime': 5.3959, 'eval_samples_per_second': 4.262, 'eval_steps_per_second': 1.112, 'epoch': 0.66}
                                                          66%|██████▋   | 4320/6500 [28:33:38<11:29:06, 18.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4320 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4320 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4320

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4320
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4320/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4320/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2351, 'learning_rate': 2.5272645634834764e-05, 'epoch': 0.66}
{'loss': 0.256, 'learning_rate': 2.5251638077094602e-05, 'epoch': 0.66}
{'loss': 0.2434, 'learning_rate': 2.523063630413357e-05, 'epoch': 0.67}
{'loss': 0.236, 'learning_rate': 2.5209640320860694e-05, 'epoch': 0.67}
{'loss': 0.2496, 'learning_rate': 2.5188650132183677e-05, 'epoch': 0.67}
 66%|██████▋   | 4321/6500 [28:34:33<18:57:51, 31.33s/it]                                                          66%|██████▋   | 4321/6500 [28:34:33<18:57:51, 31.33s/it] 66%|██████▋   | 4322/6500 [28:34:51<16:32:30, 27.34s/it]                                                          66%|██████▋   | 4322/6500 [28:34:51<16:32:30, 27.34s/it] 67%|██████▋   | 4323/6500 [28:35:09<14:50:42, 24.55s/it]                                                          67%|██████▋   | 4323/6500 [28:35:09<14:50:42, 24.55s/it] 67%|██████▋   | 4324/6500 [28:35:27<13:39:35, 22.60s/it]                                                          67%|██████▋   | 4324/6500 [28:35:27<13:39:35, 22.60s/it] 67%|██████▋   | 4325/6500 [28:35:45<12:51:48, 21.29s/it]                                                          67%|██████▋   | 4325/6500 [28:35:45<12:51:48, 21.29s/it] 67%|█{'loss': 0.2424, 'learning_rate': 2.5167665743008828e-05, 'epoch': 0.67}
{'loss': 0.2458, 'learning_rate': 2.5146687158241132e-05, 'epoch': 0.67}
{'loss': 0.2375, 'learning_rate': 2.5125714382784198e-05, 'epoch': 0.67}
{'loss': 0.2571, 'learning_rate': 2.5104747421540294e-05, 'epoch': 0.67}
{'loss': 0.237, 'learning_rate': 2.5083786279410325e-05, 'epoch': 0.67}
█████▋   | 4326/6500 [28:36:03<12:16:38, 20.33s/it]                                                          67%|██████▋   | 4326/6500 [28:36:03<12:16:38, 20.33s/it] 67%|██████▋   | 4327/6500 [28:36:21<11:52:07, 19.66s/it]                                                          67%|██████▋   | 4327/6500 [28:36:21<11:52:07, 19.66s/it] 67%|██████▋   | 4328/6500 [28:36:39<11:35:04, 19.20s/it]                                                          67%|██████▋   | 4328/6500 [28:36:39<11:35:04, 19.20s/it] 67%|██████▋   | 4329/6500 [28:36:58<11:23:15, 18.88s/it]                                                          67%|██████▋   | 4329/6500 [28:36:58<11:23:15, 18.88s/it] 67%|██████▋   | 4330/6500 [28:37:16<11:14:54, 18.66s/it]                                                          67%|██████▋   | 4330/6500 [28:37:16<11:14:54, 18.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8888494372367859, 'eval_runtime': 5.3432, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 1.123, 'epoch': 0.67}
                                                          67%|██████▋   | 4330/6500 [28:37:21<11:14:54, 18.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4330/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4330
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4330/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2317, 'learning_rate': 2.5062830961293815e-05, 'epoch': 0.67}
{'loss': 0.2277, 'learning_rate': 2.5041881472088934e-05, 'epoch': 0.67}
{'loss': 0.2384, 'learning_rate': 2.502093781669252e-05, 'epoch': 0.67}
{'loss': 0.2889, 'learning_rate': 2.500000000000001e-05, 'epoch': 0.67}
{'loss': 0.2267, 'learning_rate': 2.49790680269055e-05, 'epoch': 0.67}
 67%|██████▋   | 4331/6500 [28:38:36<22:19:44, 37.06s/it]                                                          67%|██████▋   | 4331/6500 [28:38:36<22:19:44, 37.06s/it] 67%|██████▋   | 4332/6500 [28:38:54<18:52:25, 31.34s/it]                                                          67%|██████▋   | 4332/6500 [28:38:54<18:52:25, 31.34s/it] 67%|██████▋   | 4333/6500 [28:39:12<16:27:18, 27.34s/it]                                                          67%|██████▋   | 4333/6500 [28:39:12<16:27:18, 27.34s/it] 67%|██████▋   | 4334/6500 [28:39:30<14:45:36, 24.53s/it]                                                          67%|██████▋   | 4334/6500 [28:39:30<14:45:36, 24.53s/it] 67%|██████▋   | 4335/6500 [28:39:48<13:34:54, 22.58s/it]                                                          67%|██████▋   | 4335/6500 [28:39:48<13:34:54, 22.58s/it] 67%|█{'loss': 0.2292, 'learning_rate': 2.495814190230171e-05, 'epoch': 0.67}
{'loss': 0.2445, 'learning_rate': 2.4937221631079993e-05, 'epoch': 0.67}
{'loss': 0.7207, 'learning_rate': 2.4916307218130337e-05, 'epoch': 0.67}
{'loss': 0.2481, 'learning_rate': 2.4895398668341352e-05, 'epoch': 0.67}
{'loss': 0.2353, 'learning_rate': 2.4874495986600294e-05, 'epoch': 0.67}
█████▋   | 4336/6500 [28:40:06<12:45:36, 21.23s/it]                                                          67%|██████▋   | 4336/6500 [28:40:06<12:45:36, 21.23s/it] 67%|██████▋   | 4337/6500 [28:40:24<12:11:01, 20.28s/it]                                                          67%|██████▋   | 4337/6500 [28:40:24<12:11:01, 20.28s/it] 67%|██████▋   | 4338/6500 [28:40:42<11:46:50, 19.62s/it]                                                          67%|██████▋   | 4338/6500 [28:40:42<11:46:50, 19.62s/it] 67%|██████▋   | 4339/6500 [28:41:00<11:30:30, 19.17s/it]                                                          67%|██████▋   | 4339/6500 [28:41:00<11:30:30, 19.17s/it] 67%|██████▋   | 4340/6500 [28:41:18<11:19:06, 18.86s/it]                                                          67%|██████▋   | 4340/6500 [28:41:18<11:19:06, 18.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8848863840103149, 'eval_runtime': 6.0865, 'eval_samples_per_second': 3.779, 'eval_steps_per_second': 0.986, 'epoch': 0.67}
                                                          67%|██████▋   | 4340/6500 [28:41:24<11:19:06, 18.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4340
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4340
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4340/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2322, 'learning_rate': 2.4853599177793052e-05, 'epoch': 0.67}
{'loss': 0.2231, 'learning_rate': 2.483270824680409e-05, 'epoch': 0.67}
{'loss': 0.2436, 'learning_rate': 2.4811823198516555e-05, 'epoch': 0.67}
{'loss': 0.2166, 'learning_rate': 2.4790944037812202e-05, 'epoch': 0.67}
{'loss': 0.2234, 'learning_rate': 2.4770070769571408e-05, 'epoch': 0.67}
 67%|██████▋   | 4341/6500 [28:42:16<18:18:33, 30.53s/it]                                                          67%|██████▋   | 4341/6500 [28:42:16<18:18:33, 30.53s/it] 67%|██████▋   | 4342/6500 [28:42:34<16:03:19, 26.78s/it]                                                          67%|██████▋   | 4342/6500 [28:42:34<16:03:19, 26.78s/it] 67%|██████▋   | 4343/6500 [28:42:52<14:28:42, 24.16s/it]                                                          67%|██████▋   | 4343/6500 [28:42:52<14:28:42, 24.16s/it] 67%|██████▋   | 4344/6500 [28:43:10<13:22:33, 22.33s/it]                                                          67%|██████▋   | 4344/6500 [28:43:10<13:22:33, 22.33s/it] 67%|██████▋   | 4345/6500 [28:43:28<12:36:23, 21.06s/it]                                                          67%|██████▋   | 4345/6500 [28:43:28<12:36:23, 21.06s/it] 67%|█{'loss': 0.2329, 'learning_rate': 2.4749203398673172e-05, 'epoch': 0.67}
{'loss': 0.2328, 'learning_rate': 2.4728341929995092e-05, 'epoch': 0.67}
{'loss': 0.238, 'learning_rate': 2.4707486368413445e-05, 'epoch': 0.67}
{'loss': 0.2316, 'learning_rate': 2.4686636718803086e-05, 'epoch': 0.67}
{'loss': 0.2601, 'learning_rate': 2.4665792986037507e-05, 'epoch': 0.67}
█████▋   | 4346/6500 [28:43:46<12:04:05, 20.17s/it]                                                          67%|██████▋   | 4346/6500 [28:43:46<12:04:05, 20.17s/it] 67%|██████▋   | 4347/6500 [28:44:04<11:41:38, 19.55s/it]                                                          67%|██████▋   | 4347/6500 [28:44:04<11:41:38, 19.55s/it] 67%|██████▋   | 4348/6500 [28:44:23<11:30:16, 19.25s/it]                                                          67%|██████▋   | 4348/6500 [28:44:23<11:30:16, 19.25s/it] 67%|██████▋   | 4349/6500 [28:44:41<11:17:47, 18.91s/it]                                                          67%|██████▋   | 4349/6500 [28:44:41<11:17:47, 18.91s/it] 67%|██████▋   | 4350/6500 [28:44:59<11:10:25, 18.71s/it]                                                          67%|██████▋   | 4350/6500 [28:44:59<11:10:25, 18.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8962953686714172, 'eval_runtime': 5.4827, 'eval_samples_per_second': 4.195, 'eval_steps_per_second': 1.094, 'epoch': 0.67}
                                                          67%|██████▋   | 4350/6500 [28:45:05<11:10:25, 18.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4350
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4350
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4350/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4350/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2452, 'learning_rate': 2.464495517498876e-05, 'epoch': 0.67}
{'loss': 0.2543, 'learning_rate': 2.4624123290527578e-05, 'epoch': 0.67}
{'loss': 0.2435, 'learning_rate': 2.4603297337523294e-05, 'epoch': 0.67}
{'loss': 0.2458, 'learning_rate': 2.458247732084384e-05, 'epoch': 0.67}
{'loss': 0.2465, 'learning_rate': 2.456166324535577e-05, 'epoch': 0.67}
 67%|██████▋   | 4351/6500 [28:46:16<21:33:20, 36.11s/it]                                                          67%|██████▋   | 4351/6500 [28:46:16<21:33:20, 36.11s/it] 67%|██████▋   | 4352/6500 [28:46:34<18:17:59, 30.67s/it]                                                          67%|██████▋   | 4352/6500 [28:46:34<18:17:59, 30.67s/it] 67%|██████▋   | 4353/6500 [28:46:52<16:00:57, 26.85s/it]                                                          67%|██████▋   | 4353/6500 [28:46:52<16:00:57, 26.85s/it] 67%|██████▋   | 4354/6500 [28:47:10<14:26:21, 24.22s/it]                                                          67%|██████▋   | 4354/6500 [28:47:10<14:26:21, 24.22s/it] 67%|██████▋   | 4355/6500 [28:47:28<13:20:26, 22.39s/it]                                                          67%|██████▋   | 4355/6500 [28:47:28<13:20:26, 22.39s/it] 67%|█{'loss': 0.2449, 'learning_rate': 2.454085511592425e-05, 'epoch': 0.67}
{'loss': 0.2412, 'learning_rate': 2.4520052937413058e-05, 'epoch': 0.67}
{'loss': 0.2509, 'learning_rate': 2.4499256714684565e-05, 'epoch': 0.67}
{'loss': 0.2576, 'learning_rate': 2.447846645259977e-05, 'epoch': 0.67}
{'loss': 0.2188, 'learning_rate': 2.4457682156018263e-05, 'epoch': 0.67}
█████▋   | 4356/6500 [28:47:46<12:33:21, 21.08s/it]                                                          67%|██████▋   | 4356/6500 [28:47:46<12:33:21, 21.08s/it] 67%|██████▋   | 4357/6500 [28:48:04<12:00:27, 20.17s/it]                                                          67%|██████▋   | 4357/6500 [28:48:04<12:00:27, 20.17s/it] 67%|██████▋   | 4358/6500 [28:48:22<11:37:29, 19.54s/it]                                                          67%|██████▋   | 4358/6500 [28:48:22<11:37:29, 19.54s/it] 67%|██████▋   | 4359/6500 [28:48:40<11:21:42, 19.10s/it]                                                          67%|██████▋   | 4359/6500 [28:48:40<11:21:42, 19.10s/it] 67%|██████▋   | 4360/6500 [28:48:58<11:10:52, 18.81s/it]                                                          67%|██████▋   | 4360/6500 [28:48:58<11:10:52, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8893988132476807, 'eval_runtime': 5.3287, 'eval_samples_per_second': 4.316, 'eval_steps_per_second': 1.126, 'epoch': 0.67}
                                                          67%|██████▋   | 4360/6500 [28:49:04<11:10:52, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4360
  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4360/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4360


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4360
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4360/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4360/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2328, 'learning_rate': 2.4436903829798274e-05, 'epoch': 0.67}
{'loss': 0.2293, 'learning_rate': 2.441613147879657e-05, 'epoch': 0.67}
{'loss': 0.2638, 'learning_rate': 2.4395365107868583e-05, 'epoch': 0.67}
{'loss': 0.2858, 'learning_rate': 2.437460472186832e-05, 'epoch': 0.67}
{'loss': 0.2271, 'learning_rate': 2.4353850325648404e-05, 'epoch': 0.67}
 67%|██████▋   | 4361/6500 [28:49:50<16:59:23, 28.59s/it]                                                          67%|██████▋   | 4361/6500 [28:49:50<16:59:23, 28.59s/it] 67%|██████▋   | 4362/6500 [28:50:08<15:06:20, 25.44s/it]                                                          67%|██████▋   | 4362/6500 [28:50:08<15:06:20, 25.44s/it] 67%|██████▋   | 4363/6500 [28:50:26<13:47:00, 23.22s/it]                                                          67%|██████▋   | 4363/6500 [28:50:26<13:47:00, 23.22s/it] 67%|██████▋   | 4364/6500 [28:50:44<12:54:23, 21.75s/it]                                                          67%|██████▋   | 4364/6500 [28:50:44<12:54:23, 21.75s/it] 67%|██████▋   | 4365/6500 [28:51:02<12:14:48, 20.65s/it]                                                          67%|██████▋   | 4365/6500 [28:51:02<12:14:48, 20.65s/it] 67%|█{'loss': 0.2518, 'learning_rate': 2.4333101924060035e-05, 'epoch': 0.67}
{'loss': 0.2279, 'learning_rate': 2.4312359521953045e-05, 'epoch': 0.67}
{'loss': 0.7328, 'learning_rate': 2.4291623124175822e-05, 'epoch': 0.67}
{'loss': 0.229, 'learning_rate': 2.427089273557539e-05, 'epoch': 0.67}
{'loss': 0.2513, 'learning_rate': 2.4250168360997344e-05, 'epoch': 0.67}
█████▋   | 4366/6500 [28:51:21<11:47:37, 19.90s/it]                                                          67%|██████▋   | 4366/6500 [28:51:21<11:47:37, 19.90s/it] 67%|██████▋   | 4367/6500 [28:51:39<11:28:38, 19.37s/it]                                                          67%|██████▋   | 4367/6500 [28:51:39<11:28:38, 19.37s/it] 67%|██████▋   | 4368/6500 [28:51:57<11:15:19, 19.01s/it]                                                          67%|██████▋   | 4368/6500 [28:51:57<11:15:19, 19.01s/it] 67%|██████▋   | 4369/6500 [28:52:15<11:06:32, 18.77s/it]                                                          67%|██████▋   | 4369/6500 [28:52:15<11:06:32, 18.77s/it] 67%|██████▋   | 4370/6500 [28:52:33<11:00:14, 18.60s/it]                                                          67%|██████▋   | 4370/6500 [28:52:33<11:00:14, 18.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8932679891586304, 'eval_runtime': 5.7602, 'eval_samples_per_second': 3.993, 'eval_steps_per_second': 1.042, 'epoch': 0.67}
                                                          67%|██████▋   | 4370/6500 [28:52:39<11:00:14, 18.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4370
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4370
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4370
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4370/pytorch_model.bin
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4370/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4370/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2272, 'learning_rate': 2.422945000528588e-05, 'epoch': 0.67}
{'loss': 0.243, 'learning_rate': 2.4208737673283815e-05, 'epoch': 0.67}
{'loss': 0.2343, 'learning_rate': 2.4188031369832482e-05, 'epoch': 0.67}
{'loss': 0.2155, 'learning_rate': 2.416733109977188e-05, 'epoch': 0.67}
{'loss': 0.2347, 'learning_rate': 2.4146636867940565e-05, 'epoch': 0.67}
 67%|██████▋   | 4371/6500 [28:53:58<22:41:30, 38.37s/it]                                                          67%|██████▋   | 4371/6500 [28:53:58<22:41:30, 38.37s/it] 67%|██████▋   | 4372/6500 [28:54:16<19:04:05, 32.26s/it]                                                          67%|██████▋   | 4372/6500 [28:54:16<19:04:05, 32.26s/it] 67%|██████▋   | 4373/6500 [28:54:34<16:31:54, 27.98s/it]                                                          67%|██████▋   | 4373/6500 [28:54:34<16:31:54, 27.98s/it] 67%|██████▋   | 4374/6500 [28:54:52<14:45:30, 24.99s/it]                                                          67%|██████▋   | 4374/6500 [28:54:52<14:45:30, 24.99s/it] 67%|██████▋   | 4375/6500 [28:55:10<13:31:32, 22.91s/it]                                                          67%|██████▋   | 4375/6500 [28:55:10<13:31:32, 22.91s/it] 67%|█{'loss': 0.2256, 'learning_rate': 2.4125948679175686e-05, 'epoch': 0.67}
{'loss': 0.2366, 'learning_rate': 2.4105266538312994e-05, 'epoch': 0.67}
{'loss': 0.2421, 'learning_rate': 2.4084590450186806e-05, 'epoch': 0.67}
{'loss': 0.2325, 'learning_rate': 2.4063920419630025e-05, 'epoch': 0.67}
{'loss': 0.231, 'learning_rate': 2.4043256451474162e-05, 'epoch': 0.67}
█████▋   | 4376/6500 [28:55:28<12:39:30, 21.46s/it]                                                          67%|██████▋   | 4376/6500 [28:55:28<12:39:30, 21.46s/it] 67%|██████▋   | 4377/6500 [28:55:46<12:03:09, 20.44s/it]                                                          67%|██████▋   | 4377/6500 [28:55:46<12:03:09, 20.44s/it] 67%|██████▋   | 4378/6500 [28:56:04<11:39:08, 19.77s/it]                                                          67%|██████▋   | 4378/6500 [28:56:04<11:39:08, 19.77s/it] 67%|██████▋   | 4379/6500 [28:56:22<11:21:27, 19.28s/it]                                                          67%|██████▋   | 4379/6500 [28:56:22<11:21:27, 19.28s/it] 67%|██████▋   | 4380/6500 [28:56:42<11:22:06, 19.30s/it]                                                          67%|██████▋   | 4380/6500 [28:56:42<11:22:06, 19.30s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8965941667556763, 'eval_runtime': 5.477, 'eval_samples_per_second': 4.199, 'eval_steps_per_second': 1.095, 'epoch': 0.67}
                                                          67%|██████▋   | 4380/6500 [28:56:47<11:22:06, 19.30s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4380 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4380

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4380

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4380
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4380/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2373, 'learning_rate': 2.402259855054928e-05, 'epoch': 0.67}
{'loss': 0.249, 'learning_rate': 2.400194672168404e-05, 'epoch': 0.67}
{'loss': 0.2209, 'learning_rate': 2.39813009697057e-05, 'epoch': 0.67}
{'loss': 0.2404, 'learning_rate': 2.3960661299440047e-05, 'epoch': 0.67}
{'loss': 0.2425, 'learning_rate': 2.3940027715711495e-05, 'epoch': 0.67}
 67%|██████▋   | 4381/6500 [28:57:51<20:10:21, 34.27s/it]                                                          67%|██████▋   | 4381/6500 [28:57:51<20:10:21, 34.27s/it] 67%|██████▋   | 4382/6500 [28:58:09<17:17:37, 29.39s/it]                                                          67%|██████▋   | 4382/6500 [28:58:09<17:17:37, 29.39s/it] 67%|██████▋   | 4383/6500 [28:58:27<15:16:16, 25.97s/it]                                                          67%|██████▋   | 4383/6500 [28:58:27<15:16:16, 25.97s/it] 67%|██████▋   | 4384/6500 [28:58:45<13:51:21, 23.57s/it]                                                          67%|██████▋   | 4384/6500 [28:58:45<13:51:21, 23.57s/it] 67%|██████▋   | 4385/6500 [28:59:03<12:56:36, 22.03s/it]                                                          67%|██████▋   | 4385/6500 [28:59:03<12:56:36, 22.03s/it] 67%|█{'loss': 0.2256, 'learning_rate': 2.3919400223343015e-05, 'epoch': 0.67}
{'loss': 0.2375, 'learning_rate': 2.3898778827156156e-05, 'epoch': 0.67}
{'loss': 0.2463, 'learning_rate': 2.3878163531971053e-05, 'epoch': 0.68}
{'loss': 0.235, 'learning_rate': 2.3857554342606397e-05, 'epoch': 0.68}
{'loss': 0.2276, 'learning_rate': 2.383695126387947e-05, 'epoch': 0.68}
█████▋   | 4386/6500 [28:59:21<12:14:06, 20.84s/it]                                                          67%|██████▋   | 4386/6500 [28:59:21<12:14:06, 20.84s/it] 67%|██████▋   | 4387/6500 [28:59:39<11:44:45, 20.01s/it]                                                          67%|██████▋   | 4387/6500 [28:59:39<11:44:45, 20.01s/it] 68%|██████▊   | 4388/6500 [28:59:58<11:24:13, 19.44s/it]                                                          68%|██████▊   | 4388/6500 [28:59:58<11:24:13, 19.44s/it] 68%|██████▊   | 4389/6500 [29:00:16<11:09:45, 19.04s/it]                                                          68%|██████▊   | 4389/6500 [29:00:16<11:09:45, 19.04s/it] 68%|██████▊   | 4390/6500 [29:00:34<11:00:00, 18.77s/it]                                                          68%|██████▊   | 4390/6500 [29:00:34<11:00:00, 18.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8939824104309082, 'eval_runtime': 5.6029, 'eval_samples_per_second': 4.105, 'eval_steps_per_second': 1.071, 'epoch': 0.68}
                                                          68%|██████▊   | 4390/6500 [29:00:39<11:00:00, 18.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4390the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4390
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4390
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4390/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.227, 'learning_rate': 2.381635430060611e-05, 'epoch': 0.68}
{'loss': 0.216, 'learning_rate': 2.379576345760073e-05, 'epoch': 0.68}
{'loss': 0.3009, 'learning_rate': 2.3775178739676318e-05, 'epoch': 0.68}
{'loss': 0.2318, 'learning_rate': 2.3754600151644445e-05, 'epoch': 0.68}
{'loss': 0.2186, 'learning_rate': 2.37340276983152e-05, 'epoch': 0.68}
 68%|██████▊   | 4391/6500 [29:01:31<17:45:29, 30.31s/it]                                                          68%|██████▊   | 4391/6500 [29:01:31<17:45:29, 30.31s/it] 68%|██████▊   | 4392/6500 [29:01:49<15:35:52, 26.64s/it]                                                          68%|██████▊   | 4392/6500 [29:01:49<15:35:52, 26.64s/it] 68%|██████▊   | 4393/6500 [29:02:07<14:04:47, 24.06s/it]                                                          68%|██████▊   | 4393/6500 [29:02:07<14:04:47, 24.06s/it] 68%|██████▊   | 4394/6500 [29:02:25<13:00:56, 22.25s/it]                                                          68%|██████▊   | 4394/6500 [29:02:25<13:00:56, 22.25s/it] 68%|██████▊   | 4395/6500 [29:02:43<12:16:31, 20.99s/it]                                                          68%|██████▊   | 4395/6500 [29:02:43<12:16:31, 20.99s/it] 68%|█{'loss': 0.2505, 'learning_rate': 2.371346138449727e-05, 'epoch': 0.68}
{'loss': 0.7277, 'learning_rate': 2.369290121499792e-05, 'epoch': 0.68}
{'loss': 0.2418, 'learning_rate': 2.367234719462297e-05, 'epoch': 0.68}
{'loss': 0.242, 'learning_rate': 2.3651799328176776e-05, 'epoch': 0.68}
{'loss': 0.2437, 'learning_rate': 2.3631257620462294e-05, 'epoch': 0.68}
█████▊   | 4396/6500 [29:03:02<11:48:05, 20.19s/it]                                                          68%|██████▊   | 4396/6500 [29:03:02<11:48:05, 20.19s/it] 68%|██████▊   | 4397/6500 [29:03:20<11:25:10, 19.55s/it]                                                          68%|██████▊   | 4397/6500 [29:03:20<11:25:10, 19.55s/it] 68%|██████▊   | 4398/6500 [29:03:38<11:09:20, 19.11s/it]                                                          68%|██████▊   | 4398/6500 [29:03:38<11:09:20, 19.11s/it] 68%|██████▊   | 4399/6500 [29:03:56<10:58:17, 18.80s/it]                                                          68%|██████▊   | 4399/6500 [29:03:56<10:58:17, 18.80s/it] 68%|██████▊   | 4400/6500 [29:04:14<10:50:45, 18.59s/it]                                                          68%|██████▊   | 4400/6500 [29:04:14<10:50:45, 18.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8861091732978821, 'eval_runtime': 5.3312, 'eval_samples_per_second': 4.314, 'eval_steps_per_second': 1.125, 'epoch': 0.68}
                                                          68%|██████▊   | 4400/6500 [29:04:19<10:50:45, 18.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4400
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4400
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4400
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4400/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4400/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2197, 'learning_rate': 2.3610722076281023e-05, 'epoch': 0.68}
{'loss': 0.2469, 'learning_rate': 2.3590192700433013e-05, 'epoch': 0.68}
{'loss': 0.2267, 'learning_rate': 2.3569669497716883e-05, 'epoch': 0.68}
{'loss': 0.223, 'learning_rate': 2.3549152472929808e-05, 'epoch': 0.68}
{'loss': 0.2414, 'learning_rate': 2.3528641630867526e-05, 'epoch': 0.68}
 68%|██████▊   | 4401/6500 [29:06:02<26:30:32, 45.47s/it]                                                          68%|██████▊   | 4401/6500 [29:06:02<26:30:32, 45.47s/it] 68%|██████▊   | 4402/6500 [29:06:20<21:41:07, 37.21s/it]                                                          68%|██████▊   | 4402/6500 [29:06:20<21:41:07, 37.21s/it] 68%|██████▊   | 4403/6500 [29:06:38<18:19:46, 31.47s/it]                                                          68%|██████▊   | 4403/6500 [29:06:38<18:19:46, 31.47s/it] 68%|██████▊   | 4404/6500 [29:06:56<15:58:04, 27.43s/it]                                                          68%|██████▊   | 4404/6500 [29:06:56<15:58:04, 27.43s/it] 68%|██████▊   | 4405/6500 [29:07:14<14:19:17, 24.61s/it]                                                          68%|██████▊   | 4405/6500 [29:07:14<14:19:17, 24.61s/it] 68%|█{'loss': 0.2257, 'learning_rate': 2.350813697632433e-05, 'epoch': 0.68}
{'loss': 0.2385, 'learning_rate': 2.348763851409302e-05, 'epoch': 0.68}
{'loss': 0.2283, 'learning_rate': 2.346714624896501e-05, 'epoch': 0.68}
{'loss': 0.2416, 'learning_rate': 2.3446660185730247e-05, 'epoch': 0.68}
{'loss': 0.2268, 'learning_rate': 2.3426180329177215e-05, 'epoch': 0.68}
█████▊   | 4406/6500 [29:07:32<13:10:14, 22.64s/it]                                                          68%|██████▊   | 4406/6500 [29:07:32<13:10:14, 22.64s/it] 68%|██████▊   | 4407/6500 [29:07:50<12:22:06, 21.27s/it]                                                          68%|██████▊   | 4407/6500 [29:07:50<12:22:06, 21.27s/it] 68%|██████▊   | 4408/6500 [29:08:08<11:48:38, 20.32s/it]                                                          68%|██████▊   | 4408/6500 [29:08:08<11:48:38, 20.32s/it] 68%|██████▊   | 4409/6500 [29:08:26<11:25:13, 19.66s/it]                                                          68%|██████▊   | 4409/6500 [29:08:26<11:25:13, 19.66s/it] 68%|██████▊   | 4410/6500 [29:08:45<11:08:51, 19.20s/it]                                                          68%|██████▊   | 4410/6500 [29:08:45<11:08:51, 19.20s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8951011300086975, 'eval_runtime': 5.3414, 'eval_samples_per_second': 4.306, 'eval_steps_per_second': 1.123, 'epoch': 0.68}
                                                          68%|██████▊   | 4410/6500 [29:08:50<11:08:51, 19.20s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4410
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4410
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4410

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4410
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4410/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2515, 'learning_rate': 2.3405706684092958e-05, 'epoch': 0.68}
{'loss': 0.2363, 'learning_rate': 2.3385239255263077e-05, 'epoch': 0.68}
{'loss': 0.2377, 'learning_rate': 2.336477804747169e-05, 'epoch': 0.68}
{'loss': 0.245, 'learning_rate': 2.3344323065501494e-05, 'epoch': 0.68}
{'loss': 0.2418, 'learning_rate': 2.332387431413371e-05, 'epoch': 0.68}
 68%|██████▊   | 4411/6500 [29:09:42<17:45:30, 30.60s/it]                                                          68%|██████▊   | 4411/6500 [29:09:42<17:45:30, 30.60s/it] 68%|██████▊   | 4412/6500 [29:10:00<15:37:41, 26.95s/it]                                                          68%|██████▊   | 4412/6500 [29:10:00<15:37:41, 26.95s/it] 68%|██████▊   | 4413/6500 [29:10:18<14:04:09, 24.27s/it]                                                          68%|██████▊   | 4413/6500 [29:10:18<14:04:09, 24.27s/it] 68%|██████▊   | 4414/6500 [29:10:36<12:59:08, 22.41s/it]                                                          68%|██████▊   | 4414/6500 [29:10:36<12:59:08, 22.41s/it] 68%|██████▊   | 4415/6500 [29:10:54<12:13:49, 21.12s/it]                                                          68%|██████▊   | 4415/6500 [29:10:54<12:13:49, 21.12s/it] 68%|█{'loss': 0.2435, 'learning_rate': 2.330343179814811e-05, 'epoch': 0.68}
{'loss': 0.231, 'learning_rate': 2.328299552232303e-05, 'epoch': 0.68}
{'loss': 0.2537, 'learning_rate': 2.326256549143529e-05, 'epoch': 0.68}
{'loss': 0.2327, 'learning_rate': 2.3242141710260295e-05, 'epoch': 0.68}
{'loss': 0.2254, 'learning_rate': 2.3221724183571986e-05, 'epoch': 0.68}
█████▊   | 4416/6500 [29:11:12<11:42:05, 20.21s/it]                                                          68%|██████▊   | 4416/6500 [29:11:12<11:42:05, 20.21s/it] 68%|██████▊   | 4417/6500 [29:11:31<11:20:00, 19.59s/it]                                                          68%|██████▊   | 4417/6500 [29:11:31<11:20:00, 19.59s/it] 68%|██████▊   | 4418/6500 [29:11:49<11:05:28, 19.18s/it]                                                          68%|██████▊   | 4418/6500 [29:11:49<11:05:28, 19.18s/it] 68%|██████▊   | 4419/6500 [29:12:07<10:54:36, 18.87s/it]                                                          68%|██████▊   | 4419/6500 [29:12:07<10:54:36, 18.87s/it] 68%|██████▊   | 4420/6500 [29:12:25<10:46:57, 18.66s/it]                                                          68%|██████▊   | 4420/6500 [29:12:25<10:46:57, 18.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8911526799201965, 'eval_runtime': 5.3392, 'eval_samples_per_second': 4.308, 'eval_steps_per_second': 1.124, 'epoch': 0.68}
                                                          68%|██████▊   | 4420/6500 [29:12:30<10:46:57, 18.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4420the checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4420
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4420

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4420
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4420/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2195, 'learning_rate': 2.320131291614283e-05, 'epoch': 0.68}
{'loss': 0.25, 'learning_rate': 2.318090791274385e-05, 'epoch': 0.68}
{'loss': 0.3092, 'learning_rate': 2.316050917814456e-05, 'epoch': 0.68}
{'loss': 0.2412, 'learning_rate': 2.314011671711308e-05, 'epoch': 0.68}
{'loss': 0.2306, 'learning_rate': 2.3119730534416028e-05, 'epoch': 0.68}
 68%|██████▊   | 4421/6500 [29:13:34<19:25:28, 33.64s/it]                                                          68%|██████▊   | 4421/6500 [29:13:34<19:25:28, 33.64s/it] 68%|██████▊   | 4422/6500 [29:13:52<16:42:36, 28.95s/it]                                                          68%|██████▊   | 4422/6500 [29:13:52<16:42:36, 28.95s/it] 68%|██████▊   | 4423/6500 [29:14:10<14:48:49, 25.68s/it]                                                          68%|██████▊   | 4423/6500 [29:14:10<14:48:49, 25.68s/it] 68%|██████▊   | 4424/6500 [29:14:28<13:29:28, 23.40s/it]                                                          68%|██████▊   | 4424/6500 [29:14:28<13:29:28, 23.40s/it] 68%|██████▊   | 4425/6500 [29:14:46<12:34:01, 21.80s/it]                                                          68%|██████▊   | 4425/6500 [29:14:46<12:34:01, 21.80s/it] 68%|█{'loss': 0.2521, 'learning_rate': 2.3099350634818506e-05, 'epoch': 0.68}
{'loss': 0.7322, 'learning_rate': 2.307897702308422e-05, 'epoch': 0.68}
{'loss': 0.2484, 'learning_rate': 2.305860970397537e-05, 'epoch': 0.68}
{'loss': 0.2279, 'learning_rate': 2.3038248682252693e-05, 'epoch': 0.68}
{'loss': 0.2304, 'learning_rate': 2.3017893962675458e-05, 'epoch': 0.68}
█████▊   | 4426/6500 [29:15:04<11:56:24, 20.73s/it]                                                          68%|██████▊   | 4426/6500 [29:15:04<11:56:24, 20.73s/it] 68%|██████▊   | 4427/6500 [29:15:22<11:29:29, 19.96s/it]                                                          68%|██████▊   | 4427/6500 [29:15:22<11:29:29, 19.96s/it] 68%|██████▊   | 4428/6500 [29:15:41<11:13:47, 19.51s/it]                                                          68%|██████▊   | 4428/6500 [29:15:41<11:13:47, 19.51s/it] 68%|██████▊   | 4429/6500 [29:15:59<10:59:11, 19.10s/it]                                                          68%|██████▊   | 4429/6500 [29:15:59<10:59:11, 19.10s/it] 68%|██████▊   | 4430/6500 [29:16:17<10:48:53, 18.81s/it]                                                          68%|██████▊   | 4430/6500 [29:16:17<10:48:53, 18.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8924769759178162, 'eval_runtime': 5.4806, 'eval_samples_per_second': 4.197, 'eval_steps_per_second': 1.095, 'epoch': 0.68}
                                                          68%|██████▊   | 4430/6500 [29:16:23<10:48:53, 18.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4430
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4430
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4430/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2199, 'learning_rate': 2.2997545550001455e-05, 'epoch': 0.68}
{'loss': 0.2352, 'learning_rate': 2.2977203448987007e-05, 'epoch': 0.68}
{'loss': 0.2231, 'learning_rate': 2.2956867664386945e-05, 'epoch': 0.68}
{'loss': 0.2261, 'learning_rate': 2.2936538200954644e-05, 'epoch': 0.68}
{'loss': 0.2309, 'learning_rate': 2.2916215063441985e-05, 'epoch': 0.68}
 68%|██████▊   | 4431/6500 [29:17:13<17:15:51, 30.04s/it]                                                          68%|██████▊   | 4431/6500 [29:17:13<17:15:51, 30.04s/it] 68%|██████▊   | 4432/6500 [29:17:31<15:11:11, 26.44s/it]                                                          68%|██████▊   | 4432/6500 [29:17:31<15:11:11, 26.44s/it] 68%|██████▊   | 4433/6500 [29:17:49<13:43:45, 23.91s/it]                                                          68%|██████▊   | 4433/6500 [29:17:49<13:43:45, 23.91s/it] 68%|██████▊   | 4434/6500 [29:18:07<12:42:33, 22.15s/it]                                                          68%|██████▊   | 4434/6500 [29:18:07<12:42:33, 22.15s/it] 68%|██████▊   | 4435/6500 [29:18:27<12:14:44, 21.35s/it]                                                          68%|██████▊   | 4435/6500 [29:18:27<12:14:44, 21.35s/it] 68%|█{'loss': 0.2303, 'learning_rate': 2.2895898256599392e-05, 'epoch': 0.68}
{'loss': 0.2362, 'learning_rate': 2.28755877851758e-05, 'epoch': 0.68}
{'loss': 0.2267, 'learning_rate': 2.2855283653918625e-05, 'epoch': 0.68}
{'loss': 0.2438, 'learning_rate': 2.2834985867573855e-05, 'epoch': 0.68}
{'loss': 0.2321, 'learning_rate': 2.281469443088597e-05, 'epoch': 0.68}
█████▊   | 4436/6500 [29:18:45<11:40:36, 20.37s/it]                                                          68%|██████▊   | 4436/6500 [29:18:45<11:40:36, 20.37s/it] 68%|██████▊   | 4437/6500 [29:19:03<11:16:41, 19.68s/it]                                                          68%|██████▊   | 4437/6500 [29:19:03<11:16:41, 19.68s/it] 68%|██████▊   | 4438/6500 [29:19:21<10:59:59, 19.20s/it]                                                          68%|██████▊   | 4438/6500 [29:19:21<10:59:59, 19.20s/it] 68%|██████▊   | 4439/6500 [29:19:39<10:48:26, 18.88s/it]                                                          68%|██████▊   | 4439/6500 [29:19:39<10:48:26, 18.88s/it] 68%|██████▊   | 4440/6500 [29:19:57<10:40:29, 18.65s/it]                                                          68%|██████▊   | 4440/6500 [29:19:57<10:40:29, 18.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8987261652946472, 'eval_runtime': 5.3236, 'eval_samples_per_second': 4.32, 'eval_steps_per_second': 1.127, 'epoch': 0.68}
                                                          68%|██████▊   | 4440/6500 [29:20:03<10:40:29, 18.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4440
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4440
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4440
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4440/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4440/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2385, 'learning_rate': 2.2794409348597972e-05, 'epoch': 0.68}
{'loss': 0.2296, 'learning_rate': 2.277413062545138e-05, 'epoch': 0.68}
{'loss': 0.2287, 'learning_rate': 2.2753858266186213e-05, 'epoch': 0.68}
{'loss': 0.2389, 'learning_rate': 2.273359227554102e-05, 'epoch': 0.68}
{'loss': 0.2367, 'learning_rate': 2.271333265825285e-05, 'epoch': 0.68}
 68%|██████▊   | 4441/6500 [29:21:18<21:22:06, 37.36s/it]                                                          68%|██████▊   | 4441/6500 [29:21:18<21:22:06, 37.36s/it] 68%|██████▊   | 4442/6500 [29:21:36<18:01:46, 31.54s/it]                                                          68%|██████▊   | 4442/6500 [29:21:36<18:01:46, 31.54s/it] 68%|██████▊   | 4443/6500 [29:21:54<15:41:25, 27.46s/it]                                                          68%|██████▊   | 4443/6500 [29:21:54<15:41:25, 27.46s/it] 68%|██████▊   | 4444/6500 [29:22:12<14:03:24, 24.61s/it]                                                          68%|██████▊   | 4444/6500 [29:22:12<14:03:24, 24.61s/it] 68%|██████▊   | 4445/6500 [29:22:30<12:57:36, 22.70s/it]                                                          68%|██████▊   | 4445/6500 [29:22:30<12:57:36, 22.70s/it] 68%|█{'loss': 0.2412, 'learning_rate': 2.2693079419057266e-05, 'epoch': 0.68}
{'loss': 0.238, 'learning_rate': 2.267283256268834e-05, 'epoch': 0.68}
{'loss': 0.2456, 'learning_rate': 2.2652592093878666e-05, 'epoch': 0.68}
{'loss': 0.2205, 'learning_rate': 2.2632358017359302e-05, 'epoch': 0.68}
{'loss': 0.2294, 'learning_rate': 2.261213033785985e-05, 'epoch': 0.68}
█████▊   | 4446/6500 [29:22:48<12:08:59, 21.29s/it]                                                          68%|██████▊   | 4446/6500 [29:22:48<12:08:59, 21.29s/it] 68%|██████▊   | 4447/6500 [29:23:07<11:35:07, 20.32s/it]                                                          68%|██████▊   | 4447/6500 [29:23:07<11:35:07, 20.32s/it] 68%|██████▊   | 4448/6500 [29:23:25<11:11:30, 19.63s/it]                                                          68%|██████▊   | 4448/6500 [29:23:25<11:11:30, 19.63s/it] 68%|██████▊   | 4449/6500 [29:23:43<10:55:03, 19.16s/it]                                                          68%|██████▊   | 4449/6500 [29:23:43<10:55:03, 19.16s/it] 68%|██████▊   | 4450/6500 [29:24:01<10:43:53, 18.85s/it]                                                          68%|██████▊   | 4450/6500 [29:24:01<10:43:53, 18.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.893403172492981, 'eval_runtime': 5.9128, 'eval_samples_per_second': 3.89, 'eval_steps_per_second': 1.015, 'epoch': 0.68}
                                                          68%|██████▊   | 4450/6500 [29:24:07<10:43:53, 18.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4450

 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4450
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4450

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4450
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4450/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4450/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2184, 'learning_rate': 2.2591909060108407e-05, 'epoch': 0.68}
{'loss': 0.2569, 'learning_rate': 2.2571694188831582e-05, 'epoch': 0.68}
{'loss': 0.2765, 'learning_rate': 2.2551485728754467e-05, 'epoch': 0.69}
{'loss': 0.2243, 'learning_rate': 2.253128368460068e-05, 'epoch': 0.69}
{'loss': 0.2447, 'learning_rate': 2.2511088061092318e-05, 'epoch': 0.69}
 68%|██████▊   | 4451/6500 [29:25:40<24:31:25, 43.09s/it]                                                          68%|██████▊   | 4451/6500 [29:25:40<24:31:25, 43.09s/it] 68%|██████▊   | 4452/6500 [29:25:58<20:13:23, 35.55s/it]                                                          68%|██████▊   | 4452/6500 [29:25:58<20:13:23, 35.55s/it] 69%|██████▊   | 4453/6500 [29:26:16<17:12:41, 30.27s/it]                                                          69%|██████▊   | 4453/6500 [29:26:16<17:12:41, 30.27s/it] 69%|██████▊   | 4454/6500 [29:26:34<15:06:11, 26.57s/it]                                                          69%|██████▊   | 4454/6500 [29:26:34<15:06:11, 26.57s/it] 69%|██████▊   | 4455/6500 [29:26:52<13:37:47, 23.99s/it]                                                          69%|██████▊   | 4455/6500 [29:26:52<13:37:47, 23.99s/it] 69%|█{'loss': 0.2511, 'learning_rate': 2.2490898862949987e-05, 'epoch': 0.69}
{'loss': 0.6937, 'learning_rate': 2.2470716094892785e-05, 'epoch': 0.69}
{'loss': 0.2311, 'learning_rate': 2.2450539761638316e-05, 'epoch': 0.69}
{'loss': 0.2404, 'learning_rate': 2.2430369867902694e-05, 'epoch': 0.69}
{'loss': 0.225, 'learning_rate': 2.2410206418400477e-05, 'epoch': 0.69}
█████▊   | 4456/6500 [29:27:10<12:36:02, 22.19s/it]                                                          69%|██████▊   | 4456/6500 [29:27:10<12:36:02, 22.19s/it] 69%|██████▊   | 4457/6500 [29:27:28<11:52:52, 20.94s/it]                                                          69%|██████▊   | 4457/6500 [29:27:28<11:52:52, 20.94s/it] 69%|██████▊   | 4458/6500 [29:27:46<11:23:09, 20.07s/it]                                                          69%|██████▊   | 4458/6500 [29:27:46<11:23:09, 20.07s/it] 69%|██████▊   | 4459/6500 [29:28:04<11:02:25, 19.47s/it]                                                          69%|██████▊   | 4459/6500 [29:28:04<11:02:25, 19.47s/it] 69%|██████▊   | 4460/6500 [29:28:22<10:47:56, 19.06s/it]                                                          69%|██████▊   | 4460/6500 [29:28:22<10:47:56, 19.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8900185227394104, 'eval_runtime': 5.4384, 'eval_samples_per_second': 4.229, 'eval_steps_per_second': 1.103, 'epoch': 0.69}
                                                          69%|██████▊   | 4460/6500 [29:28:28<10:47:56, 19.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4460/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4460

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4460

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4460
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4460/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4460/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2319, 'learning_rate': 2.2390049417844756e-05, 'epoch': 0.69}
{'loss': 0.2298, 'learning_rate': 2.236989887094711e-05, 'epoch': 0.69}
{'loss': 0.2048, 'learning_rate': 2.234975478241761e-05, 'epoch': 0.69}
{'loss': 0.2381, 'learning_rate': 2.232961715696481e-05, 'epoch': 0.69}
{'loss': 0.2235, 'learning_rate': 2.2309485999295765e-05, 'epoch': 0.69}
 69%|██████▊   | 4461/6500 [29:29:29<18:52:20, 33.32s/it]                                                          69%|██████▊   | 4461/6500 [29:29:29<18:52:20, 33.32s/it] 69%|██████▊   | 4462/6500 [29:29:47<16:15:34, 28.72s/it]                                                          69%|██████▊   | 4462/6500 [29:29:47<16:15:34, 28.72s/it] 69%|██████▊   | 4463/6500 [29:30:05<14:25:57, 25.51s/it]                                                          69%|██████▊   | 4463/6500 [29:30:05<14:25:57, 25.51s/it] 69%|██████▊   | 4464/6500 [29:30:23<13:09:35, 23.27s/it]                                                          69%|██████▊   | 4464/6500 [29:30:23<13:09:35, 23.27s/it] 69%|██████▊   | 4465/6500 [29:30:41<12:16:11, 21.71s/it]                                                          69%|██████▊   | 4465/6500 [29:30:41<12:16:11, 21.71s/it] 69%|█{'loss': 0.2298, 'learning_rate': 2.228936131411601e-05, 'epoch': 0.69}
{'loss': 0.238, 'learning_rate': 2.226924310612956e-05, 'epoch': 0.69}
{'loss': 0.2375, 'learning_rate': 2.2249131380038928e-05, 'epoch': 0.69}
{'loss': 0.2275, 'learning_rate': 2.2229026140545112e-05, 'epoch': 0.69}
{'loss': 0.2328, 'learning_rate': 2.2208927392347596e-05, 'epoch': 0.69}
█████▊   | 4466/6500 [29:30:59<11:38:53, 20.62s/it]                                                          69%|██████▊   | 4466/6500 [29:30:59<11:38:53, 20.62s/it] 69%|██████▊   | 4467/6500 [29:31:17<11:13:10, 19.87s/it]                                                          69%|██████▊   | 4467/6500 [29:31:17<11:13:10, 19.87s/it] 69%|██████▊   | 4468/6500 [29:31:35<10:55:01, 19.34s/it]                                                          69%|██████▊   | 4468/6500 [29:31:35<10:55:01, 19.34s/it] 69%|██████▉   | 4469/6500 [29:31:54<10:42:26, 18.98s/it]                                                          69%|██████▉   | 4469/6500 [29:31:54<10:42:26, 18.98s/it] 69%|██████▉   | 4470/6500 [29:32:12<10:35:34, 18.79s/it]                                                          69%|██████▉   | 4470/6500 [29:32:12<10:35:34, 18.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9000927209854126, 'eval_runtime': 5.3279, 'eval_samples_per_second': 4.317, 'eval_steps_per_second': 1.126, 'epoch': 0.69}
                                                          69%|██████▉   | 4470/6500 [29:32:17<10:35:34, 18.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4470
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4470/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4470


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4470
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4470/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4470/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.244, 'learning_rate': 2.2188835140144314e-05, 'epoch': 0.69}
{'loss': 0.2261, 'learning_rate': 2.2168749388631727e-05, 'epoch': 0.69}
{'loss': 0.2432, 'learning_rate': 2.214867014250475e-05, 'epoch': 0.69}
{'loss': 0.2402, 'learning_rate': 2.212859740645679e-05, 'epoch': 0.69}
{'loss': 0.2343, 'learning_rate': 2.2108531185179727e-05, 'epoch': 0.69}
 69%|██████▉   | 4471/6500 [29:33:09<17:03:05, 30.25s/it]                                                          69%|██████▉   | 4471/6500 [29:33:09<17:03:05, 30.25s/it] 69%|██████▉   | 4472/6500 [29:33:27<14:58:34, 26.58s/it]                                                          69%|██████▉   | 4472/6500 [29:33:27<14:58:34, 26.58s/it] 69%|██████▉   | 4473/6500 [29:33:45<13:31:17, 24.01s/it]                                                          69%|██████▉   | 4473/6500 [29:33:45<13:31:17, 24.01s/it] 69%|██████▉   | 4474/6500 [29:34:03<12:30:16, 22.22s/it]                                                          69%|██████▉   | 4474/6500 [29:34:03<12:30:16, 22.22s/it] 69%|██████▉   | 4475/6500 [29:34:21<11:47:38, 20.97s/it]                                                          69%|██████▉   | 4475/6500 [29:34:21<11:47:38, 20.97s/it] 69%|█{'loss': 0.2331, 'learning_rate': 2.2088471483363916e-05, 'epoch': 0.69}
{'loss': 0.2436, 'learning_rate': 2.206841830569819e-05, 'epoch': 0.69}
{'loss': 0.2316, 'learning_rate': 2.204837165686986e-05, 'epoch': 0.69}
{'loss': 0.2228, 'learning_rate': 2.20283315415647e-05, 'epoch': 0.69}
{'loss': 0.221, 'learning_rate': 2.200829796446698e-05, 'epoch': 0.69}
█████▉   | 4476/6500 [29:34:39<11:17:50, 20.09s/it]                                                          69%|██████▉   | 4476/6500 [29:34:39<11:17:50, 20.09s/it] 69%|██████▉   | 4477/6500 [29:34:57<10:59:41, 19.57s/it]                                                          69%|██████▉   | 4477/6500 [29:34:57<10:59:41, 19.57s/it] 69%|██████▉   | 4478/6500 [29:35:16<10:44:37, 19.13s/it]                                                          69%|██████▉   | 4478/6500 [29:35:16<10:44:37, 19.13s/it] 69%|██████▉   | 4479/6500 [29:35:34<10:34:08, 18.83s/it]                                                          69%|██████▉   | 4479/6500 [29:35:34<10:34:08, 18.83s/it] 69%|██████▉   | 4480/6500 [29:35:52<10:26:49, 18.62s/it]                                                          69%|██████▉   | 4480/6500 [29:35:52<10:26:49, 18.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8915344476699829, 'eval_runtime': 5.3298, 'eval_samples_per_second': 4.315, 'eval_steps_per_second': 1.126, 'epoch': 0.69}
                                                          69%|██████▉   | 4480/6500 [29:35:57<10:26:49, 18.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4480
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4480
 
/projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4480
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4480

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4480
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4480/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4480/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2238, 'learning_rate': 2.198827093025943e-05, 'epoch': 0.69}
{'loss': 0.2931, 'learning_rate': 2.1968250443623224e-05, 'epoch': 0.69}
{'loss': 0.2255, 'learning_rate': 2.1948236509238034e-05, 'epoch': 0.69}
{'loss': 0.2164, 'learning_rate': 2.1928229131782007e-05, 'epoch': 0.69}
{'loss': 0.2467, 'learning_rate': 2.190822831593174e-05, 'epoch': 0.69}
 69%|██████▉   | 4481/6500 [29:37:14<21:03:37, 37.55s/it]                                                          69%|██████▉   | 4481/6500 [29:37:14<21:03:37, 37.55s/it] 69%|██████▉   | 4482/6500 [29:37:31<17:45:17, 31.67s/it]                                                          69%|██████▉   | 4482/6500 [29:37:31<17:45:17, 31.67s/it] 69%|██████▉   | 4483/6500 [29:37:49<15:26:31, 27.56s/it]                                                          69%|██████▉   | 4483/6500 [29:37:49<15:26:31, 27.56s/it] 69%|██████▉   | 4484/6500 [29:38:07<13:49:30, 24.69s/it]                                                          69%|██████▉   | 4484/6500 [29:38:07<13:49:30, 24.69s/it] 69%|██████▉   | 4485/6500 [29:38:25<12:41:43, 22.68s/it]                                                          69%|██████▉   | 4485/6500 [29:38:25<12:41:43, 22.68s/it] 69%|█{'loss': 0.7126, 'learning_rate': 2.1888234066362302e-05, 'epoch': 0.69}
{'loss': 0.2411, 'learning_rate': 2.1868246387747232e-05, 'epoch': 0.69}
{'loss': 0.2299, 'learning_rate': 2.1848265284758524e-05, 'epoch': 0.69}
{'loss': 0.2464, 'learning_rate': 2.182829076206664e-05, 'epoch': 0.69}
{'loss': 0.2203, 'learning_rate': 2.18083228243405e-05, 'epoch': 0.69}
█████▉   | 4486/6500 [29:38:43<11:54:27, 21.28s/it]                                                          69%|██████▉   | 4486/6500 [29:38:43<11:54:27, 21.28s/it] 69%|██████▉   | 4487/6500 [29:39:02<11:21:47, 20.32s/it]                                                          69%|██████▉   | 4487/6500 [29:39:02<11:21:47, 20.32s/it] 69%|██████▉   | 4488/6500 [29:39:20<10:58:51, 19.65s/it]                                                          69%|██████▉   | 4488/6500 [29:39:20<10:58:51, 19.65s/it] 69%|██████▉   | 4489/6500 [29:39:38<10:43:14, 19.19s/it]                                                          69%|██████▉   | 4489/6500 [29:39:38<10:43:14, 19.19s/it] 69%|██████▉   | 4490/6500 [29:39:56<10:32:19, 18.88s/it]                                                          69%|██████▉   | 4490/6500 [29:39:56<10:32:19, 18.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8962951302528381, 'eval_runtime': 5.3403, 'eval_samples_per_second': 4.307, 'eval_steps_per_second': 1.124, 'epoch': 0.69}
                                                          69%|██████▉   | 4490/6500 [29:40:01<10:32:19, 18.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4490
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4490
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4490

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4490
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4490/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2355, 'learning_rate': 2.17883614762475e-05, 'epoch': 0.69}
{'loss': 0.2169, 'learning_rate': 2.1768406722453465e-05, 'epoch': 0.69}
{'loss': 0.2234, 'learning_rate': 2.1748458567622733e-05, 'epoch': 0.69}
{'loss': 0.2481, 'learning_rate': 2.1728517016418016e-05, 'epoch': 0.69}
{'loss': 0.234, 'learning_rate': 2.1708582073500554e-05, 'epoch': 0.69}
 69%|██████▉   | 4491/6500 [29:40:57<17:34:09, 31.48s/it]                                                          69%|██████▉   | 4491/6500 [29:40:57<17:34:09, 31.48s/it] 69%|██████▉   | 4492/6500 [29:41:15<15:19:12, 27.47s/it]                                                          69%|██████▉   | 4492/6500 [29:41:15<15:19:12, 27.47s/it] 69%|██████▉   | 4493/6500 [29:41:33<13:47:43, 24.75s/it]                                                          69%|██████▉   | 4493/6500 [29:41:33<13:47:43, 24.75s/it] 69%|██████▉   | 4494/6500 [29:41:51<12:39:56, 22.73s/it]                                                          69%|██████▉   | 4494/6500 [29:41:51<12:39:56, 22.73s/it] 69%|██████▉   | 4495/6500 [29:42:09<11:53:57, 21.37s/it]                                                          69%|██████▉   | 4495/6500 [29:42:09<11:53:57, 21.37s/it] 69%|█{'loss': 0.2451, 'learning_rate': 2.1688653743530023e-05, 'epoch': 0.69}
{'loss': 0.2399, 'learning_rate': 2.166873203116454e-05, 'epoch': 0.69}
{'loss': 0.2471, 'learning_rate': 2.1648816941060668e-05, 'epoch': 0.69}
{'loss': 0.2337, 'learning_rate': 2.162890847787348e-05, 'epoch': 0.69}
{'loss': 0.2543, 'learning_rate': 2.160900664625643e-05, 'epoch': 0.69}
█████▉   | 4496/6500 [29:42:28<11:20:22, 20.37s/it]                                                          69%|██████▉   | 4496/6500 [29:42:28<11:20:22, 20.37s/it] 69%|██████▉   | 4497/6500 [29:42:46<10:56:54, 19.68s/it]                                                          69%|██████▉   | 4497/6500 [29:42:46<10:56:54, 19.68s/it] 69%|██████▉   | 4498/6500 [29:43:05<10:49:01, 19.45s/it]                                                          69%|██████▉   | 4498/6500 [29:43:05<10:49:01, 19.45s/it] 69%|██████▉   | 4499/6500 [29:43:23<10:36:24, 19.08s/it]                                                          69%|██████▉   | 4499/6500 [29:43:23<10:36:24, 19.08s/it] 69%|██████▉   | 4500/6500 [29:43:41<10:26:19, 18.79s/it]                                                          69%|██████▉   | 4500/6500 [29:43:41<10:26:19, 18.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8943294286727905, 'eval_runtime': 5.5591, 'eval_samples_per_second': 4.137, 'eval_steps_per_second': 1.079, 'epoch': 0.69}
                                                          69%|██████▉   | 4500/6500 [29:43:46<10:26:19, 18.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/tmp-checkpoint-4500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4500
 /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4500
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_5_percent/checkpoint-4500/pytorch_model.bin
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/miniconda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2298, 'learning_rate': 2.1589111450861475e-05, 'epoch': 0.69}
{'loss': 0.2346, 'learning_rate': 2.1569222896338966e-05, 'epoch': 0.69}
{'loss': 0.2412, 'learning_rate': 2.154934098733774e-05, 'epoch': 0.69}
{'loss': 0.2433, 'learning_rate': 2.1529465728505078e-05, 'epoch': 0.69}
{'loss': 0.2381, 'learning_rate': 2.150959712448669e-05, 'epoch': 0.69}
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
 69%|██████▉   | 4501/6500 [29:44:53<19:15:16, 34.68s/it]                                                          69%|██████▉   | 4501/6500 [29:44:53<19:15:16, 34.68s/it] 69%|██████▉   | 4502/6500 [29:45:11<16:27:39, 29.66s/it]                                                          69%|██████▉   | 4502/6500 [29:45:11<16:27:39, 29.66s/it] 69%|██████▉   | 4503/6500 [29:45:29<14:30:40, 26.16s/it]                                                          69%|██████▉   | 4503/6500 [29:45:29<14:30:40, 26.16s/it] 69%|██████▉   | 4504/6500 [29:45:47<13:08:58, 23.72s/it]                                                          69%|██████▉   | 4504/6500 [29:45:47<13:08:58, 23.72s/it] 69%|██████▉   | 4505/6500 [29:46:05<12:11:54, 22.01s/it]                                                          69%|██████▉   | 4505/6500 [29:46:05<12:11:54, 22.01s/it]slurmstepd: error: *** STEP 2890665.1 ON gpub032 CANCELLED AT 2024-01-25T19:20:14 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 2890665 ON gpub032 CANCELLED AT 2024-01-25T19:20:14 DUE TO TIME LIMIT ***
[2024-01-25 19:20:14,228] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers
[2024-01-25 19:20:14,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2630413 closing signal SIGTERM
[2024-01-25 19:20:14,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2630414 closing signal SIGTERM
[2024-01-25 19:20:14,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2630415 closing signal SIGTERM
[2024-01-25 19:20:14,229] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2630416 closing signal SIGTERM
