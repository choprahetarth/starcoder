Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Currently Loaded Modules:
  1) gcc/11.4.0      4) cue-login-env/1.0   7) python/3.11.6
  2) openmpi/4.1.6   5) slurm-env/0.1       8) cudnn/8.9.0.131
  3) cuda/11.8.0     6) default-s11         9) anaconda3_gpu/23.9.0

 

job is starting on gpua052.delta.ncsa.illinois.edu
WARNING: A conda environment already exists at '/u/bzd2/.conda/envs/hetarth_py10'
Remove existing environment (y/[n])? 

CondaSystemExit: Exiting.

usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...
conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'build', 'content-trust', 'convert', 'debug', 'develop', 'doctor', 'index', 'inspect', 'metapackage', 'render', 'repoquery', 'skeleton', 'verify', 'repo', 'server', 'env', 'token')
Starting script...
Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: torch in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231003+cu121)
Requirement already satisfied: torchvision in /sw/external/python/anaconda3/lib/python3.9/site-packages (0.17.0.dev20231003+cu121)
Requirement already satisfied: torchaudio in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231002)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (2.1.0+6e4932cda8)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (1.24.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (2.31.0)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (9.4.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (4.37.0.dev0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.2)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/peft.git
  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-vpwtg5ft
  Resolved https://github.com/huggingface/peft.git to commit 1c1c7fdaa6e6abaa53939b865dee1eded82ad032
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.4.1)
Requirement already satisfied: torch>=1.13.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (2.2.0.dev20231003+cu121)
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.37.0.dev0)
Requirement already satisfied: tqdm in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.65.0)
Requirement already satisfied: accelerate>=0.21.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.25.0)
Requirement already satisfied: safetensors in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.3.2)
Requirement already satisfied: huggingface-hub>=0.17.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.2)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0+6e4932cda8)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (0.15.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-o37pxyw4
  Resolved https://github.com/huggingface/transformers to commit f40b87de0ca234df61f76928956c4a2118c0b548
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (0.15.0)
Collecting safetensors>=0.4.1 (from transformers==4.38.0.dev0)
  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/aa/9a/723fc6eed972b28bbb24241b246005093b3c27340bc8f7b7606d75a92834/safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.38.0.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.38.0.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.38.0.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.38.0.dev0) (2023.7.22)
Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 8.1 MB/s eta 0:00:00
Building wheels for collected packages: transformers
  Building wheel for transformers (pyproject.toml): started
  Building wheel for transformers (pyproject.toml): finished with status 'done'
  Created wheel for transformers: filename=transformers-4.38.0.dev0-py3-none-any.whl size=8408548 sha256=fa02fd48741e7663518b0af715f59a5aa8919f6a34907510b53ef070fa7ccef1
  Stored in directory: /tmp/pip-ephem-wheel-cache-w_ktlrmv/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b
Successfully built transformers
Installing collected packages: safetensors, transformers
  Attempting uninstall: transformers
    Found existing installation: transformers 4.37.0.dev0
    Uninstalling transformers-4.37.0.dev0:
      Successfully uninstalled transformers-4.37.0.dev0
Successfully installed safetensors-0.4.2 transformers-4.38.0.dev0

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: datasets in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.14.5)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (1.24.3)
Requirement already satisfied: pyarrow>=8.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.3.7)
Requirement already satisfied: pandas in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.3)
Requirement already satisfied: requests>=2.19.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (4.65.0)
Requirement already satisfied: xxhash in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.2)
Requirement already satisfied: multiprocess in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (2023.6.0)
Requirement already satisfied: aiohttp in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.5)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.19.4)
Requirement already satisfied: packaging in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (5.4.1)
Requirement already satisfied: attrs>=17.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)
Requirement already satisfied: multidict<7.0,>=4.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)
Requirement already satisfied: frozenlist>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: accelerate in /u/bzd2/.local/lib/python3.9/site-packages (0.25.0)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.4.1)
Requirement already satisfied: torch>=1.10.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (2.2.0.dev20231003+cu121)
Requirement already satisfied: huggingface-hub in /u/bzd2/.local/lib/python3.9/site-packages (from accelerate) (0.19.4)
Requirement already satisfied: safetensors>=0.3.1 in /u/bzd2/.local/lib/python3.9/site-packages (from accelerate) (0.4.2)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0+6e4932cda8)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.65.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: huggingface_hub in /u/bzd2/.local/lib/python3.9/site-packages (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface_hub) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.65.0)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (5.4.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.7.1)
Requirement already satisfied: packaging>=20.9 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (23.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: bitsandbytes in /u/bzd2/.local/lib/python3.9/site-packages (0.41.3.post2)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: wandb in /u/bzd2/.local/lib/python3.9/site-packages (0.16.1)
Requirement already satisfied: Click!=8.0.0,>=7.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (8.0.4)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (3.1.40)
Requirement already satisfied: requests<3,>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (2.31.0)
Requirement already satisfied: psutil>=5.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.9.0)
Requirement already satisfied: sentry-sdk>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (1.38.0)
Requirement already satisfied: docker-pycreds>=0.4.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: PyYAML in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.4.1)
Requirement already satisfied: setproctitle in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.3.2)
Requirement already satisfied: setuptools in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (68.0.0)
Requirement already satisfied: appdirs>=1.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.4.4)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (4.7.1)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (3.20.3)
Requirement already satisfied: six>=1.4.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)
Requirement already satisfied: smmap<6,>=3.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /sw/external/python/anaconda3/lib/python3.9/site-packages (1.3.0)
Requirement already satisfied: numpy>=1.17.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.3)
Requirement already satisfied: scipy>=1.5.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.8.1)
Requirement already satisfied: joblib>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: code_bert_score in /u/bzd2/.local/lib/python3.9/site-packages (0.4.1)
Requirement already satisfied: torch>=1.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.2.0.dev20231003+cu121)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (1.24.3)
Requirement already satisfied: pandas>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.0.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.31.0)
Requirement already satisfied: tqdm>=4.31.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (4.65.0)
Requirement already satisfied: matplotlib in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (3.7.2)
Requirement already satisfied: transformers>=3.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from code_bert_score) (4.38.0.dev0)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2.1.0+6e4932cda8)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.19.4)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.15.0)
Requirement already satisfied: safetensors>=0.4.1 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.4.2)
Requirement already satisfied: contourpy>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.0.5)
Requirement already satisfied: cycler>=0.10 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (4.25.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.4.4)
Requirement already satisfied: pillow>=6.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (9.4.0)
Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (3.0.9)
Requirement already satisfied: importlib-resources>=3.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (5.2.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2023.7.22)
Requirement already satisfied: zipp>=3.1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->code_bert_score) (3.11.0)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->code_bert_score) (1.16.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->code_bert_score) (2.1.1)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.0.0->code_bert_score) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: nltk in /sw/external/python/anaconda3/lib/python3.9/site-packages (3.8.1)
Requirement already satisfied: click in /sw/external/python/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)
Requirement already satisfied: joblib in /sw/external/python/anaconda3/lib/python3.9/site-packages (from nltk) (1.2.0)
Requirement already satisfied: regex>=2021.8.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from nltk) (2022.7.9)
Requirement already satisfied: tqdm in /sw/external/python/anaconda3/lib/python3.9/site-packages (from nltk) (4.65.0)

WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2024-01-24 17:45:30.960473: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-24 17:45:30.960497: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-24 17:45:30.960487: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-24 17:45:30.960496: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-24 17:45:30.961046: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-24 17:45:30.961053: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-24 17:45:30.961058: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-24 17:45:30.961134: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-24 17:45:30.966844: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-24 17:45:30.966853: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-24 17:45:30.966853: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-24 17:45:30.966856: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-24 17:45:32.090965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-24 17:45:32.090980: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-24 17:45:32.091008: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-24 17:45:32.091057: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'GPL-3.0-only', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'input': 'name: Delete /etc/motd file', 'download_count': '81335', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n', 'org_name': 'adriagalin', 'repo_name': 'motd'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'input': 'name: check if gogs exists', 'download_count': '16571', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n', 'org_name': 'siamaksade', 'repo_name': 'openshift_gogs'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'input': 'name: assert release profile idempotency', 'download_count': '688', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n', 'org_name': 'devopsarr', 'repo_name': 'sonarr'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'input': 'name: Start and enable anaconda', 'download_count': '9401', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n', 'org_name': 'buluma', 'repo_name': 'anaconda'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'input': 'name: Add hosts group temporary inventory group without pem path', 'download_count': '1904', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n", 'org_name': 'chrismeyersfsu', 'repo_name': 'ec2_server'}
{'repo_name': 'motd', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'input': 'name: Delete /etc/motd file', 'download_count': '81335', 'license': 'GPL-3.0-only', 'org_name': 'adriagalin'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'repo_name': 'openshift_gogs', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'input': 'name: check if gogs exists', 'download_count': '16571', 'license': '', 'org_name': 'siamaksade'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'GPL-3.0-only', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'org_name': 'adriagalin', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n', 'download_count': '81335', 'repo_name': 'motd', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'input': 'name: Delete /etc/motd file'}{'license': 'GPL-3.0-only', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'org_name': 'adriagalin', 'input': 'name: Delete /etc/motd file', 'repo_name': 'motd', 'download_count': '81335', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd'}

{'repo_name': 'sonarr', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'input': 'name: assert release profile idempotency', 'download_count': '688', 'license': '', 'org_name': 'devopsarr'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'repo_name': 'anaconda', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'input': 'name: Start and enable anaconda', 'download_count': '9401', 'license': 'Apache-2.0-Short,Apache-2.0', 'org_name': 'buluma'}{'license': '', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'org_name': 'siamaksade', 'input': 'name: check if gogs exists', 'repo_name': 'openshift_gogs', 'download_count': '16571', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs'}{'license': '', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'org_name': 'siamaksade', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n', 'download_count': '16571', 'repo_name': 'openshift_gogs', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'input': 'name: check if gogs exists'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'org_name': 'devopsarr', 'input': 'name: assert release profile idempotency', 'repo_name': 'sonarr', 'download_count': '688', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr'}{'repo_name': 'ec2_server', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n", 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'input': 'name: Add hosts group temporary inventory group without pem path', 'download_count': '1904', 'license': '', 'org_name': 'chrismeyersfsu'}
{'license': '', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'org_name': 'devopsarr', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n', 'download_count': '688', 'repo_name': 'sonarr', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'input': 'name: assert release profile idempotency'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'org_name': 'buluma', 'input': 'name: Start and enable anaconda', 'repo_name': 'anaconda', 'download_count': '9401', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'org_name': 'buluma', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n', 'download_count': '9401', 'repo_name': 'anaconda', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'input': 'name: Start and enable anaconda'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'org_name': 'chrismeyersfsu', 'input': 'name: Add hosts group temporary inventory group without pem path', 'repo_name': 'ec2_server', 'download_count': '1904', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n", 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server'}
{'license': '', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'org_name': 'chrismeyersfsu', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n", 'download_count': '1904', 'repo_name': 'ec2_server', 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'input': 'name: Add hosts group temporary inventory group without pem path'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'input': 'name: Install', 'download_count': '561', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n', 'org_name': 'lifeofguenter', 'repo_name': 'nginx'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'input': 'name: assert | Test locale_lc_time', 'download_count': '34969', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n', 'org_name': 'robertdebock', 'repo_name': 'locale'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'input': 'name: Create the APT repository (Debian)', 'download_count': '513', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n', 'org_name': 'brentwg', 'repo_name': 'visual-studio-code'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'input': 'name: Add MariaDB Repository Key for', 'download_count': '801', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n', 'org_name': 'mahdi22', 'repo_name': 'mariadb_install'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'input': 'name: ensure necessary packages are installed.', 'download_count': '747', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n', 'org_name': 'buluma', 'repo_name': 'packer_rhel'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'download_count': '4445', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n', 'org_name': 'darkwizard242', 'repo_name': 'packer'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'repo_name': 'nginx', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'input': 'name: Install', 'download_count': '561', 'license': 'MIT', 'org_name': 'lifeofguenter'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'repo_name': 'locale', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'input': 'name: assert | Test locale_lc_time', 'download_count': '34969', 'license': 'Apache-2.0-Short,Apache-2.0', 'org_name': 'robertdebock'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'org_name': 'lifeofguenter', 'input': 'name: Install', 'repo_name': 'nginx', 'download_count': '561', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx'}
{'repo_name': 'visual-studio-code', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'input': 'name: Create the APT repository (Debian)', 'download_count': '513', 'license': '', 'org_name': 'brentwg'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'org_name': 'lifeofguenter', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n', 'download_count': '561', 'repo_name': 'nginx', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'input': 'name: Install'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'org_name': 'robertdebock', 'input': 'name: assert | Test locale_lc_time', 'repo_name': 'locale', 'download_count': '34969', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'org_name': 'robertdebock', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n', 'download_count': '34969', 'repo_name': 'locale', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'input': 'name: assert | Test locale_lc_time'}{'repo_name': 'mariadb_install', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'input': 'name: Add MariaDB Repository Key for', 'download_count': '801', 'license': '', 'org_name': 'mahdi22'}

{'license': '', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'org_name': 'brentwg', 'input': 'name: Create the APT repository (Debian)', 'repo_name': 'visual-studio-code', 'download_count': '513', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'license': '', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'org_name': 'brentwg', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n', 'download_count': '513', 'repo_name': 'visual-studio-code', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'input': 'name: Create the APT repository (Debian)'}
{'license': '', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'org_name': 'mahdi22', 'input': 'name: Add MariaDB Repository Key for', 'repo_name': 'mariadb_install', 'download_count': '801', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install'}{'repo_name': 'packer_rhel', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'input': 'name: ensure necessary packages are installed.', 'download_count': '747', 'license': 'Apache-2.0-Short,Apache-2.0', 'org_name': 'buluma'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'org_name': 'mahdi22', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n', 'download_count': '801', 'repo_name': 'mariadb_install', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'input': 'name: Add MariaDB Repository Key for'}

{'license': 'Apache-2.0-Short,Apache-2.0', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'org_name': 'buluma', 'input': 'name: ensure necessary packages are installed.', 'repo_name': 'packer_rhel', 'download_count': '747', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel'}
{'repo_name': 'packer', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'download_count': '4445', 'license': 'MIT', 'org_name': 'darkwizard242'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'org_name': 'buluma', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n', 'download_count': '747', 'repo_name': 'packer_rhel', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'input': 'name: ensure necessary packages are installed.'}{'license': 'MIT', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'org_name': 'darkwizard242', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'repo_name': 'packer', 'download_count': '4445', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'org_name': 'darkwizard242', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n', 'download_count': '4445', 'repo_name': 'packer', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state'}
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<03:21,  1.98it/s]  0%|          | 1/400 [00:00<03:21,  1.98it/s]  0%|          | 1/400 [00:00<03:21,  1.98it/s]  0%|          | 1/400 [00:00<03:21,  1.98it/s]100%|██████████| 400/400 [00:00<00:00, 699.51it/s]
100%|██████████| 400/400 [00:00<00:00, 699.49it/s]
100%|██████████| 400/400 [00:00<00:00, 699.21it/s]
100%|██████████| 400/400 [00:00<00:00, 697.82it/s]
The character to token ratio of the dataset is: 3.18
Loading the model
The character to token ratio of the dataset is: 3.18
Loading the model
The character to token ratio of the dataset is: 3.18
Loading the model
The character to token ratio of the dataset is: 3.18
Loading the model
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.38.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/_utils.py:832: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/_utils.py:832: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/_utils.py:832: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/_utils.py:832: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  25%|██▌       | 1/4 [04:11<12:34, 251.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [04:12<12:36, 252.11s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [04:12<12:36, 252.12s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [04:12<12:36, 252.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [08:16<08:15, 247.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [08:17<08:16, 248.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [08:17<08:16, 248.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [08:17<08:16, 248.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [12:13<04:02, 242.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [12:14<04:03, 243.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [12:14<04:03, 243.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [12:14<04:02, 243.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [12:15<00:00, 147.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [12:15<00:00, 183.76s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 4/4 [12:15<00:00, 147.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [12:15<00:00, 183.77s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 4/4 [12:15<00:00, 147.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [12:15<00:00, 183.77s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 4/4 [12:15<00:00, 147.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [12:15<00:00, 183.77s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
Starting main loop
PyTorch: setting up devices
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
Starting main loop
PyTorch: setting up devices
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
Starting main loop
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
Starting main loop
PyTorch: setting up devices
PyTorch: setting up devices
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
TOKENIZERS_PARALLELISM=(true | false)
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
Training...
Training...
Training...
Training...
Currently training with a batch size of: 1
***** Running training *****
  Num examples = 208,000
  Num Epochs = 9,223,372,036,854,775,807
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 3,250
  Number of trainable parameters = 24,944,640
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: hetarthvader (complex_dnn). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /projects/bbvz/bzd2/wandb/run-20240124_175858-6cclgrt4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent
wandb: ⭐️ View project at https://wandb.ai/complex_dnn/huggingface
wandb: 🚀 View run at https://wandb.ai/complex_dnn/huggingface/runs/6cclgrt4
  0%|          | 0/3250 [00:00<?, ?it/s]/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9651, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 2.0186, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 1.8841, 'learning_rate': 9.999997661121407e-05, 'epoch': 0.0}
{'loss': 1.8822, 'learning_rate': 9.999990644487813e-05, 'epoch': 0.0}
{'loss': 1.7656, 'learning_rate': 9.999978950105786e-05, 'epoch': 0.0}
{'loss': 1.7687, 'learning_rate': 9.999962577986265e-05, 'epoch': 0.0}
  0%|          | 1/3250 [00:29<26:10:40, 29.01s/it]                                                     0%|          | 1/3250 [00:29<26:10:40, 29.01s/it]  0%|          | 2/3250 [00:45<19:17:32, 21.38s/it]                                                     0%|          | 2/3250 [00:45<19:17:32, 21.38s/it]  0%|          | 3/3250 [01:00<17:01:26, 18.87s/it]                                                     0%|          | 3/3250 [01:00<17:01:26, 18.87s/it]  0%|          | 4/3250 [01:16<15:57:53, 17.71s/it]                                                     0%|          | 4/3250 [01:16<15:57:53, 17.71s/it]  0%|          | 5/3250 [01:32<15:22:45, 17.06s/it]                                                     0%|          | 5/3250 [01:32<15:22:45, 17.06s/it]  0%|          | 6/3250 [01:48<15:01:15, 16.67s/it]                                                     0%|          | 6/3250 [01:48<15:01:15, 16.67s/it]  0%|          | 7/3250 [02:04<14:47:03, 16.41s/it]                             {'loss': 1.9984, 'learning_rate': 9.999941528144567e-05, 'epoch': 0.0}
{'loss': 1.6304, 'learning_rate': 9.999915800600383e-05, 'epoch': 0.0}
{'loss': 1.5816, 'learning_rate': 9.999885395377788e-05, 'epoch': 0.0}
{'loss': 1.5732, 'learning_rate': 9.999850312505221e-05, 'epoch': 0.0}
                        0%|          | 7/3250 [02:04<14:47:03, 16.41s/it]  0%|          | 8/3250 [02:20<14:38:19, 16.26s/it]                                                     0%|          | 8/3250 [02:20<14:38:19, 16.26s/it]  0%|          | 9/3250 [02:36<14:32:11, 16.15s/it]                                                     0%|          | 9/3250 [02:36<14:32:11, 16.15s/it]  0%|          | 10/3250 [02:52<14:28:03, 16.08s/it]                                                      0%|          | 10/3250 [02:52<14:28:03, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.4107571840286255, 'eval_runtime': 2.5391, 'eval_samples_per_second': 4.726, 'eval_steps_per_second': 1.182, 'epoch': 0.0}
                                                      0%|          | 10/3250 [02:54<14:28:03, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-10
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-10
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-10 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-10

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-10
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-10/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4997, 'learning_rate': 9.99981055201551e-05, 'epoch': 0.0}
{'loss': 1.3811, 'learning_rate': 9.999766113945847e-05, 'epoch': 0.0}
{'loss': 1.4357, 'learning_rate': 9.999716998337812e-05, 'epoch': 0.0}
{'loss': 1.4157, 'learning_rate': 9.99966320523735e-05, 'epoch': 0.0}
{'loss': 1.3309, 'learning_rate': 9.999604734694792e-05, 'epoch': 0.0}
{'loss': 1.3251, 'learning_rate': 9.999541586764836e-05, 'epoch': 0.0}
  0%|          | 11/3250 [03:11<15:25:54, 17.15s/it]                                                      0%|          | 11/3250 [03:11<15:25:54, 17.15s/it]  0%|          | 12/3250 [03:27<15:05:16, 16.77s/it]                                                      0%|          | 12/3250 [03:27<15:05:16, 16.77s/it]  0%|          | 13/3250 [03:43<14:50:50, 16.51s/it]                                                      0%|          | 13/3250 [03:43<14:50:50, 16.51s/it]  0%|          | 14/3250 [03:59<14:39:58, 16.32s/it]                                                      0%|          | 14/3250 [03:59<14:39:58, 16.32s/it]  0%|          | 15/3250 [04:15<14:32:39, 16.19s/it]                                                      0%|          | 15/3250 [04:15<14:32:39, 16.19s/it]  0%|          | 16/3250 [04:31<14:27:16, 16.09s/it]                                                      0%|          | 16/3250 [04:31<14:27:16, 16.09s/it]  1%|          | 17/3250 [04:47<14:34:50, 16.24s/it]          {'loss': 1.2622, 'learning_rate': 9.999473761506563e-05, 'epoch': 0.01}
{'loss': 1.2794, 'learning_rate': 9.999401258983425e-05, 'epoch': 0.01}
{'loss': 1.2748, 'learning_rate': 9.999324079263253e-05, 'epoch': 0.01}
{'loss': 1.264, 'learning_rate': 9.999242222418252e-05, 'epoch': 0.01}
                                            1%|          | 17/3250 [04:47<14:34:50, 16.24s/it]  1%|          | 18/3250 [05:03<14:28:49, 16.13s/it]                                                      1%|          | 18/3250 [05:03<14:28:49, 16.13s/it]  1%|          | 19/3250 [05:19<14:24:37, 16.06s/it]                                                      1%|          | 19/3250 [05:19<14:24:37, 16.06s/it]  1%|          | 20/3250 [05:35<14:21:38, 16.01s/it]                                                      1%|          | 20/3250 [05:35<14:21:38, 16.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.18625807762146, 'eval_runtime': 2.4948, 'eval_samples_per_second': 4.81, 'eval_steps_per_second': 1.202, 'epoch': 0.01}
                                                      1%|          | 20/3250 [05:38<14:21:38, 16.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-20
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-20
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-20/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-20


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-20
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-20/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.235, 'learning_rate': 9.999155688525004e-05, 'epoch': 0.01}
{'loss': 1.2606, 'learning_rate': 9.999064477664466e-05, 'epoch': 0.01}
{'loss': 1.2305, 'learning_rate': 9.998968589921969e-05, 'epoch': 0.01}
{'loss': 1.2227, 'learning_rate': 9.998868025387223e-05, 'epoch': 0.01}
{'loss': 1.1984, 'learning_rate': 9.998762784154308e-05, 'epoch': 0.01}
{'loss': 1.178, 'learning_rate': 9.998652866321687e-05, 'epoch': 0.01}
  1%|          | 21/3250 [05:54<15:12:09, 16.95s/it]                                                      1%|          | 21/3250 [05:54<15:12:09, 16.95s/it]  1%|          | 22/3250 [06:10<14:54:56, 16.63s/it]                                                      1%|          | 22/3250 [06:10<14:54:56, 16.63s/it]  1%|          | 23/3250 [06:26<14:42:47, 16.41s/it]                                                      1%|          | 23/3250 [06:26<14:42:47, 16.41s/it]  1%|          | 24/3250 [06:42<14:34:24, 16.26s/it]                                                      1%|          | 24/3250 [06:42<14:34:24, 16.26s/it]  1%|          | 25/3250 [06:58<14:28:21, 16.16s/it]                                                      1%|          | 25/3250 [06:58<14:28:21, 16.16s/it]  1%|          | 26/3250 [07:14<14:24:07, 16.08s/it]                                                      1%|          | 26/3250 [07:14<14:24:07, 16.08s/it]  1%|          | 27/3250 [07:30<14:21:13, 16.03s/it]          {'loss': 1.1986, 'learning_rate': 9.99853827199219e-05, 'epoch': 0.01}
{'loss': 1.1865, 'learning_rate': 9.998419001273029e-05, 'epoch': 0.01}
{'loss': 1.2075, 'learning_rate': 9.998295054275786e-05, 'epoch': 0.01}
{'loss': 1.2316, 'learning_rate': 9.99816643111642e-05, 'epoch': 0.01}
                                            1%|          | 27/3250 [07:30<14:21:13, 16.03s/it]  1%|          | 28/3250 [07:46<14:19:06, 16.00s/it]                                                      1%|          | 28/3250 [07:46<14:19:06, 16.00s/it]  1%|          | 29/3250 [08:02<14:17:45, 15.98s/it]                                                      1%|          | 29/3250 [08:02<14:17:45, 15.98s/it]  1%|          | 30/3250 [08:17<14:16:43, 15.96s/it]                                                      1%|          | 30/3250 [08:17<14:16:43, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.115474820137024, 'eval_runtime': 2.4982, 'eval_samples_per_second': 4.803, 'eval_steps_per_second': 1.201, 'epoch': 0.01}
                                                      1%|          | 30/3250 [08:20<14:16:43, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-30
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-30/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-30


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-30

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-30
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-30/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1627, 'learning_rate': 9.998033131915266e-05, 'epoch': 0.01}
{'loss': 1.172, 'learning_rate': 9.997895156797028e-05, 'epoch': 0.01}
{'loss': 1.1501, 'learning_rate': 9.997752505890794e-05, 'epoch': 0.01}
{'loss': 1.1362, 'learning_rate': 9.997605179330019e-05, 'epoch': 0.01}
{'loss': 1.1417, 'learning_rate': 9.997453177252534e-05, 'epoch': 0.01}
{'loss': 1.1723, 'learning_rate': 9.997296499800545e-05, 'epoch': 0.01}
  1%|          | 31/3250 [08:37<15:07:46, 16.92s/it]                                                      1%|          | 31/3250 [08:37<15:07:46, 16.92s/it]  1%|          | 32/3250 [08:52<14:51:02, 16.61s/it]                                                      1%|          | 32/3250 [08:52<14:51:02, 16.61s/it]  1%|          | 33/3250 [09:09<14:43:14, 16.47s/it]                                                      1%|          | 33/3250 [09:09<14:43:14, 16.47s/it]  1%|          | 34/3250 [09:25<14:33:52, 16.30s/it]                                                      1%|          | 34/3250 [09:25<14:33:52, 16.30s/it]  1%|          | 35/3250 [09:40<14:27:16, 16.19s/it]                                                      1%|          | 35/3250 [09:40<14:27:16, 16.19s/it]  1%|          | 36/3250 [09:56<14:22:43, 16.11s/it]                                                      1%|          | 36/3250 [09:56<14:22:43, 16.11s/it]  1%|          | 37/3250 [10:12<14:18:39, 16.03s/it]          {'loss': 1.5788, 'learning_rate': 9.997135147120633e-05, 'epoch': 0.01}
{'loss': 1.0465, 'learning_rate': 9.99696911936375e-05, 'epoch': 0.01}
{'loss': 1.1276, 'learning_rate': 9.996798416685228e-05, 'epoch': 0.01}
{'loss': 1.1803, 'learning_rate': 9.99662303924476e-05, 'epoch': 0.01}
                                            1%|          | 37/3250 [10:12<14:18:39, 16.03s/it]  1%|          | 38/3250 [10:28<14:16:23, 16.00s/it]                                                      1%|          | 38/3250 [10:28<14:16:23, 16.00s/it]  1%|          | 39/3250 [10:44<14:14:18, 15.96s/it]                                                      1%|          | 39/3250 [10:44<14:14:18, 15.96s/it]  1%|          | 40/3250 [11:00<14:13:15, 15.95s/it]                                                      1%|          | 40/3250 [11:00<14:13:15, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.073500156402588, 'eval_runtime': 2.498, 'eval_samples_per_second': 4.804, 'eval_steps_per_second': 1.201, 'epoch': 0.01}
                                                      1%|          | 40/3250 [11:02<14:13:15, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-40
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-40/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-40

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-40

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-40
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-40/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.122, 'learning_rate': 9.996442987206428e-05, 'epoch': 0.01}
{'loss': 1.1185, 'learning_rate': 9.996258260738676e-05, 'epoch': 0.01}
{'loss': 1.1323, 'learning_rate': 9.996068860014325e-05, 'epoch': 0.01}
{'loss': 1.1715, 'learning_rate': 9.995874785210573e-05, 'epoch': 0.01}
{'loss': 1.1535, 'learning_rate': 9.995676036508982e-05, 'epoch': 0.01}
{'loss': 1.0993, 'learning_rate': 9.995472614095495e-05, 'epoch': 0.01}
  1%|▏         | 41/3250 [11:19<15:03:43, 16.90s/it]                                                      1%|▏         | 41/3250 [11:19<15:03:43, 16.90s/it]  1%|▏         | 42/3250 [11:35<14:47:27, 16.60s/it]                                                      1%|▏         | 42/3250 [11:35<14:47:27, 16.60s/it]  1%|▏         | 43/3250 [11:51<14:36:09, 16.39s/it]                                                      1%|▏         | 43/3250 [11:51<14:36:09, 16.39s/it]  1%|▏         | 44/3250 [12:07<14:28:16, 16.25s/it]                                                      1%|▏         | 44/3250 [12:07<14:28:16, 16.25s/it]  1%|▏         | 45/3250 [12:23<14:22:35, 16.15s/it]                                                      1%|▏         | 45/3250 [12:23<14:22:35, 16.15s/it]  1%|▏         | 46/3250 [12:39<14:18:36, 16.08s/it]                                                      1%|▏         | 46/3250 [12:39<14:18:36, 16.08s/it]  1%|▏         | 47/3250 [12:55<14:15{'loss': 1.0654, 'learning_rate': 9.995264518160425e-05, 'epoch': 0.01}
{'loss': 1.1212, 'learning_rate': 9.995051748898453e-05, 'epoch': 0.01}
{'loss': 1.0881, 'learning_rate': 9.994834306508638e-05, 'epoch': 0.02}
{'loss': 1.0959, 'learning_rate': 9.994612191194406e-05, 'epoch': 0.02}
:44, 16.03s/it]                                                      1%|▏         | 47/3250 [12:55<14:15:44, 16.03s/it]  1%|▏         | 48/3250 [13:10<14:13:50, 16.00s/it]                                                      1%|▏         | 48/3250 [13:10<14:13:50, 16.00s/it]  2%|▏         | 49/3250 [13:26<14:11:49, 15.97s/it]                                                      2%|▏         | 49/3250 [13:26<14:11:49, 15.97s/it]  2%|▏         | 50/3250 [13:43<14:19:40, 16.12s/it]                                                      2%|▏         | 50/3250 [13:43<14:19:40, 16.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0427738428115845, 'eval_runtime': 2.4826, 'eval_samples_per_second': 4.834, 'eval_steps_per_second': 1.208, 'epoch': 0.02}
                                                      2%|▏         | 50/3250 [13:45<14:19:40, 16.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-50
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-50 

/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-50
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-50

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-50
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-50/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0737, 'learning_rate': 9.99438540316356e-05, 'epoch': 0.02}
{'loss': 1.0789, 'learning_rate': 9.99415394262827e-05, 'epoch': 0.02}
{'loss': 1.0893, 'learning_rate': 9.993917809805081e-05, 'epoch': 0.02}
{'loss': 1.1012, 'learning_rate': 9.993677004914907e-05, 'epoch': 0.02}
{'loss': 1.0757, 'learning_rate': 9.993431528183032e-05, 'epoch': 0.02}
{'loss': 1.0593, 'learning_rate': 9.993181379839113e-05, 'epoch': 0.02}
  2%|▏         | 51/3250 [14:34<23:36:23, 26.57s/it]                                                      2%|▏         | 51/3250 [14:34<23:36:23, 26.57s/it]  2%|▏         | 52/3250 [14:50<20:45:45, 23.37s/it]                                                      2%|▏         | 52/3250 [14:50<20:45:45, 23.37s/it]  2%|▏         | 53/3250 [15:06<18:46:02, 21.13s/it]                                                      2%|▏         | 53/3250 [15:06<18:46:02, 21.13s/it]  2%|▏         | 54/3250 [15:22<17:22:24, 19.57s/it]                                                      2%|▏         | 54/3250 [15:22<17:22:24, 19.57s/it]  2%|▏         | 55/3250 [15:37<16:23:52, 18.48s/it]                                                      2%|▏         | 55/3250 [15:37<16:23:52, 18.48s/it]  2%|▏         | 56/3250 [15:53<15:42:42, 17.71s/it]                                                      2%|▏         | 56/3250 [15:53<15:42:42, 17.71s/it]  2%|▏         | 57/3250 [16:09<15:13{'loss': 1.0575, 'learning_rate': 9.992926560117176e-05, 'epoch': 0.02}
{'loss': 1.1034, 'learning_rate': 9.992667069255619e-05, 'epoch': 0.02}
{'loss': 1.0511, 'learning_rate': 9.992402907497209e-05, 'epoch': 0.02}
{'loss': 1.1325, 'learning_rate': 9.992134075089084e-05, 'epoch': 0.02}
:35, 17.17s/it]                                                      2%|▏         | 57/3250 [16:09<15:13:35, 17.17s/it]  2%|▏         | 58/3250 [16:25<14:53:14, 16.79s/it]                                                      2%|▏         | 58/3250 [16:25<14:53:14, 16.79s/it]  2%|▏         | 59/3250 [16:41<14:38:31, 16.52s/it]                                                      2%|▏         | 59/3250 [16:41<14:38:31, 16.52s/it]  2%|▏         | 60/3250 [16:57<14:28:21, 16.33s/it]                                                      2%|▏         | 60/3250 [16:57<14:28:21, 16.33s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0195672512054443, 'eval_runtime': 2.4798, 'eval_samples_per_second': 4.839, 'eval_steps_per_second': 1.21, 'epoch': 0.02}
                                                      2%|▏         | 60/3250 [16:59<14:28:21, 16.33s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-60
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-60/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-60

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-60

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-60
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-60/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0421, 'learning_rate': 9.991860572282747e-05, 'epoch': 0.02}
{'loss': 1.082, 'learning_rate': 9.991582399334076e-05, 'epoch': 0.02}
{'loss': 1.0597, 'learning_rate': 9.991299556503318e-05, 'epoch': 0.02}
{'loss': 1.0439, 'learning_rate': 9.991012044055084e-05, 'epoch': 0.02}
{'loss': 1.0518, 'learning_rate': 9.990719862258358e-05, 'epoch': 0.02}
{'loss': 1.0483, 'learning_rate': 9.990423011386489e-05, 'epoch': 0.02}
  2%|▏         | 61/3250 [17:16<15:12:20, 17.17s/it]                                                      2%|▏         | 61/3250 [17:16<15:12:20, 17.17s/it]  2%|▏         | 62/3250 [17:32<14:51:50, 16.78s/it]                                                      2%|▏         | 62/3250 [17:32<14:51:50, 16.78s/it]  2%|▏         | 63/3250 [17:48<14:37:21, 16.52s/it]                                                      2%|▏         | 63/3250 [17:48<14:37:21, 16.52s/it]  2%|▏         | 64/3250 [18:04<14:27:10, 16.33s/it]                                                      2%|▏         | 64/3250 [18:04<14:27:10, 16.33s/it]  2%|▏         | 65/3250 [18:20<14:19:40, 16.19s/it]                                                      2%|▏         | 65/3250 [18:20<14:19:40, 16.19s/it]  2%|▏         | 66/3250 [18:36<14:18:18, 16.17s/it]                                                      2%|▏         | 66/3250 [18:36<14:18:18, 16.17s/it]  2%|▏         | 67/3250 [18:52<14:13{'loss': 1.0213, 'learning_rate': 9.990121491717201e-05, 'epoch': 0.02}
{'loss': 1.4459, 'learning_rate': 9.989815303532577e-05, 'epoch': 0.02}
{'loss': 1.0353, 'learning_rate': 9.989504447119073e-05, 'epoch': 0.02}
{'loss': 1.0868, 'learning_rate': 9.989188922767512e-05, 'epoch': 0.02}
:56, 16.10s/it]                                                      2%|▏         | 67/3250 [18:52<14:13:56, 16.10s/it]  2%|▏         | 68/3250 [19:08<14:09:52, 16.03s/it]                                                      2%|▏         | 68/3250 [19:08<14:09:52, 16.03s/it]  2%|▏         | 69/3250 [19:23<14:07:39, 15.99s/it]                                                      2%|▏         | 69/3250 [19:23<14:07:39, 15.99s/it]  2%|▏         | 70/3250 [19:39<14:06:04, 15.96s/it]                                                      2%|▏         | 70/3250 [19:39<14:06:04, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0020776987075806, 'eval_runtime': 2.4849, 'eval_samples_per_second': 4.829, 'eval_steps_per_second': 1.207, 'epoch': 0.02}
                                                      2%|▏         | 70/3250 [19:42<14:06:04, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-70
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-70/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-70

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-70

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-70
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-70/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0498, 'learning_rate': 9.988868730773082e-05, 'epoch': 0.02}
{'loss': 1.0461, 'learning_rate': 9.98854387143534e-05, 'epoch': 0.02}
{'loss': 1.0197, 'learning_rate': 9.988214345058209e-05, 'epoch': 0.02}
{'loss': 1.0933, 'learning_rate': 9.987880151949976e-05, 'epoch': 0.02}
{'loss': 1.0701, 'learning_rate': 9.987541292423298e-05, 'epoch': 0.02}
{'loss': 1.0255, 'learning_rate': 9.987197766795193e-05, 'epoch': 0.02}
  2%|▏         | 71/3250 [19:58<14:55:38, 16.90s/it]                                                      2%|▏         | 71/3250 [19:58<14:55:38, 16.90s/it]  2%|▏         | 72/3250 [20:14<14:39:16, 16.60s/it]                                                      2%|▏         | 72/3250 [20:14<14:39:16, 16.60s/it]  2%|▏         | 73/3250 [20:30<14:27:35, 16.39s/it]                                                      2%|▏         | 73/3250 [20:30<14:27:35, 16.39s/it]  2%|▏         | 74/3250 [20:46<14:19:16, 16.23s/it]                                                      2%|▏         | 74/3250 [20:46<14:19:16, 16.23s/it]  2%|▏         | 75/3250 [21:02<14:13:25, 16.13s/it]                                                      2%|▏         | 75/3250 [21:02<14:13:25, 16.13s/it]  2%|▏         | 76/3250 [21:18<14:09:37, 16.06s/it]                                                      2%|▏         | 76/3250 [21:18<14:09:37, 16.06s/it]  2%|▏         | 77/3250 [21:34<14:06{'loss': 1.0199, 'learning_rate': 9.986849575387049e-05, 'epoch': 0.02}
{'loss': 1.0322, 'learning_rate': 9.986496718524616e-05, 'epoch': 0.02}
{'loss': 1.0173, 'learning_rate': 9.986139196538011e-05, 'epoch': 0.02}
{'loss': 1.031, 'learning_rate': 9.985777009761713e-05, 'epoch': 0.02}
:41, 16.01s/it]                                                      2%|▏         | 77/3250 [21:34<14:06:41, 16.01s/it]  2%|▏         | 78/3250 [21:50<14:04:19, 15.97s/it]                                                      2%|▏         | 78/3250 [21:50<14:04:19, 15.97s/it]  2%|▏         | 79/3250 [22:06<14:02:53, 15.95s/it]                                                      2%|▏         | 79/3250 [22:06<14:02:53, 15.95s/it]  2%|▏         | 80/3250 [22:21<14:01:40, 15.93s/it]                                                      2%|▏         | 80/3250 [22:21<14:01:40, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9901047348976135, 'eval_runtime': 2.7063, 'eval_samples_per_second': 4.434, 'eval_steps_per_second': 1.109, 'epoch': 0.02}
                                                      2%|▏         | 80/3250 [22:24<14:01:40, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-80
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-80/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-80

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-80

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-80
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-80/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0203, 'learning_rate': 9.985410158534567e-05, 'epoch': 0.02}
{'loss': 1.0097, 'learning_rate': 9.98503864319978e-05, 'epoch': 0.03}
{'loss': 1.0114, 'learning_rate': 9.984662464104926e-05, 'epoch': 0.03}
{'loss': 1.0329, 'learning_rate': 9.984281621601938e-05, 'epoch': 0.03}
{'loss': 1.0282, 'learning_rate': 9.983896116047113e-05, 'epoch': 0.03}
{'loss': 0.9989, 'learning_rate': 9.983505947801115e-05, 'epoch': 0.03}
  2%|▏         | 81/3250 [22:41<14:55:11, 16.95s/it]                                                      2%|▏         | 81/3250 [22:41<14:55:11, 16.95s/it]  3%|▎         | 82/3250 [22:57<14:42:44, 16.72s/it]                                                      3%|▎         | 82/3250 [22:57<14:42:44, 16.72s/it]  3%|▎         | 83/3250 [23:13<14:29:12, 16.47s/it]                                                      3%|▎         | 83/3250 [23:13<14:29:12, 16.47s/it]  3%|▎         | 84/3250 [23:29<14:19:50, 16.30s/it]                                                      3%|▎         | 84/3250 [23:29<14:19:50, 16.30s/it]  3%|▎         | 85/3250 [23:45<14:13:20, 16.18s/it]                                                      3%|▎         | 85/3250 [23:45<14:13:20, 16.18s/it]  3%|▎         | 86/3250 [24:01<14:08:57, 16.10s/it]                                                      3%|▎         | 86/3250 [24:01<14:08:57, 16.10s/it]  3%|▎         | 87/3250 [24:16<14:05{'loss': 1.0268, 'learning_rate': 9.983111117228961e-05, 'epoch': 0.03}
{'loss': 1.0232, 'learning_rate': 9.982711624700041e-05, 'epoch': 0.03}
{'loss': 1.0041, 'learning_rate': 9.982307470588098e-05, 'epoch': 0.03}
{'loss': 1.062, 'learning_rate': 9.981898655271235e-05, 'epoch': 0.03}
:36, 16.04s/it]                                                      3%|▎         | 87/3250 [24:16<14:05:36, 16.04s/it]  3%|▎         | 88/3250 [24:32<14:02:54, 15.99s/it]                                                      3%|▎         | 88/3250 [24:32<14:02:54, 15.99s/it]  3%|▎         | 89/3250 [24:48<14:01:15, 15.97s/it]                                                      3%|▎         | 89/3250 [24:48<14:01:15, 15.97s/it]  3%|▎         | 90/3250 [25:04<14:00:09, 15.95s/it]                                                      3%|▎         | 90/3250 [25:04<14:00:09, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9768532514572144, 'eval_runtime': 2.4826, 'eval_samples_per_second': 4.834, 'eval_steps_per_second': 1.208, 'epoch': 0.03}
                                                      3%|▎         | 90/3250 [25:07<14:00:09, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-90
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-90/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-90


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-90

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-90
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-90/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0272, 'learning_rate': 9.981485179131929e-05, 'epoch': 0.03}
{'loss': 1.0074, 'learning_rate': 9.981067042557e-05, 'epoch': 0.03}
{'loss': 1.02, 'learning_rate': 9.980644245937639e-05, 'epoch': 0.03}
{'loss': 0.9716, 'learning_rate': 9.980216789669395e-05, 'epoch': 0.03}
{'loss': 1.0274, 'learning_rate': 9.979784674152175e-05, 'epoch': 0.03}
{'loss': 0.983, 'learning_rate': 9.979347899790246e-05, 'epoch': 0.03}
  3%|▎         | 91/3250 [25:23<14:49:15, 16.89s/it]                                                      3%|▎         | 91/3250 [25:23<14:49:15, 16.89s/it]  3%|▎         | 92/3250 [25:39<14:33:22, 16.59s/it]                                                      3%|▎         | 92/3250 [25:39<14:33:22, 16.59s/it]  3%|▎         | 93/3250 [25:55<14:22:09, 16.39s/it]                                                      3%|▎         | 93/3250 [25:55<14:22:09, 16.39s/it]  3%|▎         | 94/3250 [26:11<14:14:01, 16.24s/it]                                                      3%|▎         | 94/3250 [26:11<14:14:01, 16.24s/it]  3%|▎         | 95/3250 [26:27<14:08:32, 16.14s/it]                                                      3%|▎         | 95/3250 [26:27<14:08:32, 16.14s/it]  3%|▎         | 96/3250 [26:43<14:04:31, 16.07s/it]                                                      3%|▎         | 96/3250 [26:43<14:04:31, 16.07s/it]  3%|▎         | 97/3250 [26:59<14:01{'loss': 1.0201, 'learning_rate': 9.978906466992229e-05, 'epoch': 0.03}
{'loss': 1.4101, 'learning_rate': 9.978460376171112e-05, 'epoch': 0.03}
{'loss': 0.9884, 'learning_rate': 9.978009627744234e-05, 'epoch': 0.03}
{'loss': 1.0077, 'learning_rate': 9.977554222133292e-05, 'epoch': 0.03}
:32, 16.01s/it]                                                      3%|▎         | 97/3250 [26:59<14:01:32, 16.01s/it]  3%|▎         | 98/3250 [27:14<13:59:10, 15.97s/it]                                                      3%|▎         | 98/3250 [27:15<13:59:10, 15.97s/it]  3%|▎         | 99/3250 [27:31<14:05:19, 16.10s/it]                                                      3%|▎         | 99/3250 [27:31<14:05:19, 16.10s/it]  3%|▎         | 100/3250 [27:47<14:01:41, 16.03s/it]                                                       3%|▎         | 100/3250 [27:47<14:01:41, 16.03s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9665260314941406, 'eval_runtime': 2.4784, 'eval_samples_per_second': 4.842, 'eval_steps_per_second': 1.21, 'epoch': 0.03}
                                                       3%|▎         | 100/3250 [27:49<14:01:41, 16.03s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-100
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-100/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-100


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-100

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0254, 'learning_rate': 9.977094159764342e-05, 'epoch': 0.03}
{'loss': 0.9952, 'learning_rate': 9.976629441067797e-05, 'epoch': 0.03}
{'loss': 0.9748, 'learning_rate': 9.976160066478425e-05, 'epoch': 0.03}
{'loss': 0.992, 'learning_rate': 9.97568603643535e-05, 'epoch': 0.03}
{'loss': 1.0632, 'learning_rate': 9.975207351382051e-05, 'epoch': 0.03}
{'loss': 0.9979, 'learning_rate': 9.974724011766363e-05, 'epoch': 0.03}
  3%|▎         | 101/3250 [28:06<14:48:41, 16.93s/it]                                                       3%|▎         | 101/3250 [28:06<14:48:41, 16.93s/it]  3%|▎         | 102/3250 [28:22<14:32:04, 16.62s/it]                                                       3%|▎         | 102/3250 [28:22<14:32:04, 16.62s/it]  3%|▎         | 103/3250 [28:38<14:20:30, 16.41s/it]                                                       3%|▎         | 103/3250 [28:38<14:20:30, 16.41s/it]  3%|▎         | 104/3250 [28:53<14:12:04, 16.25s/it]                                                       3%|▎         | 104/3250 [28:53<14:12:04, 16.25s/it]  3%|▎         | 105/3250 [29:09<14:06:21, 16.15s/it]                                                       3%|▎         | 105/3250 [29:09<14:06:21, 16.15s/it]  3%|▎         | 106/3250 [29:25<14:02:17, 16.07s/it]                                                       3%|▎         | 106/3250 [29:25<14:02:17, 16.07s/it]  3%|▎         | 10{'loss': 0.9964, 'learning_rate': 9.974236018040474e-05, 'epoch': 0.03}
{'loss': 0.9504, 'learning_rate': 9.973743370660928e-05, 'epoch': 0.03}
{'loss': 1.0029, 'learning_rate': 9.973246070088624e-05, 'epoch': 0.03}
{'loss': 0.9928, 'learning_rate': 9.972744116788809e-05, 'epoch': 0.03}
7/3250 [29:41<13:59:18, 16.02s/it]                                                       3%|▎         | 107/3250 [29:41<13:59:18, 16.02s/it]  3%|▎         | 108/3250 [29:57<13:57:11, 15.99s/it]                                                       3%|▎         | 108/3250 [29:57<13:57:11, 15.99s/it]  3%|▎         | 109/3250 [30:13<13:55:33, 15.96s/it]                                                       3%|▎         | 109/3250 [30:13<13:55:33, 15.96s/it]  3%|▎         | 110/3250 [30:29<13:54:28, 15.95s/it]                                                       3%|▎         | 110/3250 [30:29<13:54:28, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9585030674934387, 'eval_runtime': 2.496, 'eval_samples_per_second': 4.808, 'eval_steps_per_second': 1.202, 'epoch': 0.03}
                                                       3%|▎         | 110/3250 [30:31<13:54:28, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-110
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-110
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9761, 'learning_rate': 9.972237511231087e-05, 'epoch': 0.03}
{'loss': 0.959, 'learning_rate': 9.971726253889416e-05, 'epoch': 0.03}
{'loss': 0.9779, 'learning_rate': 9.9712103452421e-05, 'epoch': 0.03}
{'loss': 0.989, 'learning_rate': 9.970689785771798e-05, 'epoch': 0.04}
{'loss': 0.996, 'learning_rate': 9.970164575965523e-05, 'epoch': 0.04}
{'loss': 0.9682, 'learning_rate': 9.969634716314635e-05, 'epoch': 0.04}
  3%|▎         | 111/3250 [31:02<18:30:29, 21.23s/it]                                                       3%|▎         | 111/3250 [31:02<18:30:29, 21.23s/it]  3%|▎         | 112/3250 [31:18<17:06:35, 19.63s/it]                                                       3%|▎         | 112/3250 [31:18<17:06:35, 19.63s/it]  3%|▎         | 113/3250 [31:34<16:08:07, 18.52s/it]                                                       3%|▎         | 113/3250 [31:34<16:08:07, 18.52s/it]  4%|▎         | 114/3250 [31:50<15:26:43, 17.73s/it]                                                       4%|▎         | 114/3250 [31:50<15:26:43, 17.73s/it]  4%|▎         | 115/3250 [32:06<15:02:30, 17.27s/it]                                                       4%|▎         | 115/3250 [32:06<15:02:30, 17.27s/it]  4%|▎         | 116/3250 [32:23<14:44:54, 16.94s/it]                                                       4%|▎         | 116/3250 [32:23<14:44:54, 16.94s/it]  4%|▎         | 11{'loss': 0.9574, 'learning_rate': 9.969100207314845e-05, 'epoch': 0.04}
{'loss': 1.0033, 'learning_rate': 9.968561049466214e-05, 'epoch': 0.04}
{'loss': 0.9781, 'learning_rate': 9.968017243273149e-05, 'epoch': 0.04}
{'loss': 0.9982, 'learning_rate': 9.967468789244412e-05, 'epoch': 0.04}
7/3250 [32:38<14:28:57, 16.64s/it]                                                       4%|▎         | 117/3250 [32:38<14:28:57, 16.64s/it]  4%|▎         | 118/3250 [32:54<14:17:33, 16.43s/it]                                                       4%|▎         | 118/3250 [32:54<14:17:33, 16.43s/it]  4%|▎         | 119/3250 [33:10<14:09:29, 16.28s/it]                                                       4%|▎         | 119/3250 [33:10<14:09:29, 16.28s/it]  4%|▎         | 120/3250 [33:26<14:03:41, 16.17s/it]                                                       4%|▎         | 120/3250 [33:26<14:03:41, 16.17s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9504378437995911, 'eval_runtime': 2.4858, 'eval_samples_per_second': 4.827, 'eval_steps_per_second': 1.207, 'epoch': 0.04}
                                                       4%|▎         | 120/3250 [33:29<14:03:41, 16.17s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-120
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-120/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-120


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0149, 'learning_rate': 9.966915687893108e-05, 'epoch': 0.04}
{'loss': 0.9592, 'learning_rate': 9.966357939736693e-05, 'epoch': 0.04}
{'loss': 0.9532, 'learning_rate': 9.965795545296967e-05, 'epoch': 0.04}
{'loss': 0.964, 'learning_rate': 9.965228505100084e-05, 'epoch': 0.04}
{'loss': 0.9769, 'learning_rate': 9.964656819676533e-05, 'epoch': 0.04}
{'loss': 0.964, 'learning_rate': 9.964080489561159e-05, 'epoch': 0.04}
  4%|▎         | 121/3250 [33:45<14:49:01, 17.05s/it]                                                       4%|▎         | 121/3250 [33:45<14:49:01, 17.05s/it]  4%|▍         | 122/3250 [34:01<14:31:18, 16.71s/it]                                                       4%|▍         | 122/3250 [34:01<14:31:18, 16.71s/it]  4%|▍         | 123/3250 [34:17<14:18:53, 16.48s/it]                                                       4%|▍         | 123/3250 [34:17<14:18:53, 16.48s/it]  4%|▍         | 124/3250 [34:33<14:10:07, 16.32s/it]                                                       4%|▍         | 124/3250 [34:33<14:10:07, 16.32s/it]  4%|▍         | 125/3250 [34:49<14:03:46, 16.20s/it]                                                       4%|▍         | 125/3250 [34:49<14:03:46, 16.20s/it]  4%|▍         | 126/3250 [35:05<13:59:35, 16.13s/it]                                                       4%|▍         | 126/3250 [35:05<13:59:35, 16.13s/it]  4%|▍         | 12{'loss': 0.9792, 'learning_rate': 9.963499515293147e-05, 'epoch': 0.04}
{'loss': 1.4258, 'learning_rate': 9.962913897416028e-05, 'epoch': 0.04}
{'loss': 0.9011, 'learning_rate': 9.96232363647768e-05, 'epoch': 0.04}
{'loss': 0.9461, 'learning_rate': 9.961728733030318e-05, 'epoch': 0.04}
7/3250 [35:21<13:56:24, 16.07s/it]                                                       4%|▍         | 127/3250 [35:21<13:56:24, 16.07s/it]  4%|▍         | 128/3250 [35:37<13:53:43, 16.02s/it]                                                       4%|▍         | 128/3250 [35:37<13:53:43, 16.02s/it]  4%|▍         | 129/3250 [35:53<13:52:24, 16.00s/it]                                                       4%|▍         | 129/3250 [35:53<13:52:24, 16.00s/it]  4%|▍         | 130/3250 [36:09<13:51:37, 15.99s/it]                                                       4%|▍         | 130/3250 [36:09<13:51:37, 15.99s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9422574043273926, 'eval_runtime': 2.4944, 'eval_samples_per_second': 4.811, 'eval_steps_per_second': 1.203, 'epoch': 0.04}
                                                       4%|▍         | 130/3250 [36:11<13:51:37, 15.99s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-130
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-130
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-130
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-130/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9972, 'learning_rate': 9.961129187630509e-05, 'epoch': 0.04}
{'loss': 0.9654, 'learning_rate': 9.960525000839159e-05, 'epoch': 0.04}
{'loss': 0.9503, 'learning_rate': 9.959916173221511e-05, 'epoch': 0.04}
{'loss': 0.9531, 'learning_rate': 9.959302705347158e-05, 'epoch': 0.04}
{'loss': 1.0181, 'learning_rate': 9.958684597790031e-05, 'epoch': 0.04}
{'loss': 0.9853, 'learning_rate': 9.958061851128402e-05, 'epoch': 0.04}
  4%|▍         | 131/3250 [36:28<14:39:55, 16.93s/it]                                                       4%|▍         | 131/3250 [36:28<14:39:55, 16.93s/it]  4%|▍         | 132/3250 [36:44<14:30:59, 16.76s/it]                                                       4%|▍         | 132/3250 [36:44<14:30:59, 16.76s/it]  4%|▍         | 133/3250 [37:00<14:17:47, 16.51s/it]                                                       4%|▍         | 133/3250 [37:00<14:17:47, 16.51s/it]  4%|▍         | 134/3250 [37:16<14:08:31, 16.34s/it]                                                       4%|▍         | 134/3250 [37:16<14:08:31, 16.34s/it]  4%|▍         | 135/3250 [37:32<14:02:06, 16.22s/it]                                                       4%|▍         | 135/3250 [37:32<14:02:06, 16.22s/it]  4%|▍         | 136/3250 [37:48<13:57:31, 16.14s/it]                                                       4%|▍         | 136/3250 [37:48<13:57:31, 16.14s/it]  4%|▍         | 13{'loss': 0.9451, 'learning_rate': 9.957434465944879e-05, 'epoch': 0.04}
{'loss': 0.9171, 'learning_rate': 9.956802442826416e-05, 'epoch': 0.04}
{'loss': 0.9807, 'learning_rate': 9.956165782364303e-05, 'epoch': 0.04}
{'loss': 0.9465, 'learning_rate': 9.955524485154168e-05, 'epoch': 0.04}
7/3250 [38:04<13:54:10, 16.08s/it]                                                       4%|▍         | 137/3250 [38:04<13:54:10, 16.08s/it]  4%|▍         | 138/3250 [38:20<13:51:56, 16.04s/it]                                                       4%|▍         | 138/3250 [38:20<13:51:56, 16.04s/it]  4%|▍         | 139/3250 [38:36<13:50:05, 16.01s/it]                                                       4%|▍         | 139/3250 [38:36<13:50:05, 16.01s/it]  4%|▍         | 140/3250 [38:52<13:48:18, 15.98s/it]                                                       4%|▍         | 140/3250 [38:52<13:48:18, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.936115026473999, 'eval_runtime': 2.4904, 'eval_samples_per_second': 4.818, 'eval_steps_per_second': 1.205, 'epoch': 0.04}
                                                       4%|▍         | 140/3250 [38:54<13:48:18, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-140
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-140/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-140


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.954, 'learning_rate': 9.954878551795976e-05, 'epoch': 0.04}
{'loss': 0.9324, 'learning_rate': 9.954227982894034e-05, 'epoch': 0.04}
{'loss': 0.9436, 'learning_rate': 9.953572779056981e-05, 'epoch': 0.04}
{'loss': 0.9585, 'learning_rate': 9.952912940897793e-05, 'epoch': 0.04}
{'loss': 0.9554, 'learning_rate': 9.952248469033785e-05, 'epoch': 0.04}
{'loss': 0.9524, 'learning_rate': 9.951579364086602e-05, 'epoch': 0.04}
  4%|▍         | 141/3250 [39:11<14:36:03, 16.91s/it]                                                       4%|▍         | 141/3250 [39:11<14:36:03, 16.91s/it]  4%|▍         | 142/3250 [39:27<14:20:58, 16.62s/it]                                                       4%|▍         | 142/3250 [39:27<14:20:58, 16.62s/it]  4%|▍         | 143/3250 [39:43<14:09:47, 16.41s/it]                                                       4%|▍         | 143/3250 [39:43<14:09:47, 16.41s/it]  4%|▍         | 144/3250 [39:59<14:01:41, 16.26s/it]                                                       4%|▍         | 144/3250 [39:59<14:01:41, 16.26s/it]  4%|▍         | 145/3250 [40:15<13:56:15, 16.16s/it]                                                       4%|▍         | 145/3250 [40:15<13:56:15, 16.16s/it]  4%|▍         | 146/3250 [40:30<13:51:44, 16.08s/it]                                                       4%|▍         | 146/3250 [40:30<13:51:44, 16.08s/it]  5%|▍         | 14{'loss': 0.9562, 'learning_rate': 9.950905626682228e-05, 'epoch': 0.05}
{'loss': 0.939, 'learning_rate': 9.95022725745098e-05, 'epoch': 0.05}
{'loss': 0.982, 'learning_rate': 9.949544257027502e-05, 'epoch': 0.05}
{'loss': 0.9317, 'learning_rate': 9.948856626050781e-05, 'epoch': 0.05}
7/3250 [40:46<13:48:38, 16.02s/it]                                                       5%|▍         | 147/3250 [40:46<13:48:38, 16.02s/it]  5%|▍         | 148/3250 [41:03<13:52:08, 16.10s/it]                                                       5%|▍         | 148/3250 [41:03<13:52:08, 16.10s/it]  5%|▍         | 149/3250 [41:19<13:49:09, 16.04s/it]                                                       5%|▍         | 149/3250 [41:19<13:49:09, 16.04s/it]  5%|▍         | 150/3250 [41:34<13:46:34, 16.00s/it]                                                       5%|▍         | 150/3250 [41:34<13:46:34, 16.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9296449422836304, 'eval_runtime': 2.4777, 'eval_samples_per_second': 4.843, 'eval_steps_per_second': 1.211, 'epoch': 0.05}
                                                       5%|▍         | 150/3250 [41:37<13:46:34, 16.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-150
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-150/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-150


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0068, 'learning_rate': 9.94816436516413e-05, 'epoch': 0.05}
{'loss': 0.9175, 'learning_rate': 9.947467475015196e-05, 'epoch': 0.05}
{'loss': 0.9585, 'learning_rate': 9.94676595625595e-05, 'epoch': 0.05}
{'loss': 0.9405, 'learning_rate': 9.946059809542707e-05, 'epoch': 0.05}
{'loss': 0.933, 'learning_rate': 9.945349035536097e-05, 'epoch': 0.05}
{'loss': 0.953, 'learning_rate': 9.944633634901088e-05, 'epoch': 0.05}
  5%|▍         | 151/3250 [41:54<14:34:03, 16.92s/it]                                                       5%|▍         | 151/3250 [41:54<14:34:03, 16.92s/it]  5%|▍         | 152/3250 [42:09<14:18:35, 16.63s/it]                                                       5%|▍         | 152/3250 [42:09<14:18:35, 16.63s/it]  5%|▍         | 153/3250 [42:25<14:07:13, 16.41s/it]                                                       5%|▍         | 153/3250 [42:25<14:07:13, 16.41s/it]  5%|▍         | 154/3250 [42:41<13:59:02, 16.26s/it]                                                       5%|▍         | 154/3250 [42:41<13:59:02, 16.26s/it]  5%|▍         | 155/3250 [42:57<13:53:38, 16.16s/it]                                                       5%|▍         | 155/3250 [42:57<13:53:38, 16.16s/it]  5%|▍         | 156/3250 [43:13<13:49:59, 16.10s/it]                                                       5%|▍         | 156/3250 [43:13<13:49:59, 16.10s/it]  5%|▍         | 15{'loss': 0.9357, 'learning_rate': 9.943913608306975e-05, 'epoch': 0.05}
{'loss': 0.9302, 'learning_rate': 9.94318895642738e-05, 'epoch': 0.05}
{'loss': 1.3554, 'learning_rate': 9.94245967994025e-05, 'epoch': 0.05}
{'loss': 0.9313, 'learning_rate': 9.941725779527861e-05, 'epoch': 0.05}
7/3250 [43:29<13:47:11, 16.05s/it]                                                       5%|▍         | 157/3250 [43:29<13:47:11, 16.05s/it]  5%|▍         | 158/3250 [43:45<13:45:08, 16.01s/it]                                                       5%|▍         | 158/3250 [43:45<13:45:08, 16.01s/it]  5%|▍         | 159/3250 [44:01<13:43:32, 15.99s/it]                                                       5%|▍         | 159/3250 [44:01<13:43:32, 15.99s/it]  5%|▍         | 160/3250 [44:17<13:42:47, 15.98s/it]                                                       5%|▍         | 160/3250 [44:17<13:42:47, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.92388916015625, 'eval_runtime': 2.4859, 'eval_samples_per_second': 4.827, 'eval_steps_per_second': 1.207, 'epoch': 0.05}
                                                       5%|▍         | 160/3250 [44:19<13:42:47, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-160
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-160/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9763, 'learning_rate': 9.940987255876817e-05, 'epoch': 0.05}
{'loss': 0.951, 'learning_rate': 9.940244109678043e-05, 'epoch': 0.05}
{'loss': 0.9441, 'learning_rate': 9.939496341626791e-05, 'epoch': 0.05}
{'loss': 0.9091, 'learning_rate': 9.938743952422636e-05, 'epoch': 0.05}
{'loss': 0.9905, 'learning_rate': 9.937986942769477e-05, 'epoch': 0.05}
{'loss': 0.9718, 'learning_rate': 9.937225313375535e-05, 'epoch': 0.05}
  5%|▍         | 161/3250 [44:36<14:32:04, 16.94s/it]                                                       5%|▍         | 161/3250 [44:36<14:32:04, 16.94s/it]  5%|▍         | 162/3250 [44:52<14:19:51, 16.71s/it]                                                       5%|▍         | 162/3250 [44:52<14:19:51, 16.71s/it]  5%|▌         | 163/3250 [45:08<14:07:27, 16.47s/it]                                                       5%|▌         | 163/3250 [45:08<14:07:27, 16.47s/it]  5%|▌         | 164/3250 [45:24<14:01:17, 16.36s/it]                                                       5%|▌         | 164/3250 [45:24<14:01:17, 16.36s/it]  5%|▌         | 165/3250 [45:40<13:54:42, 16.23s/it]                                                       5%|▌         | 165/3250 [45:40<13:54:42, 16.23s/it]  5%|▌         | 166/3250 [45:56<13:50:00, 16.15s/it]                                                       5%|▌         | 166/3250 [45:56<13:50:00, 16.15s/it]  5%|▌         | 16{'loss': 0.9172, 'learning_rate': 9.936459064953355e-05, 'epoch': 0.05}
{'loss': 0.9224, 'learning_rate': 9.935688198219801e-05, 'epoch': 0.05}
{'loss': 0.9342, 'learning_rate': 9.934912713896057e-05, 'epoch': 0.05}
{'loss': 0.9176, 'learning_rate': 9.934132612707632e-05, 'epoch': 0.05}
7/3250 [46:12<13:46:57, 16.09s/it]                                                       5%|▌         | 167/3250 [46:12<13:46:57, 16.09s/it]  5%|▌         | 168/3250 [46:28<13:44:29, 16.05s/it]                                                       5%|▌         | 168/3250 [46:28<13:44:29, 16.05s/it]  5%|▌         | 169/3250 [46:44<13:42:55, 16.03s/it]                                                       5%|▌         | 169/3250 [46:44<13:42:55, 16.03s/it]  5%|▌         | 170/3250 [47:00<13:41:53, 16.01s/it]                                                       5%|▌         | 170/3250 [47:00<13:41:53, 16.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9178791642189026, 'eval_runtime': 2.4903, 'eval_samples_per_second': 4.819, 'eval_steps_per_second': 1.205, 'epoch': 0.05}
                                                       5%|▌         | 170/3250 [47:03<13:41:53, 16.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-170
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-170/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9269, 'learning_rate': 9.933347895384346e-05, 'epoch': 0.05}
{'loss': 0.9123, 'learning_rate': 9.932558562660347e-05, 'epoch': 0.05}
{'loss': 0.9177, 'learning_rate': 9.931764615274093e-05, 'epoch': 0.05}
{'loss': 0.9189, 'learning_rate': 9.930966053968364e-05, 'epoch': 0.05}
{'loss': 0.9545, 'learning_rate': 9.930162879490257e-05, 'epoch': 0.05}
{'loss': 0.9213, 'learning_rate': 9.92935509259118e-05, 'epoch': 0.05}
  5%|▌         | 171/3250 [47:19<14:30:21, 16.96s/it]                                                       5%|▌         | 171/3250 [47:19<14:30:21, 16.96s/it]  5%|▌         | 172/3250 [47:35<14:14:38, 16.66s/it]                                                       5%|▌         | 172/3250 [47:35<14:14:38, 16.66s/it]  5%|▌         | 173/3250 [47:51<14:03:44, 16.45s/it]                                                       5%|▌         | 173/3250 [47:51<14:03:44, 16.45s/it]  5%|▌         | 174/3250 [48:07<13:55:55, 16.31s/it]                                                       5%|▌         | 174/3250 [48:07<13:55:55, 16.31s/it]  5%|▌         | 175/3250 [48:23<13:50:19, 16.20s/it]                                                       5%|▌         | 175/3250 [48:23<13:50:19, 16.20s/it]  5%|▌         | 176/3250 [48:39<13:46:06, 16.12s/it]                                                       5%|▌         | 176/3250 [48:39<13:46:06, 16.12s/it]  5%|▌         | 17{'loss': 0.905, 'learning_rate': 9.928542694026862e-05, 'epoch': 0.05}
{'loss': 0.9489, 'learning_rate': 9.927725684557338e-05, 'epoch': 0.05}
{'loss': 0.937, 'learning_rate': 9.926904064946969e-05, 'epoch': 0.06}
{'loss': 0.9091, 'learning_rate': 9.92607783596442e-05, 'epoch': 0.06}
7/3250 [48:55<13:42:54, 16.07s/it]                                                       5%|▌         | 177/3250 [48:55<13:42:54, 16.07s/it]  5%|▌         | 178/3250 [49:11<13:40:51, 16.03s/it]                                                       5%|▌         | 178/3250 [49:11<13:40:51, 16.03s/it]  6%|▌         | 179/3250 [49:27<13:39:17, 16.01s/it]                                                       6%|▌         | 179/3250 [49:27<13:39:17, 16.01s/it]  6%|▌         | 180/3250 [49:43<13:37:58, 15.99s/it]                                                       6%|▌         | 180/3250 [49:43<13:37:58, 15.99s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9136168360710144, 'eval_runtime': 2.483, 'eval_samples_per_second': 4.833, 'eval_steps_per_second': 1.208, 'epoch': 0.06}
                                                       6%|▌         | 180/3250 [49:45<13:37:58, 15.99s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-180
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-180
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-180/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-180


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9743, 'learning_rate': 9.925246998382671e-05, 'epoch': 0.06}
{'loss': 0.9303, 'learning_rate': 9.924411552979012e-05, 'epoch': 0.06}
{'loss': 0.908, 'learning_rate': 9.923571500535047e-05, 'epoch': 0.06}
{'loss': 0.9281, 'learning_rate': 9.922726841836684e-05, 'epoch': 0.06}
{'loss': 0.8851, 'learning_rate': 9.921877577674152e-05, 'epoch': 0.06}
{'loss': 0.9437, 'learning_rate': 9.921023708841975e-05, 'epoch': 0.06}
  6%|▌         | 181/3250 [50:21<19:15:43, 22.59s/it]                                                       6%|▌         | 181/3250 [50:21<19:15:43, 22.59s/it]  6%|▌         | 182/3250 [50:37<17:33:11, 20.60s/it]                                                       6%|▌         | 182/3250 [50:37<17:33:11, 20.60s/it]  6%|▌         | 183/3250 [50:53<16:21:23, 19.20s/it]                                                       6%|▌         | 183/3250 [50:53<16:21:23, 19.20s/it]  6%|▌         | 184/3250 [51:09<15:30:54, 18.22s/it]                                                       6%|▌         | 184/3250 [51:09<15:30:54, 18.22s/it]  6%|▌         | 185/3250 [51:24<14:55:25, 17.53s/it]                                                       6%|▌         | 185/3250 [51:24<14:55:25, 17.53s/it]  6%|▌         | 186/3250 [51:40<14:30:24, 17.04s/it]                                                       6%|▌         | 186/3250 [51:40<14:30:24, 17.04s/it]  6%|▌         | 18{'loss': 0.8957, 'learning_rate': 9.920165236138994e-05, 'epoch': 0.06}
{'loss': 0.9301, 'learning_rate': 9.919302160368353e-05, 'epoch': 0.06}
{'loss': 1.3446, 'learning_rate': 9.918434482337506e-05, 'epoch': 0.06}
{'loss': 0.9097, 'learning_rate': 9.917562202858208e-05, 'epoch': 0.06}
7/3250 [51:56<14:13:01, 16.71s/it]                                                       6%|▌         | 187/3250 [51:56<14:13:01, 16.71s/it]  6%|▌         | 188/3250 [52:12<14:01:04, 16.48s/it]                                                       6%|▌         | 188/3250 [52:12<14:01:04, 16.48s/it]  6%|▌         | 189/3250 [52:28<13:51:39, 16.30s/it]                                                       6%|▌         | 189/3250 [52:28<13:51:39, 16.30s/it]  6%|▌         | 190/3250 [52:44<13:45:33, 16.19s/it]                                                       6%|▌         | 190/3250 [52:44<13:45:33, 16.19s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9079829454421997, 'eval_runtime': 2.4966, 'eval_samples_per_second': 4.807, 'eval_steps_per_second': 1.202, 'epoch': 0.06}
                                                       6%|▌         | 190/3250 [52:47<13:45:33, 16.19s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-190
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-190/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9149, 'learning_rate': 9.916685322746524e-05, 'epoch': 0.06}
{'loss': 0.9474, 'learning_rate': 9.915803842822817e-05, 'epoch': 0.06}
{'loss': 0.9166, 'learning_rate': 9.914917763911759e-05, 'epoch': 0.06}
{'loss': 0.8962, 'learning_rate': 9.914027086842322e-05, 'epoch': 0.06}
{'loss': 0.9111, 'learning_rate': 9.913131812447781e-05, 'epoch': 0.06}
{'loss': 0.9777, 'learning_rate': 9.912231941565711e-05, 'epoch': 0.06}
  6%|▌         | 191/3250 [53:03<14:31:04, 17.09s/it]                                                       6%|▌         | 191/3250 [53:03<14:31:04, 17.09s/it]  6%|▌         | 192/3250 [53:19<14:12:51, 16.73s/it]                                                       6%|▌         | 192/3250 [53:19<14:12:51, 16.73s/it]  6%|▌         | 193/3250 [53:35<14:00:21, 16.49s/it]                                                       6%|▌         | 193/3250 [53:35<14:00:21, 16.49s/it]  6%|▌         | 194/3250 [53:51<13:51:26, 16.32s/it]                                                       6%|▌         | 194/3250 [53:51<13:51:26, 16.32s/it]  6%|▌         | 195/3250 [54:07<13:44:44, 16.20s/it]                                                       6%|▌         | 195/3250 [54:07<13:44:44, 16.20s/it]  6%|▌         | 196/3250 [54:23<13:40:02, 16.11s/it]                                                       6%|▌         | 196/3250 [54:23<13:40:02, 16.11s/it]  6%|▌         | 19{'loss': 0.9214, 'learning_rate': 9.911327475037985e-05, 'epoch': 0.06}
{'loss': 0.9185, 'learning_rate': 9.91041841371078e-05, 'epoch': 0.06}
{'loss': 0.873, 'learning_rate': 9.909504758434571e-05, 'epoch': 0.06}
{'loss': 0.9158, 'learning_rate': 9.908586510064127e-05, 'epoch': 0.06}
7/3250 [54:39<13:41:45, 16.15s/it]                                                       6%|▌         | 197/3250 [54:39<13:41:45, 16.15s/it]  6%|▌         | 198/3250 [54:55<13:38:05, 16.08s/it]                                                       6%|▌         | 198/3250 [54:55<13:38:05, 16.08s/it]  6%|▌         | 199/3250 [55:11<13:35:06, 16.03s/it]                                                       6%|▌         | 199/3250 [55:11<13:35:06, 16.03s/it]  6%|▌         | 200/3250 [55:27<13:32:57, 15.99s/it]                                                       6%|▌         | 200/3250 [55:27<13:32:57, 15.99s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9031010270118713, 'eval_runtime': 2.4826, 'eval_samples_per_second': 4.834, 'eval_steps_per_second': 1.208, 'epoch': 0.06}
                                                       6%|▌         | 200/3250 [55:29<13:32:57, 15.99s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-200
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-200I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-200
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9132, 'learning_rate': 9.907663669458518e-05, 'epoch': 0.06}
{'loss': 0.9107, 'learning_rate': 9.906736237481109e-05, 'epoch': 0.06}
{'loss': 0.9004, 'learning_rate': 9.905804214999558e-05, 'epoch': 0.06}
{'loss': 0.9102, 'learning_rate': 9.904867602885824e-05, 'epoch': 0.06}
{'loss': 0.9143, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.06}
{'loss': 0.9169, 'learning_rate': 9.902980613271087e-05, 'epoch': 0.06}
  6%|▌         | 201/3250 [56:15<21:41:26, 25.61s/it]                                                       6%|▌         | 201/3250 [56:15<21:41:26, 25.61s/it]  6%|▌         | 202/3250 [56:31<19:13:16, 22.70s/it]                                                       6%|▌         | 202/3250 [56:31<19:13:16, 22.70s/it]  6%|▌         | 203/3250 [56:47<17:29:24, 20.66s/it]                                                       6%|▌         | 203/3250 [56:47<17:29:24, 20.66s/it]  6%|▋         | 204/3250 [57:03<16:16:37, 19.24s/it]                                                       6%|▋         | 204/3250 [57:03<16:16:37, 19.24s/it]  6%|▋         | 205/3250 [57:19<15:25:26, 18.24s/it]                                                       6%|▋         | 205/3250 [57:19<15:25:26, 18.24s/it]  6%|▋         | 206/3250 [57:34<14:49:18, 17.53s/it]                                                       6%|▋         | 206/3250 [57:34<14:49:18, 17.53s/it]  6%|▋         | 20{'loss': 0.9054, 'learning_rate': 9.90203023753546e-05, 'epoch': 0.06}
{'loss': 0.9076, 'learning_rate': 9.9010752756984e-05, 'epoch': 0.06}
{'loss': 0.934, 'learning_rate': 9.900115728653319e-05, 'epoch': 0.06}
{'loss': 0.903, 'learning_rate': 9.899151597297923e-05, 'epoch': 0.06}
7/3250 [57:50<14:24:16, 17.04s/it]                                                       6%|▋         | 207/3250 [57:50<14:24:16, 17.04s/it]  6%|▋         | 208/3250 [58:06<14:06:36, 16.70s/it]                                                       6%|▋         | 208/3250 [58:06<14:06:36, 16.70s/it]  6%|▋         | 209/3250 [58:22<13:54:18, 16.46s/it]                                                       6%|▋         | 209/3250 [58:22<13:54:18, 16.46s/it]  6%|▋         | 210/3250 [58:38<13:45:19, 16.29s/it]                                                       6%|▋         | 210/3250 [58:38<13:45:19, 16.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8987917900085449, 'eval_runtime': 2.4733, 'eval_samples_per_second': 4.852, 'eval_steps_per_second': 1.213, 'epoch': 0.06}
                                                       6%|▋         | 210/3250 [58:40<13:45:19, 16.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-210

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-210
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-210/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-210/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9304, 'learning_rate': 9.89818288253421e-05, 'epoch': 0.06}
{'loss': 0.929, 'learning_rate': 9.897209585268458e-05, 'epoch': 0.07}
{'loss': 0.8963, 'learning_rate': 9.896231706411242e-05, 'epoch': 0.07}
{'loss': 0.8811, 'learning_rate': 9.895249246877415e-05, 'epoch': 0.07}
{'loss': 0.8933, 'learning_rate': 9.894262207586116e-05, 'epoch': 0.07}
{'loss': 0.9055, 'learning_rate': 9.893270589460775e-05, 'epoch': 0.07}
  6%|▋         | 211/3250 [58:57<14:27:42, 17.13s/it]                                                       6%|▋         | 211/3250 [58:57<14:27:42, 17.13s/it]  7%|▋         | 212/3250 [59:13<14:08:25, 16.76s/it]                                                       7%|▋         | 212/3250 [59:13<14:08:25, 16.76s/it]  7%|▋         | 213/3250 [59:29<13:54:59, 16.50s/it]                                                       7%|▋         | 213/3250 [59:29<13:54:59, 16.50s/it]  7%|▋         | 214/3250 [59:46<14:01:28, 16.63s/it]                                                       7%|▋         | 214/3250 [59:46<14:01:28, 16.63s/it]  7%|▋         | 215/3250 [1:00:02<13:49:48, 16.40s/it]                                                         7%|▋         | 215/3250 [1:00:02<13:49:48, 16.40s/it]  7%|▋         | 216/3250 [1:00:18<13:41:12, 16.24s/it]                                                         7%|▋         | 216/3250 [1:00:18<13:41:12, 16.24s/it]  7%|▋ {'loss': 0.8862, 'learning_rate': 9.8922743934291e-05, 'epoch': 0.07}
{'loss': 0.9124, 'learning_rate': 9.891273620423083e-05, 'epoch': 0.07}
{'loss': 1.3619, 'learning_rate': 9.890268271379e-05, 'epoch': 0.07}
{'loss': 0.8262, 'learning_rate': 9.889258347237404e-05, 'epoch': 0.07}
        | 217/3250 [1:00:33<13:35:28, 16.13s/it]                                                         7%|▋         | 217/3250 [1:00:33<13:35:28, 16.13s/it]  7%|▋         | 218/3250 [1:00:49<13:31:46, 16.06s/it]                                                         7%|▋         | 218/3250 [1:00:49<13:31:46, 16.06s/it]  7%|▋         | 219/3250 [1:01:05<13:28:15, 16.00s/it]                                                         7%|▋         | 219/3250 [1:01:05<13:28:15, 16.00s/it]  7%|▋         | 220/3250 [1:01:21<13:26:02, 15.96s/it]                                                         7%|▋         | 220/3250 [1:01:21<13:26:02, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8963306546211243, 'eval_runtime': 2.4717, 'eval_samples_per_second': 4.855, 'eval_steps_per_second': 1.214, 'epoch': 0.07}
                                                         7%|▋         | 220/3250 [1:01:24<13:26:02, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-220
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-220
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-220
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8714, 'learning_rate': 9.888243848943136e-05, 'epoch': 0.07}
{'loss': 0.9227, 'learning_rate': 9.887224777445308e-05, 'epoch': 0.07}
{'loss': 0.9045, 'learning_rate': 9.886201133697314e-05, 'epoch': 0.07}
{'loss': 0.8857, 'learning_rate': 9.885172918656826e-05, 'epoch': 0.07}
{'loss': 0.8895, 'learning_rate': 9.884140133285791e-05, 'epoch': 0.07}
{'loss': 0.936, 'learning_rate': 9.883102778550434e-05, 'epoch': 0.07}
  7%|▋         | 221/3250 [1:02:13<22:23:43, 26.62s/it]                                                         7%|▋         | 221/3250 [1:02:13<22:23:43, 26.62s/it]  7%|▋         | 222/3250 [1:02:28<19:40:45, 23.40s/it]                                                         7%|▋         | 222/3250 [1:02:28<19:40:45, 23.40s/it]  7%|▋         | 223/3250 [1:02:44<17:46:49, 21.15s/it]                                                         7%|▋         | 223/3250 [1:02:44<17:46:49, 21.15s/it]  7%|▋         | 224/3250 [1:03:00<16:26:54, 19.57s/it]                                                         7%|▋         | 224/3250 [1:03:00<16:26:54, 19.57s/it]  7%|▋         | 225/3250 [1:03:16<15:30:27, 18.46s/it]                                                         7%|▋         | 225/3250 [1:03:16<15:30:27, 18.46s/it]  7%|▋         | 226/3250 [1:03:32<14:51:03, 17.68s/it]                                                         7%|▋         | 226/3250 [1:03:32<14:51:{'loss': 0.9093, 'learning_rate': 9.882060855421253e-05, 'epoch': 0.07}
{'loss': 0.8751, 'learning_rate': 9.881014364873021e-05, 'epoch': 0.07}
{'loss': 0.8465, 'learning_rate': 9.879963307884784e-05, 'epoch': 0.07}
{'loss': 0.9033, 'learning_rate': 9.87890768543986e-05, 'epoch': 0.07}
03, 17.68s/it]  7%|▋         | 227/3250 [1:03:48<14:23:37, 17.14s/it]                                                         7%|▋         | 227/3250 [1:03:48<14:23:37, 17.14s/it]  7%|▋         | 228/3250 [1:04:04<14:04:18, 16.76s/it]                                                         7%|▋         | 228/3250 [1:04:04<14:04:18, 16.76s/it]  7%|▋         | 229/3250 [1:04:20<13:50:43, 16.50s/it]                                                         7%|▋         | 229/3250 [1:04:20<13:50:43, 16.50s/it]  7%|▋         | 230/3250 [1:04:36<13:49:22, 16.48s/it]                                                         7%|▋         | 230/3250 [1:04:36<13:49:22, 16.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8918418288230896, 'eval_runtime': 2.4758, 'eval_samples_per_second': 4.847, 'eval_steps_per_second': 1.212, 'epoch': 0.07}
                                                         7%|▋         | 230/3250 [1:04:38<13:49:22, 16.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-230
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-230/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-230


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8802, 'learning_rate': 9.877847498525837e-05, 'epoch': 0.07}
{'loss': 0.8849, 'learning_rate': 9.876782748134576e-05, 'epoch': 0.07}
{'loss': 0.8757, 'learning_rate': 9.875713435262203e-05, 'epoch': 0.07}
{'loss': 0.8795, 'learning_rate': 9.874639560909117e-05, 'epoch': 0.07}
{'loss': 0.8941, 'learning_rate': 9.873561126079985e-05, 'epoch': 0.07}
{'loss': 0.888, 'learning_rate': 9.872478131783736e-05, 'epoch': 0.07}
  7%|▋         | 231/3250 [1:04:55<14:29:56, 17.29s/it]                                                         7%|▋         | 231/3250 [1:04:55<14:29:56, 17.29s/it]  7%|▋         | 232/3250 [1:05:12<14:21:02, 17.12s/it]                                                         7%|▋         | 232/3250 [1:05:12<14:21:02, 17.12s/it]  7%|▋         | 233/3250 [1:05:28<14:02:05, 16.75s/it]                                                         7%|▋         | 233/3250 [1:05:28<14:02:05, 16.75s/it]  7%|▋         | 234/3250 [1:05:44<13:48:34, 16.48s/it]                                                         7%|▋         | 234/3250 [1:05:44<13:48:34, 16.48s/it]  7%|▋         | 235/3250 [1:05:59<13:38:46, 16.29s/it]                                                         7%|▋         | 235/3250 [1:05:59<13:38:46, 16.29s/it]  7%|▋         | 236/3250 [1:06:16<13:37:59, 16.28s/it]                                                         7%|▋         | 236/3250 [1:06:16<13:37:{'loss': 0.8906, 'learning_rate': 9.871390579033564e-05, 'epoch': 0.07}
{'loss': 0.8933, 'learning_rate': 9.870298468846936e-05, 'epoch': 0.07}
{'loss': 0.8761, 'learning_rate': 9.869201802245573e-05, 'epoch': 0.07}
{'loss': 0.9167, 'learning_rate': 9.868100580255466e-05, 'epoch': 0.07}
59, 16.28s/it]  7%|▋         | 237/3250 [1:06:32<13:31:26, 16.16s/it]                                                         7%|▋         | 237/3250 [1:06:32<13:31:26, 16.16s/it]  7%|▋         | 238/3250 [1:06:48<13:27:04, 16.08s/it]                                                         7%|▋         | 238/3250 [1:06:48<13:27:04, 16.08s/it]  7%|▋         | 239/3250 [1:07:03<13:23:28, 16.01s/it]                                                         7%|▋         | 239/3250 [1:07:03<13:23:28, 16.01s/it]  7%|▋         | 240/3250 [1:07:19<13:21:11, 15.97s/it]                                                         7%|▋         | 240/3250 [1:07:19<13:21:11, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8874260783195496, 'eval_runtime': 2.7479, 'eval_samples_per_second': 4.367, 'eval_steps_per_second': 1.092, 'epoch': 0.07}
                                                         7%|▋         | 240/3250 [1:07:22<13:21:11, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-240
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-240/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-240


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8617, 'learning_rate': 9.866994803906862e-05, 'epoch': 0.07}
{'loss': 0.9432, 'learning_rate': 9.865884474234274e-05, 'epoch': 0.07}
{'loss': 0.8656, 'learning_rate': 9.864769592276472e-05, 'epoch': 0.07}
{'loss': 0.8871, 'learning_rate': 9.863650159076485e-05, 'epoch': 0.08}
{'loss': 0.8686, 'learning_rate': 9.8625261756816e-05, 'epoch': 0.08}
{'loss': 0.8647, 'learning_rate': 9.861397643143362e-05, 'epoch': 0.08}
  7%|▋         | 241/3250 [1:07:39<14:11:40, 16.98s/it]                                                         7%|▋         | 241/3250 [1:07:39<14:11:40, 16.98s/it]  7%|▋         | 242/3250 [1:07:54<13:54:40, 16.65s/it]                                                         7%|▋         | 242/3250 [1:07:54<13:54:40, 16.65s/it]  7%|▋         | 243/3250 [1:08:10<13:42:53, 16.42s/it]                                                         7%|▋         | 243/3250 [1:08:10<13:42:53, 16.42s/it]  8%|▊         | 244/3250 [1:08:26<13:34:41, 16.26s/it]                                                         8%|▊         | 244/3250 [1:08:26<13:34:41, 16.26s/it]  8%|▊         | 245/3250 [1:08:42<13:28:45, 16.15s/it]                                                         8%|▊         | 245/3250 [1:08:42<13:28:45, 16.15s/it]  8%|▊         | 246/3250 [1:08:58<13:28:37, 16.15s/it]                                                         8%|▊         | 246/3250 [1:08:58<13:28:{'loss': 0.8726, 'learning_rate': 9.86026456251757e-05, 'epoch': 0.08}
{'loss': 0.8697, 'learning_rate': 9.859126934864281e-05, 'epoch': 0.08}
{'loss': 0.8687, 'learning_rate': 9.857984761247803e-05, 'epoch': 0.08}
{'loss': 1.3042, 'learning_rate': 9.856838042736699e-05, 'epoch': 0.08}
37, 16.15s/it]  8%|▊         | 247/3250 [1:09:14<13:24:41, 16.08s/it]                                                         8%|▊         | 247/3250 [1:09:14<13:24:41, 16.08s/it]  8%|▊         | 248/3250 [1:09:30<13:21:48, 16.03s/it]                                                         8%|▊         | 248/3250 [1:09:30<13:21:48, 16.03s/it]  8%|▊         | 249/3250 [1:09:46<13:19:41, 15.99s/it]                                                         8%|▊         | 249/3250 [1:09:46<13:19:41, 15.99s/it]  8%|▊         | 250/3250 [1:10:02<13:17:39, 15.95s/it]                                                         8%|▊         | 250/3250 [1:10:02<13:17:39, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8846814036369324, 'eval_runtime': 2.4864, 'eval_samples_per_second': 4.826, 'eval_steps_per_second': 1.207, 'epoch': 0.08}
                                                         8%|▊         | 250/3250 [1:10:04<13:17:39, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-250
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-250
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-250
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8623, 'learning_rate': 9.855686780403783e-05, 'epoch': 0.08}
{'loss': 0.8981, 'learning_rate': 9.854530975326117e-05, 'epoch': 0.08}
{'loss': 0.8816, 'learning_rate': 9.853370628585021e-05, 'epoch': 0.08}
{'loss': 0.8756, 'learning_rate': 9.852205741266058e-05, 'epoch': 0.08}
{'loss': 0.8463, 'learning_rate': 9.851036314459037e-05, 'epoch': 0.08}
{'loss': 0.928, 'learning_rate': 9.84986234925802e-05, 'epoch': 0.08}
  8%|▊         | 251/3250 [1:10:21<14:04:32, 16.90s/it]                                                         8%|▊         | 251/3250 [1:10:21<14:04:32, 16.90s/it]  8%|▊         | 252/3250 [1:10:37<13:49:15, 16.60s/it]                                                         8%|▊         | 252/3250 [1:10:37<13:49:15, 16.60s/it]  8%|▊         | 253/3250 [1:10:53<13:41:19, 16.44s/it]                                                         8%|▊         | 253/3250 [1:10:53<13:41:19, 16.44s/it]  8%|▊         | 254/3250 [1:11:09<13:33:09, 16.28s/it]                                                         8%|▊         | 254/3250 [1:11:09<13:33:09, 16.28s/it]  8%|▊         | 255/3250 [1:11:25<13:27:12, 16.17s/it]                                                         8%|▊         | 255/3250 [1:11:25<13:27:12, 16.17s/it]  8%|▊         | 256/3250 [1:11:41<13:23:05, 16.09s/it]                                                         8%|▊         | 256/3250 [1:11:41<13:23:{'loss': 0.9019, 'learning_rate': 9.848683846761311e-05, 'epoch': 0.08}
{'loss': 0.8539, 'learning_rate': 9.847500808071457e-05, 'epoch': 0.08}
{'loss': 0.8599, 'learning_rate': 9.846313234295256e-05, 'epoch': 0.08}
{'loss': 0.8759, 'learning_rate': 9.845121126543742e-05, 'epoch': 0.08}
05, 16.09s/it]  8%|▊         | 257/3250 [1:11:57<13:19:46, 16.03s/it]                                                         8%|▊         | 257/3250 [1:11:57<13:19:46, 16.03s/it]  8%|▊         | 258/3250 [1:12:12<13:17:23, 15.99s/it]                                                         8%|▊         | 258/3250 [1:12:12<13:17:23, 15.99s/it]  8%|▊         | 259/3250 [1:12:28<13:15:50, 15.96s/it]                                                         8%|▊         | 259/3250 [1:12:28<13:15:50, 15.96s/it]  8%|▊         | 260/3250 [1:12:44<13:14:44, 15.95s/it]                                                         8%|▊         | 260/3250 [1:12:44<13:14:44, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8795086741447449, 'eval_runtime': 2.4873, 'eval_samples_per_second': 4.824, 'eval_steps_per_second': 1.206, 'epoch': 0.08}
                                                         8%|▊         | 260/3250 [1:12:47<13:14:44, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-260
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-260
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-260
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-260

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8525, 'learning_rate': 9.843924485932194e-05, 'epoch': 0.08}
{'loss': 0.8626, 'learning_rate': 9.842723313580128e-05, 'epoch': 0.08}
{'loss': 0.8658, 'learning_rate': 9.841517610611309e-05, 'epoch': 0.08}
{'loss': 0.8489, 'learning_rate': 9.840307378153726e-05, 'epoch': 0.08}
{'loss': 0.863, 'learning_rate': 9.83909261733962e-05, 'epoch': 0.08}
{'loss': 0.89, 'learning_rate': 9.837873329305459e-05, 'epoch': 0.08}
  8%|▊         | 261/3250 [1:13:18<17:37:58, 21.24s/it]                                                         8%|▊         | 261/3250 [1:13:18<17:37:58, 21.24s/it]  8%|▊         | 262/3250 [1:13:34<16:17:39, 19.63s/it]                                                         8%|▊         | 262/3250 [1:13:34<16:17:39, 19.63s/it]  8%|▊         | 263/3250 [1:13:50<15:32:56, 18.74s/it]                                                         8%|▊         | 263/3250 [1:13:50<15:32:56, 18.74s/it]  8%|▊         | 264/3250 [1:14:06<14:49:56, 17.88s/it]                                                         8%|▊         | 264/3250 [1:14:06<14:49:56, 17.88s/it]  8%|▊         | 265/3250 [1:14:22<14:20:01, 17.29s/it]                                                         8%|▊         | 265/3250 [1:14:22<14:20:01, 17.29s/it]  8%|▊         | 266/3250 [1:14:38<13:58:50, 16.87s/it]                                                         8%|▊         | 266/3250 [1:14:38<13:58:{'loss': 0.8565, 'learning_rate': 9.836649515191949e-05, 'epoch': 0.08}
{'loss': 0.8492, 'learning_rate': 9.835421176144035e-05, 'epoch': 0.08}
{'loss': 0.8837, 'learning_rate': 9.834188313310886e-05, 'epoch': 0.08}
{'loss': 0.8791, 'learning_rate': 9.832950927845914e-05, 'epoch': 0.08}
50, 16.87s/it]  8%|▊         | 267/3250 [1:14:54<13:43:58, 16.57s/it]                                                         8%|▊         | 267/3250 [1:14:54<13:43:58, 16.57s/it]  8%|▊         | 268/3250 [1:15:10<13:33:25, 16.37s/it]                                                         8%|▊         | 268/3250 [1:15:10<13:33:25, 16.37s/it]  8%|▊         | 269/3250 [1:15:26<13:25:50, 16.22s/it]                                                         8%|▊         | 269/3250 [1:15:26<13:25:50, 16.22s/it]  8%|▊         | 270/3250 [1:15:42<13:20:30, 16.12s/it]                                                         8%|▊         | 270/3250 [1:15:42<13:20:30, 16.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8765513896942139, 'eval_runtime': 2.4968, 'eval_samples_per_second': 4.806, 'eval_steps_per_second': 1.202, 'epoch': 0.08}
                                                         8%|▊         | 270/3250 [1:15:44<13:20:30, 16.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-270
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-270
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-270/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-270/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8419, 'learning_rate': 9.831709020906754e-05, 'epoch': 0.08}
{'loss': 0.928, 'learning_rate': 9.830462593655274e-05, 'epoch': 0.08}
{'loss': 0.8678, 'learning_rate': 9.829211647257571e-05, 'epoch': 0.08}
{'loss': 0.8557, 'learning_rate': 9.82795618288397e-05, 'epoch': 0.08}
{'loss': 0.8503, 'learning_rate': 9.826696201709021e-05, 'epoch': 0.08}
{'loss': 0.8352, 'learning_rate': 9.825431704911503e-05, 'epoch': 0.08}
  8%|▊         | 271/3250 [1:16:19<18:40:26, 22.57s/it]                                                         8%|▊         | 271/3250 [1:16:19<18:40:26, 22.57s/it]  8%|▊         | 272/3250 [1:16:35<17:00:19, 20.56s/it]                                                         8%|▊         | 272/3250 [1:16:35<17:00:19, 20.56s/it]  8%|▊         | 273/3250 [1:16:51<15:50:31, 19.16s/it]                                                         8%|▊         | 273/3250 [1:16:51<15:50:31, 19.16s/it]  8%|▊         | 274/3250 [1:17:07<15:01:32, 18.18s/it]                                                         8%|▊         | 274/3250 [1:17:07<15:01:32, 18.18s/it]  8%|▊         | 275/3250 [1:17:23<14:26:55, 17.48s/it]                                                         8%|▊         | 275/3250 [1:17:23<14:26:55, 17.48s/it]  8%|▊         | 276/3250 [1:17:39<14:02:44, 17.00s/it]                                                         8%|▊         | 276/3250 [1:17:39<14:02:{'loss': 0.8863, 'learning_rate': 9.824162693674417e-05, 'epoch': 0.09}
{'loss': 0.8339, 'learning_rate': 9.82288916918499e-05, 'epoch': 0.09}
{'loss': 0.8681, 'learning_rate': 9.821611132634666e-05, 'epoch': 0.09}
{'loss': 1.2949, 'learning_rate': 9.820328585219117e-05, 'epoch': 0.09}
44, 17.00s/it]  9%|▊         | 277/3250 [1:17:54<13:45:59, 16.67s/it]                                                         9%|▊         | 277/3250 [1:17:54<13:45:59, 16.67s/it]  9%|▊         | 278/3250 [1:18:10<13:34:09, 16.44s/it]                                                         9%|▊         | 278/3250 [1:18:10<13:34:09, 16.44s/it]  9%|▊         | 279/3250 [1:18:26<13:28:58, 16.34s/it]                                                         9%|▊         | 279/3250 [1:18:26<13:28:58, 16.34s/it]  9%|▊         | 280/3250 [1:18:42<13:21:21, 16.19s/it]                                                         9%|▊         | 280/3250 [1:18:42<13:21:21, 16.19s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8738405704498291, 'eval_runtime': 2.4817, 'eval_samples_per_second': 4.835, 'eval_steps_per_second': 1.209, 'epoch': 0.09}
                                                         9%|▊         | 280/3250 [1:18:45<13:21:21, 16.19s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-280
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-280
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.84, 'learning_rate': 9.819041528138231e-05, 'epoch': 0.09}
{'loss': 0.8583, 'learning_rate': 9.817749962596115e-05, 'epoch': 0.09}
{'loss': 0.8965, 'learning_rate': 9.816453889801098e-05, 'epoch': 0.09}
{'loss': 0.8611, 'learning_rate': 9.815153310965718e-05, 'epoch': 0.09}
{'loss': 0.8226, 'learning_rate': 9.81384822730674e-05, 'epoch': 0.09}
{'loss': 0.8619, 'learning_rate': 9.812538640045132e-05, 'epoch': 0.09}
  9%|▊         | 281/3250 [1:19:30<21:12:55, 25.72s/it]                                                         9%|▊         | 281/3250 [1:19:30<21:12:55, 25.72s/it]  9%|▊         | 282/3250 [1:19:46<18:46:17, 22.77s/it]                                                         9%|▊         | 282/3250 [1:19:46<18:46:17, 22.77s/it]  9%|▊         | 283/3250 [1:20:02<17:03:39, 20.70s/it]                                                         9%|▊         | 283/3250 [1:20:02<17:03:39, 20.70s/it]  9%|▊         | 284/3250 [1:20:18<15:51:48, 19.25s/it]                                                         9%|▊         | 284/3250 [1:20:18<15:51:48, 19.25s/it]  9%|▉         | 285/3250 [1:20:34<15:01:18, 18.24s/it]                                                         9%|▉         | 285/3250 [1:20:34<15:01:18, 18.24s/it]  9%|▉         | 286/3250 [1:20:50<14:26:16, 17.54s/it]                                                         9%|▉         | 286/3250 [1:20:50<14:26:{'loss': 0.9132, 'learning_rate': 9.811224550406082e-05, 'epoch': 0.09}
{'loss': 0.8605, 'learning_rate': 9.809905959618985e-05, 'epoch': 0.09}
{'loss': 0.8529, 'learning_rate': 9.808582868917458e-05, 'epoch': 0.09}
{'loss': 0.8136, 'learning_rate': 9.807255279539313e-05, 'epoch': 0.09}
16, 17.54s/it]  9%|▉         | 287/3250 [1:21:06<14:01:30, 17.04s/it]                                                         9%|▉         | 287/3250 [1:21:06<14:01:30, 17.04s/it]  9%|▉         | 288/3250 [1:21:21<13:44:02, 16.69s/it]                                                         9%|▉         | 288/3250 [1:21:21<13:44:02, 16.69s/it]  9%|▉         | 289/3250 [1:21:37<13:31:52, 16.45s/it]                                                         9%|▉         | 289/3250 [1:21:37<13:31:52, 16.45s/it]  9%|▉         | 290/3250 [1:21:53<13:23:24, 16.29s/it]                                                         9%|▉         | 290/3250 [1:21:53<13:23:24, 16.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8706572651863098, 'eval_runtime': 2.486, 'eval_samples_per_second': 4.827, 'eval_steps_per_second': 1.207, 'epoch': 0.09}
                                                         9%|▉         | 290/3250 [1:21:56<13:23:24, 16.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-290
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-290/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-290

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-290

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-290/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8655, 'learning_rate': 9.805923192726581e-05, 'epoch': 0.09}
{'loss': 0.8629, 'learning_rate': 9.804586609725499e-05, 'epoch': 0.09}
{'loss': 0.8594, 'learning_rate': 9.803245531786506e-05, 'epoch': 0.09}
{'loss': 0.8286, 'learning_rate': 9.801899960164253e-05, 'epoch': 0.09}
{'loss': 0.8511, 'learning_rate': 9.800549896117589e-05, 'epoch': 0.09}
{'loss': 0.8613, 'learning_rate': 9.79919534090957e-05, 'epoch': 0.09}
  9%|▉         | 291/3250 [1:22:12<14:03:11, 17.10s/it]                                                         9%|▉         | 291/3250 [1:22:12<14:03:11, 17.10s/it]  9%|▉         | 292/3250 [1:22:28<13:44:43, 16.73s/it]                                                         9%|▉         | 292/3250 [1:22:28<13:44:43, 16.73s/it]  9%|▉         | 293/3250 [1:22:44<13:31:37, 16.47s/it]                                                         9%|▉         | 293/3250 [1:22:44<13:31:37, 16.47s/it]  9%|▉         | 294/3250 [1:23:00<13:22:14, 16.28s/it]                                                         9%|▉         | 294/3250 [1:23:00<13:22:14, 16.28s/it]  9%|▉         | 295/3250 [1:23:16<13:15:36, 16.15s/it]                                                         9%|▉         | 295/3250 [1:23:16<13:15:36, 16.15s/it]  9%|▉         | 296/3250 [1:23:32<13:20:05, 16.25s/it]                                                         9%|▉         | 296/3250 [1:23:32<13:20:{'loss': 0.8654, 'learning_rate': 9.79783629580745e-05, 'epoch': 0.09}
{'loss': 0.8522, 'learning_rate': 9.796472762082687e-05, 'epoch': 0.09}
{'loss': 0.8335, 'learning_rate': 9.795104741010938e-05, 'epoch': 0.09}
{'loss': 0.8819, 'learning_rate': 9.793732233872056e-05, 'epoch': 0.09}
05, 16.25s/it]  9%|▉         | 297/3250 [1:23:48<13:13:58, 16.13s/it]                                                         9%|▉         | 297/3250 [1:23:48<13:13:58, 16.13s/it]  9%|▉         | 298/3250 [1:24:04<13:09:39, 16.05s/it]                                                         9%|▉         | 298/3250 [1:24:04<13:09:39, 16.05s/it]  9%|▉         | 299/3250 [1:24:20<13:06:39, 15.99s/it]                                                         9%|▉         | 299/3250 [1:24:20<13:06:39, 15.99s/it]  9%|▉         | 300/3250 [1:24:36<13:04:29, 15.96s/it]                                                         9%|▉         | 300/3250 [1:24:36<13:04:29, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8672103881835938, 'eval_runtime': 2.471, 'eval_samples_per_second': 4.856, 'eval_steps_per_second': 1.214, 'epoch': 0.09}
                                                         9%|▉         | 300/3250 [1:24:38<13:04:29, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-300
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-300
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-300the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-300

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-300/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-300/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8404, 'learning_rate': 9.792355241950088e-05, 'epoch': 0.09}
{'loss': 0.8635, 'learning_rate': 9.790973766533288e-05, 'epoch': 0.09}
{'loss': 0.8741, 'learning_rate': 9.789587808914093e-05, 'epoch': 0.09}
{'loss': 0.8342, 'learning_rate': 9.78819737038914e-05, 'epoch': 0.09}
{'loss': 0.8211, 'learning_rate': 9.786802452259251e-05, 'epoch': 0.09}
{'loss': 0.8431, 'learning_rate': 9.785403055829449e-05, 'epoch': 0.09}
  9%|▉         | 301/3250 [1:25:08<17:08:55, 20.93s/it]                                                         9%|▉         | 301/3250 [1:25:08<17:08:55, 20.93s/it]  9%|▉         | 302/3250 [1:25:24<15:53:50, 19.41s/it]                                                         9%|▉         | 302/3250 [1:25:24<15:53:50, 19.41s/it]  9%|▉         | 303/3250 [1:25:40<15:01:30, 18.35s/it]                                                         9%|▉         | 303/3250 [1:25:40<15:01:30, 18.35s/it]  9%|▉         | 304/3250 [1:25:56<14:24:28, 17.61s/it]                                                         9%|▉         | 304/3250 [1:25:56<14:24:28, 17.61s/it]  9%|▉         | 305/3250 [1:26:12<13:58:57, 17.09s/it]                                                         9%|▉         | 305/3250 [1:26:12<13:58:57, 17.09s/it]  9%|▉         | 306/3250 [1:26:28<13:40:50, 16.73s/it]                                                         9%|▉         | 306/3250 [1:26:28<13:40:{'loss': 0.8424, 'learning_rate': 9.783999182408941e-05, 'epoch': 0.09}
{'loss': 0.838, 'learning_rate': 9.78259083331112e-05, 'epoch': 0.09}
{'loss': 0.8708, 'learning_rate': 9.781178009853568e-05, 'epoch': 0.1}
{'loss': 1.3249, 'learning_rate': 9.779760713358059e-05, 'epoch': 0.1}
50, 16.73s/it]  9%|▉         | 307/3250 [1:26:43<13:27:53, 16.47s/it]                                                         9%|▉         | 307/3250 [1:26:43<13:27:53, 16.47s/it]  9%|▉         | 308/3250 [1:26:59<13:18:55, 16.29s/it]                                                         9%|▉         | 308/3250 [1:26:59<13:18:55, 16.29s/it] 10%|▉         | 309/3250 [1:27:15<13:12:34, 16.17s/it]                                                        10%|▉         | 309/3250 [1:27:15<13:12:34, 16.17s/it] 10%|▉         | 310/3250 [1:27:31<13:07:22, 16.07s/it]                                                        10%|▉         | 310/3250 [1:27:31<13:07:22, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8648741841316223, 'eval_runtime': 2.4749, 'eval_samples_per_second': 4.849, 'eval_steps_per_second': 1.212, 'epoch': 0.1}
                                                        10%|▉         | 310/3250 [1:27:33<13:07:22, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-310
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-310/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7796, 'learning_rate': 9.778338945150542e-05, 'epoch': 0.1}
{'loss': 0.8218, 'learning_rate': 9.776912706561156e-05, 'epoch': 0.1}
{'loss': 0.8698, 'learning_rate': 9.775481998924222e-05, 'epoch': 0.1}
{'loss': 0.849, 'learning_rate': 9.77404682357824e-05, 'epoch': 0.1}
{'loss': 0.8277, 'learning_rate': 9.77260718186589e-05, 'epoch': 0.1}
{'loss': 0.8423, 'learning_rate': 9.771163075134029e-05, 'epoch': 0.1}
 10%|▉         | 311/3250 [1:27:50<13:50:57, 16.96s/it]                                                        10%|▉         | 311/3250 [1:27:50<13:50:57, 16.96s/it] 10%|▉         | 312/3250 [1:28:06<13:38:14, 16.71s/it]                                                        10%|▉         | 312/3250 [1:28:06<13:38:14, 16.71s/it] 10%|▉         | 313/3250 [1:28:22<13:25:26, 16.45s/it]                                                        10%|▉         | 313/3250 [1:28:22<13:25:26, 16.45s/it] 10%|▉         | 314/3250 [1:28:38<13:16:30, 16.28s/it]                                                        10%|▉         | 314/3250 [1:28:38<13:16:30, 16.28s/it] 10%|▉         | 315/3250 [1:28:54<13:10:17, 16.16s/it]                                                        10%|▉         | 315/3250 [1:28:54<13:10:17, 16.16s/it] 10%|▉         | 316/3250 [1:29:10<13:05:39, 16.07s/it]                                                        10%|▉         | 316/3250 [1:29:10<13:05:{'loss': 0.8806, 'learning_rate': 9.769714504733694e-05, 'epoch': 0.1}
{'loss': 0.8662, 'learning_rate': 9.768261472020099e-05, 'epoch': 0.1}
{'loss': 0.8101, 'learning_rate': 9.76680397835263e-05, 'epoch': 0.1}
{'loss': 0.7839, 'learning_rate': 9.765342025094847e-05, 'epoch': 0.1}
39, 16.07s/it] 10%|▉         | 317/3250 [1:29:25<13:02:22, 16.00s/it]                                                        10%|▉         | 317/3250 [1:29:25<13:02:22, 16.00s/it] 10%|▉         | 318/3250 [1:29:41<13:00:18, 15.97s/it]                                                        10%|▉         | 318/3250 [1:29:41<13:00:18, 15.97s/it] 10%|▉         | 319/3250 [1:29:57<12:58:30, 15.94s/it]                                                        10%|▉         | 319/3250 [1:29:57<12:58:30, 15.94s/it] 10%|▉         | 320/3250 [1:30:13<13:00:20, 15.98s/it]                                                        10%|▉         | 320/3250 [1:30:13<13:00:20, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8615541458129883, 'eval_runtime': 2.4707, 'eval_samples_per_second': 4.857, 'eval_steps_per_second': 1.214, 'epoch': 0.1}
                                                        10%|▉         | 320/3250 [1:30:16<13:00:20, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-320
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-320/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-320the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-320

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8565, 'learning_rate': 9.763875613614482e-05, 'epoch': 0.1}
{'loss': 0.8301, 'learning_rate': 9.762404745283439e-05, 'epoch': 0.1}
{'loss': 0.8333, 'learning_rate': 9.760929421477791e-05, 'epoch': 0.1}
{'loss': 0.8257, 'learning_rate': 9.759449643577778e-05, 'epoch': 0.1}
{'loss': 0.8226, 'learning_rate': 9.757965412967811e-05, 'epoch': 0.1}
{'loss': 0.8344, 'learning_rate': 9.756476731036461e-05, 'epoch': 0.1}
 10%|▉         | 321/3250 [1:30:32<13:45:22, 16.91s/it]                                                        10%|▉         | 321/3250 [1:30:32<13:45:22, 16.91s/it] 10%|▉         | 322/3250 [1:30:48<13:30:03, 16.60s/it]                                                        10%|▉         | 322/3250 [1:30:48<13:30:03, 16.60s/it] 10%|▉         | 323/3250 [1:31:04<13:19:08, 16.38s/it]                                                        10%|▉         | 323/3250 [1:31:04<13:19:08, 16.38s/it] 10%|▉         | 324/3250 [1:31:20<13:11:31, 16.23s/it]                                                        10%|▉         | 324/3250 [1:31:20<13:11:31, 16.23s/it] 10%|█         | 325/3250 [1:31:36<13:05:49, 16.12s/it]                                                        10%|█         | 325/3250 [1:31:36<13:05:49, 16.12s/it] 10%|█         | 326/3250 [1:31:52<13:02:05, 16.05s/it]                                                        10%|█         | 326/3250 [1:31:52<13:02:{'loss': 0.8344, 'learning_rate': 9.75498359917647e-05, 'epoch': 0.1}
{'loss': 0.843, 'learning_rate': 9.753486018784736e-05, 'epoch': 0.1}
{'loss': 0.8388, 'learning_rate': 9.751983991262326e-05, 'epoch': 0.1}
{'loss': 0.8235, 'learning_rate': 9.750477518014461e-05, 'epoch': 0.1}
05, 16.05s/it] 10%|█         | 327/3250 [1:32:08<12:59:19, 16.00s/it]                                                        10%|█         | 327/3250 [1:32:08<12:59:19, 16.00s/it] 10%|█         | 328/3250 [1:32:24<13:02:26, 16.07s/it]                                                        10%|█         | 328/3250 [1:32:24<13:02:26, 16.07s/it] 10%|█         | 329/3250 [1:32:40<12:59:27, 16.01s/it]                                                        10%|█         | 329/3250 [1:32:40<12:59:27, 16.01s/it] 10%|█         | 330/3250 [1:32:56<12:57:10, 15.97s/it]                                                        10%|█         | 330/3250 [1:32:56<12:57:10, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8579902052879333, 'eval_runtime': 2.4681, 'eval_samples_per_second': 4.862, 'eval_steps_per_second': 1.215, 'epoch': 0.1}
                                                        10%|█         | 330/3250 [1:32:58<12:57:10, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-330
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-330 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-330
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8581, 'learning_rate': 9.748966600450525e-05, 'epoch': 0.1}
{'loss': 0.8113, 'learning_rate': 9.74745123998406e-05, 'epoch': 0.1}
{'loss': 0.8788, 'learning_rate': 9.745931438032763e-05, 'epoch': 0.1}
{'loss': 0.8058, 'learning_rate': 9.744407196018488e-05, 'epoch': 0.1}
{'loss': 0.8251, 'learning_rate': 9.74287851536724e-05, 'epoch': 0.1}
{'loss': 0.8296, 'learning_rate': 9.74134539750918e-05, 'epoch': 0.1}
 10%|█         | 331/3250 [1:33:15<13:42:04, 16.90s/it]                                                        10%|█         | 331/3250 [1:33:15<13:42:04, 16.90s/it] 10%|█         | 332/3250 [1:33:31<13:27:02, 16.59s/it]                                                        10%|█         | 332/3250 [1:33:31<13:27:02, 16.59s/it] 10%|█         | 333/3250 [1:33:46<13:15:53, 16.37s/it]                                                        10%|█         | 333/3250 [1:33:46<13:15:53, 16.37s/it] 10%|█         | 334/3250 [1:34:02<13:08:09, 16.22s/it]                                                        10%|█         | 334/3250 [1:34:02<13:08:09, 16.22s/it] 10%|█         | 335/3250 [1:34:18<13:02:42, 16.11s/it]                                                        10%|█         | 335/3250 [1:34:18<13:02:42, 16.11s/it] 10%|█         | 336/3250 [1:34:34<12:58:36, 16.03s/it]                                                        10%|█         | 336/3250 [1:34:34<12:58:{'loss': 0.8265, 'learning_rate': 9.739807843878617e-05, 'epoch': 0.1}
{'loss': 0.822, 'learning_rate': 9.738265855914013e-05, 'epoch': 0.1}
{'loss': 0.8333, 'learning_rate': 9.736719435057976e-05, 'epoch': 0.1}
{'loss': 0.8216, 'learning_rate': 9.735168582757264e-05, 'epoch': 0.1}
36, 16.03s/it] 10%|█         | 337/3250 [1:34:50<12:55:41, 15.98s/it]                                                        10%|█         | 337/3250 [1:34:50<12:55:41, 15.98s/it] 10%|█         | 338/3250 [1:35:06<12:53:35, 15.94s/it]                                                        10%|█         | 338/3250 [1:35:06<12:53:35, 15.94s/it] 10%|█         | 339/3250 [1:35:22<12:52:04, 15.91s/it]                                                        10%|█         | 339/3250 [1:35:22<12:52:04, 15.91s/it] 10%|█         | 340/3250 [1:35:37<12:50:49, 15.89s/it]                                                        10%|█         | 340/3250 [1:35:37<12:50:49, 15.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8558340072631836, 'eval_runtime': 2.4707, 'eval_samples_per_second': 4.857, 'eval_steps_per_second': 1.214, 'epoch': 0.1}
                                                        10%|█         | 340/3250 [1:35:40<12:50:49, 15.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-340
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-340/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-340
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-340/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2691, 'learning_rate': 9.733613300462776e-05, 'epoch': 0.1}
{'loss': 0.8205, 'learning_rate': 9.732053589629561e-05, 'epoch': 0.11}
{'loss': 0.8519, 'learning_rate': 9.730489451716809e-05, 'epoch': 0.11}
{'loss': 0.827, 'learning_rate': 9.728920888187849e-05, 'epoch': 0.11}
{'loss': 0.8293, 'learning_rate': 9.727347900510155e-05, 'epoch': 0.11}
{'loss': 0.8006, 'learning_rate': 9.725770490155338e-05, 'epoch': 0.11}
 10%|█         | 341/3250 [1:36:11<17:07:57, 21.20s/it]                                                        10%|█         | 341/3250 [1:36:11<17:07:57, 21.20s/it] 11%|█         | 342/3250 [1:36:27<15:49:33, 19.59s/it]                                                        11%|█         | 342/3250 [1:36:27<15:49:33, 19.59s/it] 11%|█         | 343/3250 [1:36:43<14:55:00, 18.47s/it]                                                        11%|█         | 343/3250 [1:36:43<14:55:00, 18.47s/it] 11%|█         | 344/3250 [1:36:59<14:16:31, 17.68s/it]                                                        11%|█         | 344/3250 [1:36:59<14:16:31, 17.68s/it] 11%|█         | 345/3250 [1:37:15<13:56:09, 17.27s/it]                                                        11%|█         | 345/3250 [1:37:15<13:56:09, 17.27s/it] 11%|█         | 346/3250 [1:37:31<13:34:52, 16.84s/it]                                                        11%|█         | 346/3250 [1:37:31<13:34:{'loss': 0.8858, 'learning_rate': 9.724188658599146e-05, 'epoch': 0.11}
{'loss': 0.8413, 'learning_rate': 9.722602407321463e-05, 'epoch': 0.11}
{'loss': 0.8075, 'learning_rate': 9.721011737806309e-05, 'epoch': 0.11}
{'loss': 0.8133, 'learning_rate': 9.719416651541839e-05, 'epoch': 0.11}
52, 16.84s/it] 11%|█         | 347/3250 [1:37:46<13:20:00, 16.53s/it]                                                        11%|█         | 347/3250 [1:37:46<13:20:00, 16.53s/it] 11%|█         | 348/3250 [1:38:02<13:09:41, 16.33s/it]                                                        11%|█         | 348/3250 [1:38:02<13:09:41, 16.33s/it] 11%|█         | 349/3250 [1:38:18<13:02:22, 16.18s/it]                                                        11%|█         | 349/3250 [1:38:18<13:02:22, 16.18s/it] 11%|█         | 350/3250 [1:38:34<12:57:12, 16.08s/it]                                                        11%|█         | 350/3250 [1:38:34<12:57:12, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8507437109947205, 'eval_runtime': 2.4642, 'eval_samples_per_second': 4.87, 'eval_steps_per_second': 1.217, 'epoch': 0.11}
                                                        11%|█         | 350/3250 [1:38:36<12:57:12, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-350
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-350
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8154, 'learning_rate': 9.717817150020336e-05, 'epoch': 0.11}
{'loss': 0.8023, 'learning_rate': 9.716213234738215e-05, 'epoch': 0.11}
{'loss': 0.8217, 'learning_rate': 9.714604907196025e-05, 'epoch': 0.11}
{'loss': 0.8237, 'learning_rate': 9.712992168898436e-05, 'epoch': 0.11}
{'loss': 0.8094, 'learning_rate': 9.711375021354248e-05, 'epoch': 0.11}
{'loss': 0.8145, 'learning_rate': 9.709753466076387e-05, 'epoch': 0.11}
 11%|█         | 351/3250 [1:39:21<20:24:19, 25.34s/it]                                                        11%|█         | 351/3250 [1:39:21<20:24:19, 25.34s/it] 11%|█         | 352/3250 [1:39:37<18:06:02, 22.49s/it]                                                        11%|█         | 352/3250 [1:39:37<18:06:02, 22.49s/it] 11%|█         | 353/3250 [1:39:53<16:32:38, 20.56s/it]                                                        11%|█         | 353/3250 [1:39:53<16:32:38, 20.56s/it] 11%|█         | 354/3250 [1:40:09<15:24:01, 19.14s/it]                                                        11%|█         | 354/3250 [1:40:09<15:24:01, 19.14s/it] 11%|█         | 355/3250 [1:40:25<14:35:49, 18.15s/it]                                                        11%|█         | 355/3250 [1:40:25<14:35:49, 18.15s/it] 11%|█         | 356/3250 [1:40:40<14:01:59, 17.46s/it]                                                        11%|█         | 356/3250 [1:40:40<14:01:{'loss': 0.8467, 'learning_rate': 9.708127504581902e-05, 'epoch': 0.11}
{'loss': 0.7908, 'learning_rate': 9.706497138391961e-05, 'epoch': 0.11}
{'loss': 0.812, 'learning_rate': 9.704862369031857e-05, 'epoch': 0.11}
{'loss': 0.8369, 'learning_rate': 9.703223198031002e-05, 'epoch': 0.11}
59, 17.46s/it] 11%|█         | 357/3250 [1:40:56<13:38:18, 16.97s/it]                                                        11%|█         | 357/3250 [1:40:56<13:38:18, 16.97s/it] 11%|█         | 358/3250 [1:41:12<13:21:29, 16.63s/it]                                                        11%|█         | 358/3250 [1:41:12<13:21:29, 16.63s/it] 11%|█         | 359/3250 [1:41:28<13:09:51, 16.39s/it]                                                        11%|█         | 359/3250 [1:41:28<13:09:51, 16.39s/it] 11%|█         | 360/3250 [1:41:44<13:01:29, 16.22s/it]                                                        11%|█         | 360/3250 [1:41:44<13:01:29, 16.22s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8481104373931885, 'eval_runtime': 2.4767, 'eval_samples_per_second': 4.845, 'eval_steps_per_second': 1.211, 'epoch': 0.11}
                                                        11%|█         | 360/3250 [1:41:46<13:01:29, 16.22s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-360
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-360the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-360
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-360

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-360/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-360/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8218, 'learning_rate': 9.701579626922922e-05, 'epoch': 0.11}
{'loss': 0.7889, 'learning_rate': 9.699931657245263e-05, 'epoch': 0.11}
{'loss': 0.8737, 'learning_rate': 9.698279290539788e-05, 'epoch': 0.11}
{'loss': 0.8157, 'learning_rate': 9.696622528352368e-05, 'epoch': 0.11}
{'loss': 0.8056, 'learning_rate': 9.694961372232991e-05, 'epoch': 0.11}
{'loss': 0.8045, 'learning_rate': 9.693295823735753e-05, 'epoch': 0.11}
 11%|█         | 361/3250 [1:42:30<20:15:48, 25.25s/it]                                                        11%|█         | 361/3250 [1:42:30<20:15:48, 25.25s/it] 11%|█         | 362/3250 [1:42:46<17:59:27, 22.43s/it]                                                        11%|█         | 362/3250 [1:42:46<17:59:27, 22.43s/it] 11%|█         | 363/3250 [1:43:02<16:24:10, 20.45s/it]                                                        11%|█         | 363/3250 [1:43:02<16:24:10, 20.45s/it] 11%|█         | 364/3250 [1:43:18<15:17:23, 19.07s/it]                                                        11%|█         | 364/3250 [1:43:18<15:17:23, 19.07s/it] 11%|█         | 365/3250 [1:43:33<14:30:24, 18.10s/it]                                                        11%|█         | 365/3250 [1:43:33<14:30:24, 18.10s/it] 11%|█▏        | 366/3250 [1:43:49<13:57:42, 17.43s/it]                                                        11%|█▏        | 366/3250 [1:43:49<13{'loss': 0.7848, 'learning_rate': 9.69162588441886e-05, 'epoch': 0.11}
{'loss': 0.8302, 'learning_rate': 9.689951555844628e-05, 'epoch': 0.11}
{'loss': 0.7957, 'learning_rate': 9.688272839579477e-05, 'epoch': 0.11}
{'loss': 0.8233, 'learning_rate': 9.686589737193929e-05, 'epoch': 0.11}
:57:42, 17.43s/it] 11%|█▏        | 367/3250 [1:44:05<13:34:40, 16.95s/it]                                                        11%|█▏        | 367/3250 [1:44:05<13:34:40, 16.95s/it] 11%|█▏        | 368/3250 [1:44:21<13:18:28, 16.62s/it]                                                        11%|█▏        | 368/3250 [1:44:21<13:18:28, 16.62s/it] 11%|█▏        | 369/3250 [1:44:37<13:07:09, 16.39s/it]                                                        11%|█▏        | 369/3250 [1:44:37<13:07:09, 16.39s/it] 11%|█▏        | 370/3250 [1:44:53<12:59:08, 16.23s/it]                                                        11%|█▏        | 370/3250 [1:44:53<12:59:08, 16.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8441508412361145, 'eval_runtime': 2.7041, 'eval_samples_per_second': 4.438, 'eval_steps_per_second': 1.109, 'epoch': 0.11}
                                                        11%|█▏        | 370/3250 [1:44:55<12:59:08, 16.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-370
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-370/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2586, 'learning_rate': 9.684902250262618e-05, 'epoch': 0.11}
{'loss': 0.7908, 'learning_rate': 9.683210380364272e-05, 'epoch': 0.11}
{'loss': 0.8062, 'learning_rate': 9.681514129081724e-05, 'epoch': 0.11}
{'loss': 0.8445, 'learning_rate': 9.6798134980019e-05, 'epoch': 0.12}
{'loss': 0.8122, 'learning_rate': 9.678108488715833e-05, 'epoch': 0.12}
{'loss': 0.7736, 'learning_rate': 9.676399102818646e-05, 'epoch': 0.12}
 11%|█▏        | 371/3250 [1:45:12<13:41:16, 17.12s/it]                                                        11%|█▏        | 371/3250 [1:45:12<13:41:16, 17.12s/it] 11%|█▏        | 372/3250 [1:45:28<13:22:59, 16.74s/it]                                                        11%|█▏        | 372/3250 [1:45:28<13:22:59, 16.74s/it] 11%|█▏        | 373/3250 [1:45:44<13:10:19, 16.48s/it]                                                        11%|█▏        | 373/3250 [1:45:44<13:10:19, 16.48s/it] 12%|█▏        | 374/3250 [1:45:59<13:01:22, 16.30s/it]                                                        12%|█▏        | 374/3250 [1:45:59<13:01:22, 16.30s/it] 12%|█▏        | 375/3250 [1:46:15<12:54:54, 16.17s/it]                                                        12%|█▏        | 375/3250 [1:46:15<12:54:54, 16.17s/it] 12%|█▏        | 376/3250 [1:46:31<12:50:19, 16.08s/it]                                                        12%|█▏        | {'loss': 0.804, 'learning_rate': 9.674685341909552e-05, 'epoch': 0.12}
{'loss': 0.8768, 'learning_rate': 9.67296720759187e-05, 'epoch': 0.12}
{'loss': 0.8087, 'learning_rate': 9.671244701472999e-05, 'epoch': 0.12}
{'loss': 0.8086, 'learning_rate': 9.669517825164434e-05, 'epoch': 0.12}
376/3250 [1:46:31<12:50:19, 16.08s/it] 12%|█▏        | 377/3250 [1:46:47<12:47:10, 16.02s/it]                                                        12%|█▏        | 377/3250 [1:46:47<12:47:10, 16.02s/it] 12%|█▏        | 378/3250 [1:47:03<12:48:20, 16.05s/it]                                                        12%|█▏        | 378/3250 [1:47:03<12:48:20, 16.05s/it] 12%|█▏        | 379/3250 [1:47:19<12:45:40, 16.00s/it]                                                        12%|█▏        | 379/3250 [1:47:19<12:45:40, 16.00s/it] 12%|█▏        | 380/3250 [1:47:35<12:43:30, 15.96s/it]                                                        12%|█▏        | 380/3250 [1:47:35<12:43:30, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8435649275779724, 'eval_runtime': 2.4781, 'eval_samples_per_second': 4.842, 'eval_steps_per_second': 1.211, 'epoch': 0.12}
                                                        12%|█▏        | 380/3250 [1:47:37<12:43:30, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-380
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-380/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-380

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-380

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7634, 'learning_rate': 9.667786580281755e-05, 'epoch': 0.12}
{'loss': 0.8073, 'learning_rate': 9.66605096844463e-05, 'epoch': 0.12}
{'loss': 0.8092, 'learning_rate': 9.664310991276815e-05, 'epoch': 0.12}
{'loss': 0.8065, 'learning_rate': 9.662566650406146e-05, 'epoch': 0.12}
{'loss': 0.7719, 'learning_rate': 9.660817947464547e-05, 'epoch': 0.12}
{'loss': 0.8098, 'learning_rate': 9.659064884088016e-05, 'epoch': 0.12}
 12%|█▏        | 381/3250 [1:47:54<13:26:25, 16.86s/it]                                                        12%|█▏        | 381/3250 [1:47:54<13:26:25, 16.86s/it] 12%|█▏        | 382/3250 [1:48:10<13:11:58, 16.57s/it]                                                        12%|█▏        | 382/3250 [1:48:10<13:11:58, 16.57s/it] 12%|█▏        | 383/3250 [1:48:26<13:01:41, 16.36s/it]                                                        12%|█▏        | 383/3250 [1:48:26<13:01:41, 16.36s/it] 12%|█▏        | 384/3250 [1:48:42<12:54:37, 16.22s/it]                                                        12%|█▏        | 384/3250 [1:48:42<12:54:37, 16.22s/it] 12%|█▏        | 385/3250 [1:48:57<12:49:07, 16.11s/it]                                                        12%|█▏        | 385/3250 [1:48:57<12:49:07, 16.11s/it] 12%|█▏        | 386/3250 [1:49:13<12:45:42, 16.04s/it]                                                        12%|█▏        | {'loss': 0.8143, 'learning_rate': 9.657307461916635e-05, 'epoch': 0.12}
{'loss': 0.8158, 'learning_rate': 9.655545682594566e-05, 'epoch': 0.12}
{'loss': 0.8068, 'learning_rate': 9.65377954777004e-05, 'epoch': 0.12}
{'loss': 0.7876, 'learning_rate': 9.652009059095369e-05, 'epoch': 0.12}
386/3250 [1:49:13<12:45:42, 16.04s/it] 12%|█▏        | 387/3250 [1:49:29<12:42:48, 15.99s/it]                                                        12%|█▏        | 387/3250 [1:49:29<12:42:48, 15.99s/it] 12%|█▏        | 388/3250 [1:49:45<12:40:56, 15.95s/it]                                                        12%|█▏        | 388/3250 [1:49:45<12:40:56, 15.95s/it] 12%|█▏        | 389/3250 [1:50:01<12:39:33, 15.93s/it]                                                        12%|█▏        | 389/3250 [1:50:01<12:39:33, 15.93s/it] 12%|█▏        | 390/3250 [1:50:17<12:38:37, 15.92s/it]                                                        12%|█▏        | 390/3250 [1:50:17<12:38:37, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8398249745368958, 'eval_runtime': 2.4739, 'eval_samples_per_second': 4.851, 'eval_steps_per_second': 1.213, 'epoch': 0.12}
                                                        12%|█▏        | 390/3250 [1:50:19<12:38:37, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-390
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-390 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8344, 'learning_rate': 9.650234218226934e-05, 'epoch': 0.12}
{'loss': 0.7923, 'learning_rate': 9.648455026825194e-05, 'epoch': 0.12}
{'loss': 0.8144, 'learning_rate': 9.64667148655467e-05, 'epoch': 0.12}
{'loss': 0.8283, 'learning_rate': 9.644883599083958e-05, 'epoch': 0.12}
{'loss': 0.7858, 'learning_rate': 9.643091366085717e-05, 'epoch': 0.12}
{'loss': 0.7773, 'learning_rate': 9.641294789236676e-05, 'epoch': 0.12}
 12%|█▏        | 391/3250 [1:50:49<16:34:36, 20.87s/it]                                                        12%|█▏        | 391/3250 [1:50:49<16:34:36, 20.87s/it] 12%|█▏        | 392/3250 [1:51:05<15:22:53, 19.37s/it]                                                        12%|█▏        | 392/3250 [1:51:05<15:22:53, 19.37s/it] 12%|█▏        | 393/3250 [1:51:21<14:32:36, 18.33s/it]                                                        12%|█▏        | 393/3250 [1:51:21<14:32:36, 18.33s/it] 12%|█▏        | 394/3250 [1:51:37<14:06:33, 17.78s/it]                                                        12%|█▏        | 394/3250 [1:51:37<14:06:33, 17.78s/it] 12%|█▏        | 395/3250 [1:51:53<13:38:53, 17.21s/it]                                                        12%|█▏        | 395/3250 [1:51:53<13:38:53, 17.21s/it] 12%|█▏        | 396/3250 [1:52:09<13:19:24, 16.81s/it]                                                        12%|█▏        | {'loss': 0.7889, 'learning_rate': 9.639493870217622e-05, 'epoch': 0.12}
{'loss': 0.8041, 'learning_rate': 9.637688610713409e-05, 'epoch': 0.12}
{'loss': 0.7917, 'learning_rate': 9.635879012412951e-05, 'epoch': 0.12}
{'loss': 0.8183, 'learning_rate': 9.634065077009218e-05, 'epoch': 0.12}
396/3250 [1:52:09<13:19:24, 16.81s/it] 12%|█▏        | 397/3250 [1:52:25<13:05:37, 16.52s/it]                                                        12%|█▏        | 397/3250 [1:52:25<13:05:37, 16.52s/it] 12%|█▏        | 398/3250 [1:52:41<12:55:43, 16.32s/it]                                                        12%|█▏        | 398/3250 [1:52:41<12:55:43, 16.32s/it] 12%|█▏        | 399/3250 [1:52:57<12:49:40, 16.20s/it]                                                        12%|█▏        | 399/3250 [1:52:57<12:49:40, 16.20s/it] 12%|█▏        | 400/3250 [1:53:13<12:45:05, 16.11s/it]                                                        12%|█▏        | 400/3250 [1:53:13<12:45:05, 16.11s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8367049694061279, 'eval_runtime': 2.4804, 'eval_samples_per_second': 4.838, 'eval_steps_per_second': 1.209, 'epoch': 0.12}
                                                        12%|█▏        | 400/3250 [1:53:15<12:45:05, 16.11s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-400
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-400
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-400

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2795, 'learning_rate': 9.632246806199241e-05, 'epoch': 0.12}
{'loss': 0.7451, 'learning_rate': 9.630424201684105e-05, 'epoch': 0.12}
{'loss': 0.7654, 'learning_rate': 9.628597265168953e-05, 'epoch': 0.12}
{'loss': 0.8163, 'learning_rate': 9.626765998362974e-05, 'epoch': 0.12}
{'loss': 0.806, 'learning_rate': 9.624930402979416e-05, 'epoch': 0.12}
{'loss': 0.7804, 'learning_rate': 9.62309048073557e-05, 'epoch': 0.12}
 12%|█▏        | 401/3250 [1:53:32<13:25:55, 16.97s/it]                                                        12%|█▏        | 401/3250 [1:53:32<13:25:55, 16.97s/it] 12%|█▏        | 402/3250 [1:53:48<13:10:08, 16.65s/it]                                                        12%|█▏        | 402/3250 [1:53:48<13:10:08, 16.65s/it] 12%|█▏        | 403/3250 [1:54:03<12:58:55, 16.42s/it]                                                        12%|█▏        | 403/3250 [1:54:03<12:58:55, 16.42s/it] 12%|█▏        | 404/3250 [1:54:19<12:51:22, 16.26s/it]                                                        12%|█▏        | 404/3250 [1:54:19<12:51:22, 16.26s/it] 12%|█▏        | 405/3250 [1:54:35<12:45:59, 16.15s/it]                                                        12%|█▏        | 405/3250 [1:54:35<12:45:59, 16.15s/it] 12%|█▏        | 406/3250 [1:54:51<12:41:54, 16.07s/it]                                                        12%|█▏        | {'loss': 0.7792, 'learning_rate': 9.62124623335278e-05, 'epoch': 0.13}
{'loss': 0.8452, 'learning_rate': 9.619397662556435e-05, 'epoch': 0.13}
{'loss': 0.8157, 'learning_rate': 9.617544770075965e-05, 'epoch': 0.13}
{'loss': 0.776, 'learning_rate': 9.615687557644848e-05, 'epoch': 0.13}
406/3250 [1:54:51<12:41:54, 16.07s/it] 13%|█▎        | 407/3250 [1:55:07<12:39:00, 16.02s/it]                                                        13%|█▎        | 407/3250 [1:55:07<12:39:00, 16.02s/it] 13%|█▎        | 408/3250 [1:55:23<12:38:14, 16.01s/it]                                                        13%|█▎        | 408/3250 [1:55:23<12:38:14, 16.01s/it] 13%|█▎        | 409/3250 [1:55:39<12:36:04, 15.97s/it]                                                        13%|█▎        | 409/3250 [1:55:39<12:36:04, 15.97s/it] 13%|█▎        | 410/3250 [1:55:56<12:52:35, 16.32s/it]                                                        13%|█▎        | 410/3250 [1:55:56<12:52:35, 16.32s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8351997137069702, 'eval_runtime': 2.4807, 'eval_samples_per_second': 4.837, 'eval_steps_per_second': 1.209, 'epoch': 0.13}
                                                        13%|█▎        | 410/3250 [1:55:59<12:52:35, 16.32s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-410
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-410the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-410

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-410/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7378, 'learning_rate': 9.613826027000601e-05, 'epoch': 0.13}
{'loss': 0.8126, 'learning_rate': 9.611960179884783e-05, 'epoch': 0.13}
{'loss': 0.7921, 'learning_rate': 9.61009001804299e-05, 'epoch': 0.13}
{'loss': 0.7883, 'learning_rate': 9.608215543224853e-05, 'epoch': 0.13}
{'loss': 0.7842, 'learning_rate': 9.60633675718404e-05, 'epoch': 0.13}
{'loss': 0.7853, 'learning_rate': 9.604453661678253e-05, 'epoch': 0.13}
 13%|█▎        | 411/3250 [1:56:34<17:52:21, 22.66s/it]                                                        13%|█▎        | 411/3250 [1:56:34<17:52:21, 22.66s/it] 13%|█▎        | 412/3250 [1:56:49<16:15:32, 20.62s/it]                                                        13%|█▎        | 412/3250 [1:56:49<16:15:32, 20.62s/it] 13%|█▎        | 413/3250 [1:57:05<15:07:44, 19.20s/it]                                                        13%|█▎        | 413/3250 [1:57:05<15:07:44, 19.20s/it] 13%|█▎        | 414/3250 [1:57:21<14:20:29, 18.20s/it]                                                        13%|█▎        | 414/3250 [1:57:21<14:20:29, 18.20s/it] 13%|█▎        | 415/3250 [1:57:37<13:47:04, 17.50s/it]                                                        13%|█▎        | 415/3250 [1:57:37<13:47:04, 17.50s/it] 13%|█▎        | 416/3250 [1:57:53<13:23:34, 17.01s/it]                                                        13%|█▎        | {'loss': 0.7854, 'learning_rate': 9.602566258469225e-05, 'epoch': 0.13}
{'loss': 0.7945, 'learning_rate': 9.600674549322717e-05, 'epoch': 0.13}
{'loss': 0.7912, 'learning_rate': 9.598778536008522e-05, 'epoch': 0.13}
{'loss': 0.8007, 'learning_rate': 9.596878220300454e-05, 'epoch': 0.13}
416/3250 [1:57:53<13:23:34, 17.01s/it] 13%|█▎        | 417/3250 [1:58:09<13:07:04, 16.67s/it]                                                        13%|█▎        | 417/3250 [1:58:09<13:07:04, 16.67s/it] 13%|█▎        | 418/3250 [1:58:25<12:55:54, 16.44s/it]                                                        13%|█▎        | 418/3250 [1:58:25<12:55:54, 16.44s/it] 13%|█▎        | 419/3250 [1:58:41<12:47:22, 16.26s/it]                                                        13%|█▎        | 419/3250 [1:58:41<12:47:22, 16.26s/it] 13%|█▎        | 420/3250 [1:58:56<12:41:28, 16.14s/it]                                                        13%|█▎        | 420/3250 [1:58:56<12:41:28, 16.14s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8332681655883789, 'eval_runtime': 2.4853, 'eval_samples_per_second': 4.828, 'eval_steps_per_second': 1.207, 'epoch': 0.13}
                                                        13%|█▎        | 420/3250 [1:58:59<12:41:28, 16.14s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-420
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-420/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-420

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-420

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-420/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-420/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7807, 'learning_rate': 9.594973603976363e-05, 'epoch': 0.13}
{'loss': 0.8175, 'learning_rate': 9.59306468881811e-05, 'epoch': 0.13}
{'loss': 0.7521, 'learning_rate': 9.591151476611584e-05, 'epoch': 0.13}
{'loss': 0.8399, 'learning_rate': 9.589233969146695e-05, 'epoch': 0.13}
{'loss': 0.7584, 'learning_rate': 9.58731216821737e-05, 'epoch': 0.13}
{'loss': 0.7718, 'learning_rate': 9.585386075621554e-05, 'epoch': 0.13}
 13%|█▎        | 421/3250 [1:59:30<16:46:11, 21.34s/it]                                                        13%|█▎        | 421/3250 [1:59:30<16:46:11, 21.34s/it] 13%|█▎        | 422/3250 [1:59:46<15:28:12, 19.69s/it]                                                        13%|█▎        | 422/3250 [1:59:46<15:28:12, 19.69s/it] 13%|█▎        | 423/3250 [2:00:02<14:33:33, 18.54s/it]                                                        13%|█▎        | 423/3250 [2:00:02<14:33:33, 18.54s/it] 13%|█▎        | 424/3250 [2:00:17<13:55:09, 17.73s/it]                                                        13%|█▎        | 424/3250 [2:00:17<13:55:09, 17.73s/it] 13%|█▎        | 425/3250 [2:00:33<13:28:27, 17.17s/it]                                                        13%|█▎        | 425/3250 [2:00:33<13:28:27, 17.17s/it] 13%|█▎        | 426/3250 [2:00:49<13:09:37, 16.78s/it]                                                        13%|█▎        | {'loss': 0.7761, 'learning_rate': 9.583455693161201e-05, 'epoch': 0.13}
{'loss': 0.7763, 'learning_rate': 9.581521022642286e-05, 'epoch': 0.13}
{'loss': 0.7766, 'learning_rate': 9.579582065874793e-05, 'epoch': 0.13}
{'loss': 0.7894, 'learning_rate': 9.577638824672715e-05, 'epoch': 0.13}
426/3250 [2:00:49<13:09:37, 16.78s/it] 13%|█▎        | 427/3250 [2:01:06<13:17:44, 16.96s/it]                                                        13%|█▎        | 427/3250 [2:01:06<13:17:44, 16.96s/it] 13%|█▎        | 428/3250 [2:01:22<13:01:54, 16.62s/it]                                                        13%|█▎        | 428/3250 [2:01:22<13:01:54, 16.62s/it] 13%|█▎        | 429/3250 [2:01:38<12:50:58, 16.40s/it]                                                        13%|█▎        | 429/3250 [2:01:38<12:50:58, 16.40s/it] 13%|█▎        | 430/3250 [2:01:54<12:42:55, 16.23s/it]                                                        13%|█▎        | 430/3250 [2:01:54<12:42:55, 16.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8289763331413269, 'eval_runtime': 2.4679, 'eval_samples_per_second': 4.862, 'eval_steps_per_second': 1.216, 'epoch': 0.13}
                                                        13%|█▎        | 430/3250 [2:01:57<12:42:55, 16.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-430
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-430/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7798, 'learning_rate': 9.575691300854055e-05, 'epoch': 0.13}
{'loss': 1.2271, 'learning_rate': 9.57373949624082e-05, 'epoch': 0.13}
{'loss': 0.7663, 'learning_rate': 9.571783412659027e-05, 'epoch': 0.13}
{'loss': 0.8015, 'learning_rate': 9.56982305193869e-05, 'epoch': 0.13}
{'loss': 0.7775, 'learning_rate': 9.567858415913826e-05, 'epoch': 0.13}
{'loss': 0.7822, 'learning_rate': 9.565889506422456e-05, 'epoch': 0.13}
 13%|█▎        | 431/3250 [2:02:13<13:21:33, 17.06s/it]                                                        13%|█▎        | 431/3250 [2:02:13<13:21:33, 17.06s/it] 13%|█▎        | 432/3250 [2:02:29<13:03:56, 16.69s/it]                                                        13%|█▎        | 432/3250 [2:02:29<13:03:56, 16.69s/it] 13%|█▎        | 433/3250 [2:02:45<12:52:02, 16.44s/it]                                                        13%|█▎        | 433/3250 [2:02:45<12:52:02, 16.44s/it] 13%|█▎        | 434/3250 [2:03:01<12:43:43, 16.27s/it]                                                        13%|█▎        | 434/3250 [2:03:01<12:43:43, 16.27s/it] 13%|█▎        | 435/3250 [2:03:16<12:37:44, 16.15s/it]                                                        13%|█▎        | 435/3250 [2:03:16<12:37:44, 16.15s/it] 13%|█▎        | 436/3250 [2:03:32<12:33:26, 16.06s/it]                                                        13%|█▎        | {'loss': 0.7547, 'learning_rate': 9.563916325306594e-05, 'epoch': 0.13}
{'loss': 0.8274, 'learning_rate': 9.561938874412255e-05, 'epoch': 0.13}
{'loss': 0.8045, 'learning_rate': 9.559957155589444e-05, 'epoch': 0.14}
{'loss': 0.7746, 'learning_rate': 9.557971170692161e-05, 'epoch': 0.14}
436/3250 [2:03:32<12:33:26, 16.06s/it] 13%|█▎        | 437/3250 [2:03:48<12:30:05, 16.00s/it]                                                        13%|█▎        | 437/3250 [2:03:48<12:30:05, 16.00s/it] 13%|█▎        | 438/3250 [2:04:04<12:27:47, 15.96s/it]                                                        13%|█▎        | 438/3250 [2:04:04<12:27:47, 15.96s/it] 14%|█▎        | 439/3250 [2:04:20<12:25:57, 15.92s/it]                                                        14%|█▎        | 439/3250 [2:04:20<12:25:57, 15.92s/it] 14%|█▎        | 440/3250 [2:04:36<12:24:47, 15.90s/it]                                                        14%|█▎        | 440/3250 [2:04:36<12:24:47, 15.90s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8264591097831726, 'eval_runtime': 2.4704, 'eval_samples_per_second': 4.857, 'eval_steps_per_second': 1.214, 'epoch': 0.14}
                                                        14%|█▎        | 440/3250 [2:04:38<12:24:47, 15.90s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-440
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-440
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7681, 'learning_rate': 9.555980921578398e-05, 'epoch': 0.14}
{'loss': 0.7735, 'learning_rate': 9.553986410110134e-05, 'epoch': 0.14}
{'loss': 0.75, 'learning_rate': 9.551987638153339e-05, 'epoch': 0.14}
{'loss': 0.7759, 'learning_rate': 9.549984607577964e-05, 'epoch': 0.14}
{'loss': 0.766, 'learning_rate': 9.54797732025795e-05, 'epoch': 0.14}
{'loss': 0.7568, 'learning_rate': 9.545965778071218e-05, 'epoch': 0.14}
 14%|█▎        | 441/3250 [2:05:08<16:17:02, 20.87s/it]                                                        14%|█▎        | 441/3250 [2:05:08<16:17:02, 20.87s/it] 14%|█▎        | 442/3250 [2:05:24<15:06:24, 19.37s/it]                                                        14%|█▎        | 442/3250 [2:05:24<15:06:24, 19.37s/it] 14%|█▎        | 443/3250 [2:05:40<14:22:05, 18.43s/it]                                                        14%|█▎        | 443/3250 [2:05:40<14:22:05, 18.43s/it] 14%|█▎        | 444/3250 [2:05:57<13:50:47, 17.76s/it]                                                        14%|█▎        | 444/3250 [2:05:57<13:50:47, 17.76s/it] 14%|█▎        | 445/3250 [2:06:12<13:23:59, 17.20s/it]                                                        14%|█▎        | 445/3250 [2:06:12<13:23:59, 17.20s/it] 14%|█▎        | 446/3250 [2:06:28<13:04:58, 16.80s/it]                                                        14%|█▎        | {'loss': 0.7725, 'learning_rate': 9.543949982899667e-05, 'epoch': 0.14}
{'loss': 0.7916, 'learning_rate': 9.541929936629175e-05, 'epoch': 0.14}
{'loss': 0.7641, 'learning_rate': 9.539905641149605e-05, 'epoch': 0.14}
{'loss': 0.7661, 'learning_rate': 9.537877098354786e-05, 'epoch': 0.14}
446/3250 [2:06:28<13:04:58, 16.80s/it] 14%|█▍        | 447/3250 [2:06:44<12:51:33, 16.52s/it]                                                        14%|█▍        | 447/3250 [2:06:44<12:51:33, 16.52s/it] 14%|█▍        | 448/3250 [2:07:00<12:42:16, 16.32s/it]                                                        14%|█▍        | 448/3250 [2:07:00<12:42:16, 16.32s/it] 14%|█▍        | 449/3250 [2:07:16<12:35:39, 16.19s/it]                                                        14%|█▍        | 449/3250 [2:07:16<12:35:39, 16.19s/it] 14%|█▍        | 450/3250 [2:07:32<12:30:51, 16.09s/it]                                                        14%|█▍        | 450/3250 [2:07:32<12:30:51, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8264481425285339, 'eval_runtime': 2.4678, 'eval_samples_per_second': 4.863, 'eval_steps_per_second': 1.216, 'epoch': 0.14}
                                                        14%|█▍        | 450/3250 [2:07:34<12:30:51, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-450
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-450
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-450the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-450

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-450/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-450/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7862, 'learning_rate': 9.535844310142524e-05, 'epoch': 0.14}
{'loss': 0.782, 'learning_rate': 9.533807278414597e-05, 'epoch': 0.14}
{'loss': 0.7401, 'learning_rate': 9.531766005076755e-05, 'epoch': 0.14}
{'loss': 0.8326, 'learning_rate': 9.529720492038712e-05, 'epoch': 0.14}
{'loss': 0.7744, 'learning_rate': 9.527670741214152e-05, 'epoch': 0.14}
{'loss': 0.7537, 'learning_rate': 9.525616754520721e-05, 'epoch': 0.14}
 14%|█▍        | 451/3250 [2:08:04<16:22:58, 21.07s/it]                                                        14%|█▍        | 451/3250 [2:08:04<16:22:58, 21.07s/it] 14%|█▍        | 452/3250 [2:08:20<15:09:53, 19.51s/it]                                                        14%|█▍        | 452/3250 [2:08:20<15:09:53, 19.51s/it] 14%|█▍        | 453/3250 [2:08:36<14:18:32, 18.42s/it]                                                        14%|█▍        | 453/3250 [2:08:36<14:18:32, 18.42s/it] 14%|█▍        | 454/3250 [2:08:52<13:42:46, 17.66s/it]                                                        14%|█▍        | 454/3250 [2:08:52<13:42:46, 17.66s/it] 14%|█▍        | 455/3250 [2:09:08<13:17:29, 17.12s/it]                                                        14%|█▍        | 455/3250 [2:09:08<13:17:29, 17.12s/it] 14%|█▍        | 456/3250 [2:09:24<13:01:14, 16.78s/it]                                                        14%|█▍        | {'loss': 0.7451, 'learning_rate': 9.52355853388003e-05, 'epoch': 0.14}
{'loss': 0.7454, 'learning_rate': 9.521496081217651e-05, 'epoch': 0.14}
{'loss': 0.7877, 'learning_rate': 9.519429398463114e-05, 'epoch': 0.14}
{'loss': 0.7538, 'learning_rate': 9.517358487549906e-05, 'epoch': 0.14}
456/3250 [2:09:24<13:01:14, 16.78s/it] 14%|█▍        | 457/3250 [2:09:40<12:52:19, 16.59s/it]                                                        14%|█▍        | 457/3250 [2:09:40<12:52:19, 16.59s/it] 14%|█▍        | 458/3250 [2:09:56<12:46:18, 16.47s/it]                                                        14%|█▍        | 458/3250 [2:09:56<12:46:18, 16.47s/it] 14%|█▍        | 459/3250 [2:10:13<12:44:17, 16.43s/it]                                                        14%|█▍        | 459/3250 [2:10:13<12:44:17, 16.43s/it] 14%|█▍        | 460/3250 [2:10:28<12:35:51, 16.25s/it]                                                        14%|█▍        | 460/3250 [2:10:28<12:35:51, 16.25s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8211565017700195, 'eval_runtime': 2.5792, 'eval_samples_per_second': 4.653, 'eval_steps_per_second': 1.163, 'epoch': 0.14}
                                                        14%|█▍        | 460/3250 [2:10:31<12:35:51, 16.25s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-460
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-460

the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-460
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-460

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-460
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7803, 'learning_rate': 9.51528335041547e-05, 'epoch': 0.14}
{'loss': 1.2252, 'learning_rate': 9.513203989001207e-05, 'epoch': 0.14}
{'loss': 0.7515, 'learning_rate': 9.511120405252464e-05, 'epoch': 0.14}
{'loss': 0.7754, 'learning_rate': 9.509032601118541e-05, 'epoch': 0.14}
{'loss': 0.7965, 'learning_rate': 9.506940578552688e-05, 'epoch': 0.14}
{'loss': 0.7691, 'learning_rate': 9.504844339512095e-05, 'epoch': 0.14}
 14%|█▍        | 461/3250 [2:10:48<13:19:07, 17.19s/it]                                                        14%|█▍        | 461/3250 [2:10:48<13:19:07, 17.19s/it] 14%|█▍        | 462/3250 [2:11:04<13:00:09, 16.79s/it]                                                        14%|█▍        | 462/3250 [2:11:04<13:00:09, 16.79s/it] 14%|█▍        | 463/3250 [2:11:20<12:47:19, 16.52s/it]                                                        14%|█▍        | 463/3250 [2:11:20<12:47:19, 16.52s/it] 14%|█▍        | 464/3250 [2:11:35<12:37:49, 16.32s/it]                                                        14%|█▍        | 464/3250 [2:11:35<12:37:49, 16.32s/it] 14%|█▍        | 465/3250 [2:11:51<12:31:19, 16.19s/it]                                                        14%|█▍        | 465/3250 [2:11:51<12:31:19, 16.19s/it] 14%|█▍        | 466/3250 [2:12:07<12:26:26, 16.09s/it]                                                        14%|█▍        | {'loss': 0.7459, 'learning_rate': 9.502743885957907e-05, 'epoch': 0.14}
{'loss': 0.7536, 'learning_rate': 9.500639219855206e-05, 'epoch': 0.14}
{'loss': 0.8362, 'learning_rate': 9.49853034317301e-05, 'epoch': 0.14}
{'loss': 0.7658, 'learning_rate': 9.496417257884285e-05, 'epoch': 0.14}
466/3250 [2:12:07<12:26:26, 16.09s/it] 14%|█▍        | 467/3250 [2:12:23<12:22:54, 16.02s/it]                                                        14%|█▍        | 467/3250 [2:12:23<12:22:54, 16.02s/it] 14%|█▍        | 468/3250 [2:12:39<12:20:20, 15.97s/it]                                                        14%|█▍        | 468/3250 [2:12:39<12:20:20, 15.97s/it] 14%|█▍        | 469/3250 [2:12:55<12:18:30, 15.93s/it]                                                        14%|█▍        | 469/3250 [2:12:55<12:18:30, 15.93s/it] 14%|█▍        | 470/3250 [2:13:11<12:17:16, 15.91s/it]                                                        14%|█▍        | 470/3250 [2:13:11<12:17:16, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8190761208534241, 'eval_runtime': 2.4742, 'eval_samples_per_second': 4.85, 'eval_steps_per_second': 1.213, 'epoch': 0.14}
                                                        14%|█▍        | 470/3250 [2:13:13<12:17:16, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-470
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-470I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7586, 'learning_rate': 9.494299965965933e-05, 'epoch': 0.14}
{'loss': 0.7082, 'learning_rate': 9.492178469398787e-05, 'epoch': 0.15}
{'loss': 0.7604, 'learning_rate': 9.490052770167617e-05, 'epoch': 0.15}
{'loss': 0.7756, 'learning_rate': 9.487922870261122e-05, 'epoch': 0.15}
{'loss': 0.7682, 'learning_rate': 9.485788771671935e-05, 'epoch': 0.15}
{'loss': 0.736, 'learning_rate': 9.483650476396615e-05, 'epoch': 0.15}
 14%|█▍        | 471/3250 [2:13:30<13:00:16, 16.85s/it]                                                        14%|█▍        | 471/3250 [2:13:30<13:00:16, 16.85s/it] 15%|█▍        | 472/3250 [2:13:45<12:46:27, 16.55s/it]                                                        15%|█▍        | 472/3250 [2:13:45<12:46:27, 16.55s/it] 15%|█▍        | 473/3250 [2:14:02<12:43:36, 16.50s/it]                                                        15%|█▍        | 473/3250 [2:14:02<12:43:36, 16.50s/it] 15%|█▍        | 474/3250 [2:14:18<12:34:39, 16.31s/it]                                                        15%|█▍        | 474/3250 [2:14:18<12:34:39, 16.31s/it] 15%|█▍        | 475/3250 [2:14:34<12:28:17, 16.18s/it]                                                        15%|█▍        | 475/3250 [2:14:34<12:28:17, 16.18s/it] 15%|█▍        | 476/3250 [2:14:50<12:32:51, 16.28s/it]                                                        15%|█▍        | {'loss': 0.7606, 'learning_rate': 9.481507986435647e-05, 'epoch': 0.15}
{'loss': 0.7679, 'learning_rate': 9.47936130379344e-05, 'epoch': 0.15}
{'loss': 0.7739, 'learning_rate': 9.477210430478327e-05, 'epoch': 0.15}
{'loss': 0.7693, 'learning_rate': 9.475055368502559e-05, 'epoch': 0.15}
476/3250 [2:14:50<12:32:51, 16.28s/it] 15%|█▍        | 477/3250 [2:15:06<12:26:50, 16.16s/it]                                                        15%|█▍        | 477/3250 [2:15:06<12:26:50, 16.16s/it] 15%|█▍        | 478/3250 [2:15:22<12:22:46, 16.08s/it]                                                        15%|█▍        | 478/3250 [2:15:22<12:22:46, 16.08s/it] 15%|█▍        | 479/3250 [2:15:38<12:19:41, 16.02s/it]                                                        15%|█▍        | 479/3250 [2:15:38<12:19:41, 16.02s/it] 15%|█▍        | 480/3250 [2:15:54<12:17:22, 15.97s/it]                                                        15%|█▍        | 480/3250 [2:15:54<12:17:22, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8210015892982483, 'eval_runtime': 2.4746, 'eval_samples_per_second': 4.849, 'eval_steps_per_second': 1.212, 'epoch': 0.15}
                                                        15%|█▍        | 480/3250 [2:15:56<12:17:22, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-480
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-480 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-480
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-480

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7442, 'learning_rate': 9.472896119882308e-05, 'epoch': 0.15}
{'loss': 0.7837, 'learning_rate': 9.470732686637664e-05, 'epoch': 0.15}
{'loss': 0.7507, 'learning_rate': 9.468565070792628e-05, 'epoch': 0.15}
{'loss': 0.7681, 'learning_rate': 9.466393274375116e-05, 'epoch': 0.15}
{'loss': 0.7794, 'learning_rate': 9.464217299416956e-05, 'epoch': 0.15}
{'loss': 0.7389, 'learning_rate': 9.462037147953886e-05, 'epoch': 0.15}
 15%|█▍        | 481/3250 [2:16:15<13:39:17, 17.75s/it]                                                        15%|█▍        | 481/3250 [2:16:15<13:39:17, 17.75s/it] 15%|█▍        | 482/3250 [2:16:31<13:13:22, 17.20s/it]                                                        15%|█▍        | 482/3250 [2:16:31<13:13:22, 17.20s/it] 15%|█▍        | 483/3250 [2:16:47<12:55:04, 16.81s/it]                                                        15%|█▍        | 483/3250 [2:16:47<12:55:04, 16.81s/it] 15%|█▍        | 484/3250 [2:17:03<12:42:08, 16.53s/it]                                                        15%|█▍        | 484/3250 [2:17:03<12:42:08, 16.53s/it] 15%|█▍        | 485/3250 [2:17:19<12:33:02, 16.34s/it]                                                        15%|█▍        | 485/3250 [2:17:19<12:33:02, 16.34s/it] 15%|█▍        | 486/3250 [2:17:35<12:26:46, 16.21s/it]                                                        15%|█▍        | {'loss': 0.7325, 'learning_rate': 9.459852822025546e-05, 'epoch': 0.15}
{'loss': 0.7414, 'learning_rate': 9.457664323675489e-05, 'epoch': 0.15}
{'loss': 0.7723, 'learning_rate': 9.455471654951165e-05, 'epoch': 0.15}
{'loss': 0.7441, 'learning_rate': 9.453274817903931e-05, 'epoch': 0.15}
486/3250 [2:17:35<12:26:46, 16.21s/it] 15%|█▍        | 487/3250 [2:17:51<12:22:07, 16.12s/it]                                                        15%|█▍        | 487/3250 [2:17:51<12:22:07, 16.12s/it] 15%|█▌        | 488/3250 [2:18:07<12:18:29, 16.04s/it]                                                        15%|█▌        | 488/3250 [2:18:07<12:18:29, 16.04s/it] 15%|█▌        | 489/3250 [2:18:23<12:16:01, 15.99s/it]                                                        15%|█▌        | 489/3250 [2:18:23<12:16:01, 15.99s/it] 15%|█▌        | 490/3250 [2:18:39<12:15:58, 16.00s/it]                                                        15%|█▌        | 490/3250 [2:18:39<12:15:58, 16.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.814184308052063, 'eval_runtime': 4.2922, 'eval_samples_per_second': 2.796, 'eval_steps_per_second': 0.699, 'epoch': 0.15}
                                                        15%|█▌        | 490/3250 [2:18:43<12:15:58, 16.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-490
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-490/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-490
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-490

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7733, 'learning_rate': 9.45107381458904e-05, 'epoch': 0.15}
{'loss': 1.2474, 'learning_rate': 9.448868647065642e-05, 'epoch': 0.15}
{'loss': 0.6925, 'learning_rate': 9.446659317396787e-05, 'epoch': 0.15}
{'loss': 0.7378, 'learning_rate': 9.444445827649415e-05, 'epoch': 0.15}
{'loss': 0.7743, 'learning_rate': 9.442228179894362e-05, 'epoch': 0.15}
{'loss': 0.7663, 'learning_rate': 9.440006376206349e-05, 'epoch': 0.15}
 15%|█▌        | 491/3250 [2:19:00<13:24:32, 17.50s/it]                                                        15%|█▌        | 491/3250 [2:19:00<13:24:32, 17.50s/it] 15%|█▌        | 492/3250 [2:19:16<13:05:05, 17.08s/it]                                                        15%|█▌        | 492/3250 [2:19:16<13:05:05, 17.08s/it] 15%|█▌        | 493/3250 [2:19:32<12:48:13, 16.72s/it]                                                        15%|█▌        | 493/3250 [2:19:32<12:48:13, 16.72s/it] 15%|█▌        | 494/3250 [2:19:47<12:36:32, 16.47s/it]                                                        15%|█▌        | 494/3250 [2:19:48<12:36:32, 16.47s/it] 15%|█▌        | 495/3250 [2:20:03<12:27:51, 16.29s/it]                                                        15%|█▌        | 495/3250 [2:20:03<12:27:51, 16.29s/it] 15%|█▌        | 496/3250 [2:20:19<12:21:38, 16.16s/it]                                                        15%|█▌        | {'loss': 0.7504, 'learning_rate': 9.437780418663988e-05, 'epoch': 0.15}
{'loss': 0.7396, 'learning_rate': 9.435550309349777e-05, 'epoch': 0.15}
{'loss': 0.8043, 'learning_rate': 9.433316050350099e-05, 'epoch': 0.15}
{'loss': 0.7783, 'learning_rate': 9.431077643755217e-05, 'epoch': 0.15}
496/3250 [2:20:19<12:21:38, 16.16s/it] 15%|█▌        | 497/3250 [2:20:35<12:17:10, 16.07s/it]                                                        15%|█▌        | 497/3250 [2:20:35<12:17:10, 16.07s/it] 15%|█▌        | 498/3250 [2:20:51<12:16:20, 16.05s/it]                                                        15%|█▌        | 498/3250 [2:20:51<12:16:20, 16.05s/it] 15%|█▌        | 499/3250 [2:21:07<12:13:27, 16.00s/it]                                                        15%|█▌        | 499/3250 [2:21:07<12:13:27, 16.00s/it] 15%|█▌        | 500/3250 [2:21:23<12:11:29, 15.96s/it]                                                        15%|█▌        | 500/3250 [2:21:23<12:11:29, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8111589550971985, 'eval_runtime': 2.4928, 'eval_samples_per_second': 4.814, 'eval_steps_per_second': 1.203, 'epoch': 0.15}
                                                        15%|█▌        | 500/3250 [2:21:25<12:11:29, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-500
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-500
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7346, 'learning_rate': 9.428835091659276e-05, 'epoch': 0.15}
{'loss': 0.7004, 'learning_rate': 9.426588396160299e-05, 'epoch': 0.15}
{'loss': 0.7657, 'learning_rate': 9.424337559360183e-05, 'epoch': 0.15}
{'loss': 0.7381, 'learning_rate': 9.422082583364706e-05, 'epoch': 0.16}
{'loss': 0.7548, 'learning_rate': 9.419823470283511e-05, 'epoch': 0.16}
{'loss': 0.7401, 'learning_rate': 9.417560222230115e-05, 'epoch': 0.16}
 15%|█▌        | 501/3250 [2:22:00<17:06:15, 22.40s/it]                                                        15%|█▌        | 501/3250 [2:22:00<17:06:15, 22.40s/it] 15%|█▌        | 502/3250 [2:22:16<15:36:15, 20.44s/it]                                                        15%|█▌        | 502/3250 [2:22:16<15:36:15, 20.44s/it] 15%|█▌        | 503/3250 [2:22:32<14:33:18, 19.07s/it]                                                        15%|█▌        | 503/3250 [2:22:32<14:33:18, 19.07s/it] 16%|█▌        | 504/3250 [2:22:48<13:55:16, 18.25s/it]                                                        16%|█▌        | 504/3250 [2:22:48<13:55:16, 18.25s/it] 16%|█▌        | 505/3250 [2:23:04<13:22:27, 17.54s/it]                                                        16%|█▌        | 505/3250 [2:23:04<13:22:27, 17.54s/it] 16%|█▌        | 506/3250 [2:23:20<12:59:29, 17.04s/it]                                                        16%|█▌        | {'loss': 0.7392, 'learning_rate': 9.415292841321903e-05, 'epoch': 0.16}
{'loss': 0.7471, 'learning_rate': 9.413021329680128e-05, 'epoch': 0.16}
{'loss': 0.7584, 'learning_rate': 9.4107456894299e-05, 'epoch': 0.16}
{'loss': 0.7583, 'learning_rate': 9.408465922700206e-05, 'epoch': 0.16}
506/3250 [2:23:20<12:59:29, 17.04s/it] 16%|█▌        | 507/3250 [2:23:36<12:43:13, 16.69s/it]                                                        16%|█▌        | 507/3250 [2:23:36<12:43:13, 16.69s/it] 16%|█▌        | 508/3250 [2:23:52<12:31:57, 16.45s/it]                                                        16%|█▌        | 508/3250 [2:23:52<12:31:57, 16.45s/it] 16%|█▌        | 509/3250 [2:24:08<12:29:21, 16.40s/it]                                                        16%|█▌        | 509/3250 [2:24:08<12:29:21, 16.40s/it] 16%|█▌        | 510/3250 [2:24:24<12:21:59, 16.25s/it]                                                        16%|█▌        | 510/3250 [2:24:24<12:21:59, 16.25s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8126214742660522, 'eval_runtime': 2.481, 'eval_samples_per_second': 4.837, 'eval_steps_per_second': 1.209, 'epoch': 0.16}
                                                        16%|█▌        | 510/3250 [2:24:27<12:21:59, 16.25s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-510
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-510/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-510

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-510

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.744, 'learning_rate': 9.40618203162388e-05, 'epoch': 0.16}
{'loss': 0.7337, 'learning_rate': 9.403894018337622e-05, 'epoch': 0.16}
{'loss': 0.7766, 'learning_rate': 9.401601884981983e-05, 'epoch': 0.16}
{'loss': 0.7168, 'learning_rate': 9.399305633701373e-05, 'epoch': 0.16}
{'loss': 0.8008, 'learning_rate': 9.397005266644054e-05, 'epoch': 0.16}
{'loss': 0.7315, 'learning_rate': 9.394700785962139e-05, 'epoch': 0.16}
 16%|█▌        | 511/3250 [2:24:43<13:00:22, 17.09s/it]                                                        16%|█▌        | 511/3250 [2:24:43<13:00:22, 17.09s/it] 16%|█▌        | 512/3250 [2:24:59<12:43:41, 16.74s/it]                                                        16%|█▌        | 512/3250 [2:24:59<12:43:41, 16.74s/it] 16%|█▌        | 513/3250 [2:25:15<12:31:50, 16.48s/it]                                                        16%|█▌        | 513/3250 [2:25:15<12:31:50, 16.48s/it] 16%|█▌        | 514/3250 [2:25:31<12:23:29, 16.30s/it]                                                        16%|█▌        | 514/3250 [2:25:31<12:23:29, 16.30s/it] 16%|█▌        | 515/3250 [2:25:47<12:17:32, 16.18s/it]                                                        16%|█▌        | 515/3250 [2:25:47<12:17:32, 16.18s/it] 16%|█▌        | 516/3250 [2:26:03<12:13:24, 16.10s/it]                                                        16%|█▌        | {'loss': 0.7296, 'learning_rate': 9.392392193811584e-05, 'epoch': 0.16}
{'loss': 0.7327, 'learning_rate': 9.390079492352199e-05, 'epoch': 0.16}
{'loss': 0.7352, 'learning_rate': 9.387762683747636e-05, 'epoch': 0.16}
{'loss': 0.7356, 'learning_rate': 9.385441770165385e-05, 'epoch': 0.16}
516/3250 [2:26:03<12:13:24, 16.10s/it] 16%|█▌        | 517/3250 [2:26:18<12:10:20, 16.03s/it]                                                        16%|█▌        | 517/3250 [2:26:18<12:10:20, 16.03s/it] 16%|█▌        | 518/3250 [2:26:34<12:08:07, 15.99s/it]                                                        16%|█▌        | 518/3250 [2:26:34<12:08:07, 15.99s/it] 16%|█▌        | 519/3250 [2:26:50<12:06:20, 15.96s/it]                                                        16%|█▌        | 519/3250 [2:26:50<12:06:20, 15.96s/it] 16%|█▌        | 520/3250 [2:27:06<12:05:12, 15.94s/it]                                                        16%|█▌        | 520/3250 [2:27:06<12:05:12, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8095623850822449, 'eval_runtime': 2.4729, 'eval_samples_per_second': 4.853, 'eval_steps_per_second': 1.213, 'epoch': 0.16}
                                                        16%|█▌        | 520/3250 [2:27:09<12:05:12, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-520
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-520/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-520


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-520

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-520/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.745, 'learning_rate': 9.383116753776784e-05, 'epoch': 0.16}
{'loss': 0.7436, 'learning_rate': 9.380787636757002e-05, 'epoch': 0.16}
{'loss': 1.1946, 'learning_rate': 9.378454421285049e-05, 'epoch': 0.16}
{'loss': 0.7236, 'learning_rate': 9.376117109543769e-05, 'epoch': 0.16}
{'loss': 0.7654, 'learning_rate': 9.373775703719836e-05, 'epoch': 0.16}
{'loss': 0.7398, 'learning_rate': 9.371430206003758e-05, 'epoch': 0.16}
 16%|█▌        | 521/3250 [2:27:25<12:46:55, 16.86s/it]                                                        16%|█▌        | 521/3250 [2:27:25<12:46:55, 16.86s/it] 16%|█▌        | 522/3250 [2:27:41<12:33:23, 16.57s/it]                                                        16%|█▌        | 522/3250 [2:27:41<12:33:23, 16.57s/it] 16%|█▌        | 523/3250 [2:27:57<12:23:32, 16.36s/it]                                                        16%|█▌        | 523/3250 [2:27:57<12:23:32, 16.36s/it] 16%|█▌        | 524/3250 [2:28:13<12:16:42, 16.22s/it]                                                        16%|█▌        | 524/3250 [2:28:13<12:16:42, 16.22s/it] 16%|█▌        | 525/3250 [2:28:29<12:18:42, 16.26s/it]                                                        16%|█▌        | 525/3250 [2:28:29<12:18:42, 16.26s/it] 16%|█▌        | 526/3250 [2:28:45<12:13:07, 16.15s/it]                                                        16%|█▌        | {'loss': 0.7434, 'learning_rate': 9.369080618589864e-05, 'epoch': 0.16}
{'loss': 0.7162, 'learning_rate': 9.366726943676321e-05, 'epoch': 0.16}
{'loss': 0.7872, 'learning_rate': 9.364369183465106e-05, 'epoch': 0.16}
{'loss': 0.7626, 'learning_rate': 9.362007340162029e-05, 'epoch': 0.16}
526/3250 [2:28:45<12:13:07, 16.15s/it] 16%|█▌        | 527/3250 [2:29:01<12:09:18, 16.07s/it]                                                        16%|█▌        | 527/3250 [2:29:01<12:09:18, 16.07s/it] 16%|█▌        | 528/3250 [2:29:17<12:06:28, 16.01s/it]                                                        16%|█▌        | 528/3250 [2:29:17<12:06:28, 16.01s/it] 16%|█▋        | 529/3250 [2:29:33<12:04:45, 15.98s/it]                                                        16%|█▋        | 529/3250 [2:29:33<12:04:45, 15.98s/it] 16%|█▋        | 530/3250 [2:29:49<12:03:10, 15.95s/it]                                                        16%|█▋        | 530/3250 [2:29:49<12:03:10, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8065019845962524, 'eval_runtime': 2.4821, 'eval_samples_per_second': 4.835, 'eval_steps_per_second': 1.209, 'epoch': 0.16}
                                                        16%|█▋        | 530/3250 [2:29:51<12:03:10, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-530
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-530
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-530the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-530

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7246, 'learning_rate': 9.359641415976714e-05, 'epoch': 0.16}
{'loss': 0.7242, 'learning_rate': 9.357271413122606e-05, 'epoch': 0.16}
{'loss': 0.74, 'learning_rate': 9.354897333816963e-05, 'epoch': 0.16}
{'loss': 0.7191, 'learning_rate': 9.35251918028086e-05, 'epoch': 0.16}
{'loss': 0.7353, 'learning_rate': 9.350136954739183e-05, 'epoch': 0.16}
{'loss': 0.7467, 'learning_rate': 9.347750659420623e-05, 'epoch': 0.16}
 16%|█▋        | 531/3250 [2:30:21<15:52:09, 21.01s/it]                                                        16%|█▋        | 531/3250 [2:30:21<15:52:09, 21.01s/it] 16%|█▋        | 532/3250 [2:30:37<14:42:22, 19.48s/it]                                                        16%|█▋        | 532/3250 [2:30:37<14:42:22, 19.48s/it] 16%|█▋        | 533/3250 [2:30:53<13:53:17, 18.40s/it]                                                        16%|█▋        | 533/3250 [2:30:53<13:53:17, 18.40s/it] 16%|█▋        | 534/3250 [2:31:09<13:18:24, 17.64s/it]                                                        16%|█▋        | 534/3250 [2:31:09<13:18:24, 17.64s/it] 16%|█▋        | 535/3250 [2:31:25<12:54:09, 17.11s/it]                                                        16%|█▋        | 535/3250 [2:31:25<12:54:09, 17.11s/it] 16%|█▋        | 536/3250 [2:31:41<12:36:57, 16.73s/it]                                                        16%|█▋        | {'loss': 0.726, 'learning_rate': 9.345360296557684e-05, 'epoch': 0.17}
{'loss': 0.7306, 'learning_rate': 9.342965868386674e-05, 'epoch': 0.17}
{'loss': 0.7575, 'learning_rate': 9.340567377147702e-05, 'epoch': 0.17}
{'loss': 0.7241, 'learning_rate': 9.338164825084682e-05, 'epoch': 0.17}
536/3250 [2:31:41<12:36:57, 16.73s/it] 17%|█▋        | 537/3250 [2:31:57<12:24:43, 16.47s/it]                                                        17%|█▋        | 537/3250 [2:31:57<12:24:43, 16.47s/it] 17%|█▋        | 538/3250 [2:32:13<12:16:08, 16.29s/it]                                                        17%|█▋        | 538/3250 [2:32:13<12:16:08, 16.29s/it] 17%|█▋        | 539/3250 [2:32:28<12:10:11, 16.16s/it]                                                        17%|█▋        | 539/3250 [2:32:28<12:10:11, 16.16s/it] 17%|█▋        | 540/3250 [2:32:44<12:05:34, 16.06s/it]                                                        17%|█▋        | 540/3250 [2:32:44<12:05:34, 16.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8042236566543579, 'eval_runtime': 2.4743, 'eval_samples_per_second': 4.85, 'eval_steps_per_second': 1.212, 'epoch': 0.17}
                                                        17%|█▋        | 540/3250 [2:32:47<12:05:34, 16.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-540
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-540the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-540
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-540

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7342, 'learning_rate': 9.335758214445324e-05, 'epoch': 0.17}
{'loss': 0.741, 'learning_rate': 9.333347547481135e-05, 'epoch': 0.17}
{'loss': 0.7532, 'learning_rate': 9.330932826447421e-05, 'epoch': 0.17}
{'loss': 0.6913, 'learning_rate': 9.328514053603272e-05, 'epoch': 0.17}
{'loss': 0.7779, 'learning_rate': 9.326091231211582e-05, 'epoch': 0.17}
{'loss': 0.7344, 'learning_rate': 9.323664361539019e-05, 'epoch': 0.17}
 17%|█▋        | 541/3250 [2:33:18<16:08:14, 21.44s/it]                                                        17%|█▋        | 541/3250 [2:33:18<16:08:14, 21.44s/it] 17%|█▋        | 542/3250 [2:33:34<14:52:38, 19.78s/it]                                                        17%|█▋        | 542/3250 [2:33:34<14:52:38, 19.78s/it] 17%|█▋        | 543/3250 [2:33:50<13:59:42, 18.61s/it]                                                        17%|█▋        | 543/3250 [2:33:50<13:59:42, 18.61s/it] 17%|█▋        | 544/3250 [2:34:06<13:22:32, 17.79s/it]                                                        17%|█▋        | 544/3250 [2:34:06<13:22:32, 17.79s/it] 17%|█▋        | 545/3250 [2:34:22<12:56:27, 17.22s/it]                                                        17%|█▋        | 545/3250 [2:34:22<12:56:27, 17.22s/it] 17%|█▋        | 546/3250 [2:34:38<12:38:04, 16.82s/it]                                                        17%|█▋        | {'loss': 0.7143, 'learning_rate': 9.32123344685605e-05, 'epoch': 0.17}
{'loss': 0.7133, 'learning_rate': 9.318798489436917e-05, 'epoch': 0.17}
{'loss': 0.6962, 'learning_rate': 9.31635949155965e-05, 'epoch': 0.17}
{'loss': 0.7453, 'learning_rate': 9.313916455506055e-05, 'epoch': 0.17}
546/3250 [2:34:38<12:38:04, 16.82s/it] 17%|█▋        | 547/3250 [2:34:54<12:25:05, 16.54s/it]                                                        17%|█▋        | 547/3250 [2:34:54<12:25:05, 16.54s/it] 17%|█▋        | 548/3250 [2:35:09<12:15:46, 16.34s/it]                                                        17%|█▋        | 548/3250 [2:35:09<12:15:46, 16.34s/it] 17%|█▋        | 549/3250 [2:35:25<12:08:57, 16.19s/it]                                                        17%|█▋        | 549/3250 [2:35:25<12:08:57, 16.19s/it] 17%|█▋        | 550/3250 [2:35:41<12:04:32, 16.10s/it]                                                        17%|█▋        | 550/3250 [2:35:41<12:04:32, 16.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8036959767341614, 'eval_runtime': 2.4889, 'eval_samples_per_second': 4.821, 'eval_steps_per_second': 1.205, 'epoch': 0.17}
                                                        17%|█▋        | 550/3250 [2:35:44<12:04:32, 16.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-550
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-550 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-550the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-550

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.707, 'learning_rate': 9.311469383561719e-05, 'epoch': 0.17}
{'loss': 0.7434, 'learning_rate': 9.309018278016004e-05, 'epoch': 0.17}
{'loss': 1.1937, 'learning_rate': 9.306563141162046e-05, 'epoch': 0.17}
{'loss': 0.7076, 'learning_rate': 9.30410397529675e-05, 'epoch': 0.17}
{'loss': 0.7317, 'learning_rate': 9.301640782720792e-05, 'epoch': 0.17}
{'loss': 0.7458, 'learning_rate': 9.299173565738617e-05, 'epoch': 0.17}
 17%|█▋        | 551/3250 [2:36:00<12:44:21, 16.99s/it]                                                        17%|█▋        | 551/3250 [2:36:00<12:44:21, 16.99s/it] 17%|█▋        | 552/3250 [2:36:16<12:29:12, 16.66s/it]                                                        17%|█▋        | 552/3250 [2:36:16<12:29:12, 16.66s/it] 17%|█▋        | 553/3250 [2:36:32<12:18:08, 16.42s/it]                                                        17%|█▋        | 553/3250 [2:36:32<12:18:08, 16.42s/it] 17%|█▋        | 554/3250 [2:36:48<12:10:42, 16.26s/it]                                                        17%|█▋        | 554/3250 [2:36:48<12:10:42, 16.26s/it] 17%|█▋        | 555/3250 [2:37:04<12:05:32, 16.15s/it]                                                        17%|█▋        | 555/3250 [2:37:04<12:05:32, 16.15s/it] 17%|█▋        | 556/3250 [2:37:20<12:01:42, 16.07s/it]                                                        17%|█▋        | {'loss': 0.7357, 'learning_rate': 9.296702326658433e-05, 'epoch': 0.17}
{'loss': 0.6885, 'learning_rate': 9.294227067792211e-05, 'epoch': 0.17}
{'loss': 0.7089, 'learning_rate': 9.291747791455682e-05, 'epoch': 0.17}
{'loss': 0.7985, 'learning_rate': 9.289264499968339e-05, 'epoch': 0.17}
556/3250 [2:37:20<12:01:42, 16.07s/it] 17%|█▋        | 557/3250 [2:37:36<11:59:04, 16.02s/it]                                                        17%|█▋        | 557/3250 [2:37:36<11:59:04, 16.02s/it] 17%|█▋        | 558/3250 [2:37:52<12:03:36, 16.13s/it]                                                        17%|█▋        | 558/3250 [2:37:52<12:03:36, 16.13s/it] 17%|█▋        | 559/3250 [2:38:08<12:00:20, 16.06s/it]                                                        17%|█▋        | 559/3250 [2:38:08<12:00:20, 16.06s/it] 17%|█▋        | 560/3250 [2:38:24<11:57:37, 16.01s/it]                                                        17%|█▋        | 560/3250 [2:38:24<11:57:37, 16.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7996041178703308, 'eval_runtime': 2.4854, 'eval_samples_per_second': 4.828, 'eval_steps_per_second': 1.207, 'epoch': 0.17}
                                                        17%|█▋        | 560/3250 [2:38:26<11:57:37, 16.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-560
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-560
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-560
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-560

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7266, 'learning_rate': 9.286777195653426e-05, 'epoch': 0.17}
{'loss': 0.7288, 'learning_rate': 9.284285880837946e-05, 'epoch': 0.17}
{'loss': 0.6885, 'learning_rate': 9.281790557852652e-05, 'epoch': 0.17}
{'loss': 0.7159, 'learning_rate': 9.279291229032048e-05, 'epoch': 0.17}
{'loss': 0.7302, 'learning_rate': 9.276787896714382e-05, 'epoch': 0.17}
{'loss': 0.7239, 'learning_rate': 9.27428056324165e-05, 'epoch': 0.17}
 17%|█▋        | 561/3250 [2:38:57<15:50:59, 21.22s/it]                                                        17%|█▋        | 561/3250 [2:38:57<15:50:59, 21.22s/it] 17%|█▋        | 562/3250 [2:39:13<14:38:57, 19.62s/it]                                                        17%|█▋        | 562/3250 [2:39:13<14:38:57, 19.62s/it] 17%|█▋        | 563/3250 [2:39:29<13:48:33, 18.50s/it]                                                        17%|█▋        | 563/3250 [2:39:29<13:48:33, 18.50s/it] 17%|█▋        | 564/3250 [2:39:45<13:13:04, 17.72s/it]                                                        17%|█▋        | 564/3250 [2:39:45<13:13:04, 17.72s/it] 17%|█▋        | 565/3250 [2:40:01<12:48:15, 17.17s/it]                                                        17%|█▋        | 565/3250 [2:40:01<12:48:15, 17.17s/it] 17%|█▋        | 566/3250 [2:40:17<12:30:54, 16.79s/it]                                                        17%|█▋        | {'loss': 0.6954, 'learning_rate': 9.271769230959596e-05, 'epoch': 0.17}
{'loss': 0.7195, 'learning_rate': 9.269253902217696e-05, 'epoch': 0.17}
{'loss': 0.7352, 'learning_rate': 9.266734579369172e-05, 'epoch': 0.18}
{'loss': 0.7386, 'learning_rate': 9.264211264770976e-05, 'epoch': 0.18}
566/3250 [2:40:17<12:30:54, 16.79s/it] 17%|█▋        | 567/3250 [2:40:32<12:18:18, 16.51s/it]                                                        17%|█▋        | 567/3250 [2:40:32<12:18:18, 16.51s/it] 17%|█▋        | 568/3250 [2:40:48<12:09:29, 16.32s/it]                                                        17%|█▋        | 568/3250 [2:40:48<12:09:29, 16.32s/it] 18%|█▊        | 569/3250 [2:41:04<12:03:19, 16.19s/it]                                                        18%|█▊        | 569/3250 [2:41:04<12:03:19, 16.19s/it] 18%|█▊        | 570/3250 [2:41:20<11:58:34, 16.09s/it]                                                        18%|█▊        | 570/3250 [2:41:20<11:58:34, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7977831959724426, 'eval_runtime': 2.464, 'eval_samples_per_second': 4.87, 'eval_steps_per_second': 1.218, 'epoch': 0.18}
                                                        18%|█▊        | 570/3250 [2:41:22<11:58:34, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-570
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-570
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-570
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-570

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7369, 'learning_rate': 9.261683960783804e-05, 'epoch': 0.18}
{'loss': 0.7053, 'learning_rate': 9.259152669772078e-05, 'epoch': 0.18}
{'loss': 0.7501, 'learning_rate': 9.256617394103946e-05, 'epoch': 0.18}
{'loss': 0.713, 'learning_rate': 9.254078136151295e-05, 'epoch': 0.18}
{'loss': 0.7285, 'learning_rate': 9.251534898289726e-05, 'epoch': 0.18}
{'loss': 0.7275, 'learning_rate': 9.248987682898575e-05, 'epoch': 0.18}
 18%|█▊        | 571/3250 [2:41:39<12:37:04, 16.96s/it]                                                        18%|█▊        | 571/3250 [2:41:39<12:37:04, 16.96s/it] 18%|█▊        | 572/3250 [2:41:55<12:21:58, 16.62s/it]                                                        18%|█▊        | 572/3250 [2:41:55<12:21:58, 16.62s/it] 18%|█▊        | 573/3250 [2:42:11<12:11:26, 16.39s/it]                                                        18%|█▊        | 573/3250 [2:42:11<12:11:26, 16.39s/it] 18%|█▊        | 574/3250 [2:42:27<12:09:14, 16.35s/it]                                                        18%|█▊        | 574/3250 [2:42:27<12:09:14, 16.35s/it] 18%|█▊        | 575/3250 [2:42:43<12:02:25, 16.20s/it]                                                        18%|█▊        | 575/3250 [2:42:43<12:02:25, 16.20s/it] 18%|█▊        | 576/3250 [2:42:59<11:57:31, 16.10s/it]                                                        18%|█▊        | {'loss': 0.6966, 'learning_rate': 9.246436492360888e-05, 'epoch': 0.18}
{'loss': 0.6986, 'learning_rate': 9.243881329063435e-05, 'epoch': 0.18}
{'loss': 0.7019, 'learning_rate': 9.241322195396707e-05, 'epoch': 0.18}
{'loss': 0.7391, 'learning_rate': 9.2387590937549e-05, 'epoch': 0.18}
576/3250 [2:42:59<11:57:31, 16.10s/it] 18%|█▊        | 577/3250 [2:43:15<11:54:13, 16.03s/it]                                                        18%|█▊        | 577/3250 [2:43:15<11:54:13, 16.03s/it] 18%|█▊        | 578/3250 [2:43:30<11:51:32, 15.98s/it]                                                        18%|█▊        | 578/3250 [2:43:30<11:51:32, 15.98s/it] 18%|█▊        | 579/3250 [2:43:46<11:49:58, 15.95s/it]                                                        18%|█▊        | 579/3250 [2:43:46<11:49:58, 15.95s/it] 18%|█▊        | 580/3250 [2:44:02<11:48:38, 15.92s/it]                                                        18%|█▊        | 580/3250 [2:44:02<11:48:38, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7960510849952698, 'eval_runtime': 2.695, 'eval_samples_per_second': 4.453, 'eval_steps_per_second': 1.113, 'epoch': 0.18}
                                                        18%|█▊        | 580/3250 [2:44:05<11:48:38, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-580
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-580/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-580
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-580

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7, 'learning_rate': 9.23619202653593e-05, 'epoch': 0.18}
{'loss': 0.7416, 'learning_rate': 9.233620996141421e-05, 'epoch': 0.18}
{'loss': 1.2158, 'learning_rate': 9.231046004976704e-05, 'epoch': 0.18}
{'loss': 0.6605, 'learning_rate': 9.228467055450813e-05, 'epoch': 0.18}
{'loss': 0.6927, 'learning_rate': 9.225884149976493e-05, 'epoch': 0.18}
{'loss': 0.7347, 'learning_rate': 9.22329729097018e-05, 'epoch': 0.18}
 18%|█▊        | 581/3250 [2:44:21<12:33:18, 16.93s/it]                                                        18%|█▊        | 581/3250 [2:44:21<12:33:18, 16.93s/it] 18%|█▊        | 582/3250 [2:44:37<12:18:57, 16.62s/it]                                                        18%|█▊        | 582/3250 [2:44:37<12:18:57, 16.62s/it] 18%|█▊        | 583/3250 [2:44:53<12:08:26, 16.39s/it]                                                        18%|█▊        | 583/3250 [2:44:53<12:08:26, 16.39s/it] 18%|█▊        | 584/3250 [2:45:09<12:01:01, 16.23s/it]                                                        18%|█▊        | 584/3250 [2:45:09<12:01:01, 16.23s/it] 18%|█▊        | 585/3250 [2:45:25<11:56:04, 16.12s/it]                                                        18%|█▊        | 585/3250 [2:45:25<11:56:04, 16.12s/it] 18%|█▊        | 586/3250 [2:45:41<11:52:29, 16.05s/it]                                                        18%|█▊        | {'loss': 0.7332, 'learning_rate': 9.220706480852016e-05, 'epoch': 0.18}
{'loss': 0.695, 'learning_rate': 9.218111722045837e-05, 'epoch': 0.18}
{'loss': 0.7142, 'learning_rate': 9.215513016979172e-05, 'epoch': 0.18}
{'loss': 0.762, 'learning_rate': 9.212910368083245e-05, 'epoch': 0.18}
586/3250 [2:45:41<11:52:29, 16.05s/it] 18%|█▊        | 587/3250 [2:45:57<11:49:42, 15.99s/it]                                                        18%|█▊        | 587/3250 [2:45:57<11:49:42, 15.99s/it] 18%|█▊        | 588/3250 [2:46:13<11:47:52, 15.96s/it]                                                        18%|█▊        | 588/3250 [2:46:13<11:47:52, 15.96s/it] 18%|█▊        | 589/3250 [2:46:28<11:46:31, 15.93s/it]                                                        18%|█▊        | 589/3250 [2:46:28<11:46:31, 15.93s/it] 18%|█▊        | 590/3250 [2:46:44<11:45:40, 15.92s/it]                                                        18%|█▊        | 590/3250 [2:46:44<11:45:40, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7946682572364807, 'eval_runtime': 2.488, 'eval_samples_per_second': 4.823, 'eval_steps_per_second': 1.206, 'epoch': 0.18}
                                                        18%|█▊        | 590/3250 [2:46:47<11:45:40, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-590
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-590
 I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-590

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-590

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7309, 'learning_rate': 9.210303777792968e-05, 'epoch': 0.18}
{'loss': 0.7038, 'learning_rate': 9.20769324854694e-05, 'epoch': 0.18}
{'loss': 0.6661, 'learning_rate': 9.205078782787445e-05, 'epoch': 0.18}
{'loss': 0.727, 'learning_rate': 9.202460382960448e-05, 'epoch': 0.18}
{'loss': 0.7111, 'learning_rate': 9.1998380515156e-05, 'epoch': 0.18}
{'loss': 0.7176, 'learning_rate': 9.197211790906227e-05, 'epoch': 0.18}
 18%|█▊        | 591/3250 [2:47:22<16:36:56, 22.50s/it]                                                        18%|█▊        | 591/3250 [2:47:22<16:36:56, 22.50s/it] 18%|█▊        | 592/3250 [2:47:38<15:09:16, 20.53s/it]                                                        18%|█▊        | 592/3250 [2:47:38<15:09:16, 20.53s/it] 18%|█▊        | 593/3250 [2:47:54<14:07:52, 19.15s/it]                                                        18%|█▊        | 593/3250 [2:47:54<14:07:52, 19.15s/it] 18%|█▊        | 594/3250 [2:48:10<13:24:43, 18.18s/it]                                                        18%|█▊        | 594/3250 [2:48:10<13:24:43, 18.18s/it] 18%|█▊        | 595/3250 [2:48:26<12:54:32, 17.50s/it]                                                        18%|█▊        | 595/3250 [2:48:26<12:54:32, 17.50s/it] 18%|█▊        | 596/3250 [2:48:42<12:33:17, 17.03s/it]                                                        18%|█▊        | {'loss': 0.7176, 'learning_rate': 9.194581603589328e-05, 'epoch': 0.18}
{'loss': 0.7041, 'learning_rate': 9.191947492025582e-05, 'epoch': 0.18}
{'loss': 0.7092, 'learning_rate': 9.189309458679331e-05, 'epoch': 0.18}
{'loss': 0.7294, 'learning_rate': 9.186667506018596e-05, 'epoch': 0.18}
596/3250 [2:48:42<12:33:17, 17.03s/it] 18%|█▊        | 597/3250 [2:48:58<12:18:22, 16.70s/it]                                                        18%|█▊        | 597/3250 [2:48:58<12:18:22, 16.70s/it] 18%|█▊        | 598/3250 [2:49:14<12:07:55, 16.47s/it]                                                        18%|█▊        | 598/3250 [2:49:14<12:07:55, 16.47s/it] 18%|█▊        | 599/3250 [2:49:30<12:00:17, 16.30s/it]                                                        18%|█▊        | 599/3250 [2:49:30<12:00:17, 16.30s/it] 18%|█▊        | 600/3250 [2:49:45<11:54:56, 16.19s/it]                                                        18%|█▊        | 600/3250 [2:49:45<11:54:56, 16.19s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7925639152526855, 'eval_runtime': 2.4932, 'eval_samples_per_second': 4.813, 'eval_steps_per_second': 1.203, 'epoch': 0.18}
                                                        18%|█▊        | 600/3250 [2:49:48<11:54:56, 16.19s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-600
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-600/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7292, 'learning_rate': 9.184021636515058e-05, 'epoch': 0.18}
{'loss': 0.722, 'learning_rate': 9.181371852644063e-05, 'epoch': 0.19}
{'loss': 0.7046, 'learning_rate': 9.178718156884621e-05, 'epoch': 0.19}
{'loss': 0.7372, 'learning_rate': 9.1760605517194e-05, 'epoch': 0.19}
{'loss': 0.6776, 'learning_rate': 9.173399039634729e-05, 'epoch': 0.19}
{'loss': 0.7637, 'learning_rate': 9.170733623120585e-05, 'epoch': 0.19}
 18%|█▊        | 601/3250 [2:50:05<12:32:56, 17.05s/it]                                                        18%|█▊        | 601/3250 [2:50:05<12:32:56, 17.05s/it] 19%|█▊        | 602/3250 [2:50:20<12:17:58, 16.72s/it]                                                        19%|█▊        | 602/3250 [2:50:20<12:17:58, 16.72s/it] 19%|█▊        | 603/3250 [2:50:36<12:07:16, 16.49s/it]                                                        19%|█▊        | 603/3250 [2:50:36<12:07:16, 16.49s/it] 19%|█▊        | 604/3250 [2:50:52<11:59:36, 16.32s/it]                                                        19%|█▊        | 604/3250 [2:50:52<11:59:36, 16.32s/it] 19%|█▊        | 605/3250 [2:51:08<11:54:02, 16.20s/it]                                                        19%|█▊        | 605/3250 [2:51:08<11:54:02, 16.20s/it] 19%|█▊        | 606/3250 [2:51:24<11:50:16, 16.12s/it]                                                        19%|█▊        | {'loss': 0.679, 'learning_rate': 9.168064304670606e-05, 'epoch': 0.19}
{'loss': 0.6968, 'learning_rate': 9.165391086782074e-05, 'epoch': 0.19}
{'loss': 0.7014, 'learning_rate': 9.162713971955925e-05, 'epoch': 0.19}
{'loss': 0.6831, 'learning_rate': 9.160032962696734e-05, 'epoch': 0.19}
606/3250 [2:51:24<11:50:16, 16.12s/it] 19%|█▊        | 607/3250 [2:51:41<11:56:08, 16.26s/it]                                                        19%|█▊        | 607/3250 [2:51:41<11:56:08, 16.26s/it] 19%|█▊        | 608/3250 [2:51:57<11:51:31, 16.16s/it]                                                        19%|█▊        | 608/3250 [2:51:57<11:51:31, 16.16s/it] 19%|█▊        | 609/3250 [2:52:13<11:48:21, 16.09s/it]                                                        19%|█▊        | 609/3250 [2:52:13<11:48:21, 16.09s/it] 19%|█▉        | 610/3250 [2:52:29<11:45:55, 16.04s/it]                                                        19%|█▉        | 610/3250 [2:52:29<11:45:55, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7935246229171753, 'eval_runtime': 2.4691, 'eval_samples_per_second': 4.86, 'eval_steps_per_second': 1.215, 'epoch': 0.19}
                                                        19%|█▉        | 610/3250 [2:52:31<11:45:55, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-610
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-610
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-610
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-610

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7052, 'learning_rate': 9.157348061512727e-05, 'epoch': 0.19}
{'loss': 0.7201, 'learning_rate': 9.154659270915764e-05, 'epoch': 0.19}
{'loss': 0.6998, 'learning_rate': 9.151966593421347e-05, 'epoch': 0.19}
{'loss': 1.1598, 'learning_rate': 9.149270031548617e-05, 'epoch': 0.19}
{'loss': 0.6869, 'learning_rate': 9.146569587820344e-05, 'epoch': 0.19}
{'loss': 0.7262, 'learning_rate': 9.143865264762931e-05, 'epoch': 0.19}
 19%|█▉        | 611/3250 [2:53:01<15:20:42, 20.93s/it]                                                        19%|█▉        | 611/3250 [2:53:01<15:20:42, 20.93s/it] 19%|█▉        | 612/3250 [2:53:17<14:14:17, 19.43s/it]                                                        19%|█▉        | 612/3250 [2:53:17<14:14:17, 19.43s/it] 19%|█▉        | 613/3250 [2:53:33<13:27:50, 18.38s/it]                                                        19%|█▉        | 613/3250 [2:53:33<13:27:50, 18.38s/it] 19%|█▉        | 614/3250 [2:53:49<12:54:49, 17.64s/it]                                                        19%|█▉        | 614/3250 [2:53:49<12:54:49, 17.64s/it] 19%|█▉        | 615/3250 [2:54:05<12:32:05, 17.13s/it]                                                        19%|█▉        | 615/3250 [2:54:05<12:32:05, 17.13s/it] 19%|█▉        | 616/3250 [2:54:21<12:16:08, 16.77s/it]                                                        19%|█▉        | {'loss': 0.6944, 'learning_rate': 9.141157064906414e-05, 'epoch': 0.19}
{'loss': 0.7146, 'learning_rate': 9.138444990784453e-05, 'epoch': 0.19}
{'loss': 0.6815, 'learning_rate': 9.135729044934331e-05, 'epoch': 0.19}
{'loss': 0.7489, 'learning_rate': 9.133009229896957e-05, 'epoch': 0.19}
616/3250 [2:54:21<12:16:08, 16.77s/it] 19%|█▉        | 617/3250 [2:54:36<12:05:00, 16.52s/it]                                                        19%|█▉        | 617/3250 [2:54:36<12:05:00, 16.52s/it] 19%|█▉        | 618/3250 [2:54:52<11:56:56, 16.34s/it]                                                        19%|█▉        | 618/3250 [2:54:52<11:56:56, 16.34s/it] 19%|█▉        | 619/3250 [2:55:08<11:51:09, 16.22s/it]                                                        19%|█▉        | 619/3250 [2:55:08<11:51:09, 16.22s/it] 19%|█▉        | 620/3250 [2:55:24<11:47:10, 16.13s/it]                                                        19%|█▉        | 620/3250 [2:55:24<11:47:10, 16.13s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7900785803794861, 'eval_runtime': 2.4808, 'eval_samples_per_second': 4.837, 'eval_steps_per_second': 1.209, 'epoch': 0.19}
                                                        19%|█▉        | 620/3250 [2:55:27<11:47:10, 16.13s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-620
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-620/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-620

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-620

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7246, 'learning_rate': 9.130285548216857e-05, 'epoch': 0.19}
{'loss': 0.695, 'learning_rate': 9.127558002442174e-05, 'epoch': 0.19}
{'loss': 0.6815, 'learning_rate': 9.124826595124671e-05, 'epoch': 0.19}
{'loss': 0.7005, 'learning_rate': 9.122091328819715e-05, 'epoch': 0.19}
{'loss': 0.6891, 'learning_rate': 9.119352206086293e-05, 'epoch': 0.19}
{'loss': 0.702, 'learning_rate': 9.116609229486992e-05, 'epoch': 0.19}
 19%|█▉        | 621/3250 [2:55:43<12:25:55, 17.02s/it]                                                        19%|█▉        | 621/3250 [2:55:43<12:25:55, 17.02s/it] 19%|█▉        | 622/3250 [2:55:59<12:11:23, 16.70s/it]                                                        19%|█▉        | 622/3250 [2:55:59<12:11:23, 16.70s/it] 19%|█▉        | 623/3250 [2:56:16<12:06:05, 16.58s/it]                                                        19%|█▉        | 623/3250 [2:56:16<12:06:05, 16.58s/it] 19%|█▉        | 624/3250 [2:56:32<11:57:45, 16.40s/it]                                                        19%|█▉        | 624/3250 [2:56:32<11:57:45, 16.40s/it] 19%|█▉        | 625/3250 [2:56:48<11:51:37, 16.27s/it]                                                        19%|█▉        | 625/3250 [2:56:48<11:51:37, 16.27s/it] 19%|█▉        | 626/3250 [2:57:03<11:47:19, 16.17s/it]                                                        19%|█▉        | {'loss': 0.6992, 'learning_rate': 9.113862401588009e-05, 'epoch': 0.19}
{'loss': 0.6872, 'learning_rate': 9.111111724959143e-05, 'epoch': 0.19}
{'loss': 0.7042, 'learning_rate': 9.108357202173794e-05, 'epoch': 0.19}
{'loss': 0.7223, 'learning_rate': 9.105598835808957e-05, 'epoch': 0.19}
626/3250 [2:57:03<11:47:19, 16.17s/it] 19%|█▉        | 627/3250 [2:57:19<11:44:15, 16.11s/it]                                                        19%|█▉        | 627/3250 [2:57:19<11:44:15, 16.11s/it] 19%|█▉        | 628/3250 [2:57:35<11:42:09, 16.07s/it]                                                        19%|█▉        | 628/3250 [2:57:35<11:42:09, 16.07s/it] 19%|█▉        | 629/3250 [2:57:51<11:40:19, 16.03s/it]                                                        19%|█▉        | 629/3250 [2:57:51<11:40:19, 16.03s/it] 19%|█▉        | 630/3250 [2:58:07<11:39:01, 16.01s/it]                                                        19%|█▉        | 630/3250 [2:58:07<11:39:01, 16.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7863833904266357, 'eval_runtime': 2.4826, 'eval_samples_per_second': 4.834, 'eval_steps_per_second': 1.208, 'epoch': 0.19}
                                                        19%|█▉        | 630/3250 [2:58:10<11:39:01, 16.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-630
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-630 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-630
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6981, 'learning_rate': 9.10283662844523e-05, 'epoch': 0.19}
{'loss': 0.7052, 'learning_rate': 9.100070582666795e-05, 'epoch': 0.19}
{'loss': 0.6997, 'learning_rate': 9.097300701061434e-05, 'epoch': 0.19}
{'loss': 0.7186, 'learning_rate': 9.094526986220512e-05, 'epoch': 0.2}
{'loss': 0.6549, 'learning_rate': 9.091749440738984e-05, 'epoch': 0.2}
{'loss': 0.7495, 'learning_rate': 9.088968067215383e-05, 'epoch': 0.2}
 19%|█▉        | 631/3250 [2:58:27<12:21:14, 16.98s/it]                                                        19%|█▉        | 631/3250 [2:58:27<12:21:14, 16.98s/it] 19%|█▉        | 632/3250 [2:58:43<12:07:35, 16.67s/it]                                                        19%|█▉        | 632/3250 [2:58:43<12:07:35, 16.67s/it] 19%|█▉        | 633/3250 [2:58:58<11:57:52, 16.46s/it]                                                        19%|█▉        | 633/3250 [2:58:58<11:57:52, 16.46s/it] 20%|█▉        | 634/3250 [2:59:14<11:50:55, 16.31s/it]                                                        20%|█▉        | 634/3250 [2:59:14<11:50:55, 16.31s/it] 20%|█▉        | 635/3250 [2:59:30<11:46:09, 16.20s/it]                                                        20%|█▉        | 635/3250 [2:59:30<11:46:09, 16.20s/it] 20%|█▉        | 636/3250 [2:59:46<11:42:53, 16.13s/it]                                                        20%|█▉        | {'loss': 0.6959, 'learning_rate': 9.08618286825183e-05, 'epoch': 0.2}
{'loss': 0.6796, 'learning_rate': 9.08339384645402e-05, 'epoch': 0.2}
{'loss': 0.6788, 'learning_rate': 9.080601004431229e-05, 'epoch': 0.2}
{'loss': 0.6739, 'learning_rate': 9.077804344796302e-05, 'epoch': 0.2}
636/3250 [2:59:46<11:42:53, 16.13s/it] 20%|█▉        | 637/3250 [3:00:02<11:40:15, 16.08s/it]                                                        20%|█▉        | 637/3250 [3:00:02<11:40:15, 16.08s/it] 20%|█▉        | 638/3250 [3:00:18<11:38:13, 16.04s/it]                                                        20%|█▉        | 638/3250 [3:00:18<11:38:13, 16.04s/it] 20%|█▉        | 639/3250 [3:00:34<11:36:50, 16.01s/it]                                                        20%|█▉        | 639/3250 [3:00:34<11:36:50, 16.01s/it] 20%|█▉        | 640/3250 [3:00:51<11:42:14, 16.14s/it]                                                        20%|█▉        | 640/3250 [3:00:51<11:42:14, 16.14s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7881773114204407, 'eval_runtime': 2.4842, 'eval_samples_per_second': 4.831, 'eval_steps_per_second': 1.208, 'epoch': 0.2}
                                                        20%|█▉        | 640/3250 [3:00:53<11:42:14, 16.14s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-640
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-640/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-640

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-640

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7083, 'learning_rate': 9.075003870165657e-05, 'epoch': 0.2}
{'loss': 0.677, 'learning_rate': 9.072199583159286e-05, 'epoch': 0.2}
{'loss': 0.7064, 'learning_rate': 9.069391486400741e-05, 'epoch': 0.2}
{'loss': 1.165, 'learning_rate': 9.066579582517139e-05, 'epoch': 0.2}
{'loss': 0.6826, 'learning_rate': 9.063763874139164e-05, 'epoch': 0.2}
{'loss': 0.6885, 'learning_rate': 9.060944363901056e-05, 'epoch': 0.2}
 20%|█▉        | 641/3250 [3:01:10<12:20:49, 17.04s/it]                                                        20%|█▉        | 641/3250 [3:01:10<12:20:49, 17.04s/it] 20%|█▉        | 642/3250 [3:01:26<12:06:26, 16.71s/it]                                                        20%|█▉        | 642/3250 [3:01:26<12:06:26, 16.71s/it] 20%|█▉        | 643/3250 [3:01:42<11:56:12, 16.48s/it]                                                        20%|█▉        | 643/3250 [3:01:42<11:56:12, 16.48s/it] 20%|█▉        | 644/3250 [3:01:58<11:48:34, 16.31s/it]                                                        20%|█▉        | 644/3250 [3:01:58<11:48:34, 16.31s/it] 20%|█▉        | 645/3250 [3:02:14<11:43:29, 16.20s/it]                                                        20%|█▉        | 645/3250 [3:02:14<11:43:29, 16.20s/it] 20%|█▉        | 646/3250 [3:02:30<11:39:53, 16.13s/it]                                                        20%|█▉        | {'loss': 0.713, 'learning_rate': 9.058121054440612e-05, 'epoch': 0.2}
{'loss': 0.7016, 'learning_rate': 9.055293948399179e-05, 'epoch': 0.2}
{'loss': 0.6566, 'learning_rate': 9.052463048421665e-05, 'epoch': 0.2}
{'loss': 0.6762, 'learning_rate': 9.04962835715652e-05, 'epoch': 0.2}
646/3250 [3:02:30<11:39:53, 16.13s/it] 20%|█▉        | 647/3250 [3:02:45<11:37:31, 16.08s/it]                                                        20%|█▉        | 647/3250 [3:02:45<11:37:31, 16.08s/it] 20%|█▉        | 648/3250 [3:03:01<11:35:39, 16.04s/it]                                                        20%|█▉        | 648/3250 [3:03:01<11:35:39, 16.04s/it] 20%|█▉        | 649/3250 [3:03:17<11:34:23, 16.02s/it]                                                        20%|█▉        | 649/3250 [3:03:17<11:34:23, 16.02s/it] 20%|██        | 650/3250 [3:03:33<11:33:18, 16.00s/it]                                                        20%|██        | 650/3250 [3:03:33<11:33:18, 16.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7861911058425903, 'eval_runtime': 2.4797, 'eval_samples_per_second': 4.839, 'eval_steps_per_second': 1.21, 'epoch': 0.2}
                                                        20%|██        | 650/3250 [3:03:36<11:33:18, 16.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-650
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-650/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7575, 'learning_rate': 9.046789877255746e-05, 'epoch': 0.2}
{'loss': 0.6994, 'learning_rate': 9.043947611374886e-05, 'epoch': 0.2}
{'loss': 0.6909, 'learning_rate': 9.041101562173023e-05, 'epoch': 0.2}
{'loss': 0.6374, 'learning_rate': 9.038251732312783e-05, 'epoch': 0.2}
{'loss': 0.6944, 'learning_rate': 9.035398124460333e-05, 'epoch': 0.2}
{'loss': 0.6986, 'learning_rate': 9.032540741285367e-05, 'epoch': 0.2}
 20%|██        | 651/3250 [3:03:53<12:14:10, 16.95s/it]                                                        20%|██        | 651/3250 [3:03:53<12:14:10, 16.95s/it] 20%|██        | 652/3250 [3:04:08<12:01:00, 16.65s/it]                                                        20%|██        | 652/3250 [3:04:08<12:01:00, 16.65s/it] 20%|██        | 653/3250 [3:04:24<11:51:45, 16.44s/it]                                                        20%|██        | 653/3250 [3:04:24<11:51:45, 16.44s/it] 20%|██        | 654/3250 [3:04:40<11:44:56, 16.29s/it]                                                        20%|██        | 654/3250 [3:04:40<11:44:56, 16.29s/it] 20%|██        | 655/3250 [3:04:56<11:40:13, 16.19s/it]                                                        20%|██        | 655/3250 [3:04:56<11:40:13, 16.19s/it] 20%|██        | 656/3250 [3:05:13<11:41:40, 16.23s/it]                                                        20%|██        | {'loss': 0.6753, 'learning_rate': 9.029679585461113e-05, 'epoch': 0.2}
{'loss': 0.668, 'learning_rate': 9.026814659664331e-05, 'epoch': 0.2}
{'loss': 0.686, 'learning_rate': 9.023945966575304e-05, 'epoch': 0.2}
{'loss': 0.7015, 'learning_rate': 9.021073508877845e-05, 'epoch': 0.2}
656/3250 [3:05:13<11:41:40, 16.23s/it] 20%|██        | 657/3250 [3:05:29<11:37:53, 16.15s/it]                                                        20%|██        | 657/3250 [3:05:29<11:37:53, 16.15s/it] 20%|██        | 658/3250 [3:05:45<11:34:44, 16.08s/it]                                                        20%|██        | 658/3250 [3:05:45<11:34:44, 16.08s/it] 20%|██        | 659/3250 [3:06:01<11:38:39, 16.18s/it]                                                        20%|██        | 659/3250 [3:06:01<11:38:39, 16.18s/it] 20%|██        | 660/3250 [3:06:17<11:35:47, 16.12s/it]                                                        20%|██        | 660/3250 [3:06:17<11:35:47, 16.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7811927199363708, 'eval_runtime': 3.6936, 'eval_samples_per_second': 3.249, 'eval_steps_per_second': 0.812, 'epoch': 0.2}
                                                        20%|██        | 660/3250 [3:06:21<11:35:47, 16.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-660
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-660/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-660

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-660

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.71, 'learning_rate': 9.018197289259285e-05, 'epoch': 0.2}
{'loss': 0.6925, 'learning_rate': 9.015317310410474e-05, 'epoch': 0.2}
{'loss': 0.6773, 'learning_rate': 9.012433575025783e-05, 'epoch': 0.2}
{'loss': 0.7027, 'learning_rate': 9.009546085803092e-05, 'epoch': 0.2}
{'loss': 0.6858, 'learning_rate': 9.006654845443797e-05, 'epoch': 0.2}
{'loss': 0.6951, 'learning_rate': 9.003759856652803e-05, 'epoch': 0.2}
 20%|██        | 661/3250 [3:06:37<12:29:36, 17.37s/it]                                                        20%|██        | 661/3250 [3:06:37<12:29:36, 17.37s/it] 20%|██        | 662/3250 [3:06:53<12:10:53, 16.95s/it]                                                        20%|██        | 662/3250 [3:06:53<12:10:53, 16.95s/it] 20%|██        | 663/3250 [3:07:09<11:57:33, 16.64s/it]                                                        20%|██        | 663/3250 [3:07:09<11:57:33, 16.64s/it] 20%|██        | 664/3250 [3:07:25<11:48:22, 16.44s/it]                                                        20%|██        | 664/3250 [3:07:25<11:48:22, 16.44s/it] 20%|██        | 665/3250 [3:07:41<11:41:51, 16.29s/it]                                                        20%|██        | 665/3250 [3:07:41<11:41:51, 16.29s/it] 20%|██        | 666/3250 [3:07:57<11:37:11, 16.19s/it]                                                        20%|██        | {'loss': 0.6992, 'learning_rate': 9.000861122138517e-05, 'epoch': 0.21}
{'loss': 0.6756, 'learning_rate': 8.997958644612861e-05, 'epoch': 0.21}
{'loss': 0.6675, 'learning_rate': 8.995052426791247e-05, 'epoch': 0.21}
{'loss': 0.6639, 'learning_rate': 8.99214247139259e-05, 'epoch': 0.21}
666/3250 [3:07:57<11:37:11, 16.19s/it] 21%|██        | 667/3250 [3:08:13<11:33:49, 16.12s/it]                                                        21%|██        | 667/3250 [3:08:13<11:33:49, 16.12s/it] 21%|██        | 668/3250 [3:08:29<11:31:26, 16.07s/it]                                                        21%|██        | 668/3250 [3:08:29<11:31:26, 16.07s/it] 21%|██        | 669/3250 [3:08:45<11:29:34, 16.03s/it]                                                        21%|██        | 669/3250 [3:08:45<11:29:34, 16.03s/it] 21%|██        | 670/3250 [3:09:01<11:28:23, 16.01s/it]                                                        21%|██        | 670/3250 [3:09:01<11:28:23, 16.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.781577467918396, 'eval_runtime': 2.4832, 'eval_samples_per_second': 4.832, 'eval_steps_per_second': 1.208, 'epoch': 0.21}
                                                        21%|██        | 670/3250 [3:09:03<11:28:23, 16.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-670
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-670
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-670
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-670

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-670/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-670/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7046, 'learning_rate': 8.989228781139307e-05, 'epoch': 0.21}
{'loss': 0.668, 'learning_rate': 8.986311358757304e-05, 'epoch': 0.21}
{'loss': 0.702, 'learning_rate': 8.98339020697598e-05, 'epoch': 0.21}
{'loss': 1.1838, 'learning_rate': 8.980465328528219e-05, 'epoch': 0.21}
{'loss': 0.6377, 'learning_rate': 8.977536726150399e-05, 'epoch': 0.21}
{'loss': 0.6547, 'learning_rate': 8.974604402582379e-05, 'epoch': 0.21}
 21%|██        | 671/3250 [3:09:33<15:02:32, 21.00s/it]                                                        21%|██        | 671/3250 [3:09:33<15:02:32, 21.00s/it] 21%|██        | 672/3250 [3:09:49<13:57:16, 19.49s/it]                                                        21%|██        | 672/3250 [3:09:49<13:57:16, 19.49s/it] 21%|██        | 673/3250 [3:10:06<13:15:21, 18.52s/it]                                                        21%|██        | 673/3250 [3:10:06<13:15:21, 18.52s/it] 21%|██        | 674/3250 [3:10:22<12:41:37, 17.74s/it]                                                        21%|██        | 674/3250 [3:10:22<12:41:37, 17.74s/it] 21%|██        | 675/3250 [3:10:37<12:18:01, 17.20s/it]                                                        21%|██        | 675/3250 [3:10:37<12:18:01, 17.20s/it] 21%|██        | 676/3250 [3:10:53<12:01:35, 16.82s/it]                                                        21%|██        | {'loss': 0.6972, 'learning_rate': 8.971668360567496e-05, 'epoch': 0.21}
{'loss': 0.6983, 'learning_rate': 8.968728602852569e-05, 'epoch': 0.21}
{'loss': 0.6633, 'learning_rate': 8.965785132187894e-05, 'epoch': 0.21}
{'loss': 0.6768, 'learning_rate': 8.962837951327236e-05, 'epoch': 0.21}
676/3250 [3:10:53<12:01:35, 16.82s/it] 21%|██        | 677/3250 [3:11:09<11:49:59, 16.56s/it]                                                        21%|██        | 677/3250 [3:11:09<11:49:59, 16.56s/it] 21%|██        | 678/3250 [3:11:25<11:41:45, 16.37s/it]                                                        21%|██        | 678/3250 [3:11:25<11:41:45, 16.37s/it] 21%|██        | 679/3250 [3:11:41<11:35:58, 16.24s/it]                                                        21%|██        | 679/3250 [3:11:41<11:35:58, 16.24s/it] 21%|██        | 680/3250 [3:11:57<11:31:47, 16.15s/it]                                                        21%|██        | 680/3250 [3:11:57<11:31:47, 16.15s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7815179824829102, 'eval_runtime': 2.4831, 'eval_samples_per_second': 4.833, 'eval_steps_per_second': 1.208, 'epoch': 0.21}
                                                        21%|██        | 680/3250 [3:12:00<11:31:47, 16.15s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-680
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-680
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-680

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7278, 'learning_rate': 8.959887063027837e-05, 'epoch': 0.21}
{'loss': 0.7036, 'learning_rate': 8.956932470050404e-05, 'epoch': 0.21}
{'loss': 0.6619, 'learning_rate': 8.953974175159111e-05, 'epoch': 0.21}
{'loss': 0.6353, 'learning_rate': 8.951012181121594e-05, 'epoch': 0.21}
{'loss': 0.691, 'learning_rate': 8.948046490708953e-05, 'epoch': 0.21}
{'loss': 0.6707, 'learning_rate': 8.94507710669574e-05, 'epoch': 0.21}
 21%|██        | 681/3250 [3:12:16<12:09:05, 17.03s/it]                                                        21%|██        | 681/3250 [3:12:16<12:09:05, 17.03s/it] 21%|██        | 682/3250 [3:12:32<11:54:44, 16.70s/it]                                                        21%|██        | 682/3250 [3:12:32<11:54:44, 16.70s/it] 21%|██        | 683/3250 [3:12:48<11:44:36, 16.47s/it]                                                        21%|██        | 683/3250 [3:12:48<11:44:36, 16.47s/it] 21%|██        | 684/3250 [3:13:04<11:37:31, 16.31s/it]                                                        21%|██        | 684/3250 [3:13:04<11:37:31, 16.31s/it] 21%|██        | 685/3250 [3:13:20<11:33:01, 16.21s/it]                                                        21%|██        | 685/3250 [3:13:20<11:33:01, 16.21s/it] 21%|██        | 686/3250 [3:13:36<11:29:17, 16.13s/it]                                                        21%|██        | {'loss': 0.6856, 'learning_rate': 8.942104031859972e-05, 'epoch': 0.21}
{'loss': 0.6518, 'learning_rate': 8.939127268983108e-05, 'epoch': 0.21}
{'loss': 0.6665, 'learning_rate': 8.936146820850067e-05, 'epoch': 0.21}
{'loss': 0.6633, 'learning_rate': 8.933162690249208e-05, 'epoch': 0.21}
686/3250 [3:13:36<11:29:17, 16.13s/it] 21%|██        | 687/3250 [3:13:52<11:26:20, 16.07s/it]                                                        21%|██        | 687/3250 [3:13:52<11:26:20, 16.07s/it] 21%|██        | 688/3250 [3:14:08<11:24:25, 16.03s/it]                                                        21%|██        | 688/3250 [3:14:08<11:24:25, 16.03s/it] 21%|██        | 689/3250 [3:14:24<11:31:16, 16.20s/it]                                                        21%|██        | 689/3250 [3:14:25<11:31:16, 16.20s/it] 21%|██        | 690/3250 [3:14:42<11:51:27, 16.67s/it]                                                        21%|██        | 690/3250 [3:14:42<11:51:27, 16.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7780988216400146, 'eval_runtime': 2.5626, 'eval_samples_per_second': 4.683, 'eval_steps_per_second': 1.171, 'epoch': 0.21}
                                                        21%|██        | 690/3250 [3:14:45<11:51:27, 16.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-690
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-690
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-690
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6918, 'learning_rate': 8.930174879972342e-05, 'epoch': 0.21}
{'loss': 0.691, 'learning_rate': 8.927183392814718e-05, 'epoch': 0.21}
{'loss': 0.676, 'learning_rate': 8.924188231575024e-05, 'epoch': 0.21}
{'loss': 0.6773, 'learning_rate': 8.921189399055389e-05, 'epoch': 0.21}
{'loss': 0.6981, 'learning_rate': 8.918186898061376e-05, 'epoch': 0.21}
 21%|██▏       | 691/3250 [3:15:16<15:33:23, 21.88s/it]                                                        21%|██▏       | 691/3250 [3:15:16<15:33:23, 21.88s/it] 21%|██▏       | 692/3250 [3:15:32<14:16:27, 20.09s/it]                                                        21%|██▏       | 692/3250 [3:15:32<14:16:27, 20.09s/it] 21%|██▏       | 693/3250 [3:15:54<14:33:03, 20.49s/it]                                                        21%|██▏       | 693/3250 [3:15:54<14:33:03, 20.49s/it] 21%|██▏       | 694/3250 [3:16:10<13:36:19, 19.16s/it]                                                        21%|██▏       | 694/3250 [3:16:10<13:36:19, 19.16s/it] 21%|██▏       | 695/3250 [3:16:26<12:54:25, 18.19s/it]                                                        21%|██▏       | 695/3250 [3:16:26<12:54:25, 18.19s/it] 21%|██▏       | 696/3250 [3:16:41<12:24:54, 17.50s/it]                                                       {'loss': 0.6391, 'learning_rate': 8.915180731401978e-05, 'epoch': 0.21}
{'loss': 0.7143, 'learning_rate': 8.91217090188962e-05, 'epoch': 0.21}
{'loss': 0.6543, 'learning_rate': 8.90915741234015e-05, 'epoch': 0.21}
{'loss': 0.6708, 'learning_rate': 8.906140265572843e-05, 'epoch': 0.22}
{'loss': 0.6635, 'learning_rate': 8.903119464410397e-05, 'epoch': 0.22}
 21%|██▏       | 696/3250 [3:16:41<12:24:54, 17.50s/it] 21%|██▏       | 697/3250 [3:16:57<12:04:17, 17.02s/it]                                                        21%|██▏       | 697/3250 [3:16:57<12:04:17, 17.02s/it] 21%|██▏       | 698/3250 [3:17:13<11:49:53, 16.69s/it]                                                        21%|██▏       | 698/3250 [3:17:13<11:49:53, 16.69s/it] 22%|██▏       | 699/3250 [3:17:29<11:39:32, 16.45s/it]                                                        22%|██▏       | 699/3250 [3:17:29<11:39:32, 16.45s/it] 22%|██▏       | 700/3250 [3:17:45<11:32:18, 16.29s/it]                                                        22%|██▏       | 700/3250 [3:17:45<11:32:18, 16.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7766256928443909, 'eval_runtime': 2.7156, 'eval_samples_per_second': 4.419, 'eval_steps_per_second': 1.105, 'epoch': 0.22}
                                                        22%|██▏       | 700/3250 [3:17:48<11:32:18, 16.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-700
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-700/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-700


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-700

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-700/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-700/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-700/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6611, 'learning_rate': 8.900095011678924e-05, 'epoch': 0.22}
{'loss': 0.6663, 'learning_rate': 8.897066910207958e-05, 'epoch': 0.22}
{'loss': 0.6861, 'learning_rate': 8.894035162830443e-05, 'epoch': 0.22}
{'loss': 0.6669, 'learning_rate': 8.890999772382732e-05, 'epoch': 0.22}
{'loss': 1.1377, 'learning_rate': 8.887960741704592e-05, 'epoch': 0.22}
 22%|██▏       | 701/3250 [3:18:05<12:13:32, 17.27s/it]                                                        22%|██▏       | 701/3250 [3:18:05<12:13:32, 17.27s/it] 22%|██▏       | 702/3250 [3:18:21<11:55:57, 16.86s/it]                                                        22%|██▏       | 702/3250 [3:18:21<11:55:57, 16.86s/it] 22%|██▏       | 703/3250 [3:18:36<11:43:27, 16.57s/it]                                                        22%|██▏       | 703/3250 [3:18:36<11:43:27, 16.57s/it] 22%|██▏       | 704/3250 [3:18:52<11:34:33, 16.37s/it]                                                        22%|██▏       | 704/3250 [3:18:52<11:34:33, 16.37s/it] 22%|██▏       | 705/3250 [3:19:09<11:33:59, 16.36s/it]                                                        22%|██▏       | 705/3250 [3:19:09<11:33:59, 16.36s/it] 22%|██▏       | 706/3250 [3:19:25<11:27:42, 16.22s/it]                                                       {'loss': 0.6526, 'learning_rate': 8.88491807363919e-05, 'epoch': 0.22}
{'loss': 0.688, 'learning_rate': 8.881871771033102e-05, 'epoch': 0.22}
{'loss': 0.6783, 'learning_rate': 8.878821836736297e-05, 'epoch': 0.22}
{'loss': 0.6773, 'learning_rate': 8.875768273602148e-05, 'epoch': 0.22}
{'loss': 0.6523, 'learning_rate': 8.872711084487418e-05, 'epoch': 0.22}
 22%|██▏       | 706/3250 [3:19:25<11:27:42, 16.22s/it] 22%|██▏       | 707/3250 [3:19:40<11:23:28, 16.13s/it]                                                        22%|██▏       | 707/3250 [3:19:40<11:23:28, 16.13s/it] 22%|██▏       | 708/3250 [3:19:56<11:20:11, 16.05s/it]                                                        22%|██▏       | 708/3250 [3:19:56<11:20:11, 16.05s/it] 22%|██▏       | 709/3250 [3:20:12<11:17:57, 16.01s/it]                                                        22%|██▏       | 709/3250 [3:20:12<11:17:57, 16.01s/it] 22%|██▏       | 710/3250 [3:20:28<11:15:56, 15.97s/it]                                                        22%|██▏       | 710/3250 [3:20:28<11:15:56, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7771182060241699, 'eval_runtime': 2.5174, 'eval_samples_per_second': 4.767, 'eval_steps_per_second': 1.192, 'epoch': 0.22}
                                                        22%|██▏       | 710/3250 [3:20:31<11:15:56, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-710
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-710
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-710
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-710

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-710
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-710/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-710/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7182, 'learning_rate': 8.869650272252267e-05, 'epoch': 0.22}
{'loss': 0.6987, 'learning_rate': 8.866585839760242e-05, 'epoch': 0.22}
{'loss': 0.6598, 'learning_rate': 8.863517789878275e-05, 'epoch': 0.22}
{'loss': 0.6609, 'learning_rate': 8.860446125476687e-05, 'epoch': 0.22}
{'loss': 0.674, 'learning_rate': 8.857370849429178e-05, 'epoch': 0.22}
 22%|██▏       | 711/3250 [3:21:01<14:49:52, 21.03s/it]                                                        22%|██▏       | 711/3250 [3:21:01<14:49:52, 21.03s/it] 22%|██▏       | 712/3250 [3:21:17<13:44:09, 19.48s/it]                                                        22%|██▏       | 712/3250 [3:21:17<13:44:09, 19.48s/it] 22%|██▏       | 713/3250 [3:21:33<12:58:09, 18.40s/it]                                                        22%|██▏       | 713/3250 [3:21:33<12:58:09, 18.40s/it] 22%|██▏       | 714/3250 [3:21:49<12:25:48, 17.65s/it]                                                        22%|██▏       | 714/3250 [3:21:49<12:25:48, 17.65s/it] 22%|██▏       | 715/3250 [3:22:04<12:03:16, 17.12s/it]                                                        22%|██▏       | 715/3250 [3:22:04<12:03:16, 17.12s/it] 22%|██▏       | 716/3250 [3:22:20<11:47:23, 16.75s/it]                                                       {'loss': 0.6626, 'learning_rate': 8.854291964612825e-05, 'epoch': 0.22}
{'loss': 0.6735, 'learning_rate': 8.851209473908083e-05, 'epoch': 0.22}
{'loss': 0.6631, 'learning_rate': 8.848123380198783e-05, 'epoch': 0.22}
{'loss': 0.6549, 'learning_rate': 8.845033686372123e-05, 'epoch': 0.22}
{'loss': 0.6767, 'learning_rate': 8.84194039531867e-05, 'epoch': 0.22}
 22%|██▏       | 716/3250 [3:22:20<11:47:23, 16.75s/it] 22%|██▏       | 717/3250 [3:22:36<11:36:09, 16.49s/it]                                                        22%|██▏       | 717/3250 [3:22:36<11:36:09, 16.49s/it] 22%|██▏       | 718/3250 [3:22:52<11:28:15, 16.31s/it]                                                        22%|██▏       | 718/3250 [3:22:52<11:28:15, 16.31s/it] 22%|██▏       | 719/3250 [3:23:08<11:22:38, 16.18s/it]                                                        22%|██▏       | 719/3250 [3:23:08<11:22:38, 16.18s/it] 22%|██▏       | 720/3250 [3:23:24<11:18:31, 16.09s/it]                                                        22%|██▏       | 720/3250 [3:23:24<11:18:31, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7784121036529541, 'eval_runtime': 2.4651, 'eval_samples_per_second': 4.868, 'eval_steps_per_second': 1.217, 'epoch': 0.22}
                                                        22%|██▏       | 720/3250 [3:23:26<11:18:31, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-720
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-720
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-720
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-720

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6761, 'learning_rate': 8.83884350993236e-05, 'epoch': 0.22}
{'loss': 0.6631, 'learning_rate': 8.835743033110482e-05, 'epoch': 0.22}
{'loss': 0.6763, 'learning_rate': 8.832638967753699e-05, 'epoch': 0.22}
{'loss': 0.668, 'learning_rate': 8.82953131676602e-05, 'epoch': 0.22}
{'loss': 0.6839, 'learning_rate': 8.826420083054812e-05, 'epoch': 0.22}
 22%|██▏       | 721/3250 [3:23:43<11:55:42, 16.98s/it]                                                        22%|██▏       | 721/3250 [3:23:43<11:55:42, 16.98s/it] 22%|██▏       | 722/3250 [3:24:00<11:56:53, 17.01s/it]                                                        22%|██▏       | 722/3250 [3:24:00<11:56:53, 17.01s/it] 22%|██▏       | 723/3250 [3:24:16<11:42:44, 16.69s/it]                                                        22%|██▏       | 723/3250 [3:24:16<11:42:44, 16.69s/it] 22%|██▏       | 724/3250 [3:24:32<11:32:47, 16.46s/it]                                                        22%|██▏       | 724/3250 [3:24:32<11:32:47, 16.46s/it] 22%|██▏       | 725/3250 [3:24:48<11:25:31, 16.29s/it]                                                        22%|██▏       | 725/3250 [3:24:48<11:25:31, 16.29s/it] 22%|██▏       | 726/3250 [3:25:04<11:20:48, 16.18s/it]                                                       {'loss': 0.616, 'learning_rate': 8.823305269530795e-05, 'epoch': 0.22}
{'loss': 0.7197, 'learning_rate': 8.820186879108038e-05, 'epoch': 0.22}
{'loss': 0.6602, 'learning_rate': 8.817064914703954e-05, 'epoch': 0.22}
{'loss': 0.6482, 'learning_rate': 8.813939379239303e-05, 'epoch': 0.22}
{'loss': 0.65, 'learning_rate': 8.810810275638183e-05, 'epoch': 0.22}
 22%|██▏       | 726/3250 [3:25:04<11:20:48, 16.18s/it] 22%|██▏       | 727/3250 [3:25:20<11:16:59, 16.10s/it]                                                        22%|██▏       | 727/3250 [3:25:20<11:16:59, 16.10s/it] 22%|██▏       | 728/3250 [3:25:36<11:14:20, 16.04s/it]                                                        22%|██▏       | 728/3250 [3:25:36<11:14:20, 16.04s/it] 22%|██▏       | 729/3250 [3:25:51<11:12:50, 16.01s/it]                                                        22%|██▏       | 729/3250 [3:25:51<11:12:50, 16.01s/it] 22%|██▏       | 730/3250 [3:26:07<11:11:08, 15.98s/it]                                                        22%|██▏       | 730/3250 [3:26:07<11:11:08, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7742197513580322, 'eval_runtime': 2.4648, 'eval_samples_per_second': 4.868, 'eval_steps_per_second': 1.217, 'epoch': 0.22}
                                                        22%|██▏       | 730/3250 [3:26:10<11:11:08, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-730
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-730 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-730the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-730

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-730/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-730/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6327, 'learning_rate': 8.80767760682803e-05, 'epoch': 0.22}
{'loss': 0.6852, 'learning_rate': 8.804541375739623e-05, 'epoch': 0.23}
{'loss': 0.6459, 'learning_rate': 8.801401585307058e-05, 'epoch': 0.23}
{'loss': 0.6685, 'learning_rate': 8.79825823846778e-05, 'epoch': 0.23}
{'loss': 1.1438, 'learning_rate': 8.795111338162544e-05, 'epoch': 0.23}
 22%|██▏       | 731/3250 [3:26:27<11:50:52, 16.93s/it]                                                        22%|██▏       | 731/3250 [3:26:27<11:50:52, 16.93s/it] 23%|██▎       | 732/3250 [3:26:42<11:37:45, 16.63s/it]                                                        23%|██▎       | 732/3250 [3:26:42<11:37:45, 16.63s/it] 23%|██▎       | 733/3250 [3:26:58<11:28:27, 16.41s/it]                                                        23%|██▎       | 733/3250 [3:26:58<11:28:27, 16.41s/it] 23%|██▎       | 734/3250 [3:27:14<11:21:55, 16.26s/it]                                                        23%|██▎       | 734/3250 [3:27:14<11:21:55, 16.26s/it] 23%|██▎       | 735/3250 [3:27:30<11:16:51, 16.15s/it]                                                        23%|██▎       | 735/3250 [3:27:30<11:16:51, 16.15s/it] 23%|██▎       | 736/3250 [3:27:46<11:13:35, 16.08s/it]                                                       {'loss': 0.6583, 'learning_rate': 8.791960887335441e-05, 'epoch': 0.23}
{'loss': 0.6492, 'learning_rate': 8.788806888933881e-05, 'epoch': 0.23}
{'loss': 0.6815, 'learning_rate': 8.785649345908588e-05, 'epoch': 0.23}
{'loss': 0.676, 'learning_rate': 8.782488261213608e-05, 'epoch': 0.23}
{'loss': 0.6335, 'learning_rate': 8.779323637806299e-05, 'epoch': 0.23}
 23%|██▎       | 736/3250 [3:27:46<11:13:35, 16.08s/it] 23%|██▎       | 737/3250 [3:28:02<11:11:29, 16.03s/it]                                                        23%|██▎       | 737/3250 [3:28:02<11:11:29, 16.03s/it] 23%|██▎       | 738/3250 [3:28:18<11:13:05, 16.08s/it]                                                        23%|██▎       | 738/3250 [3:28:18<11:13:05, 16.08s/it] 23%|██▎       | 739/3250 [3:28:34<11:10:29, 16.02s/it]                                                        23%|██▎       | 739/3250 [3:28:34<11:10:29, 16.02s/it] 23%|██▎       | 740/3250 [3:28:50<11:08:32, 15.98s/it]                                                        23%|██▎       | 740/3250 [3:28:50<11:08:32, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7724984288215637, 'eval_runtime': 2.4851, 'eval_samples_per_second': 4.829, 'eval_steps_per_second': 1.207, 'epoch': 0.23}
                                                        23%|██▎       | 740/3250 [3:28:52<11:08:32, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-740
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-740the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-740

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-740

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-740/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-740/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6473, 'learning_rate': 8.776155478647326e-05, 'epoch': 0.23}
{'loss': 0.7248, 'learning_rate': 8.772983786700668e-05, 'epoch': 0.23}
{'loss': 0.6563, 'learning_rate': 8.769808564933605e-05, 'epoch': 0.23}
{'loss': 0.666, 'learning_rate': 8.766629816316721e-05, 'epoch': 0.23}
{'loss': 0.6112, 'learning_rate': 8.763447543823896e-05, 'epoch': 0.23}
 23%|██▎       | 741/3250 [3:29:09<11:47:09, 16.91s/it]                                                        23%|██▎       | 741/3250 [3:29:09<11:47:09, 16.91s/it] 23%|██▎       | 742/3250 [3:29:25<11:34:44, 16.62s/it]                                                        23%|██▎       | 742/3250 [3:29:25<11:34:44, 16.62s/it] 23%|██▎       | 743/3250 [3:29:41<11:25:18, 16.40s/it]                                                        23%|██▎       | 743/3250 [3:29:41<11:25:18, 16.40s/it] 23%|██▎       | 744/3250 [3:29:57<11:18:50, 16.25s/it]                                                        23%|██▎       | 744/3250 [3:29:57<11:18:50, 16.25s/it] 23%|██▎       | 745/3250 [3:30:13<11:19:02, 16.26s/it]                                                        23%|██▎       | 745/3250 [3:30:13<11:19:02, 16.26s/it] 23%|██▎       | 746/3250 [3:30:29<11:13:28, 16.14s/it]                                                       {'loss': 0.6556, 'learning_rate': 8.760261750432313e-05, 'epoch': 0.23}
{'loss': 0.6748, 'learning_rate': 8.757072439122444e-05, 'epoch': 0.23}
{'loss': 0.6584, 'learning_rate': 8.753879612878054e-05, 'epoch': 0.23}
{'loss': 0.6355, 'learning_rate': 8.750683274686196e-05, 'epoch': 0.23}
{'loss': 0.6555, 'learning_rate': 8.747483427537209e-05, 'epoch': 0.23}
 23%|██▎       | 746/3250 [3:30:29<11:13:28, 16.14s/it] 23%|██▎       | 747/3250 [3:30:45<11:09:18, 16.04s/it]                                                        23%|██▎       | 747/3250 [3:30:45<11:09:18, 16.04s/it] 23%|██▎       | 748/3250 [3:31:01<11:06:32, 15.98s/it]                                                        23%|██▎       | 748/3250 [3:31:01<11:06:32, 15.98s/it] 23%|██▎       | 749/3250 [3:31:16<11:04:22, 15.94s/it]                                                        23%|██▎       | 749/3250 [3:31:16<11:04:22, 15.94s/it] 23%|██▎       | 750/3250 [3:31:32<11:02:43, 15.91s/it]                                                        23%|██▎       | 750/3250 [3:31:32<11:02:43, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7783478498458862, 'eval_runtime': 2.4663, 'eval_samples_per_second': 4.866, 'eval_steps_per_second': 1.216, 'epoch': 0.23}
                                                        23%|██▎       | 750/3250 [3:31:35<11:02:43, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-750
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-750
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-750

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-750

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.672, 'learning_rate': 8.744280074424713e-05, 'epoch': 0.23}
{'loss': 0.6804, 'learning_rate': 8.741073218345614e-05, 'epoch': 0.23}
{'loss': 0.6781, 'learning_rate': 8.737862862300085e-05, 'epoch': 0.23}
{'loss': 0.6482, 'learning_rate': 8.734649009291585e-05, 'epoch': 0.23}
{'loss': 0.6847, 'learning_rate': 8.731431662326835e-05, 'epoch': 0.23}
 23%|██▎       | 751/3250 [3:32:29<19:32:46, 28.16s/it]                                                        23%|██▎       | 751/3250 [3:32:29<19:32:46, 28.16s/it] 23%|██▎       | 752/3250 [3:32:45<16:58:12, 24.46s/it]                                                        23%|██▎       | 752/3250 [3:32:45<16:58:12, 24.46s/it] 23%|██▎       | 753/3250 [3:33:01<15:09:53, 21.86s/it]                                                        23%|██▎       | 753/3250 [3:33:01<15:09:53, 21.86s/it] 23%|██▎       | 754/3250 [3:33:17<13:59:02, 20.17s/it]                                                        23%|██▎       | 754/3250 [3:33:17<13:59:02, 20.17s/it] 23%|██▎       | 755/3250 [3:33:33<13:04:48, 18.87s/it]                                                        23%|██▎       | 755/3250 [3:33:33<13:04:48, 18.87s/it] 23%|██▎       | 756/3250 [3:33:49<12:26:30, 17.96s/it]                                                       {'loss': 0.6474, 'learning_rate': 8.728210824415827e-05, 'epoch': 0.23}
{'loss': 0.659, 'learning_rate': 8.724986498571828e-05, 'epoch': 0.23}
{'loss': 0.6699, 'learning_rate': 8.721758687811352e-05, 'epoch': 0.23}
{'loss': 0.6428, 'learning_rate': 8.718527395154187e-05, 'epoch': 0.23}
{'loss': 0.6362, 'learning_rate': 8.715292623623373e-05, 'epoch': 0.23}
 23%|██▎       | 756/3250 [3:33:49<12:26:30, 17.96s/it] 23%|██▎       | 757/3250 [3:34:04<11:59:39, 17.32s/it]                                                        23%|██▎       | 757/3250 [3:34:04<11:59:39, 17.32s/it] 23%|██▎       | 758/3250 [3:34:20<11:40:48, 16.87s/it]                                                        23%|██▎       | 758/3250 [3:34:20<11:40:48, 16.87s/it] 23%|██▎       | 759/3250 [3:34:36<11:27:42, 16.56s/it]                                                        23%|██▎       | 759/3250 [3:34:36<11:27:42, 16.56s/it] 23%|██▎       | 760/3250 [3:34:52<11:18:11, 16.34s/it]                                                        23%|██▎       | 760/3250 [3:34:52<11:18:11, 16.34s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.772956371307373, 'eval_runtime': 2.4633, 'eval_samples_per_second': 4.872, 'eval_steps_per_second': 1.218, 'epoch': 0.23}
                                                        23%|██▎       | 760/3250 [3:34:54<11:18:11, 16.34s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-760
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-760
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-760
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-760

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6352, 'learning_rate': 8.712054376245202e-05, 'epoch': 0.23}
{'loss': 0.6721, 'learning_rate': 8.708812656049226e-05, 'epoch': 0.23}
{'loss': 0.639, 'learning_rate': 8.705567466068237e-05, 'epoch': 0.23}
{'loss': 0.6772, 'learning_rate': 8.702318809338278e-05, 'epoch': 0.24}
{'loss': 1.1599, 'learning_rate': 8.699066688898635e-05, 'epoch': 0.24}
 23%|██▎       | 761/3250 [3:35:11<11:51:41, 17.16s/it]                                                        23%|██▎       | 761/3250 [3:35:11<11:51:41, 17.16s/it] 23%|██▎       | 762/3250 [3:35:27<11:34:42, 16.75s/it]                                                        23%|██▎       | 762/3250 [3:35:27<11:34:42, 16.75s/it] 23%|██▎       | 763/3250 [3:35:43<11:22:59, 16.48s/it]                                                        23%|██▎       | 763/3250 [3:35:43<11:22:59, 16.48s/it] 24%|██▎       | 764/3250 [3:35:58<11:14:36, 16.28s/it]                                                        24%|██▎       | 764/3250 [3:35:58<11:14:36, 16.28s/it] 24%|██▎       | 765/3250 [3:36:14<11:08:25, 16.14s/it]                                                        24%|██▎       | 765/3250 [3:36:14<11:08:25, 16.14s/it] 24%|██▎       | 766/3250 [3:36:30<11:04:08, 16.04s/it]                                                       {'loss': 0.6046, 'learning_rate': 8.695811107791836e-05, 'epoch': 0.24}
{'loss': 0.628, 'learning_rate': 8.692552069063641e-05, 'epoch': 0.24}
{'loss': 0.6641, 'learning_rate': 8.689289575763051e-05, 'epoch': 0.24}
{'loss': 0.6689, 'learning_rate': 8.686023630942292e-05, 'epoch': 0.24}
{'loss': 0.6299, 'learning_rate': 8.68275423765683e-05, 'epoch': 0.24}
 24%|██▎       | 766/3250 [3:36:30<11:04:08, 16.04s/it] 24%|██▎       | 767/3250 [3:36:46<11:01:15, 15.98s/it]                                                        24%|██▎       | 767/3250 [3:36:46<11:01:15, 15.98s/it] 24%|██▎       | 768/3250 [3:37:02<10:59:04, 15.93s/it]                                                        24%|██▎       | 768/3250 [3:37:02<10:59:04, 15.93s/it] 24%|██▎       | 769/3250 [3:37:17<10:57:18, 15.90s/it]                                                        24%|██▎       | 769/3250 [3:37:17<10:57:18, 15.90s/it] 24%|██▎       | 770/3250 [3:37:33<10:56:20, 15.88s/it]                                                        24%|██▎       | 770/3250 [3:37:33<10:56:20, 15.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7706395387649536, 'eval_runtime': 2.4602, 'eval_samples_per_second': 4.878, 'eval_steps_per_second': 1.219, 'epoch': 0.24}
                                                        24%|██▎       | 770/3250 [3:37:36<10:56:20, 15.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-770
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-770
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-770

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-770

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-770
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-770/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6565, 'learning_rate': 8.679481398965347e-05, 'epoch': 0.24}
{'loss': 0.6896, 'learning_rate': 8.676205117929752e-05, 'epoch': 0.24}
{'loss': 0.6667, 'learning_rate': 8.672925397615173e-05, 'epoch': 0.24}
{'loss': 0.6274, 'learning_rate': 8.669642241089959e-05, 'epoch': 0.24}
{'loss': 0.6065, 'learning_rate': 8.666355651425672e-05, 'epoch': 0.24}
 24%|██▎       | 771/3250 [3:37:53<11:41:03, 16.97s/it]                                                        24%|██▎       | 771/3250 [3:37:53<11:41:03, 16.97s/it] 24%|██▍       | 772/3250 [3:38:09<11:26:58, 16.63s/it]                                                        24%|██▍       | 772/3250 [3:38:09<11:26:58, 16.63s/it] 24%|██▍       | 773/3250 [3:38:25<11:16:54, 16.40s/it]                                                        24%|██▍       | 773/3250 [3:38:25<11:16:54, 16.40s/it] 24%|██▍       | 774/3250 [3:38:40<11:09:52, 16.23s/it]                                                        24%|██▍       | 774/3250 [3:38:40<11:09:52, 16.23s/it] 24%|██▍       | 775/3250 [3:38:56<11:04:42, 16.11s/it]                                                        24%|██▍       | 775/3250 [3:38:56<11:04:42, 16.11s/it] 24%|██▍       | 776/3250 [3:39:12<11:01:04, 16.03s/it]                                                       {'loss': 0.6565, 'learning_rate': 8.663065631697085e-05, 'epoch': 0.24}
{'loss': 0.643, 'learning_rate': 8.65977218498218e-05, 'epoch': 0.24}
{'loss': 0.6486, 'learning_rate': 8.656475314362148e-05, 'epoch': 0.24}
{'loss': 0.6307, 'learning_rate': 8.65317502292138e-05, 'epoch': 0.24}
{'loss': 0.6412, 'learning_rate': 8.649871313747466e-05, 'epoch': 0.24}
 24%|██▍       | 776/3250 [3:39:12<11:01:04, 16.03s/it] 24%|██▍       | 777/3250 [3:39:28<10:58:09, 15.97s/it]                                                        24%|██▍       | 777/3250 [3:39:28<10:58:09, 15.97s/it] 24%|██▍       | 778/3250 [3:39:44<10:56:02, 15.92s/it]                                                        24%|██▍       | 778/3250 [3:39:44<10:56:02, 15.92s/it] 24%|██▍       | 779/3250 [3:39:59<10:54:24, 15.89s/it]                                                        24%|██▍       | 779/3250 [3:39:59<10:54:24, 15.89s/it] 24%|██▍       | 780/3250 [3:40:15<10:53:21, 15.87s/it]                                                        24%|██▍       | 780/3250 [3:40:15<10:53:21, 15.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7735660672187805, 'eval_runtime': 2.4638, 'eval_samples_per_second': 4.87, 'eval_steps_per_second': 1.218, 'epoch': 0.24}
                                                        24%|██▍       | 780/3250 [3:40:18<10:53:21, 15.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-780
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-780/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-780

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-780

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6514, 'learning_rate': 8.646564189931199e-05, 'epoch': 0.24}
{'loss': 0.6683, 'learning_rate': 8.64325365456656e-05, 'epoch': 0.24}
{'loss': 0.6625, 'learning_rate': 8.63993971075073e-05, 'epoch': 0.24}
{'loss': 0.6568, 'learning_rate': 8.636622361584072e-05, 'epoch': 0.24}
{'loss': 0.6358, 'learning_rate': 8.633301610170135e-05, 'epoch': 0.24}
 24%|██▍       | 781/3250 [3:40:48<14:26:30, 21.06s/it]                                                        24%|██▍       | 781/3250 [3:40:48<14:26:30, 21.06s/it] 24%|██▍       | 782/3250 [3:41:04<13:21:33, 19.49s/it]                                                        24%|██▍       | 782/3250 [3:41:04<13:21:33, 19.49s/it] 24%|██▍       | 783/3250 [3:41:20<12:35:52, 18.38s/it]                                                        24%|██▍       | 783/3250 [3:41:20<12:35:52, 18.38s/it] 24%|██▍       | 784/3250 [3:41:36<12:04:10, 17.62s/it]                                                        24%|██▍       | 784/3250 [3:41:36<12:04:10, 17.62s/it] 24%|██▍       | 785/3250 [3:41:52<11:41:57, 17.09s/it]                                                        24%|██▍       | 785/3250 [3:41:52<11:41:57, 17.09s/it] 24%|██▍       | 786/3250 [3:42:08<11:26:15, 16.71s/it]                                                       {'loss': 0.6672, 'learning_rate': 8.629977459615655e-05, 'epoch': 0.24}
{'loss': 0.6063, 'learning_rate': 8.626649913030545e-05, 'epoch': 0.24}
{'loss': 0.6959, 'learning_rate': 8.623318973527893e-05, 'epoch': 0.24}
{'loss': 0.6308, 'learning_rate': 8.619984644223968e-05, 'epoch': 0.24}
{'loss': 0.6369, 'learning_rate': 8.616646928238205e-05, 'epoch': 0.24}
 24%|██▍       | 786/3250 [3:42:08<11:26:15, 16.71s/it] 24%|██▍       | 787/3250 [3:42:24<11:19:46, 16.56s/it]                                                        24%|██▍       | 787/3250 [3:42:24<11:19:46, 16.56s/it] 24%|██▍       | 788/3250 [3:42:40<11:10:36, 16.34s/it]                                                        24%|██▍       | 788/3250 [3:42:40<11:10:36, 16.34s/it] 24%|██▍       | 789/3250 [3:42:55<11:04:04, 16.19s/it]                                                        24%|██▍       | 789/3250 [3:42:56<11:04:04, 16.19s/it] 24%|██▍       | 790/3250 [3:43:11<10:59:29, 16.09s/it]                                                        24%|██▍       | 790/3250 [3:43:11<10:59:29, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.769767701625824, 'eval_runtime': 2.4601, 'eval_samples_per_second': 4.878, 'eval_steps_per_second': 1.219, 'epoch': 0.24}
                                                        24%|██▍       | 790/3250 [3:43:14<10:59:29, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-790
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-790/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-790

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-790

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6325, 'learning_rate': 8.613305828693212e-05, 'epoch': 0.24}
{'loss': 0.6365, 'learning_rate': 8.609961348714756e-05, 'epoch': 0.24}
{'loss': 0.6335, 'learning_rate': 8.60661349143177e-05, 'epoch': 0.24}
{'loss': 0.6645, 'learning_rate': 8.603262259976348e-05, 'epoch': 0.24}
{'loss': 0.6417, 'learning_rate': 8.59990765748374e-05, 'epoch': 0.24}
 24%|██▍       | 791/3250 [3:43:44<14:23:25, 21.07s/it]                                                        24%|██▍       | 791/3250 [3:43:44<14:23:25, 21.07s/it] 24%|██▍       | 792/3250 [3:44:00<13:18:44, 19.50s/it]                                                        24%|██▍       | 792/3250 [3:44:00<13:18:44, 19.50s/it] 24%|██▍       | 793/3250 [3:44:16<12:33:43, 18.41s/it]                                                        24%|██▍       | 793/3250 [3:44:16<12:33:43, 18.41s/it] 24%|██▍       | 794/3250 [3:44:32<12:01:52, 17.64s/it]                                                        24%|██▍       | 794/3250 [3:44:32<12:01:52, 17.64s/it] 24%|██▍       | 795/3250 [3:44:47<11:39:44, 17.10s/it]                                                        24%|██▍       | 795/3250 [3:44:47<11:39:44, 17.10s/it] 24%|██▍       | 796/3250 [3:45:03<11:23:42, 16.72s/it]                                                       {'loss': 1.1224, 'learning_rate': 8.596549687092348e-05, 'epoch': 0.24}
{'loss': 0.6358, 'learning_rate': 8.593188351943726e-05, 'epoch': 0.25}
{'loss': 0.6648, 'learning_rate': 8.589823655182576e-05, 'epoch': 0.25}
{'loss': 0.6494, 'learning_rate': 8.586455599956746e-05, 'epoch': 0.25}
{'loss': 0.6519, 'learning_rate': 8.583084189417224e-05, 'epoch': 0.25}
 24%|██▍       | 796/3250 [3:45:03<11:23:42, 16.72s/it] 25%|██▍       | 797/3250 [3:45:19<11:12:42, 16.45s/it]                                                        25%|██▍       | 797/3250 [3:45:19<11:12:42, 16.45s/it] 25%|██▍       | 798/3250 [3:45:35<11:04:54, 16.27s/it]                                                        25%|██▍       | 798/3250 [3:45:35<11:04:54, 16.27s/it] 25%|██▍       | 799/3250 [3:45:51<10:59:33, 16.15s/it]                                                        25%|██▍       | 799/3250 [3:45:51<10:59:33, 16.15s/it] 25%|██▍       | 800/3250 [3:46:07<10:55:30, 16.05s/it]                                                        25%|██▍       | 800/3250 [3:46:07<10:55:30, 16.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7664897441864014, 'eval_runtime': 2.4643, 'eval_samples_per_second': 4.869, 'eval_steps_per_second': 1.217, 'epoch': 0.25}
                                                        25%|██▍       | 800/3250 [3:46:09<10:55:30, 16.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-800
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-800/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-800

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-800

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.613, 'learning_rate': 8.579709426718137e-05, 'epoch': 0.25}
{'loss': 0.687, 'learning_rate': 8.576331315016753e-05, 'epoch': 0.25}
{'loss': 0.6677, 'learning_rate': 8.572949857473462e-05, 'epoch': 0.25}
{'loss': 0.6201, 'learning_rate': 8.569565057251799e-05, 'epoch': 0.25}
{'loss': 0.6305, 'learning_rate': 8.566176917518416e-05, 'epoch': 0.25}
 25%|██▍       | 801/3250 [3:46:44<15:17:16, 22.47s/it]                                                        25%|██▍       | 801/3250 [3:46:44<15:17:16, 22.47s/it] 25%|██▍       | 802/3250 [3:47:00<13:55:42, 20.48s/it]                                                        25%|██▍       | 802/3250 [3:47:00<13:55:42, 20.48s/it] 25%|██▍       | 803/3250 [3:47:16<12:58:22, 19.09s/it]                                                        25%|██▍       | 803/3250 [3:47:16<12:58:22, 19.09s/it] 25%|██▍       | 804/3250 [3:47:32<12:25:38, 18.29s/it]                                                        25%|██▍       | 804/3250 [3:47:32<12:25:38, 18.29s/it] 25%|██▍       | 805/3250 [3:47:48<11:55:12, 17.55s/it]                                                        25%|██▍       | 805/3250 [3:47:48<11:55:12, 17.55s/it] 25%|██▍       | 806/3250 [3:48:04<11:34:04, 17.04s/it]                                                       {'loss': 0.6297, 'learning_rate': 8.56278544144309e-05, 'epoch': 0.25}
{'loss': 0.6228, 'learning_rate': 8.559390632198723e-05, 'epoch': 0.25}
{'loss': 0.6402, 'learning_rate': 8.555992492961334e-05, 'epoch': 0.25}
{'loss': 0.6342, 'learning_rate': 8.552591026910058e-05, 'epoch': 0.25}
{'loss': 0.6197, 'learning_rate': 8.549186237227138e-05, 'epoch': 0.25}
 25%|██▍       | 806/3250 [3:48:04<11:34:04, 17.04s/it] 25%|██▍       | 807/3250 [3:48:20<11:18:53, 16.67s/it]                                                        25%|██▍       | 807/3250 [3:48:20<11:18:53, 16.67s/it] 25%|██▍       | 808/3250 [3:48:35<11:08:11, 16.42s/it]                                                        25%|██▍       | 808/3250 [3:48:35<11:08:11, 16.42s/it] 25%|██▍       | 809/3250 [3:48:51<11:00:44, 16.24s/it]                                                        25%|██▍       | 809/3250 [3:48:51<11:00:44, 16.24s/it] 25%|██▍       | 810/3250 [3:49:07<10:55:17, 16.11s/it]                                                        25%|██▍       | 810/3250 [3:49:07<10:55:17, 16.11s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.770380973815918, 'eval_runtime': 2.468, 'eval_samples_per_second': 4.862, 'eval_steps_per_second': 1.216, 'epoch': 0.25}
                                                        25%|██▍       | 810/3250 [3:49:10<10:55:17, 16.11s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-810
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-810 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-810
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-810

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-810/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-810/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-810/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-810/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6542, 'learning_rate': 8.545778127097933e-05, 'epoch': 0.25}
{'loss': 0.6479, 'learning_rate': 8.542366699710905e-05, 'epoch': 0.25}
{'loss': 0.6353, 'learning_rate': 8.538951958257617e-05, 'epoch': 0.25}
{'loss': 0.65, 'learning_rate': 8.535533905932738e-05, 'epoch': 0.25}
{'loss': 0.6351, 'learning_rate': 8.532112545934032e-05, 'epoch': 0.25}
 25%|██▍       | 811/3250 [3:49:26<11:32:10, 17.03s/it]                                                        25%|██▍       | 811/3250 [3:49:26<11:32:10, 17.03s/it] 25%|██▍       | 812/3250 [3:49:42<11:17:02, 16.66s/it]                                                        25%|██▍       | 812/3250 [3:49:42<11:17:02, 16.66s/it] 25%|██▌       | 813/3250 [3:49:58<11:06:37, 16.41s/it]                                                        25%|██▌       | 813/3250 [3:49:58<11:06:37, 16.41s/it] 25%|██▌       | 814/3250 [3:50:14<10:59:15, 16.24s/it]                                                        25%|██▌       | 814/3250 [3:50:14<10:59:15, 16.24s/it] 25%|██▌       | 815/3250 [3:50:30<10:54:12, 16.12s/it]                                                        25%|██▌       | 815/3250 [3:50:30<10:54:12, 16.12s/it] 25%|██▌       | 816/3250 [3:50:45<10:50:32, 16.04s/it]                                                       {'loss': 0.6501, 'learning_rate': 8.528687881462357e-05, 'epoch': 0.25}
{'loss': 0.5909, 'learning_rate': 8.52525991572166e-05, 'epoch': 0.25}
{'loss': 0.686, 'learning_rate': 8.521828651918981e-05, 'epoch': 0.25}
{'loss': 0.6365, 'learning_rate': 8.518394093264448e-05, 'epoch': 0.25}
{'loss': 0.6261, 'learning_rate': 8.514956242971262e-05, 'epoch': 0.25}
 25%|██▌       | 816/3250 [3:50:45<10:50:32, 16.04s/it] 25%|██▌       | 817/3250 [3:51:01<10:47:55, 15.98s/it]                                                        25%|██▌       | 817/3250 [3:51:01<10:47:55, 15.98s/it] 25%|██▌       | 818/3250 [3:51:17<10:45:52, 15.93s/it]                                                        25%|██▌       | 818/3250 [3:51:17<10:45:52, 15.93s/it] 25%|██▌       | 819/3250 [3:51:33<10:44:26, 15.91s/it]                                                        25%|██▌       | 819/3250 [3:51:33<10:44:26, 15.91s/it] 25%|██▌       | 820/3250 [3:51:49<10:46:42, 15.97s/it]                                                        25%|██▌       | 820/3250 [3:51:49<10:46:42, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7678108215332031, 'eval_runtime': 2.4734, 'eval_samples_per_second': 4.852, 'eval_steps_per_second': 1.213, 'epoch': 0.25}
                                                        25%|██▌       | 820/3250 [3:51:52<10:46:42, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-820
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-820
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-820
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-820

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6211, 'learning_rate': 8.51151510425571e-05, 'epoch': 0.25}
{'loss': 0.609, 'learning_rate': 8.508070680337152e-05, 'epoch': 0.25}
{'loss': 0.6522, 'learning_rate': 8.504622974438028e-05, 'epoch': 0.25}
{'loss': 0.6141, 'learning_rate': 8.501171989783845e-05, 'epoch': 0.25}
{'loss': 0.6387, 'learning_rate': 8.497717729603172e-05, 'epoch': 0.25}
 25%|██▌       | 821/3250 [3:52:08<11:23:41, 16.89s/it]                                                        25%|██▌       | 821/3250 [3:52:08<11:23:41, 16.89s/it] 25%|██▌       | 822/3250 [3:52:24<11:13:42, 16.65s/it]                                                        25%|██▌       | 822/3250 [3:52:24<11:13:42, 16.65s/it] 25%|██▌       | 823/3250 [3:52:40<11:03:37, 16.41s/it]                                                        25%|██▌       | 823/3250 [3:52:40<11:03:37, 16.41s/it] 25%|██▌       | 824/3250 [3:52:56<10:56:38, 16.24s/it]                                                        25%|██▌       | 824/3250 [3:52:56<10:56:38, 16.24s/it] 25%|██▌       | 825/3250 [3:53:12<10:51:50, 16.13s/it]                                                        25%|██▌       | 825/3250 [3:53:12<10:51:50, 16.13s/it] 25%|██▌       | 826/3250 [3:53:28<10:47:49, 16.04s/it]                                                       {'loss': 1.1167, 'learning_rate': 8.49426019712765e-05, 'epoch': 0.25}
{'loss': 0.6272, 'learning_rate': 8.490799395591977e-05, 'epoch': 0.25}
{'loss': 0.6171, 'learning_rate': 8.487335328233912e-05, 'epoch': 0.25}
{'loss': 0.659, 'learning_rate': 8.483867998294266e-05, 'epoch': 0.26}
{'loss': 0.6462, 'learning_rate': 8.480397409016909e-05, 'epoch': 0.26}
 25%|██▌       | 826/3250 [3:53:28<10:47:49, 16.04s/it] 25%|██▌       | 827/3250 [3:53:43<10:45:27, 15.98s/it]                                                        25%|██▌       | 827/3250 [3:53:43<10:45:27, 15.98s/it] 25%|██▌       | 828/3250 [3:53:59<10:43:18, 15.94s/it]                                                        25%|██▌       | 828/3250 [3:53:59<10:43:18, 15.94s/it] 26%|██▌       | 829/3250 [3:54:15<10:44:30, 15.97s/it]                                                        26%|██▌       | 829/3250 [3:54:15<10:44:30, 15.97s/it] 26%|██▌       | 830/3250 [3:54:31<10:43:03, 15.94s/it]                                                        26%|██▌       | 830/3250 [3:54:31<10:43:03, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7643564343452454, 'eval_runtime': 2.4699, 'eval_samples_per_second': 4.858, 'eval_steps_per_second': 1.215, 'epoch': 0.26}
                                                        26%|██▌       | 830/3250 [3:54:34<10:43:03, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-830
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-830
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-830
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-830

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6026, 'learning_rate': 8.476923563648752e-05, 'epoch': 0.26}
{'loss': 0.6069, 'learning_rate': 8.473446465439758e-05, 'epoch': 0.26}
{'loss': 0.696, 'learning_rate': 8.469966117642929e-05, 'epoch': 0.26}
{'loss': 0.6203, 'learning_rate': 8.46648252351431e-05, 'epoch': 0.26}
{'loss': 0.6287, 'learning_rate': 8.462995686312985e-05, 'epoch': 0.26}
 26%|██▌       | 831/3250 [3:54:50<11:19:22, 16.85s/it]                                                        26%|██▌       | 831/3250 [3:54:50<11:19:22, 16.85s/it] 26%|██▌       | 832/3250 [3:55:06<11:06:54, 16.55s/it]                                                        26%|██▌       | 832/3250 [3:55:06<11:06:54, 16.55s/it] 26%|██▌       | 833/3250 [3:55:22<10:58:08, 16.34s/it]                                                        26%|██▌       | 833/3250 [3:55:22<10:58:08, 16.34s/it] 26%|██▌       | 834/3250 [3:55:38<10:51:50, 16.19s/it]                                                        26%|██▌       | 834/3250 [3:55:38<10:51:50, 16.19s/it] 26%|██▌       | 835/3250 [3:55:54<10:47:21, 16.08s/it]                                                        26%|██▌       | 835/3250 [3:55:54<10:47:21, 16.08s/it] 26%|██▌       | 836/3250 [3:56:09<10:44:12, 16.01s/it]                                                       {'loss': 0.5823, 'learning_rate': 8.459505609301069e-05, 'epoch': 0.26}
{'loss': 0.6179, 'learning_rate': 8.456012295743706e-05, 'epoch': 0.26}
{'loss': 0.6532, 'learning_rate': 8.452515748909069e-05, 'epoch': 0.26}
{'loss': 0.6225, 'learning_rate': 8.449015972068363e-05, 'epoch': 0.26}
{'loss': 0.616, 'learning_rate': 8.445512968495806e-05, 'epoch': 0.26}
 26%|██▌       | 836/3250 [3:56:09<10:44:12, 16.01s/it] 26%|██▌       | 837/3250 [3:56:26<10:46:12, 16.07s/it]                                                        26%|██▌       | 837/3250 [3:56:26<10:46:12, 16.07s/it] 26%|██▌       | 838/3250 [3:56:41<10:42:48, 15.99s/it]                                                        26%|██▌       | 838/3250 [3:56:41<10:42:48, 15.99s/it] 26%|██▌       | 839/3250 [3:56:57<10:40:29, 15.94s/it]                                                        26%|██▌       | 839/3250 [3:56:57<10:40:29, 15.94s/it] 26%|██▌       | 840/3250 [3:57:13<10:38:43, 15.90s/it]                                                        26%|██▌       | 840/3250 [3:57:13<10:38:43, 15.90s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7630007863044739, 'eval_runtime': 2.4729, 'eval_samples_per_second': 4.853, 'eval_steps_per_second': 1.213, 'epoch': 0.26}
                                                        26%|██▌       | 840/3250 [3:57:15<10:38:43, 15.90s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-840
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-840/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6182, 'learning_rate': 8.442006741468639e-05, 'epoch': 0.26}
{'loss': 0.6336, 'learning_rate': 8.438497294267117e-05, 'epoch': 0.26}
{'loss': 0.645, 'learning_rate': 8.434984630174509e-05, 'epoch': 0.26}
{'loss': 0.6437, 'learning_rate': 8.431468752477091e-05, 'epoch': 0.26}
{'loss': 0.6176, 'learning_rate': 8.427949664464152e-05, 'epoch': 0.26}
 26%|██▌       | 841/3250 [3:57:32<11:17:11, 16.87s/it]                                                        26%|██▌       | 841/3250 [3:57:32<11:17:11, 16.87s/it] 26%|██▌       | 842/3250 [3:57:48<11:04:28, 16.56s/it]                                                        26%|██▌       | 842/3250 [3:57:48<11:04:28, 16.56s/it] 26%|██▌       | 843/3250 [3:58:04<10:55:31, 16.34s/it]                                                        26%|██▌       | 843/3250 [3:58:04<10:55:31, 16.34s/it] 26%|██▌       | 844/3250 [3:58:20<10:49:07, 16.19s/it]                                                        26%|██▌       | 844/3250 [3:58:20<10:49:07, 16.19s/it] 26%|██▌       | 845/3250 [3:58:35<10:44:35, 16.08s/it]                                                        26%|██▌       | 845/3250 [3:58:35<10:44:35, 16.08s/it] 26%|██▌       | 846/3250 [3:58:51<10:41:35, 16.01s/it]                                                       {'loss': 0.649, 'learning_rate': 8.424427369427974e-05, 'epoch': 0.26}
{'loss': 0.6168, 'learning_rate': 8.42090187066385e-05, 'epoch': 0.26}
{'loss': 0.6314, 'learning_rate': 8.417373171470063e-05, 'epoch': 0.26}
{'loss': 0.6331, 'learning_rate': 8.413841275147892e-05, 'epoch': 0.26}
{'loss': 0.6061, 'learning_rate': 8.410306185001611e-05, 'epoch': 0.26}
 26%|██▌       | 846/3250 [3:58:51<10:41:35, 16.01s/it] 26%|██▌       | 847/3250 [3:59:07<10:38:51, 15.95s/it]                                                        26%|██▌       | 847/3250 [3:59:07<10:38:51, 15.95s/it] 26%|██▌       | 848/3250 [3:59:23<10:36:56, 15.91s/it]                                                        26%|██▌       | 848/3250 [3:59:23<10:36:56, 15.91s/it] 26%|██▌       | 849/3250 [3:59:39<10:35:47, 15.89s/it]                                                        26%|██▌       | 849/3250 [3:59:39<10:35:47, 15.89s/it] 26%|██▌       | 850/3250 [3:59:55<10:34:51, 15.87s/it]                                                        26%|██▌       | 850/3250 [3:59:55<10:34:51, 15.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7662050724029541, 'eval_runtime': 2.4702, 'eval_samples_per_second': 4.858, 'eval_steps_per_second': 1.214, 'epoch': 0.26}
                                                        26%|██▌       | 850/3250 [3:59:57<10:34:51, 15.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-850
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-850
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-850 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-850


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6135, 'learning_rate': 8.406767904338475e-05, 'epoch': 0.26}
{'loss': 0.6069, 'learning_rate': 8.403226436468728e-05, 'epoch': 0.26}
{'loss': 0.6429, 'learning_rate': 8.399681784705599e-05, 'epoch': 0.26}
{'loss': 0.604, 'learning_rate': 8.396133952365288e-05, 'epoch': 0.26}
{'loss': 0.6359, 'learning_rate': 8.392582942766976e-05, 'epoch': 0.26}
 26%|██▌       | 851/3250 [4:01:03<21:00:42, 31.53s/it]                                                        26%|██▌       | 851/3250 [4:01:03<21:00:42, 31.53s/it] 26%|██▌       | 852/3250 [4:01:19<17:51:59, 26.82s/it]                                                        26%|██▌       | 852/3250 [4:01:19<17:51:59, 26.82s/it] 26%|██▌       | 853/3250 [4:01:35<15:45:31, 23.67s/it]                                                        26%|██▌       | 853/3250 [4:01:35<15:45:31, 23.67s/it] 26%|██▋       | 854/3250 [4:01:51<14:11:10, 21.31s/it]                                                        26%|██▋       | 854/3250 [4:01:51<14:11:10, 21.31s/it] 26%|██▋       | 855/3250 [4:02:06<13:04:54, 19.66s/it]                                                        26%|██▋       | 855/3250 [4:02:06<13:04:54, 19.66s/it] 26%|██▋       | 856/3250 [4:02:22<12:18:23, 18.51s/it]                                                       {'loss': 1.1411, 'learning_rate': 8.389028759232815e-05, 'epoch': 0.26}
{'loss': 0.5893, 'learning_rate': 8.385471405087927e-05, 'epoch': 0.26}
{'loss': 0.5891, 'learning_rate': 8.3819108836604e-05, 'epoch': 0.26}
{'loss': 0.6344, 'learning_rate': 8.378347198281284e-05, 'epoch': 0.26}
{'loss': 0.6392, 'learning_rate': 8.37478035228459e-05, 'epoch': 0.26}
 26%|██▋       | 856/3250 [4:02:22<12:18:23, 18.51s/it] 26%|██▋       | 857/3250 [4:02:38<11:45:39, 17.69s/it]                                                        26%|██▋       | 857/3250 [4:02:38<11:45:39, 17.69s/it] 26%|██▋       | 858/3250 [4:02:54<11:22:51, 17.13s/it]                                                        26%|██▋       | 858/3250 [4:02:54<11:22:51, 17.13s/it] 26%|██▋       | 859/3250 [4:03:10<11:06:58, 16.74s/it]                                                        26%|██▋       | 859/3250 [4:03:10<11:06:58, 16.74s/it] 26%|██▋       | 860/3250 [4:03:26<10:55:49, 16.46s/it]                                                        26%|██▋       | 860/3250 [4:03:26<10:55:49, 16.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7634669542312622, 'eval_runtime': 2.4666, 'eval_samples_per_second': 4.865, 'eval_steps_per_second': 1.216, 'epoch': 0.26}
                                                        26%|██▋       | 860/3250 [4:03:28<10:55:49, 16.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-860
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-860/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-860/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-860/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5918, 'learning_rate': 8.371210349007286e-05, 'epoch': 0.26}
{'loss': 0.622, 'learning_rate': 8.367637191789296e-05, 'epoch': 0.27}
{'loss': 0.6693, 'learning_rate': 8.364060883973489e-05, 'epoch': 0.27}
{'loss': 0.6484, 'learning_rate': 8.360481428905686e-05, 'epoch': 0.27}
{'loss': 0.6048, 'learning_rate': 8.356898829934652e-05, 'epoch': 0.27}
 26%|██▋       | 861/3250 [4:03:45<11:26:32, 17.24s/it]                                                        26%|██▋       | 861/3250 [4:03:45<11:26:32, 17.24s/it] 27%|██▋       | 862/3250 [4:04:00<11:09:23, 16.82s/it]                                                        27%|██▋       | 862/3250 [4:04:00<11:09:23, 16.82s/it] 27%|██▋       | 863/3250 [4:04:16<10:57:24, 16.52s/it]                                                        27%|██▋       | 863/3250 [4:04:16<10:57:24, 16.52s/it] 27%|██▋       | 864/3250 [4:04:32<10:48:50, 16.32s/it]                                                        27%|██▋       | 864/3250 [4:04:32<10:48:50, 16.32s/it] 27%|██▋       | 865/3250 [4:04:48<10:42:45, 16.17s/it]                                                        27%|██▋       | 865/3250 [4:04:48<10:42:45, 16.17s/it] 27%|██▋       | 866/3250 [4:05:04<10:38:34, 16.07s/it]                                                       {'loss': 0.5907, 'learning_rate': 8.353313090412093e-05, 'epoch': 0.27}
{'loss': 0.6376, 'learning_rate': 8.349724213692651e-05, 'epoch': 0.27}
{'loss': 0.6109, 'learning_rate': 8.346132203133906e-05, 'epoch': 0.27}
{'loss': 0.62, 'learning_rate': 8.34253706209637e-05, 'epoch': 0.27}
{'loss': 0.6141, 'learning_rate': 8.338938793943478e-05, 'epoch': 0.27}
 27%|██▋       | 866/3250 [4:05:04<10:38:34, 16.07s/it] 27%|██▋       | 867/3250 [4:05:20<10:35:29, 16.00s/it]                                                        27%|██▋       | 867/3250 [4:05:20<10:35:29, 16.00s/it] 27%|██▋       | 868/3250 [4:05:35<10:32:54, 15.94s/it]                                                        27%|██▋       | 868/3250 [4:05:35<10:32:54, 15.94s/it] 27%|██▋       | 869/3250 [4:05:52<10:35:57, 16.03s/it]                                                        27%|██▋       | 869/3250 [4:05:52<10:35:57, 16.03s/it] 27%|██▋       | 870/3250 [4:06:07<10:33:02, 15.96s/it]                                                        27%|██▋       | 870/3250 [4:06:07<10:33:02, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7617202997207642, 'eval_runtime': 2.4598, 'eval_samples_per_second': 4.879, 'eval_steps_per_second': 1.22, 'epoch': 0.27}
                                                        27%|██▋       | 870/3250 [4:06:10<10:33:02, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-870
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-870/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-870

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-870

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.616, 'learning_rate': 8.335337402041601e-05, 'epoch': 0.27}
{'loss': 0.6121, 'learning_rate': 8.33173288976002e-05, 'epoch': 0.27}
{'loss': 0.6421, 'learning_rate': 8.328125260470947e-05, 'epoch': 0.27}
{'loss': 0.64, 'learning_rate': 8.3245145175495e-05, 'epoch': 0.27}
{'loss': 0.6196, 'learning_rate': 8.320900664373719e-05, 'epoch': 0.27}
 27%|██▋       | 871/3250 [4:06:26<11:09:07, 16.88s/it]                                                        27%|██▋       | 871/3250 [4:06:26<11:09:07, 16.88s/it] 27%|██▋       | 872/3250 [4:06:42<10:56:24, 16.56s/it]                                                        27%|██▋       | 872/3250 [4:06:42<10:56:24, 16.56s/it] 27%|██▋       | 873/3250 [4:06:58<10:47:18, 16.34s/it]                                                        27%|██▋       | 873/3250 [4:06:58<10:47:18, 16.34s/it] 27%|██▋       | 874/3250 [4:07:14<10:40:40, 16.18s/it]                                                        27%|██▋       | 874/3250 [4:07:14<10:40:40, 16.18s/it] 27%|██▋       | 875/3250 [4:07:30<10:36:26, 16.08s/it]                                                        27%|██▋       | 875/3250 [4:07:30<10:36:26, 16.08s/it] 27%|██▋       | 876/3250 [4:07:46<10:33:13, 16.00s/it]                                                       {'loss': 0.618, 'learning_rate': 8.317283704324549e-05, 'epoch': 0.27}
{'loss': 0.6429, 'learning_rate': 8.313663640785839e-05, 'epoch': 0.27}
{'loss': 0.577, 'learning_rate': 8.310040477144347e-05, 'epoch': 0.27}
{'loss': 0.6584, 'learning_rate': 8.30641421678973e-05, 'epoch': 0.27}
{'loss': 0.5952, 'learning_rate': 8.30278486311454e-05, 'epoch': 0.27}
 27%|██▋       | 876/3250 [4:07:46<10:33:13, 16.00s/it] 27%|██▋       | 877/3250 [4:08:01<10:30:54, 15.95s/it]                                                        27%|██▋       | 877/3250 [4:08:01<10:30:54, 15.95s/it] 27%|██▋       | 878/3250 [4:08:17<10:31:53, 15.98s/it]                                                        27%|██▋       | 878/3250 [4:08:17<10:31:53, 15.98s/it] 27%|██▋       | 879/3250 [4:08:33<10:29:40, 15.93s/it]                                                        27%|██▋       | 879/3250 [4:08:33<10:29:40, 15.93s/it] 27%|██▋       | 880/3250 [4:08:49<10:28:22, 15.91s/it]                                                        27%|██▋       | 880/3250 [4:08:49<10:28:22, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.766096830368042, 'eval_runtime': 2.4587, 'eval_samples_per_second': 4.881, 'eval_steps_per_second': 1.22, 'epoch': 0.27}
                                                        27%|██▋       | 880/3250 [4:08:52<10:28:22, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-880
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-880 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-880
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-880

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6047, 'learning_rate': 8.29915241951422e-05, 'epoch': 0.27}
{'loss': 0.5996, 'learning_rate': 8.295516889387115e-05, 'epoch': 0.27}
{'loss': 0.6094, 'learning_rate': 8.291878276134447e-05, 'epoch': 0.27}
{'loss': 0.6106, 'learning_rate': 8.288236583160322e-05, 'epoch': 0.27}
{'loss': 0.635, 'learning_rate': 8.284591813871738e-05, 'epoch': 0.27}
 27%|██▋       | 881/3250 [4:09:08<11:04:56, 16.84s/it]                                                        27%|██▋       | 881/3250 [4:09:08<11:04:56, 16.84s/it] 27%|██▋       | 882/3250 [4:09:24<10:52:26, 16.53s/it]                                                        27%|██▋       | 882/3250 [4:09:24<10:52:26, 16.53s/it] 27%|██▋       | 883/3250 [4:09:40<10:43:39, 16.32s/it]                                                        27%|██▋       | 883/3250 [4:09:40<10:43:39, 16.32s/it] 27%|██▋       | 884/3250 [4:09:56<10:37:19, 16.16s/it]                                                        27%|██▋       | 884/3250 [4:09:56<10:37:19, 16.16s/it] 27%|██▋       | 885/3250 [4:10:11<10:32:49, 16.05s/it]                                                        27%|██▋       | 885/3250 [4:10:11<10:32:49, 16.05s/it] 27%|██▋       | 886/3250 [4:10:27<10:32:53, 16.06s/it]                                                       {'loss': 0.6162, 'learning_rate': 8.280943971678562e-05, 'epoch': 0.27}
{'loss': 1.096, 'learning_rate': 8.277293059993535e-05, 'epoch': 0.27}
{'loss': 0.6061, 'learning_rate': 8.273639082232276e-05, 'epoch': 0.27}
{'loss': 0.6338, 'learning_rate': 8.269982041813267e-05, 'epoch': 0.27}
{'loss': 0.6193, 'learning_rate': 8.26632194215786e-05, 'epoch': 0.27}
 27%|██▋       | 886/3250 [4:10:27<10:32:53, 16.06s/it] 27%|██▋       | 887/3250 [4:10:43<10:29:15, 15.98s/it]                                                        27%|██▋       | 887/3250 [4:10:43<10:29:15, 15.98s/it] 27%|██▋       | 888/3250 [4:10:59<10:26:59, 15.93s/it]                                                        27%|██▋       | 888/3250 [4:10:59<10:26:59, 15.93s/it] 27%|██▋       | 889/3250 [4:11:15<10:25:33, 15.90s/it]                                                        27%|██▋       | 889/3250 [4:11:15<10:25:33, 15.90s/it] 27%|██▋       | 890/3250 [4:11:34<10:59:06, 16.76s/it]                                                        27%|██▋       | 890/3250 [4:11:34<10:59:06, 16.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.761405885219574, 'eval_runtime': 3.4298, 'eval_samples_per_second': 3.499, 'eval_steps_per_second': 0.875, 'epoch': 0.27}
                                                        27%|██▋       | 890/3250 [4:11:37<10:59:06, 16.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-890
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6178, 'learning_rate': 8.262658786690262e-05, 'epoch': 0.27}
{'loss': 0.5976, 'learning_rate': 8.258992578837548e-05, 'epoch': 0.27}
{'loss': 0.6669, 'learning_rate': 8.255323322029642e-05, 'epoch': 0.27}
{'loss': 0.6497, 'learning_rate': 8.251651019699322e-05, 'epoch': 0.28}
{'loss': 0.593, 'learning_rate': 8.247975675282218e-05, 'epoch': 0.28}
 27%|██▋       | 891/3250 [4:11:54<11:41:02, 17.83s/it]                                                        27%|██▋       | 891/3250 [4:11:54<11:41:02, 17.83s/it] 27%|██▋       | 892/3250 [4:12:10<11:17:55, 17.25s/it]                                                        27%|██▋       | 892/3250 [4:12:10<11:17:55, 17.25s/it] 27%|██▋       | 893/3250 [4:12:26<11:01:23, 16.84s/it]                                                        27%|██▋       | 893/3250 [4:12:26<11:01:23, 16.84s/it] 28%|██▊       | 894/3250 [4:12:42<10:49:52, 16.55s/it]                                                        28%|██▊       | 894/3250 [4:12:42<10:49:52, 16.55s/it] 28%|██▊       | 895/3250 [4:12:58<10:41:52, 16.35s/it]                                                        28%|██▊       | 895/3250 [4:12:58<10:41:52, 16.35s/it] 28%|██▊       | 896/3250 [4:13:13<10:36:01, 16.21s/it]                                                       {'loss': 0.5979, 'learning_rate': 8.244297292216802e-05, 'epoch': 0.28}
{'loss': 0.6226, 'learning_rate': 8.240615873944391e-05, 'epoch': 0.28}
{'loss': 0.607, 'learning_rate': 8.236931423909138e-05, 'epoch': 0.28}
{'loss': 0.6141, 'learning_rate': 8.233243945558042e-05, 'epoch': 0.28}
{'loss': 0.6153, 'learning_rate': 8.229553442340922e-05, 'epoch': 0.28}
 28%|██▊       | 896/3250 [4:13:13<10:36:01, 16.21s/it] 28%|██▊       | 897/3250 [4:13:29<10:31:28, 16.10s/it]                                                        28%|██▊       | 897/3250 [4:13:29<10:31:28, 16.10s/it] 28%|██▊       | 898/3250 [4:13:45<10:28:19, 16.03s/it]                                                        28%|██▊       | 898/3250 [4:13:45<10:28:19, 16.03s/it] 28%|██▊       | 899/3250 [4:14:01<10:25:47, 15.97s/it]                                                        28%|██▊       | 899/3250 [4:14:01<10:25:47, 15.97s/it] 28%|██▊       | 900/3250 [4:14:17<10:24:05, 15.93s/it]                                                        28%|██▊       | 900/3250 [4:14:17<10:24:05, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7587068676948547, 'eval_runtime': 2.4609, 'eval_samples_per_second': 4.876, 'eval_steps_per_second': 1.219, 'epoch': 0.28}
                                                        28%|██▊       | 900/3250 [4:14:19<10:24:05, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-900
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-900 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-900
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6013, 'learning_rate': 8.225859917710439e-05, 'epoch': 0.28}
{'loss': 0.6189, 'learning_rate': 8.222163375122072e-05, 'epoch': 0.28}
{'loss': 0.6256, 'learning_rate': 8.218463818034128e-05, 'epoch': 0.28}
{'loss': 0.6039, 'learning_rate': 8.214761249907732e-05, 'epoch': 0.28}
{'loss': 0.6161, 'learning_rate': 8.211055674206828e-05, 'epoch': 0.28}
 28%|██▊       | 901/3250 [4:14:36<10:59:39, 16.85s/it]                                                        28%|██▊       | 901/3250 [4:14:36<10:59:39, 16.85s/it] 28%|██▊       | 902/3250 [4:14:53<11:01:09, 16.90s/it]                                                        28%|██▊       | 902/3250 [4:14:53<11:01:09, 16.90s/it] 28%|██▊       | 903/3250 [4:15:09<10:48:31, 16.58s/it]                                                        28%|██▊       | 903/3250 [4:15:09<10:48:31, 16.58s/it] 28%|██▊       | 904/3250 [4:15:24<10:39:33, 16.36s/it]                                                        28%|██▊       | 904/3250 [4:15:24<10:39:33, 16.36s/it] 28%|██▊       | 905/3250 [4:15:40<10:33:23, 16.21s/it]                                                        28%|██▊       | 905/3250 [4:15:40<10:33:23, 16.21s/it] 28%|██▊       | 906/3250 [4:15:56<10:28:44, 16.09s/it]                                                       {'loss': 0.6221, 'learning_rate': 8.207347094398173e-05, 'epoch': 0.28}
{'loss': 0.6251, 'learning_rate': 8.203635513951331e-05, 'epoch': 0.28}
{'loss': 0.5625, 'learning_rate': 8.199920936338681e-05, 'epoch': 0.28}
{'loss': 0.6445, 'learning_rate': 8.1962033650354e-05, 'epoch': 0.28}
{'loss': 0.6035, 'learning_rate': 8.192482803519465e-05, 'epoch': 0.28}
 28%|██▊       | 906/3250 [4:15:56<10:28:44, 16.09s/it] 28%|██▊       | 907/3250 [4:16:12<10:25:42, 16.02s/it]                                                        28%|██▊       | 907/3250 [4:16:12<10:25:42, 16.02s/it] 28%|██▊       | 908/3250 [4:16:28<10:23:17, 15.97s/it]                                                        28%|██▊       | 908/3250 [4:16:28<10:23:17, 15.97s/it] 28%|██▊       | 909/3250 [4:16:44<10:21:39, 15.93s/it]                                                        28%|██▊       | 909/3250 [4:16:44<10:21:39, 15.93s/it] 28%|██▊       | 910/3250 [4:17:00<10:20:21, 15.91s/it]                                                        28%|██▊       | 910/3250 [4:17:00<10:20:21, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7656182646751404, 'eval_runtime': 2.4551, 'eval_samples_per_second': 4.888, 'eval_steps_per_second': 1.222, 'epoch': 0.28}
                                                        28%|██▊       | 910/3250 [4:17:02<10:20:21, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-910
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-910/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-910

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-910

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-910/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-910/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5862, 'learning_rate': 8.188759255271654e-05, 'epoch': 0.28}
{'loss': 0.5899, 'learning_rate': 8.185032723775539e-05, 'epoch': 0.28}
{'loss': 0.5812, 'learning_rate': 8.181303212517479e-05, 'epoch': 0.28}
{'loss': 0.6175, 'learning_rate': 8.177570724986628e-05, 'epoch': 0.28}
{'loss': 0.5934, 'learning_rate': 8.173835264674916e-05, 'epoch': 0.28}
 28%|██▊       | 911/3250 [4:17:19<10:56:56, 16.85s/it]                                                        28%|██▊       | 911/3250 [4:17:19<10:56:56, 16.85s/it] 28%|██▊       | 912/3250 [4:17:34<10:44:54, 16.55s/it]                                                        28%|██▊       | 912/3250 [4:17:34<10:44:54, 16.55s/it] 28%|██▊       | 913/3250 [4:17:50<10:36:21, 16.34s/it]                                                        28%|██▊       | 913/3250 [4:17:50<10:36:21, 16.34s/it] 28%|██▊       | 914/3250 [4:18:06<10:30:14, 16.19s/it]                                                        28%|██▊       | 914/3250 [4:18:06<10:30:14, 16.19s/it] 28%|██▊       | 915/3250 [4:18:22<10:25:53, 16.08s/it]                                                        28%|██▊       | 915/3250 [4:18:22<10:25:53, 16.08s/it] 28%|██▊       | 916/3250 [4:18:38<10:23:01, 16.02s/it]                                                       {'loss': 0.6241, 'learning_rate': 8.17009683507706e-05, 'epoch': 0.28}
{'loss': 1.0962, 'learning_rate': 8.166355439690553e-05, 'epoch': 0.28}
{'loss': 0.5969, 'learning_rate': 8.162611082015665e-05, 'epoch': 0.28}
{'loss': 0.6017, 'learning_rate': 8.15886376555543e-05, 'epoch': 0.28}
{'loss': 0.6179, 'learning_rate': 8.15511349381566e-05, 'epoch': 0.28}
 28%|██▊       | 916/3250 [4:18:38<10:23:01, 16.02s/it] 28%|██▊       | 917/3250 [4:18:54<10:20:40, 15.96s/it]                                                        28%|██▊       | 917/3250 [4:18:54<10:20:40, 15.96s/it] 28%|██▊       | 918/3250 [4:19:10<10:21:14, 15.98s/it]                                                        28%|██▊       | 918/3250 [4:19:10<10:21:14, 15.98s/it] 28%|██▊       | 919/3250 [4:19:26<10:19:39, 15.95s/it]                                                        28%|██▊       | 919/3250 [4:19:26<10:19:39, 15.95s/it] 28%|██▊       | 920/3250 [4:19:41<10:18:25, 15.92s/it]                                                        28%|██▊       | 920/3250 [4:19:41<10:18:25, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7571749687194824, 'eval_runtime': 2.6976, 'eval_samples_per_second': 4.448, 'eval_steps_per_second': 1.112, 'epoch': 0.28}
                                                        28%|██▊       | 920/3250 [4:19:44<10:18:25, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-920
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-920/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6121, 'learning_rate': 8.151360270304927e-05, 'epoch': 0.28}
{'loss': 0.5684, 'learning_rate': 8.14760409853456e-05, 'epoch': 0.28}
{'loss': 0.5934, 'learning_rate': 8.143844982018655e-05, 'epoch': 0.28}
{'loss': 0.6643, 'learning_rate': 8.14008292427406e-05, 'epoch': 0.28}
{'loss': 0.6019, 'learning_rate': 8.13631792882037e-05, 'epoch': 0.28}
 28%|██▊       | 921/3250 [4:20:01<10:58:11, 16.96s/it]                                                        28%|██▊       | 921/3250 [4:20:01<10:58:11, 16.96s/it] 28%|██▊       | 922/3250 [4:20:17<10:45:14, 16.63s/it]                                                        28%|██▊       | 922/3250 [4:20:17<10:45:14, 16.63s/it] 28%|██▊       | 923/3250 [4:20:33<10:36:07, 16.40s/it]                                                        28%|██▊       | 923/3250 [4:20:33<10:36:07, 16.40s/it] 28%|██▊       | 924/3250 [4:20:48<10:29:45, 16.24s/it]                                                        28%|██▊       | 924/3250 [4:20:48<10:29:45, 16.24s/it] 28%|██▊       | 925/3250 [4:21:04<10:25:14, 16.14s/it]                                                        28%|██▊       | 925/3250 [4:21:04<10:25:14, 16.14s/it] 28%|██▊       | 926/3250 [4:21:20<10:21:45, 16.05s/it]                                                       {'loss': 0.6065, 'learning_rate': 8.132549999179933e-05, 'epoch': 0.28}
{'loss': 0.5629, 'learning_rate': 8.128779138877843e-05, 'epoch': 0.29}
{'loss': 0.6129, 'learning_rate': 8.12500535144193e-05, 'epoch': 0.29}
{'loss': 0.6202, 'learning_rate': 8.12122864040277e-05, 'epoch': 0.29}
{'loss': 0.5901, 'learning_rate': 8.117449009293668e-05, 'epoch': 0.29}
 28%|██▊       | 926/3250 [4:21:20<10:21:45, 16.05s/it] 29%|██▊       | 927/3250 [4:21:36<10:19:13, 15.99s/it]                                                        29%|██▊       | 927/3250 [4:21:36<10:19:13, 15.99s/it] 29%|██▊       | 928/3250 [4:21:52<10:17:34, 15.96s/it]                                                        29%|██▊       | 928/3250 [4:21:52<10:17:34, 15.96s/it] 29%|██▊       | 929/3250 [4:22:08<10:16:11, 15.93s/it]                                                        29%|██▊       | 929/3250 [4:22:08<10:16:11, 15.93s/it] 29%|██▊       | 930/3250 [4:22:24<10:15:11, 15.91s/it]                                                        29%|██▊       | 930/3250 [4:22:24<10:15:11, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7547623515129089, 'eval_runtime': 2.493, 'eval_samples_per_second': 4.813, 'eval_steps_per_second': 1.203, 'epoch': 0.29}
                                                        29%|██▊       | 930/3250 [4:22:26<10:15:11, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-930
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-930
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-930
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-930

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5751, 'learning_rate': 8.113666461650664e-05, 'epoch': 0.29}
{'loss': 0.6013, 'learning_rate': 8.109881001012527e-05, 'epoch': 0.29}
{'loss': 0.6186, 'learning_rate': 8.106092630920749e-05, 'epoch': 0.29}
{'loss': 0.6187, 'learning_rate': 8.102301354919544e-05, 'epoch': 0.29}
{'loss': 0.6083, 'learning_rate': 8.098507176555849e-05, 'epoch': 0.29}
 29%|██▊       | 931/3250 [4:22:57<13:40:28, 21.23s/it]                                                        29%|██▊       | 931/3250 [4:22:57<13:40:28, 21.23s/it] 29%|██▊       | 932/3250 [4:23:13<12:37:47, 19.62s/it]                                                        29%|██▊       | 932/3250 [4:23:13<12:37:47, 19.62s/it] 29%|██▊       | 933/3250 [4:23:29<11:53:59, 18.49s/it]                                                        29%|██▊       | 933/3250 [4:23:29<11:53:59, 18.49s/it] 29%|██▊       | 934/3250 [4:23:45<11:23:11, 17.70s/it]                                                        29%|██▊       | 934/3250 [4:23:45<11:23:11, 17.70s/it] 29%|██▉       | 935/3250 [4:24:02<11:11:19, 17.40s/it]                                                        29%|██▉       | 935/3250 [4:24:02<11:11:19, 17.40s/it] 29%|██▉       | 936/3250 [4:24:17<10:53:23, 16.94s/it]                                                       {'loss': 0.5916, 'learning_rate': 8.09471009937931e-05, 'epoch': 0.29}
{'loss': 0.617, 'learning_rate': 8.090910126942288e-05, 'epoch': 0.29}
{'loss': 0.5858, 'learning_rate': 8.087107262799855e-05, 'epoch': 0.29}
{'loss': 0.6075, 'learning_rate': 8.083301510509784e-05, 'epoch': 0.29}
{'loss': 0.5932, 'learning_rate': 8.079492873632554e-05, 'epoch': 0.29}
 29%|██▉       | 936/3250 [4:24:17<10:53:23, 16.94s/it] 29%|██▉       | 937/3250 [4:24:33<10:40:44, 16.62s/it]                                                        29%|██▉       | 937/3250 [4:24:33<10:40:44, 16.62s/it] 29%|██▉       | 938/3250 [4:24:49<10:31:49, 16.40s/it]                                                        29%|██▉       | 938/3250 [4:24:49<10:31:49, 16.40s/it] 29%|██▉       | 939/3250 [4:25:05<10:25:18, 16.23s/it]                                                        29%|██▉       | 939/3250 [4:25:05<10:25:18, 16.23s/it] 29%|██▉       | 940/3250 [4:25:21<10:20:44, 16.12s/it]                                                        29%|██▉       | 940/3250 [4:25:21<10:20:44, 16.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7654827833175659, 'eval_runtime': 2.468, 'eval_samples_per_second': 4.862, 'eval_steps_per_second': 1.216, 'epoch': 0.29}
                                                        29%|██▉       | 940/3250 [4:25:23<10:20:44, 16.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-940
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-940/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5818, 'learning_rate': 8.075681355731338e-05, 'epoch': 0.29}
{'loss': 0.5746, 'learning_rate': 8.07186696037201e-05, 'epoch': 0.29}
{'loss': 0.5745, 'learning_rate': 8.06804969112313e-05, 'epoch': 0.29}
{'loss': 0.6232, 'learning_rate': 8.064229551555951e-05, 'epoch': 0.29}
{'loss': 0.5817, 'learning_rate': 8.060406545244413e-05, 'epoch': 0.29}
 29%|██▉       | 941/3250 [4:25:40<10:54:33, 17.01s/it]                                                        29%|██▉       | 941/3250 [4:25:40<10:54:33, 17.01s/it] 29%|██▉       | 942/3250 [4:25:56<10:41:32, 16.68s/it]                                                        29%|██▉       | 942/3250 [4:25:56<10:41:32, 16.68s/it] 29%|██▉       | 943/3250 [4:26:12<10:32:22, 16.45s/it]                                                        29%|██▉       | 943/3250 [4:26:12<10:32:22, 16.45s/it] 29%|██▉       | 944/3250 [4:26:28<10:25:30, 16.28s/it]                                                        29%|██▉       | 944/3250 [4:26:28<10:25:30, 16.28s/it] 29%|██▉       | 945/3250 [4:26:43<10:20:40, 16.16s/it]                                                        29%|██▉       | 945/3250 [4:26:43<10:20:40, 16.16s/it] 29%|██▉       | 946/3250 [4:26:59<10:17:16, 16.07s/it]                                                       {'loss': 0.6216, 'learning_rate': 8.05658067576513e-05, 'epoch': 0.29}
{'loss': 1.1129, 'learning_rate': 8.052751946697403e-05, 'epoch': 0.29}
{'loss': 0.559, 'learning_rate': 8.048920361623202e-05, 'epoch': 0.29}
{'loss': 0.5884, 'learning_rate': 8.045085924127177e-05, 'epoch': 0.29}
{'loss': 0.6023, 'learning_rate': 8.041248637796637e-05, 'epoch': 0.29}
 29%|██▉       | 946/3250 [4:26:59<10:17:16, 16.07s/it] 29%|██▉       | 947/3250 [4:27:15<10:14:34, 16.01s/it]                                                        29%|██▉       | 947/3250 [4:27:15<10:14:34, 16.01s/it] 29%|██▉       | 948/3250 [4:27:31<10:12:44, 15.97s/it]                                                        29%|██▉       | 948/3250 [4:27:32<10:12:44, 15.97s/it] 29%|██▉       | 949/3250 [4:27:48<10:17:47, 16.11s/it]                                                        29%|██▉       | 949/3250 [4:27:48<10:17:47, 16.11s/it] 29%|██▉       | 950/3250 [4:28:03<10:14:44, 16.04s/it]                                                        29%|██▉       | 950/3250 [4:28:03<10:14:44, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7599313855171204, 'eval_runtime': 2.5692, 'eval_samples_per_second': 4.671, 'eval_steps_per_second': 1.168, 'epoch': 0.29}
                                                        29%|██▉       | 950/3250 [4:28:06<10:14:44, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-950
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-950
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-950
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-950

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6154, 'learning_rate': 8.037408506221563e-05, 'epoch': 0.29}
{'loss': 0.5802, 'learning_rate': 8.033565532994593e-05, 'epoch': 0.29}
{'loss': 0.5985, 'learning_rate': 8.029719721711031e-05, 'epoch': 0.29}
{'loss': 0.6446, 'learning_rate': 8.025871075968828e-05, 'epoch': 0.29}
{'loss': 0.6133, 'learning_rate': 8.022019599368588e-05, 'epoch': 0.29}
 29%|██▉       | 951/3250 [4:28:36<13:27:19, 21.07s/it]                                                        29%|██▉       | 951/3250 [4:28:36<13:27:19, 21.07s/it] 29%|██▉       | 952/3250 [4:28:52<12:28:40, 19.55s/it]                                                        29%|██▉       | 952/3250 [4:28:52<12:28:40, 19.55s/it] 29%|██▉       | 953/3250 [4:29:08<11:46:02, 18.44s/it]                                                        29%|██▉       | 953/3250 [4:29:08<11:46:02, 18.44s/it] 29%|██▉       | 954/3250 [4:29:24<11:16:00, 17.67s/it]                                                        29%|██▉       | 954/3250 [4:29:24<11:16:00, 17.67s/it] 29%|██▉       | 955/3250 [4:29:40<10:55:10, 17.13s/it]                                                        29%|██▉       | 955/3250 [4:29:40<10:55:10, 17.13s/it] 29%|██▉       | 956/3250 [4:29:56<10:40:28, 16.75s/it]                                                       {'loss': 0.5785, 'learning_rate': 8.018165295513569e-05, 'epoch': 0.29}
{'loss': 0.5687, 'learning_rate': 8.014308168009668e-05, 'epoch': 0.29}
{'loss': 0.6115, 'learning_rate': 8.01044822046543e-05, 'epoch': 0.29}
{'loss': 0.6006, 'learning_rate': 8.006585456492029e-05, 'epoch': 0.3}
{'loss': 0.6063, 'learning_rate': 8.002719879703284e-05, 'epoch': 0.3}
 29%|██▉       | 956/3250 [4:29:56<10:40:28, 16.75s/it] 29%|██▉       | 957/3250 [4:30:12<10:30:03, 16.49s/it]                                                        29%|██▉       | 957/3250 [4:30:12<10:30:03, 16.49s/it] 29%|██▉       | 958/3250 [4:30:27<10:22:49, 16.30s/it]                                                        29%|██▉       | 958/3250 [4:30:27<10:22:49, 16.30s/it] 30%|██▉       | 959/3250 [4:30:43<10:17:37, 16.18s/it]                                                        30%|██▉       | 959/3250 [4:30:43<10:17:37, 16.18s/it] 30%|██▉       | 960/3250 [4:30:59<10:13:47, 16.08s/it]                                                        30%|██▉       | 960/3250 [4:30:59<10:13:47, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7525497078895569, 'eval_runtime': 3.3683, 'eval_samples_per_second': 3.563, 'eval_steps_per_second': 0.891, 'epoch': 0.3}
                                                        30%|██▉       | 960/3250 [4:31:03<10:13:47, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-960
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-960/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-960

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-960

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5867, 'learning_rate': 7.99885149371564e-05, 'epoch': 0.3}
{'loss': 0.6017, 'learning_rate': 7.99498030214817e-05, 'epoch': 0.3}
{'loss': 0.5845, 'learning_rate': 7.991106308622572e-05, 'epoch': 0.3}
{'loss': 0.618, 'learning_rate': 7.987229516763168e-05, 'epoch': 0.3}
{'loss': 0.6001, 'learning_rate': 7.983349930196896e-05, 'epoch': 0.3}
 30%|██▉       | 961/3250 [4:31:19<10:57:34, 17.24s/it]                                                        30%|██▉       | 961/3250 [4:31:19<10:57:34, 17.24s/it] 30%|██▉       | 962/3250 [4:31:35<10:41:34, 16.82s/it]                                                        30%|██▉       | 962/3250 [4:31:35<10:41:34, 16.82s/it] 30%|██▉       | 963/3250 [4:31:51<10:30:10, 16.53s/it]                                                        30%|██▉       | 963/3250 [4:31:51<10:30:10, 16.53s/it] 30%|██▉       | 964/3250 [4:32:07<10:22:07, 16.33s/it]                                                        30%|██▉       | 964/3250 [4:32:07<10:22:07, 16.33s/it] 30%|██▉       | 965/3250 [4:32:22<10:16:18, 16.18s/it]                                                        30%|██▉       | 965/3250 [4:32:23<10:16:18, 16.18s/it] 30%|██▉       | 966/3250 [4:32:38<10:12:22, 16.09s/it]                                                       {'loss': 0.5945, 'learning_rate': 7.979467552553308e-05, 'epoch': 0.3}
{'loss': 0.5945, 'learning_rate': 7.975582387464568e-05, 'epoch': 0.3}
{'loss': 0.6113, 'learning_rate': 7.97169443856545e-05, 'epoch': 0.3}
{'loss': 0.5614, 'learning_rate': 7.967803709493325e-05, 'epoch': 0.3}
{'loss': 0.6315, 'learning_rate': 7.963910203888177e-05, 'epoch': 0.3}
 30%|██▉       | 966/3250 [4:32:38<10:12:22, 16.09s/it] 30%|██▉       | 967/3250 [4:32:55<10:17:12, 16.22s/it]                                                        30%|██▉       | 967/3250 [4:32:55<10:17:12, 16.22s/it] 30%|██▉       | 968/3250 [4:33:11<10:17:47, 16.24s/it]                                                        30%|██▉       | 968/3250 [4:33:11<10:17:47, 16.24s/it] 30%|██▉       | 969/3250 [4:33:27<10:13:05, 16.13s/it]                                                        30%|██▉       | 969/3250 [4:33:27<10:13:05, 16.13s/it] 30%|██▉       | 970/3250 [4:33:43<10:09:39, 16.04s/it]                                                        30%|██▉       | 970/3250 [4:33:43<10:09:39, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7566299438476562, 'eval_runtime': 2.4618, 'eval_samples_per_second': 4.874, 'eval_steps_per_second': 1.219, 'epoch': 0.3}
                                                        30%|██▉       | 970/3250 [4:33:45<10:09:39, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-970
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-970 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-970
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-970

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-970/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-970/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5782, 'learning_rate': 7.960013925392576e-05, 'epoch': 0.3}
{'loss': 0.5814, 'learning_rate': 7.95611487765169e-05, 'epoch': 0.3}
{'loss': 0.5748, 'learning_rate': 7.952213064313283e-05, 'epoch': 0.3}
{'loss': 0.5848, 'learning_rate': 7.9483084890277e-05, 'epoch': 0.3}
{'loss': 0.5849, 'learning_rate': 7.944401155447871e-05, 'epoch': 0.3}
 30%|██▉       | 971/3250 [4:34:02<10:43:33, 16.94s/it]                                                        30%|██▉       | 971/3250 [4:34:02<10:43:33, 16.94s/it] 30%|██▉       | 972/3250 [4:34:18<10:30:18, 16.60s/it]                                                        30%|██▉       | 972/3250 [4:34:18<10:30:18, 16.60s/it] 30%|██▉       | 973/3250 [4:34:34<10:21:03, 16.37s/it]                                                        30%|██▉       | 973/3250 [4:34:34<10:21:03, 16.37s/it] 30%|██▉       | 974/3250 [4:34:49<10:14:34, 16.20s/it]                                                        30%|██▉       | 974/3250 [4:34:49<10:14:34, 16.20s/it] 30%|███       | 975/3250 [4:35:05<10:09:48, 16.08s/it]                                                        30%|███       | 975/3250 [4:35:05<10:09:48, 16.08s/it] 30%|███       | 976/3250 [4:35:21<10:06:22, 16.00s/it]                                                       {'loss': 0.6093, 'learning_rate': 7.940491067229311e-05, 'epoch': 0.3}
{'loss': 0.5855, 'learning_rate': 7.936578228030105e-05, 'epoch': 0.3}
{'loss': 1.0853, 'learning_rate': 7.932662641510915e-05, 'epoch': 0.3}
{'loss': 0.5853, 'learning_rate': 7.928744311334977e-05, 'epoch': 0.3}
{'loss': 0.6017, 'learning_rate': 7.92482324116809e-05, 'epoch': 0.3}
 30%|███       | 976/3250 [4:35:21<10:06:22, 16.00s/it] 30%|███       | 977/3250 [4:35:37<10:03:58, 15.94s/it]                                                        30%|███       | 977/3250 [4:35:37<10:03:58, 15.94s/it] 30%|███       | 978/3250 [4:35:53<10:01:45, 15.89s/it]                                                        30%|███       | 978/3250 [4:35:53<10:01:45, 15.89s/it] 30%|███       | 979/3250 [4:36:08<10:00:40, 15.87s/it]                                                        30%|███       | 979/3250 [4:36:08<10:00:40, 15.87s/it] 30%|███       | 980/3250 [4:36:24<9:59:37, 15.85s/it]                                                        30%|███       | 980/3250 [4:36:24<9:59:37, 15.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7539452910423279, 'eval_runtime': 2.458, 'eval_samples_per_second': 4.882, 'eval_steps_per_second': 1.221, 'epoch': 0.3}
                                                       30%|███       | 980/3250 [4:36:27<9:59:37, 15.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-980
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-980the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-980
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-980

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-980/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-980/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-980/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6038, 'learning_rate': 7.920899434678612e-05, 'epoch': 0.3}
{'loss': 0.5929, 'learning_rate': 7.916972895537471e-05, 'epoch': 0.3}
{'loss': 0.5663, 'learning_rate': 7.913043627418144e-05, 'epoch': 0.3}
{'loss': 0.6244, 'learning_rate': 7.909111633996664e-05, 'epoch': 0.3}
{'loss': 0.6192, 'learning_rate': 7.905176918951613e-05, 'epoch': 0.3}
 30%|███       | 981/3250 [4:37:18<17:12:56, 27.31s/it]                                                        30%|███       | 981/3250 [4:37:18<17:12:56, 27.31s/it] 30%|███       | 982/3250 [4:37:34<15:02:06, 23.87s/it]                                                        30%|███       | 982/3250 [4:37:34<15:02:06, 23.87s/it] 30%|███       | 983/3250 [4:37:50<13:30:24, 21.45s/it]                                                        30%|███       | 983/3250 [4:37:50<13:30:24, 21.45s/it] 30%|███       | 984/3250 [4:38:06<12:33:20, 19.95s/it]                                                        30%|███       | 984/3250 [4:38:06<12:33:20, 19.95s/it] 30%|███       | 985/3250 [4:38:22<11:46:03, 18.70s/it]                                                        30%|███       | 985/3250 [4:38:22<11:46:03, 18.70s/it] 30%|███       | 986/3250 [4:38:38<11:13:01, 17.84s/it]                                                       {'loss': 0.5775, 'learning_rate': 7.901239485964121e-05, 'epoch': 0.3}
{'loss': 0.5685, 'learning_rate': 7.897299338717854e-05, 'epoch': 0.3}
{'loss': 0.6026, 'learning_rate': 7.89335648089903e-05, 'epoch': 0.3}
{'loss': 0.5842, 'learning_rate': 7.889410916196389e-05, 'epoch': 0.3}
{'loss': 0.596, 'learning_rate': 7.885462648301212e-05, 'epoch': 0.3}
 30%|███       | 986/3250 [4:38:38<11:13:01, 17.84s/it] 30%|███       | 987/3250 [4:38:54<10:49:43, 17.23s/it]                                                        30%|███       | 987/3250 [4:38:54<10:49:43, 17.23s/it] 30%|███       | 988/3250 [4:39:10<10:33:19, 16.80s/it]                                                        30%|███       | 988/3250 [4:39:10<10:33:19, 16.80s/it] 30%|███       | 989/3250 [4:39:25<10:22:01, 16.51s/it]                                                        30%|███       | 989/3250 [4:39:25<10:22:01, 16.51s/it] 30%|███       | 990/3250 [4:39:41<10:13:53, 16.30s/it]                                                        30%|███       | 990/3250 [4:39:41<10:13:53, 16.30s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7516659498214722, 'eval_runtime': 2.4747, 'eval_samples_per_second': 4.849, 'eval_steps_per_second': 1.212, 'epoch': 0.3}
                                                        30%|███       | 990/3250 [4:39:44<10:13:53, 16.30s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-990
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-990/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-990

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-990

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5788, 'learning_rate': 7.881511680907307e-05, 'epoch': 0.3}
{'loss': 0.5793, 'learning_rate': 7.877558017711007e-05, 'epoch': 0.31}
{'loss': 0.5855, 'learning_rate': 7.873601662411167e-05, 'epoch': 0.31}
{'loss': 0.6005, 'learning_rate': 7.86964261870916e-05, 'epoch': 0.31}
{'loss': 0.5928, 'learning_rate': 7.865680890308879e-05, 'epoch': 0.31}
 30%|███       | 991/3250 [4:40:00<10:44:53, 17.13s/it]                                                        30%|███       | 991/3250 [4:40:00<10:44:53, 17.13s/it] 31%|███       | 992/3250 [4:40:16<10:29:46, 16.73s/it]                                                        31%|███       | 992/3250 [4:40:16<10:29:46, 16.73s/it] 31%|███       | 993/3250 [4:40:32<10:19:09, 16.46s/it]                                                        31%|███       | 993/3250 [4:40:32<10:19:09, 16.46s/it] 31%|███       | 994/3250 [4:40:48<10:11:39, 16.27s/it]                                                        31%|███       | 994/3250 [4:40:48<10:11:39, 16.27s/it] 31%|███       | 995/3250 [4:41:04<10:06:08, 16.13s/it]                                                        31%|███       | 995/3250 [4:41:04<10:06:08, 16.13s/it] 31%|███       | 996/3250 [4:41:19<10:02:20, 16.03s/it]                                                       {'loss': 0.5782, 'learning_rate': 7.86171648091672e-05, 'epoch': 0.31}
{'loss': 0.5908, 'learning_rate': 7.857749394241593e-05, 'epoch': 0.31}
{'loss': 0.6115, 'learning_rate': 7.853779633994913e-05, 'epoch': 0.31}
{'loss': 0.5381, 'learning_rate': 7.849807203890595e-05, 'epoch': 0.31}
{'loss': 0.6158, 'learning_rate': 7.84583210764505e-05, 'epoch': 0.31}
 31%|███       | 996/3250 [4:41:19<10:02:20, 16.03s/it] 31%|███       | 997/3250 [4:41:35<9:59:40, 15.97s/it]                                                        31%|███       | 997/3250 [4:41:35<9:59:40, 15.97s/it] 31%|███       | 998/3250 [4:41:51<9:59:10, 15.96s/it]                                                       31%|███       | 998/3250 [4:41:51<9:59:10, 15.96s/it] 31%|███       | 999/3250 [4:42:07<9:57:06, 15.92s/it]                                                       31%|███       | 999/3250 [4:42:07<9:57:06, 15.92s/it] 31%|███       | 1000/3250 [4:42:23<10:00:15, 16.01s/it]                                                         31%|███       | 1000/3250 [4:42:23<10:00:15, 16.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7555659413337708, 'eval_runtime': 2.4531, 'eval_samples_per_second': 4.892, 'eval_steps_per_second': 1.223, 'epoch': 0.31}
                                                         31%|███       | 1000/3250 [4:42:26<10:00:15, 16.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1000
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1000
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1000

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1000
/u/bzd2/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.
  warnings.warn("Repo card metadata block was not found. Setting CardData to empty.")
/u/bzd2/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.
  warnings.warn("Repo card metadata block was not found. Setting CardData to empty.")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5773, 'learning_rate': 7.841854348977186e-05, 'epoch': 0.31}
{'loss': 0.5733, 'learning_rate': 7.837873931608401e-05, 'epoch': 0.31}
{'loss': 0.5739, 'learning_rate': 7.833890859262579e-05, 'epoch': 0.31}
{'loss': 0.5677, 'learning_rate': 7.829905135666091e-05, 'epoch': 0.31}
{'loss': 0.5915, 'learning_rate': 7.825916764547787e-05, 'epoch': 0.31}
 31%|███       | 1001/3250 [4:43:17<17:01:51, 27.26s/it]                                                         31%|███       | 1001/3250 [4:43:17<17:01:51, 27.26s/it] 31%|███       | 1002/3250 [4:43:32<14:52:43, 23.83s/it]                                                         31%|███       | 1002/3250 [4:43:32<14:52:43, 23.83s/it] 31%|███       | 1003/3250 [4:43:48<13:22:24, 21.43s/it]                                                         31%|███       | 1003/3250 [4:43:48<13:22:24, 21.43s/it] 31%|███       | 1004/3250 [4:44:04<12:19:04, 19.74s/it]                                                         31%|███       | 1004/3250 [4:44:04<12:19:04, 19.74s/it] 31%|███       | 1005/3250 [4:44:20<11:34:40, 18.57s/it]                                                         31%|███       | 1005/3250 [4:44:20<11:34:40, 18.57s/it] 31%|███       | 1006/3250 [4:44:36<11:03:41, 17.75s/it]                                        {'loss': 0.5754, 'learning_rate': 7.82192574963899e-05, 'epoch': 0.31}
{'loss': 0.5999, 'learning_rate': 7.817932094673501e-05, 'epoch': 0.31}
{'loss': 1.0707, 'learning_rate': 7.813935803387591e-05, 'epoch': 0.31}
{'loss': 0.5835, 'learning_rate': 7.809936879519994e-05, 'epoch': 0.31}
{'loss': 0.5801, 'learning_rate': 7.805935326811912e-05, 'epoch': 0.31}
                 31%|███       | 1006/3250 [4:44:36<11:03:41, 17.75s/it] 31%|███       | 1007/3250 [4:44:52<10:41:52, 17.17s/it]                                                         31%|███       | 1007/3250 [4:44:52<10:41:52, 17.17s/it] 31%|███       | 1008/3250 [4:45:07<10:26:12, 16.76s/it]                                                         31%|███       | 1008/3250 [4:45:07<10:26:12, 16.76s/it] 31%|███       | 1009/3250 [4:45:23<10:15:25, 16.48s/it]                                                         31%|███       | 1009/3250 [4:45:23<10:15:25, 16.48s/it] 31%|███       | 1010/3250 [4:45:39<10:07:45, 16.28s/it]                                                         31%|███       | 1010/3250 [4:45:39<10:07:45, 16.28s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7502701282501221, 'eval_runtime': 2.4666, 'eval_samples_per_second': 4.865, 'eval_steps_per_second': 1.216, 'epoch': 0.31}
                                                         31%|███       | 1010/3250 [4:45:41<10:07:45, 16.28s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1010
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1010/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1010

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1010

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1010/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5994, 'learning_rate': 7.801931149007001e-05, 'epoch': 0.31}
{'loss': 0.5914, 'learning_rate': 7.797924349851376e-05, 'epoch': 0.31}
{'loss': 0.564, 'learning_rate': 7.793914933093604e-05, 'epoch': 0.31}
{'loss': 0.5708, 'learning_rate': 7.7899029024847e-05, 'epoch': 0.31}
{'loss': 0.6488, 'learning_rate': 7.785888261778124e-05, 'epoch': 0.31}
 31%|███       | 1011/3250 [4:45:58<10:38:33, 17.11s/it]                                                         31%|███       | 1011/3250 [4:45:58<10:38:33, 17.11s/it] 31%|███       | 1012/3250 [4:46:14<10:23:51, 16.73s/it]                                                         31%|███       | 1012/3250 [4:46:14<10:23:51, 16.73s/it] 31%|███       | 1013/3250 [4:46:30<10:13:32, 16.46s/it]                                                         31%|███       | 1013/3250 [4:46:30<10:13:32, 16.46s/it] 31%|███       | 1014/3250 [4:46:46<10:06:11, 16.27s/it]                                                         31%|███       | 1014/3250 [4:46:46<10:06:11, 16.27s/it] 31%|███       | 1015/3250 [4:47:01<10:01:00, 16.13s/it]                                                         31%|███       | 1015/3250 [4:47:01<10:01:00, 16.13s/it] 31%|███▏      | 1016/3250 [4:47:17<9:57:17, 16.04s/it]                                       {'loss': 0.5879, 'learning_rate': 7.781871014729781e-05, 'epoch': 0.31}
{'loss': 0.585, 'learning_rate': 7.777851165098012e-05, 'epoch': 0.31}
{'loss': 0.5468, 'learning_rate': 7.773828716643591e-05, 'epoch': 0.31}
{'loss': 0.6, 'learning_rate': 7.769803673129727e-05, 'epoch': 0.31}
{'loss': 0.597, 'learning_rate': 7.765776038322057e-05, 'epoch': 0.31}
                  31%|███▏      | 1016/3250 [4:47:17<9:57:17, 16.04s/it] 31%|███▏      | 1017/3250 [4:47:34<10:01:28, 16.16s/it]                                                         31%|███▏      | 1017/3250 [4:47:34<10:01:28, 16.16s/it] 31%|███▏      | 1018/3250 [4:47:49<9:57:21, 16.06s/it]                                                         31%|███▏      | 1018/3250 [4:47:49<9:57:21, 16.06s/it] 31%|███▏      | 1019/3250 [4:48:05<9:54:26, 15.99s/it]                                                        31%|███▏      | 1019/3250 [4:48:05<9:54:26, 15.99s/it] 31%|███▏      | 1020/3250 [4:48:21<9:52:17, 15.94s/it]                                                        31%|███▏      | 1020/3250 [4:48:21<9:52:17, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7514569759368896, 'eval_runtime': 2.464, 'eval_samples_per_second': 4.87, 'eval_steps_per_second': 1.218, 'epoch': 0.31}
                                                        31%|███▏      | 1020/3250 [4:48:24<9:52:17, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1020
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1020/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1020

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1020

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1020/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1020/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5804, 'learning_rate': 7.761745815988637e-05, 'epoch': 0.31}
{'loss': 0.5614, 'learning_rate': 7.757713009899949e-05, 'epoch': 0.31}
{'loss': 0.5721, 'learning_rate': 7.753677623828892e-05, 'epoch': 0.31}
{'loss': 0.5882, 'learning_rate': 7.749639661550775e-05, 'epoch': 0.32}
{'loss': 0.5986, 'learning_rate': 7.745599126843319e-05, 'epoch': 0.32}
 31%|███▏      | 1021/3250 [4:48:40<10:26:33, 16.87s/it]                                                         31%|███▏      | 1021/3250 [4:48:40<10:26:33, 16.87s/it] 31%|███▏      | 1022/3250 [4:48:56<10:14:31, 16.55s/it]                                                         31%|███▏      | 1022/3250 [4:48:56<10:14:31, 16.55s/it] 31%|███▏      | 1023/3250 [4:49:12<10:06:06, 16.33s/it]                                                         31%|███▏      | 1023/3250 [4:49:12<10:06:06, 16.33s/it] 32%|███▏      | 1024/3250 [4:49:28<10:00:05, 16.17s/it]                                                         32%|███▏      | 1024/3250 [4:49:28<10:00:05, 16.17s/it] 32%|███▏      | 1025/3250 [4:49:43<9:55:59, 16.07s/it]                                                         32%|███▏      | 1025/3250 [4:49:43<9:55:59, 16.07s/it] 32%|███▏      | 1026/3250 [4:49:59<9:52:52, 15.99s/it]                     {'loss': 0.5857, 'learning_rate': 7.741556023486654e-05, 'epoch': 0.32}
{'loss': 0.5668, 'learning_rate': 7.737510355263311e-05, 'epoch': 0.32}
{'loss': 0.6011, 'learning_rate': 7.733462125958219e-05, 'epoch': 0.32}
{'loss': 0.5723, 'learning_rate': 7.729411339358708e-05, 'epoch': 0.32}
{'loss': 0.5904, 'learning_rate': 7.725357999254492e-05, 'epoch': 0.32}
                                   32%|███▏      | 1026/3250 [4:49:59<9:52:52, 15.99s/it] 32%|███▏      | 1027/3250 [4:50:15<9:50:34, 15.94s/it]                                                        32%|███▏      | 1027/3250 [4:50:15<9:50:34, 15.94s/it] 32%|███▏      | 1028/3250 [4:50:31<9:48:50, 15.90s/it]                                                        32%|███▏      | 1028/3250 [4:50:31<9:48:50, 15.90s/it] 32%|███▏      | 1029/3250 [4:50:47<9:47:41, 15.88s/it]                                                        32%|███▏      | 1029/3250 [4:50:47<9:47:41, 15.88s/it] 32%|███▏      | 1030/3250 [4:51:02<9:46:35, 15.85s/it]                                                        32%|███▏      | 1030/3250 [4:51:02<9:46:35, 15.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7549830675125122, 'eval_runtime': 2.4625, 'eval_samples_per_second': 4.873, 'eval_steps_per_second': 1.218, 'epoch': 0.32}
                                                        32%|███▏      | 1030/3250 [4:51:05<9:46:35, 15.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1030
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1030/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1030

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1030

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5817, 'learning_rate': 7.721302109437685e-05, 'epoch': 0.32}
{'loss': 0.5709, 'learning_rate': 7.717243673702777e-05, 'epoch': 0.32}
{'loss': 0.5564, 'learning_rate': 7.713182695846643e-05, 'epoch': 0.32}
{'loss': 0.5552, 'learning_rate': 7.709119179668538e-05, 'epoch': 0.32}
{'loss': 0.5942, 'learning_rate': 7.70505312897009e-05, 'epoch': 0.32}
 32%|███▏      | 1031/3250 [4:51:21<10:21:27, 16.80s/it]                                                         32%|███▏      | 1031/3250 [4:51:21<10:21:27, 16.80s/it] 32%|███▏      | 1032/3250 [4:51:37<10:10:19, 16.51s/it]                                                         32%|███▏      | 1032/3250 [4:51:37<10:10:19, 16.51s/it] 32%|███▏      | 1033/3250 [4:51:53<10:05:19, 16.38s/it]                                                         32%|███▏      | 1033/3250 [4:51:53<10:05:19, 16.38s/it] 32%|███▏      | 1034/3250 [4:52:09<9:58:47, 16.21s/it]                                                         32%|███▏      | 1034/3250 [4:52:09<9:58:47, 16.21s/it] 32%|███▏      | 1035/3250 [4:52:25<9:54:13, 16.10s/it]                                                        32%|███▏      | 1035/3250 [4:52:25<9:54:13, 16.10s/it] 32%|███▏      | 1036/3250 [4:52:41<9:51:00, 16.02s/it]                        {'loss': 0.5563, 'learning_rate': 7.700984547555299e-05, 'epoch': 0.32}
{'loss': 0.599, 'learning_rate': 7.696913439230534e-05, 'epoch': 0.32}
{'loss': 1.093, 'learning_rate': 7.692839807804521e-05, 'epoch': 0.32}
{'loss': 0.5451, 'learning_rate': 7.688763657088358e-05, 'epoch': 0.32}
{'loss': 0.562, 'learning_rate': 7.68468499089549e-05, 'epoch': 0.32}
                                32%|███▏      | 1036/3250 [4:52:41<9:51:00, 16.02s/it] 32%|███▏      | 1037/3250 [4:52:57<9:48:31, 15.96s/it]                                                        32%|███▏      | 1037/3250 [4:52:57<9:48:31, 15.96s/it] 32%|███▏      | 1038/3250 [4:53:12<9:46:35, 15.91s/it]                                                        32%|███▏      | 1038/3250 [4:53:12<9:46:35, 15.91s/it] 32%|███▏      | 1039/3250 [4:53:28<9:45:11, 15.88s/it]                                                        32%|███▏      | 1039/3250 [4:53:28<9:45:11, 15.88s/it] 32%|███▏      | 1040/3250 [4:53:44<9:44:10, 15.86s/it]                                                        32%|███▏      | 1040/3250 [4:53:44<9:44:10, 15.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7519528865814209, 'eval_runtime': 2.6902, 'eval_samples_per_second': 4.461, 'eval_steps_per_second': 1.115, 'epoch': 0.32}
                                                        32%|███▏      | 1040/3250 [4:53:47<9:44:10, 15.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1040
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1040
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5883, 'learning_rate': 7.680603813041718e-05, 'epoch': 0.32}
{'loss': 0.5965, 'learning_rate': 7.676520127345197e-05, 'epoch': 0.32}
{'loss': 0.5566, 'learning_rate': 7.672433937626423e-05, 'epoch': 0.32}
{'loss': 0.5749, 'learning_rate': 7.668345247708236e-05, 'epoch': 0.32}
{'loss': 0.6132, 'learning_rate': 7.664254061415818e-05, 'epoch': 0.32}
 32%|███▏      | 1041/3250 [4:54:18<12:59:19, 21.17s/it]                                                         32%|███▏      | 1041/3250 [4:54:18<12:59:19, 21.17s/it] 32%|███▏      | 1042/3250 [4:54:33<12:00:01, 19.57s/it]                                                         32%|███▏      | 1042/3250 [4:54:33<12:00:01, 19.57s/it] 32%|███▏      | 1043/3250 [4:54:49<11:18:20, 18.44s/it]                                                         32%|███▏      | 1043/3250 [4:54:49<11:18:20, 18.44s/it] 32%|███▏      | 1044/3250 [4:55:05<10:48:57, 17.65s/it]                                                         32%|███▏      | 1044/3250 [4:55:05<10:48:57, 17.65s/it] 32%|███▏      | 1045/3250 [4:55:21<10:28:32, 17.10s/it]                                                         32%|███▏      | 1045/3250 [4:55:21<10:28:32, 17.10s/it] 32%|███▏      | 1046/3250 [4:55:37<10:14:10, 16.72s/it]                  {'loss': 0.5925, 'learning_rate': 7.660160382576683e-05, 'epoch': 0.32}
{'loss': 0.5532, 'learning_rate': 7.65606421502068e-05, 'epoch': 0.32}
{'loss': 0.5366, 'learning_rate': 7.651965562579979e-05, 'epoch': 0.32}
{'loss': 0.5864, 'learning_rate': 7.647864429089087e-05, 'epoch': 0.32}
{'loss': 0.5722, 'learning_rate': 7.64376081838482e-05, 'epoch': 0.32}
                                       32%|███▏      | 1046/3250 [4:55:37<10:14:10, 16.72s/it] 32%|███▏      | 1047/3250 [4:55:53<10:04:02, 16.45s/it]                                                         32%|███▏      | 1047/3250 [4:55:53<10:04:02, 16.45s/it] 32%|███▏      | 1048/3250 [4:56:08<9:56:57, 16.27s/it]                                                         32%|███▏      | 1048/3250 [4:56:08<9:56:57, 16.27s/it] 32%|███▏      | 1049/3250 [4:56:24<9:52:05, 16.14s/it]                                                        32%|███▏      | 1049/3250 [4:56:24<9:52:05, 16.14s/it] 32%|███▏      | 1050/3250 [4:56:40<9:52:35, 16.16s/it]                                                        32%|███▏      | 1050/3250 [4:56:40<9:52:35, 16.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7523186206817627, 'eval_runtime': 2.4658, 'eval_samples_per_second': 4.867, 'eval_steps_per_second': 1.217, 'epoch': 0.32}
                                                        32%|███▏      | 1050/3250 [4:56:43<9:52:35, 16.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1050
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1050I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1050/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5757, 'learning_rate': 7.639654734306321e-05, 'epoch': 0.32}
{'loss': 0.5549, 'learning_rate': 7.635546180695038e-05, 'epoch': 0.32}
{'loss': 0.5774, 'learning_rate': 7.63143516139474e-05, 'epoch': 0.32}
{'loss': 0.572, 'learning_rate': 7.627321680251494e-05, 'epoch': 0.32}
{'loss': 0.5958, 'learning_rate': 7.623205741113673e-05, 'epoch': 0.32}
 32%|███▏      | 1051/3250 [4:57:00<10:24:32, 17.04s/it]                                                         32%|███▏      | 1051/3250 [4:57:00<10:24:32, 17.04s/it] 32%|███▏      | 1052/3250 [4:57:15<10:11:02, 16.68s/it]                                                         32%|███▏      | 1052/3250 [4:57:15<10:11:02, 16.68s/it] 32%|███▏      | 1053/3250 [4:57:31<10:01:23, 16.42s/it]                                                         32%|███▏      | 1053/3250 [4:57:31<10:01:23, 16.42s/it] 32%|███▏      | 1054/3250 [4:57:47<9:54:34, 16.25s/it]                                                         32%|███▏      | 1054/3250 [4:57:47<9:54:34, 16.25s/it] 32%|███▏      | 1055/3250 [4:58:03<9:49:34, 16.12s/it]                                                        32%|███▏      | 1055/3250 [4:58:03<9:49:34, 16.12s/it] 32%|███▏      | 1056/3250 [4:58:19<9:45:58, 16.02s/it]                        {'loss': 0.5731, 'learning_rate': 7.61908734783195e-05, 'epoch': 0.32}
{'loss': 0.5816, 'learning_rate': 7.614966504259293e-05, 'epoch': 0.33}
{'loss': 0.5709, 'learning_rate': 7.610843214250964e-05, 'epoch': 0.33}
{'loss': 0.5855, 'learning_rate': 7.606717481664514e-05, 'epoch': 0.33}
{'loss': 0.5379, 'learning_rate': 7.602589310359778e-05, 'epoch': 0.33}
                                32%|███▏      | 1056/3250 [4:58:19<9:45:58, 16.02s/it] 33%|███▎      | 1057/3250 [4:58:34<9:43:23, 15.96s/it]                                                        33%|███▎      | 1057/3250 [4:58:34<9:43:23, 15.96s/it] 33%|███▎      | 1058/3250 [4:58:50<9:41:36, 15.92s/it]                                                        33%|███▎      | 1058/3250 [4:58:50<9:41:36, 15.92s/it] 33%|███▎      | 1059/3250 [4:59:06<9:40:13, 15.89s/it]                                                        33%|███▎      | 1059/3250 [4:59:06<9:40:13, 15.89s/it] 33%|███▎      | 1060/3250 [4:59:22<9:39:15, 15.87s/it]                                                        33%|███▎      | 1060/3250 [4:59:22<9:39:15, 15.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7495867013931274, 'eval_runtime': 2.4625, 'eval_samples_per_second': 4.873, 'eval_steps_per_second': 1.218, 'epoch': 0.33}
                                                        33%|███▎      | 1060/3250 [4:59:24<9:39:15, 15.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1060
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1060/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1060I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1060

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1060/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1060/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6137, 'learning_rate': 7.598458704198869e-05, 'epoch': 0.33}
{'loss': 0.5507, 'learning_rate': 7.594325667046186e-05, 'epoch': 0.33}
{'loss': 0.553, 'learning_rate': 7.590190202768394e-05, 'epoch': 0.33}
{'loss': 0.549, 'learning_rate': 7.586052315234437e-05, 'epoch': 0.33}
{'loss': 0.5625, 'learning_rate': 7.58191200831552e-05, 'epoch': 0.33}
 33%|███▎      | 1061/3250 [4:59:41<10:13:22, 16.81s/it]                                                         33%|███▎      | 1061/3250 [4:59:41<10:13:22, 16.81s/it] 33%|███▎      | 1062/3250 [4:59:57<10:02:21, 16.52s/it]                                                         33%|███▎      | 1062/3250 [4:59:57<10:02:21, 16.52s/it] 33%|███▎      | 1063/3250 [5:00:13<9:54:19, 16.31s/it]                                                         33%|███▎      | 1063/3250 [5:00:13<9:54:19, 16.31s/it] 33%|███▎      | 1064/3250 [5:00:28<9:48:49, 16.16s/it]                                                        33%|███▎      | 1064/3250 [5:00:28<9:48:49, 16.16s/it] 33%|███▎      | 1065/3250 [5:00:44<9:44:50, 16.06s/it]                                                        33%|███▎      | 1065/3250 [5:00:44<9:44:50, 16.06s/it] 33%|███▎      | 1066/3250 [5:01:01<9:50:10, 16.21s/it]                           {'loss': 0.5582, 'learning_rate': 7.577769285885109e-05, 'epoch': 0.33}
{'loss': 0.5841, 'learning_rate': 7.57362415181894e-05, 'epoch': 0.33}
{'loss': 0.5662, 'learning_rate': 7.569476609994994e-05, 'epoch': 0.33}
{'loss': 1.0621, 'learning_rate': 7.565326664293512e-05, 'epoch': 0.33}
{'loss': 0.5508, 'learning_rate': 7.561174318596983e-05, 'epoch': 0.33}
                             33%|███▎      | 1066/3250 [5:01:01<9:50:10, 16.21s/it] 33%|███▎      | 1067/3250 [5:01:17<9:45:40, 16.10s/it]                                                        33%|███▎      | 1067/3250 [5:01:17<9:45:40, 16.10s/it] 33%|███▎      | 1068/3250 [5:01:32<9:42:25, 16.02s/it]                                                        33%|███▎      | 1068/3250 [5:01:32<9:42:25, 16.02s/it] 33%|███▎      | 1069/3250 [5:01:48<9:39:45, 15.95s/it]                                                        33%|███▎      | 1069/3250 [5:01:48<9:39:45, 15.95s/it] 33%|███▎      | 1070/3250 [5:02:04<9:38:02, 15.91s/it]                                                        33%|███▎      | 1070/3250 [5:02:04<9:38:02, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7478235363960266, 'eval_runtime': 2.4693, 'eval_samples_per_second': 4.86, 'eval_steps_per_second': 1.215, 'epoch': 0.33}
                                                        33%|███▎      | 1070/3250 [5:02:07<9:38:02, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1070
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1070 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1070
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1070

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5764, 'learning_rate': 7.557019576790138e-05, 'epoch': 0.33}
{'loss': 0.5726, 'learning_rate': 7.552862442759954e-05, 'epoch': 0.33}
{'loss': 0.567, 'learning_rate': 7.548702920395638e-05, 'epoch': 0.33}
{'loss': 0.5367, 'learning_rate': 7.544541013588645e-05, 'epoch': 0.33}
{'loss': 0.6166, 'learning_rate': 7.540376726232648e-05, 'epoch': 0.33}
 33%|███▎      | 1071/3250 [5:02:23<10:11:17, 16.83s/it]                                                         33%|███▎      | 1071/3250 [5:02:23<10:11:17, 16.83s/it] 33%|███▎      | 1072/3250 [5:02:39<10:00:04, 16.53s/it]                                                         33%|███▎      | 1072/3250 [5:02:39<10:00:04, 16.53s/it] 33%|███▎      | 1073/3250 [5:02:55<9:52:11, 16.32s/it]                                                         33%|███▎      | 1073/3250 [5:02:55<9:52:11, 16.32s/it] 33%|███▎      | 1074/3250 [5:03:11<9:46:22, 16.17s/it]                                                        33%|███▎      | 1074/3250 [5:03:11<9:46:22, 16.17s/it] 33%|███▎      | 1075/3250 [5:03:26<9:42:18, 16.06s/it]                                                        33%|███▎      | 1075/3250 [5:03:26<9:42:18, 16.06s/it] 33%|███▎      | 1076/3250 [5:03:42<9:39:22, 15.99s/it]                           {'loss': 0.5917, 'learning_rate': 7.536210062223552e-05, 'epoch': 0.33}
{'loss': 0.5513, 'learning_rate': 7.532041025459488e-05, 'epoch': 0.33}
{'loss': 0.5459, 'learning_rate': 7.527869619840801e-05, 'epoch': 0.33}
{'loss': 0.5817, 'learning_rate': 7.523695849270061e-05, 'epoch': 0.33}
{'loss': 0.5499, 'learning_rate': 7.519519717652039e-05, 'epoch': 0.33}
                             33%|███▎      | 1076/3250 [5:03:42<9:39:22, 15.99s/it] 33%|███▎      | 1077/3250 [5:03:58<9:37:20, 15.94s/it]                                                        33%|███▎      | 1077/3250 [5:03:58<9:37:20, 15.94s/it] 33%|███▎      | 1078/3250 [5:04:14<9:35:38, 15.90s/it]                                                        33%|███▎      | 1078/3250 [5:04:14<9:35:38, 15.90s/it] 33%|███▎      | 1079/3250 [5:04:30<9:34:42, 15.88s/it]                                                        33%|███▎      | 1079/3250 [5:04:30<9:34:42, 15.88s/it] 33%|███▎      | 1080/3250 [5:04:45<9:33:45, 15.86s/it]                                                        33%|███▎      | 1080/3250 [5:04:45<9:33:45, 15.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7484052181243896, 'eval_runtime': 2.4701, 'eval_samples_per_second': 4.858, 'eval_steps_per_second': 1.215, 'epoch': 0.33}
                                                        33%|███▎      | 1080/3250 [5:04:48<9:33:45, 15.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1080
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1080/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1080

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1080

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.569, 'learning_rate': 7.515341228893725e-05, 'epoch': 0.33}
{'loss': 0.5506, 'learning_rate': 7.511160386904306e-05, 'epoch': 0.33}
{'loss': 0.554, 'learning_rate': 7.50697719559518e-05, 'epoch': 0.33}
{'loss': 0.5682, 'learning_rate': 7.502791658879932e-05, 'epoch': 0.33}
{'loss': 0.5807, 'learning_rate': 7.498603780674352e-05, 'epoch': 0.33}
 33%|███▎      | 1081/3250 [5:05:05<10:08:21, 16.83s/it]                                                         33%|███▎      | 1081/3250 [5:05:05<10:08:21, 16.83s/it] 33%|███▎      | 1082/3250 [5:05:21<10:02:00, 16.66s/it]                                                         33%|███▎      | 1082/3250 [5:05:21<10:02:00, 16.66s/it] 33%|███▎      | 1083/3250 [5:05:37<9:53:17, 16.43s/it]                                                         33%|███▎      | 1083/3250 [5:05:37<9:53:17, 16.43s/it] 33%|███▎      | 1084/3250 [5:05:53<9:46:56, 16.26s/it]                                                        33%|███▎      | 1084/3250 [5:05:53<9:46:56, 16.26s/it] 33%|███▎      | 1085/3250 [5:06:08<9:42:35, 16.15s/it]                                                        33%|███▎      | 1085/3250 [5:06:08<9:42:35, 16.15s/it] 33%|███▎      | 1086/3250 [5:06:24<9:39:24, 16.07s/it]                           {'loss': 0.5672, 'learning_rate': 7.494413564896414e-05, 'epoch': 0.33}
{'loss': 0.5656, 'learning_rate': 7.490221015466279e-05, 'epoch': 0.33}
{'loss': 0.573, 'learning_rate': 7.486026136306293e-05, 'epoch': 0.33}
{'loss': 0.5884, 'learning_rate': 7.481828931340983e-05, 'epoch': 0.34}
{'loss': 0.513, 'learning_rate': 7.477629404497048e-05, 'epoch': 0.34}
                             33%|███▎      | 1086/3250 [5:06:24<9:39:24, 16.07s/it] 33%|███▎      | 1087/3250 [5:06:40<9:37:13, 16.01s/it]                                                        33%|███▎      | 1087/3250 [5:06:40<9:37:13, 16.01s/it] 33%|███▎      | 1088/3250 [5:06:56<9:35:30, 15.97s/it]                                                        33%|███▎      | 1088/3250 [5:06:56<9:35:30, 15.97s/it] 34%|███▎      | 1089/3250 [5:07:12<9:34:10, 15.94s/it]                                                        34%|███▎      | 1089/3250 [5:07:12<9:34:10, 15.94s/it] 34%|███▎      | 1090/3250 [5:07:28<9:33:11, 15.92s/it]                                                        34%|███▎      | 1090/3250 [5:07:28<9:33:11, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7511879801750183, 'eval_runtime': 2.4694, 'eval_samples_per_second': 4.86, 'eval_steps_per_second': 1.215, 'epoch': 0.34}
                                                        34%|███▎      | 1090/3250 [5:07:30<9:33:11, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1090
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1090
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1090/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1090


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1090/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5874, 'learning_rate': 7.47342755970336e-05, 'epoch': 0.34}
{'loss': 0.5499, 'learning_rate': 7.469223400890966e-05, 'epoch': 0.34}
{'loss': 0.5513, 'learning_rate': 7.465016931993069e-05, 'epoch': 0.34}
{'loss': 0.5481, 'learning_rate': 7.460808156945036e-05, 'epoch': 0.34}
{'loss': 0.5463, 'learning_rate': 7.456597079684397e-05, 'epoch': 0.34}
 34%|███▎      | 1091/3250 [5:07:47<10:06:19, 16.85s/it]                                                         34%|███▎      | 1091/3250 [5:07:47<10:06:19, 16.85s/it] 34%|███▎      | 1092/3250 [5:08:03<9:55:08, 16.55s/it]                                                         34%|███▎      | 1092/3250 [5:08:03<9:55:08, 16.55s/it] 34%|███▎      | 1093/3250 [5:08:19<9:47:10, 16.33s/it]                                                        34%|███▎      | 1093/3250 [5:08:19<9:47:10, 16.33s/it] 34%|███▎      | 1094/3250 [5:08:34<9:41:54, 16.19s/it]                                                        34%|███▎      | 1094/3250 [5:08:34<9:41:54, 16.19s/it] 34%|███▎      | 1095/3250 [5:08:50<9:37:39, 16.08s/it]                                                        34%|███▎      | 1095/3250 [5:08:50<9:37:39, 16.08s/it] 34%|███▎      | 1096/3250 [5:09:06<9:34:42, 16.01s/it]                              {'loss': 0.5716, 'learning_rate': 7.452383704150828e-05, 'epoch': 0.34}
{'loss': 0.5584, 'learning_rate': 7.44816803428616e-05, 'epoch': 0.34}
{'loss': 0.5768, 'learning_rate': 7.443950074034368e-05, 'epoch': 0.34}
{'loss': 1.057, 'learning_rate': 7.43972982734157e-05, 'epoch': 0.34}
{'loss': 0.565, 'learning_rate': 7.435507298156026e-05, 'epoch': 0.34}
                          34%|███▎      | 1096/3250 [5:09:06<9:34:42, 16.01s/it] 34%|███▍      | 1097/3250 [5:09:22<9:32:33, 15.96s/it]                                                        34%|███▍      | 1097/3250 [5:09:22<9:32:33, 15.96s/it] 34%|███▍      | 1098/3250 [5:09:38<9:31:03, 15.92s/it]                                                        34%|███▍      | 1098/3250 [5:09:38<9:31:03, 15.92s/it] 34%|███▍      | 1099/3250 [5:09:54<9:34:49, 16.03s/it]                                                        34%|███▍      | 1099/3250 [5:09:54<9:34:49, 16.03s/it] 34%|███▍      | 1100/3250 [5:10:10<9:32:21, 15.97s/it]                                                        34%|███▍      | 1100/3250 [5:10:10<9:32:21, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7476528286933899, 'eval_runtime': 2.4686, 'eval_samples_per_second': 4.861, 'eval_steps_per_second': 1.215, 'epoch': 0.34}
                                                        34%|███▍      | 1100/3250 [5:10:12<9:32:21, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1100
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1100/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1100

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1100

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1100
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5612, 'learning_rate': 7.431282490428129e-05, 'epoch': 0.34}
{'loss': 0.5769, 'learning_rate': 7.427055408110403e-05, 'epoch': 0.34}
{'loss': 0.5778, 'learning_rate': 7.422826055157501e-05, 'epoch': 0.34}
{'loss': 0.5279, 'learning_rate': 7.4185944355262e-05, 'epoch': 0.34}
{'loss': 0.5513, 'learning_rate': 7.414360553175397e-05, 'epoch': 0.34}
 34%|███▍      | 1101/3250 [5:10:29<10:06:43, 16.94s/it]                                                         34%|███▍      | 1101/3250 [5:10:29<10:06:43, 16.94s/it] 34%|███▍      | 1102/3250 [5:10:45<9:54:32, 16.61s/it]                                                         34%|███▍      | 1102/3250 [5:10:45<9:54:32, 16.61s/it] 34%|███▍      | 1103/3250 [5:11:01<9:46:03, 16.38s/it]                                                        34%|███▍      | 1103/3250 [5:11:01<9:46:03, 16.38s/it] 34%|███▍      | 1104/3250 [5:11:17<9:39:52, 16.21s/it]                                                        34%|███▍      | 1104/3250 [5:11:17<9:39:52, 16.21s/it] 34%|███▍      | 1105/3250 [5:11:32<9:35:31, 16.10s/it]                                                        34%|███▍      | 1105/3250 [5:11:32<9:35:31, 16.10s/it] 34%|███▍      | 1106/3250 [5:11:48<9:32:25, 16.02s/it]                              {'loss': 0.6214, 'learning_rate': 7.41012441206611e-05, 'epoch': 0.34}
{'loss': 0.5703, 'learning_rate': 7.405886016161465e-05, 'epoch': 0.34}
{'loss': 0.5613, 'learning_rate': 7.401645369426697e-05, 'epoch': 0.34}
{'loss': 0.5273, 'learning_rate': 7.397402475829152e-05, 'epoch': 0.34}
{'loss': 0.5731, 'learning_rate': 7.393157339338276e-05, 'epoch': 0.34}
                          34%|███▍      | 1106/3250 [5:11:48<9:32:25, 16.02s/it] 34%|███▍      | 1107/3250 [5:12:04<9:30:11, 15.96s/it]                                                        34%|███▍      | 1107/3250 [5:12:04<9:30:11, 15.96s/it] 34%|███▍      | 1108/3250 [5:12:20<9:28:30, 15.92s/it]                                                        34%|███▍      | 1108/3250 [5:12:20<9:28:30, 15.92s/it] 34%|███▍      | 1109/3250 [5:12:36<9:27:20, 15.90s/it]                                                        34%|███▍      | 1109/3250 [5:12:36<9:27:20, 15.90s/it] 34%|███▍      | 1110/3250 [5:12:52<9:26:23, 15.88s/it]                                                        34%|███▍      | 1110/3250 [5:12:52<9:26:23, 15.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7470207810401917, 'eval_runtime': 2.4755, 'eval_samples_per_second': 4.847, 'eval_steps_per_second': 1.212, 'epoch': 0.34}
                                                        34%|███▍      | 1110/3250 [5:12:54<9:26:23, 15.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1110
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1110/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5803, 'learning_rate': 7.388909963925611e-05, 'epoch': 0.34}
{'loss': 0.549, 'learning_rate': 7.384660353564794e-05, 'epoch': 0.34}
{'loss': 0.5379, 'learning_rate': 7.380408512231557e-05, 'epoch': 0.34}
{'loss': 0.5529, 'learning_rate': 7.376154443903713e-05, 'epoch': 0.34}
{'loss': 0.5628, 'learning_rate': 7.371898152561166e-05, 'epoch': 0.34}
 34%|███▍      | 1111/3250 [5:13:11<10:00:14, 16.84s/it]                                                         34%|███▍      | 1111/3250 [5:13:11<10:00:14, 16.84s/it] 34%|███▍      | 1112/3250 [5:13:26<9:49:20, 16.54s/it]                                                         34%|███▍      | 1112/3250 [5:13:26<9:49:20, 16.54s/it] 34%|███▍      | 1113/3250 [5:13:42<9:41:27, 16.33s/it]                                                        34%|███▍      | 1113/3250 [5:13:42<9:41:27, 16.33s/it] 34%|███▍      | 1114/3250 [5:13:58<9:35:50, 16.18s/it]                                                        34%|███▍      | 1114/3250 [5:13:58<9:35:50, 16.18s/it] 34%|███▍      | 1115/3250 [5:14:15<9:38:16, 16.25s/it]                                                        34%|███▍      | 1115/3250 [5:14:15<9:38:16, 16.25s/it] 34%|███▍      | 1116/3250 [5:14:30<9:33:45, 16.13s/it]                              {'loss': 0.5769, 'learning_rate': 7.367639642185891e-05, 'epoch': 0.34}
{'loss': 0.5689, 'learning_rate': 7.363378916761945e-05, 'epoch': 0.34}
{'loss': 0.5417, 'learning_rate': 7.359115980275455e-05, 'epoch': 0.34}
{'loss': 0.5774, 'learning_rate': 7.354850836714621e-05, 'epoch': 0.34}
{'loss': 0.554, 'learning_rate': 7.350583490069701e-05, 'epoch': 0.34}
                          34%|███▍      | 1116/3250 [5:14:30<9:33:45, 16.13s/it] 34%|███▍      | 1117/3250 [5:14:46<9:30:32, 16.05s/it]                                                        34%|███▍      | 1117/3250 [5:14:46<9:30:32, 16.05s/it] 34%|███▍      | 1118/3250 [5:15:02<9:27:57, 15.98s/it]                                                        34%|███▍      | 1118/3250 [5:15:02<9:27:57, 15.98s/it] 34%|███▍      | 1119/3250 [5:15:18<9:26:11, 15.94s/it]                                                        34%|███▍      | 1119/3250 [5:15:18<9:26:11, 15.94s/it] 34%|███▍      | 1120/3250 [5:15:34<9:24:55, 15.91s/it]                                                        34%|███▍      | 1120/3250 [5:15:34<9:24:55, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7482699155807495, 'eval_runtime': 2.475, 'eval_samples_per_second': 4.848, 'eval_steps_per_second': 1.212, 'epoch': 0.34}
                                                        34%|███▍      | 1120/3250 [5:15:36<9:24:55, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1120
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1120 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1120
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5666, 'learning_rate': 7.346313944333016e-05, 'epoch': 0.34}
{'loss': 0.5537, 'learning_rate': 7.342042203498951e-05, 'epoch': 0.35}
{'loss': 0.5416, 'learning_rate': 7.337768271563935e-05, 'epoch': 0.35}
{'loss': 0.5339, 'learning_rate': 7.333492152526452e-05, 'epoch': 0.35}
{'loss': 0.5384, 'learning_rate': 7.329213850387031e-05, 'epoch': 0.35}
 34%|███▍      | 1121/3250 [5:15:53<9:57:42, 16.84s/it]                                                        34%|███▍      | 1121/3250 [5:15:53<9:57:42, 16.84s/it] 35%|███▍      | 1122/3250 [5:16:09<9:46:39, 16.54s/it]                                                        35%|███▍      | 1122/3250 [5:16:09<9:46:39, 16.54s/it] 35%|███▍      | 1123/3250 [5:16:25<9:49:10, 16.62s/it]                                                        35%|███▍      | 1123/3250 [5:16:25<9:49:10, 16.62s/it] 35%|███▍      | 1124/3250 [5:16:42<9:43:40, 16.47s/it]                                                        35%|███▍      | 1124/3250 [5:16:42<9:43:40, 16.47s/it] 35%|███▍      | 1125/3250 [5:16:57<9:36:26, 16.28s/it]                                                        35%|███▍      | 1125/3250 [5:16:57<9:36:26, 16.28s/it] 35%|███▍      | 1126/3250 [5:17:13<9:31:18, 16.14s/it]                                  {'loss': 0.5667, 'learning_rate': 7.324933369148243e-05, 'epoch': 0.35}
{'loss': 0.5406, 'learning_rate': 7.3206507128147e-05, 'epoch': 0.35}
{'loss': 0.5788, 'learning_rate': 7.316365885393048e-05, 'epoch': 0.35}
{'loss': 1.0716, 'learning_rate': 7.312078890891963e-05, 'epoch': 0.35}
{'loss': 0.5283, 'learning_rate': 7.307789733322146e-05, 'epoch': 0.35}
                      35%|███▍      | 1126/3250 [5:17:13<9:31:18, 16.14s/it] 35%|███▍      | 1127/3250 [5:17:29<9:27:43, 16.04s/it]                                                        35%|███▍      | 1127/3250 [5:17:29<9:27:43, 16.04s/it] 35%|███▍      | 1128/3250 [5:17:45<9:25:01, 15.98s/it]                                                        35%|███▍      | 1128/3250 [5:17:45<9:25:01, 15.98s/it] 35%|███▍      | 1129/3250 [5:18:01<9:22:41, 15.92s/it]                                                        35%|███▍      | 1129/3250 [5:18:01<9:22:41, 15.92s/it] 35%|███▍      | 1130/3250 [5:18:16<9:21:06, 15.88s/it]                                                        35%|███▍      | 1130/3250 [5:18:16<9:21:06, 15.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7441046833992004, 'eval_runtime': 3.1485, 'eval_samples_per_second': 3.811, 'eval_steps_per_second': 0.953, 'epoch': 0.35}
                                                        35%|███▍      | 1130/3250 [5:18:20<9:21:06, 15.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1130
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1130
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1130 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1130
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1130/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5439, 'learning_rate': 7.303498416696328e-05, 'epoch': 0.35}
{'loss': 0.5658, 'learning_rate': 7.299204945029254e-05, 'epoch': 0.35}
{'loss': 0.5803, 'learning_rate': 7.294909322337689e-05, 'epoch': 0.35}
{'loss': 0.5307, 'learning_rate': 7.29061155264041e-05, 'epoch': 0.35}
{'loss': 0.5622, 'learning_rate': 7.286311639958197e-05, 'epoch': 0.35}
 35%|███▍      | 1131/3250 [5:18:55<13:18:30, 22.61s/it]                                                         35%|███▍      | 1131/3250 [5:18:55<13:18:30, 22.61s/it] 35%|███▍      | 1132/3250 [5:19:11<12:09:32, 20.67s/it]                                                         35%|███▍      | 1132/3250 [5:19:11<12:09:32, 20.67s/it] 35%|███▍      | 1133/3250 [5:19:27<11:17:55, 19.21s/it]                                                         35%|███▍      | 1133/3250 [5:19:27<11:17:55, 19.21s/it] 35%|███▍      | 1134/3250 [5:19:43<10:41:32, 18.19s/it]                                                         35%|███▍      | 1134/3250 [5:19:43<10:41:32, 18.19s/it] 35%|███▍      | 1135/3250 [5:19:59<10:28:33, 17.83s/it]                                                         35%|███▍      | 1135/3250 [5:20:00<10:28:33, 17.83s/it] 35%|███▍      | 1136/3250 [5:20:15<10:07:26, 17.24s/it]                  {'loss': 0.5987, 'learning_rate': 7.282009588313845e-05, 'epoch': 0.35}
{'loss': 0.5775, 'learning_rate': 7.277705401732143e-05, 'epoch': 0.35}
{'loss': 0.5366, 'learning_rate': 7.273399084239878e-05, 'epoch': 0.35}
{'loss': 0.5232, 'learning_rate': 7.26909063986583e-05, 'epoch': 0.35}
{'loss': 0.5822, 'learning_rate': 7.264780072640774e-05, 'epoch': 0.35}
                                       35%|███▍      | 1136/3250 [5:20:15<10:07:26, 17.24s/it] 35%|███▍      | 1137/3250 [5:20:31<9:52:24, 16.82s/it]                                                         35%|███▍      | 1137/3250 [5:20:31<9:52:24, 16.82s/it] 35%|███▌      | 1138/3250 [5:20:47<9:41:52, 16.53s/it]                                                        35%|███▌      | 1138/3250 [5:20:47<9:41:52, 16.53s/it] 35%|███▌      | 1139/3250 [5:21:03<9:34:29, 16.33s/it]                                                        35%|███▌      | 1139/3250 [5:21:03<9:34:29, 16.33s/it] 35%|███▌      | 1140/3250 [5:21:19<9:29:08, 16.18s/it]                                                        35%|███▌      | 1140/3250 [5:21:19<9:29:08, 16.18s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7435443997383118, 'eval_runtime': 2.4828, 'eval_samples_per_second': 4.833, 'eval_steps_per_second': 1.208, 'epoch': 0.35}
                                                        35%|███▌      | 1140/3250 [5:21:21<9:29:08, 16.18s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1140
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1140/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5594, 'learning_rate': 7.260467386597466e-05, 'epoch': 0.35}
{'loss': 0.5585, 'learning_rate': 7.256152585770643e-05, 'epoch': 0.35}
{'loss': 0.5396, 'learning_rate': 7.251835674197029e-05, 'epoch': 0.35}
{'loss': 0.5471, 'learning_rate': 7.24751665591531e-05, 'epoch': 0.35}
{'loss': 0.555, 'learning_rate': 7.243195534966152e-05, 'epoch': 0.35}
 35%|███▌      | 1141/3250 [5:21:38<10:01:43, 17.12s/it]                                                         35%|███▌      | 1141/3250 [5:21:38<10:01:43, 17.12s/it] 35%|███▌      | 1142/3250 [5:21:54<9:48:04, 16.74s/it]                                                         35%|███▌      | 1142/3250 [5:21:54<9:48:04, 16.74s/it] 35%|███▌      | 1143/3250 [5:22:10<9:38:23, 16.47s/it]                                                        35%|███▌      | 1143/3250 [5:22:10<9:38:23, 16.47s/it] 35%|███▌      | 1144/3250 [5:22:26<9:31:34, 16.28s/it]                                                        35%|███▌      | 1144/3250 [5:22:26<9:31:34, 16.28s/it] 35%|███▌      | 1145/3250 [5:22:41<9:26:47, 16.16s/it]                                                        35%|███▌      | 1145/3250 [5:22:41<9:26:47, 16.16s/it] 35%|███▌      | 1146/3250 [5:22:57<9:23:26, 16.07s/it]                              {'loss': 0.5708, 'learning_rate': 7.238872315392189e-05, 'epoch': 0.35}
{'loss': 0.5564, 'learning_rate': 7.234547001238012e-05, 'epoch': 0.35}
{'loss': 0.5562, 'learning_rate': 7.230219596550176e-05, 'epoch': 0.35}
{'loss': 0.5527, 'learning_rate': 7.22589010537719e-05, 'epoch': 0.35}
{'loss': 0.5573, 'learning_rate': 7.221558531769519e-05, 'epoch': 0.35}
                          35%|███▌      | 1146/3250 [5:22:57<9:23:26, 16.07s/it] 35%|███▌      | 1147/3250 [5:23:13<9:21:03, 16.01s/it]                                                        35%|███▌      | 1147/3250 [5:23:13<9:21:03, 16.01s/it] 35%|███▌      | 1148/3250 [5:23:30<9:31:04, 16.30s/it]                                                        35%|███▌      | 1148/3250 [5:23:30<9:31:04, 16.30s/it] 35%|███▌      | 1149/3250 [5:23:46<9:26:09, 16.17s/it]                                                        35%|███▌      | 1149/3250 [5:23:46<9:26:09, 16.17s/it] 35%|███▌      | 1150/3250 [5:24:02<9:22:37, 16.08s/it]                                                        35%|███▌      | 1150/3250 [5:24:02<9:22:37, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7506702542304993, 'eval_runtime': 2.467, 'eval_samples_per_second': 4.864, 'eval_steps_per_second': 1.216, 'epoch': 0.35}
                                                        35%|███▌      | 1150/3250 [5:24:04<9:22:37, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1150
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1150/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1150/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5232, 'learning_rate': 7.217224879779567e-05, 'epoch': 0.35}
{'loss': 0.5773, 'learning_rate': 7.212889153461694e-05, 'epoch': 0.35}
{'loss': 0.5226, 'learning_rate': 7.20855135687219e-05, 'epoch': 0.35}
{'loss': 0.5261, 'learning_rate': 7.204211494069292e-05, 'epoch': 0.36}
{'loss': 0.5316, 'learning_rate': 7.199869569113161e-05, 'epoch': 0.36}
 35%|███▌      | 1151/3250 [5:24:35<12:23:16, 21.25s/it]                                                         35%|███▌      | 1151/3250 [5:24:35<12:23:16, 21.25s/it] 35%|███▌      | 1152/3250 [5:24:51<11:26:33, 19.63s/it]                                                         35%|███▌      | 1152/3250 [5:24:51<11:26:33, 19.63s/it] 35%|███▌      | 1153/3250 [5:25:07<10:48:57, 18.57s/it]                                                         35%|███▌      | 1153/3250 [5:25:07<10:48:57, 18.57s/it] 36%|███▌      | 1154/3250 [5:25:23<10:20:42, 17.77s/it]                                                         36%|███▌      | 1154/3250 [5:25:23<10:20:42, 17.77s/it] 36%|███▌      | 1155/3250 [5:25:39<10:00:51, 17.21s/it]                                                         36%|███▌      | 1155/3250 [5:25:39<10:00:51, 17.21s/it] 36%|███▌      | 1156/3250 [5:25:55<9:46:50, 16.82s/it]                   {'loss': 0.5517, 'learning_rate': 7.195525586065892e-05, 'epoch': 0.36}
{'loss': 0.54, 'learning_rate': 7.191179548991507e-05, 'epoch': 0.36}
{'loss': 0.5656, 'learning_rate': 7.186831461955943e-05, 'epoch': 0.36}
{'loss': 0.544, 'learning_rate': 7.182481329027061e-05, 'epoch': 0.36}
{'loss': 1.0413, 'learning_rate': 7.178129154274636e-05, 'epoch': 0.36}
                                      36%|███▌      | 1156/3250 [5:25:55<9:46:50, 16.82s/it] 36%|███▌      | 1157/3250 [5:26:11<9:36:52, 16.54s/it]                                                        36%|███▌      | 1157/3250 [5:26:11<9:36:52, 16.54s/it] 36%|███▌      | 1158/3250 [5:26:27<9:29:51, 16.34s/it]                                                        36%|███▌      | 1158/3250 [5:26:27<9:29:51, 16.34s/it] 36%|███▌      | 1159/3250 [5:26:43<9:24:42, 16.20s/it]                                                        36%|███▌      | 1159/3250 [5:26:43<9:24:42, 16.20s/it] 36%|███▌      | 1160/3250 [5:26:58<9:20:47, 16.10s/it]                                                        36%|███▌      | 1160/3250 [5:26:58<9:20:47, 16.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7466601729393005, 'eval_runtime': 2.5163, 'eval_samples_per_second': 4.769, 'eval_steps_per_second': 1.192, 'epoch': 0.36}
                                                        36%|███▌      | 1160/3250 [5:27:01<9:20:47, 16.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1160
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1160
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1160
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.537, 'learning_rate': 7.17377494177035e-05, 'epoch': 0.36}
{'loss': 0.5699, 'learning_rate': 7.169418695587791e-05, 'epoch': 0.36}
{'loss': 0.5544, 'learning_rate': 7.165060419802453e-05, 'epoch': 0.36}
{'loss': 0.5481, 'learning_rate': 7.160700118491728e-05, 'epoch': 0.36}
{'loss': 0.5274, 'learning_rate': 7.1563377957349e-05, 'epoch': 0.36}
 36%|███▌      | 1161/3250 [5:27:35<12:56:33, 22.30s/it]                                                         36%|███▌      | 1161/3250 [5:27:35<12:56:33, 22.30s/it] 36%|███▌      | 1162/3250 [5:27:51<11:49:18, 20.38s/it]                                                         36%|███▌      | 1162/3250 [5:27:51<11:49:18, 20.38s/it] 36%|███▌      | 1163/3250 [5:28:07<11:02:07, 19.04s/it]                                                         36%|███▌      | 1163/3250 [5:28:07<11:02:07, 19.04s/it] 36%|███▌      | 1164/3250 [5:28:23<10:31:49, 18.17s/it]                                                         36%|███▌      | 1164/3250 [5:28:23<10:31:49, 18.17s/it] 36%|███▌      | 1165/3250 [5:28:39<10:07:29, 17.48s/it]                                                         36%|███▌      | 1165/3250 [5:28:39<10:07:29, 17.48s/it] 36%|███▌      | 1166/3250 [5:28:55<9:51:04, 17.02s/it]                   {'loss': 0.5874, 'learning_rate': 7.15197345561315e-05, 'epoch': 0.36}
{'loss': 0.5787, 'learning_rate': 7.147607102209538e-05, 'epoch': 0.36}
{'loss': 0.5268, 'learning_rate': 7.143238739609016e-05, 'epoch': 0.36}
{'loss': 0.5254, 'learning_rate': 7.13886837189841e-05, 'epoch': 0.36}
{'loss': 0.5571, 'learning_rate': 7.134496003166423e-05, 'epoch': 0.36}
                                      36%|███▌      | 1166/3250 [5:28:55<9:51:04, 17.02s/it] 36%|███▌      | 1167/3250 [5:29:11<9:39:08, 16.68s/it]                                                        36%|███▌      | 1167/3250 [5:29:11<9:39:08, 16.68s/it] 36%|███▌      | 1168/3250 [5:29:27<9:30:40, 16.45s/it]                                                        36%|███▌      | 1168/3250 [5:29:27<9:30:40, 16.45s/it] 36%|███▌      | 1169/3250 [5:29:43<9:24:27, 16.27s/it]                                                        36%|███▌      | 1169/3250 [5:29:43<9:24:27, 16.27s/it] 36%|███▌      | 1170/3250 [5:29:58<9:20:06, 16.16s/it]                                                        36%|███▌      | 1170/3250 [5:29:58<9:20:06, 16.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7444620132446289, 'eval_runtime': 2.48, 'eval_samples_per_second': 4.839, 'eval_steps_per_second': 1.21, 'epoch': 0.36}
                                                        36%|███▌      | 1170/3250 [5:30:01<9:20:06, 16.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1170
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1170/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5456, 'learning_rate': 7.130121637503632e-05, 'epoch': 0.36}
{'loss': 0.5601, 'learning_rate': 7.125745279002482e-05, 'epoch': 0.36}
{'loss': 0.5385, 'learning_rate': 7.121366931757281e-05, 'epoch': 0.36}
{'loss': 0.5392, 'learning_rate': 7.116986599864197e-05, 'epoch': 0.36}
{'loss': 0.5479, 'learning_rate': 7.112604287421256e-05, 'epoch': 0.36}
 36%|███▌      | 1171/3250 [5:30:47<14:55:35, 25.85s/it]                                                         36%|███▌      | 1171/3250 [5:30:47<14:55:35, 25.85s/it] 36%|███▌      | 1172/3250 [5:31:03<13:11:32, 22.85s/it]                                                         36%|███▌      | 1172/3250 [5:31:03<13:11:32, 22.85s/it] 36%|███▌      | 1173/3250 [5:31:19<11:58:54, 20.77s/it]                                                         36%|███▌      | 1173/3250 [5:31:19<11:58:54, 20.77s/it] 36%|███▌      | 1174/3250 [5:31:35<11:08:01, 19.31s/it]                                                         36%|███▌      | 1174/3250 [5:31:35<11:08:01, 19.31s/it] 36%|███▌      | 1175/3250 [5:31:50<10:32:15, 18.28s/it]                                                         36%|███▌      | 1175/3250 [5:31:50<10:32:15, 18.28s/it] 36%|███▌      | 1176/3250 [5:32:06<10:06:58, 17.56s/it]                  {'loss': 0.5675, 'learning_rate': 7.108219998528337e-05, 'epoch': 0.36}
{'loss': 0.5422, 'learning_rate': 7.103833737287168e-05, 'epoch': 0.36}
{'loss': 0.544, 'learning_rate': 7.099445507801323e-05, 'epoch': 0.36}
{'loss': 0.5504, 'learning_rate': 7.095055314176216e-05, 'epoch': 0.36}
{'loss': 0.5698, 'learning_rate': 7.090663160519095e-05, 'epoch': 0.36}
                                       36%|███▌      | 1176/3250 [5:32:06<10:06:58, 17.56s/it] 36%|███▌      | 1177/3250 [5:32:22<9:49:14, 17.05s/it]                                                         36%|███▌      | 1177/3250 [5:32:22<9:49:14, 17.05s/it] 36%|███▌      | 1178/3250 [5:32:38<9:37:23, 16.72s/it]                                                        36%|███▌      | 1178/3250 [5:32:38<9:37:23, 16.72s/it] 36%|███▋      | 1179/3250 [5:32:54<9:28:33, 16.47s/it]                                                        36%|███▋      | 1179/3250 [5:32:54<9:28:33, 16.47s/it] 36%|███▋      | 1180/3250 [5:33:10<9:22:22, 16.30s/it]                                                        36%|███▋      | 1180/3250 [5:33:10<9:22:22, 16.30s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7475385665893555, 'eval_runtime': 2.7507, 'eval_samples_per_second': 4.363, 'eval_steps_per_second': 1.091, 'epoch': 0.36}
                                                        36%|███▋      | 1180/3250 [5:33:13<9:22:22, 16.30s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1180
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1180 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1180
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1180

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5029, 'learning_rate': 7.086269050939051e-05, 'epoch': 0.36}
{'loss': 0.5846, 'learning_rate': 7.081872989546998e-05, 'epoch': 0.36}
{'loss': 0.541, 'learning_rate': 7.077474980455678e-05, 'epoch': 0.36}
{'loss': 0.5248, 'learning_rate': 7.073075027779651e-05, 'epoch': 0.36}
{'loss': 0.5154, 'learning_rate': 7.068673135635302e-05, 'epoch': 0.36}
 36%|███▋      | 1181/3250 [5:33:31<10:09:01, 17.66s/it]                                                         36%|███▋      | 1181/3250 [5:33:31<10:09:01, 17.66s/it] 36%|███▋      | 1182/3250 [5:33:47<9:50:08, 17.12s/it]                                                         36%|███▋      | 1182/3250 [5:33:47<9:50:08, 17.12s/it] 36%|███▋      | 1183/3250 [5:34:03<9:36:52, 16.75s/it]                                                        36%|███▋      | 1183/3250 [5:34:03<9:36:52, 16.75s/it] 36%|███▋      | 1184/3250 [5:34:18<9:27:32, 16.48s/it]                                                        36%|███▋      | 1184/3250 [5:34:18<9:27:32, 16.48s/it] 36%|███▋      | 1185/3250 [5:34:34<9:20:52, 16.30s/it]                                                        36%|███▋      | 1185/3250 [5:34:34<9:20:52, 16.30s/it] 36%|███▋      | 1186/3250 [5:34:50<9:16:00, 16.16s/it]                              {'loss': 0.5267, 'learning_rate': 7.06426930814083e-05, 'epoch': 0.36}
{'loss': 0.5534, 'learning_rate': 7.059863549416237e-05, 'epoch': 0.37}
{'loss': 0.5301, 'learning_rate': 7.05545586358334e-05, 'epoch': 0.37}
{'loss': 0.5563, 'learning_rate': 7.051046254765755e-05, 'epoch': 0.37}
{'loss': 1.0392, 'learning_rate': 7.046634727088898e-05, 'epoch': 0.37}
                          36%|███▋      | 1186/3250 [5:34:50<9:16:00, 16.16s/it] 37%|███▋      | 1187/3250 [5:35:06<9:14:51, 16.14s/it]                                                        37%|███▋      | 1187/3250 [5:35:06<9:14:51, 16.14s/it] 37%|███▋      | 1188/3250 [5:35:22<9:12:09, 16.07s/it]                                                        37%|███▋      | 1188/3250 [5:35:22<9:12:09, 16.07s/it] 37%|███▋      | 1189/3250 [5:35:38<9:10:04, 16.01s/it]                                                        37%|███▋      | 1189/3250 [5:35:38<9:10:04, 16.01s/it] 37%|███▋      | 1190/3250 [5:35:54<9:08:14, 15.97s/it]                                                        37%|███▋      | 1190/3250 [5:35:54<9:08:14, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7423928380012512, 'eval_runtime': 2.4811, 'eval_samples_per_second': 4.837, 'eval_steps_per_second': 1.209, 'epoch': 0.37}
                                                        37%|███▋      | 1190/3250 [5:35:56<9:08:14, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1190
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1190/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5412, 'learning_rate': 7.042221284679982e-05, 'epoch': 0.37}
{'loss': 0.5426, 'learning_rate': 7.037805931668005e-05, 'epoch': 0.37}
{'loss': 0.5464, 'learning_rate': 7.03338867218376e-05, 'epoch': 0.37}
{'loss': 0.5478, 'learning_rate': 7.02896951035982e-05, 'epoch': 0.37}
{'loss': 0.5013, 'learning_rate': 7.02454845033054e-05, 'epoch': 0.37}
 37%|███▋      | 1191/3250 [5:36:14<9:47:00, 17.11s/it]                                                        37%|███▋      | 1191/3250 [5:36:14<9:47:00, 17.11s/it] 37%|███▋      | 1192/3250 [5:36:30<9:34:29, 16.75s/it]                                                        37%|███▋      | 1192/3250 [5:36:30<9:34:29, 16.75s/it] 37%|███▋      | 1193/3250 [5:36:45<9:25:31, 16.50s/it]                                                        37%|███▋      | 1193/3250 [5:36:45<9:25:31, 16.50s/it] 37%|███▋      | 1194/3250 [5:37:01<9:19:14, 16.32s/it]                                                        37%|███▋      | 1194/3250 [5:37:01<9:19:14, 16.32s/it] 37%|███▋      | 1195/3250 [5:37:17<9:14:44, 16.20s/it]                                                        37%|███▋      | 1195/3250 [5:37:17<9:14:44, 16.20s/it] 37%|███▋      | 1196/3250 [5:37:33<9:11:18, 16.10s/it]                                  {'loss': 0.534, 'learning_rate': 7.020125496232044e-05, 'epoch': 0.37}
{'loss': 0.591, 'learning_rate': 7.015700652202237e-05, 'epoch': 0.37}
{'loss': 0.5405, 'learning_rate': 7.01127392238079e-05, 'epoch': 0.37}
{'loss': 0.5431, 'learning_rate': 7.006845310909131e-05, 'epoch': 0.37}
{'loss': 0.5402, 'learning_rate': 7.002414821930458e-05, 'epoch': 0.37}
                      37%|███▋      | 1196/3250 [5:37:33<9:11:18, 16.10s/it] 37%|███▋      | 1197/3250 [5:37:49<9:10:29, 16.09s/it]                                                        37%|███▋      | 1197/3250 [5:37:49<9:10:29, 16.09s/it] 37%|███▋      | 1198/3250 [5:38:05<9:08:03, 16.03s/it]                                                        37%|███▋      | 1198/3250 [5:38:05<9:08:03, 16.03s/it] 37%|███▋      | 1199/3250 [5:38:21<9:06:33, 15.99s/it]                                                        37%|███▋      | 1199/3250 [5:38:21<9:06:33, 15.99s/it] 37%|███▋      | 1200/3250 [5:38:37<9:04:57, 15.95s/it]                                                        37%|███▋      | 1200/3250 [5:38:37<9:04:57, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7415599226951599, 'eval_runtime': 2.9313, 'eval_samples_per_second': 4.094, 'eval_steps_per_second': 1.023, 'epoch': 0.37}
                                                        37%|███▋      | 1200/3250 [5:38:40<9:04:57, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1200
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1200/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1200/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1200/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.527, 'learning_rate': 6.99798245958972e-05, 'epoch': 0.37}
{'loss': 0.5551, 'learning_rate': 6.993548228033618e-05, 'epoch': 0.37}
{'loss': 0.5359, 'learning_rate': 6.989112131410607e-05, 'epoch': 0.37}
{'loss': 0.5351, 'learning_rate': 6.984674173870882e-05, 'epoch': 0.37}
{'loss': 0.5294, 'learning_rate': 6.98023435956638e-05, 'epoch': 0.37}
 37%|███▋      | 1201/3250 [5:38:57<9:42:57, 17.07s/it]                                                        37%|███▋      | 1201/3250 [5:38:57<9:42:57, 17.07s/it] 37%|███▋      | 1202/3250 [5:39:12<9:30:23, 16.71s/it]                                                        37%|███▋      | 1202/3250 [5:39:12<9:30:23, 16.71s/it] 37%|███▋      | 1203/3250 [5:39:28<9:21:24, 16.46s/it]                                                        37%|███▋      | 1203/3250 [5:39:28<9:21:24, 16.46s/it] 37%|███▋      | 1204/3250 [5:39:44<9:14:51, 16.27s/it]                                                        37%|███▋      | 1204/3250 [5:39:44<9:14:51, 16.27s/it] 37%|███▋      | 1205/3250 [5:40:00<9:10:28, 16.15s/it]                                                        37%|███▋      | 1205/3250 [5:40:00<9:10:28, 16.15s/it] 37%|███▋      | 1206/3250 [5:40:16<9:07:25, 16.07s/it]                                  {'loss': 0.5474, 'learning_rate': 6.975792692650777e-05, 'epoch': 0.37}
{'loss': 0.5571, 'learning_rate': 6.971349177279481e-05, 'epoch': 0.37}
{'loss': 0.5446, 'learning_rate': 6.966903817609629e-05, 'epoch': 0.37}
{'loss': 0.5222, 'learning_rate': 6.962456617800081e-05, 'epoch': 0.37}
{'loss': 0.5637, 'learning_rate': 6.958007582011426e-05, 'epoch': 0.37}
                      37%|███▋      | 1206/3250 [5:40:16<9:07:25, 16.07s/it] 37%|███▋      | 1207/3250 [5:40:32<9:04:58, 16.01s/it]                                                        37%|███▋      | 1207/3250 [5:40:32<9:04:58, 16.01s/it] 37%|███▋      | 1208/3250 [5:40:48<9:03:15, 15.96s/it]                                                        37%|███▋      | 1208/3250 [5:40:48<9:03:15, 15.96s/it] 37%|███▋      | 1209/3250 [5:41:03<9:02:05, 15.94s/it]                                                        37%|███▋      | 1209/3250 [5:41:03<9:02:05, 15.94s/it] 37%|███▋      | 1210/3250 [5:41:19<9:00:59, 15.91s/it]                                                        37%|███▋      | 1210/3250 [5:41:19<9:00:59, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7473938465118408, 'eval_runtime': 2.5057, 'eval_samples_per_second': 4.789, 'eval_steps_per_second': 1.197, 'epoch': 0.37}
                                                        37%|███▋      | 1210/3250 [5:41:22<9:00:59, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1210
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1210
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1210
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5251, 'learning_rate': 6.95355671440596e-05, 'epoch': 0.37}
{'loss': 0.5483, 'learning_rate': 6.949104019147703e-05, 'epoch': 0.37}
{'loss': 0.5344, 'learning_rate': 6.94464950040238e-05, 'epoch': 0.37}
{'loss': 0.5259, 'learning_rate': 6.940193162337421e-05, 'epoch': 0.37}
{'loss': 0.5132, 'learning_rate': 6.935735009121958e-05, 'epoch': 0.37}
 37%|███▋      | 1211/3250 [5:41:57<12:39:51, 22.36s/it]                                                         37%|███▋      | 1211/3250 [5:41:57<12:39:51, 22.36s/it] 37%|███▋      | 1212/3250 [5:42:13<11:33:32, 20.42s/it]                                                         37%|███▋      | 1212/3250 [5:42:13<11:33:32, 20.42s/it] 37%|███▋      | 1213/3250 [5:42:29<10:50:52, 19.17s/it]                                                         37%|███▋      | 1213/3250 [5:42:29<10:50:52, 19.17s/it] 37%|███▋      | 1214/3250 [5:42:45<10:17:18, 18.19s/it]                                                         37%|███▋      | 1214/3250 [5:42:45<10:17:18, 18.19s/it] 37%|███▋      | 1215/3250 [5:43:01<9:53:37, 17.50s/it]                                                         37%|███▋      | 1215/3250 [5:43:01<9:53:37, 17.50s/it] 37%|███▋      | 1216/3250 [5:43:17<9:36:59, 17.02s/it]                     {'loss': 0.5214, 'learning_rate': 6.931275044926828e-05, 'epoch': 0.37}
{'loss': 0.54, 'learning_rate': 6.926813273924553e-05, 'epoch': 0.37}
{'loss': 0.5222, 'learning_rate': 6.922349700289348e-05, 'epoch': 0.37}
{'loss': 0.5533, 'learning_rate': 6.91788432819712e-05, 'epoch': 0.38}
{'loss': 1.0427, 'learning_rate': 6.91341716182545e-05, 'epoch': 0.38}
                                   37%|███▋      | 1216/3250 [5:43:17<9:36:59, 17.02s/it] 37%|███▋      | 1217/3250 [5:43:32<9:25:17, 16.68s/it]                                                        37%|███▋      | 1217/3250 [5:43:32<9:25:17, 16.68s/it] 37%|███▋      | 1218/3250 [5:43:48<9:17:08, 16.45s/it]                                                        37%|███▋      | 1218/3250 [5:43:48<9:17:08, 16.45s/it] 38%|███▊      | 1219/3250 [5:44:04<9:10:50, 16.27s/it]                                                        38%|███▊      | 1219/3250 [5:44:04<9:10:50, 16.27s/it] 38%|███▊      | 1220/3250 [5:44:20<9:05:59, 16.14s/it]                                                        38%|███▊      | 1220/3250 [5:44:20<9:05:59, 16.14s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.742817223072052, 'eval_runtime': 2.4802, 'eval_samples_per_second': 4.838, 'eval_steps_per_second': 1.21, 'epoch': 0.38}
                                                        38%|███▊      | 1220/3250 [5:44:23<9:05:59, 16.14s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1220
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1220/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5244, 'learning_rate': 6.908948205353603e-05, 'epoch': 0.38}
{'loss': 0.5275, 'learning_rate': 6.904477462962521e-05, 'epoch': 0.38}
{'loss': 0.5516, 'learning_rate': 6.900004938834809e-05, 'epoch': 0.38}
{'loss': 0.5572, 'learning_rate': 6.895530637154745e-05, 'epoch': 0.38}
{'loss': 0.522, 'learning_rate': 6.891054562108273e-05, 'epoch': 0.38}
 38%|███▊      | 1221/3250 [5:44:39<9:36:16, 17.04s/it]                                                        38%|███▊      | 1221/3250 [5:44:39<9:36:16, 17.04s/it] 38%|███▊      | 1222/3250 [5:44:55<9:24:04, 16.69s/it]                                                        38%|███▊      | 1222/3250 [5:44:55<9:24:04, 16.69s/it] 38%|███▊      | 1223/3250 [5:45:11<9:15:28, 16.44s/it]                                                        38%|███▊      | 1223/3250 [5:45:11<9:15:28, 16.44s/it] 38%|███▊      | 1224/3250 [5:45:27<9:09:16, 16.27s/it]                                                        38%|███▊      | 1224/3250 [5:45:27<9:09:16, 16.27s/it] 38%|███▊      | 1225/3250 [5:45:43<9:05:01, 16.15s/it]                                                        38%|███▊      | 1225/3250 [5:45:43<9:05:01, 16.15s/it] 38%|███▊      | 1226/3250 [5:45:58<9:01:45, 16.06s/it]                                  {'loss': 0.5279, 'learning_rate': 6.886576717882982e-05, 'epoch': 0.38}
{'loss': 0.5766, 'learning_rate': 6.882097108668132e-05, 'epoch': 0.38}
{'loss': 0.5564, 'learning_rate': 6.877615738654628e-05, 'epoch': 0.38}
{'loss': 0.5272, 'learning_rate': 6.87313261203502e-05, 'epoch': 0.38}
{'loss': 0.5022, 'learning_rate': 6.868647733003502e-05, 'epoch': 0.38}
                      38%|███▊      | 1226/3250 [5:45:58<9:01:45, 16.06s/it] 38%|███▊      | 1227/3250 [5:46:14<8:59:30, 16.00s/it]                                                        38%|███▊      | 1227/3250 [5:46:14<8:59:30, 16.00s/it] 38%|███▊      | 1228/3250 [5:46:30<8:57:45, 15.96s/it]                                                        38%|███▊      | 1228/3250 [5:46:30<8:57:45, 15.96s/it] 38%|███▊      | 1229/3250 [5:46:46<8:56:24, 15.93s/it]                                                        38%|███▊      | 1229/3250 [5:46:46<8:56:24, 15.93s/it] 38%|███▊      | 1230/3250 [5:47:03<9:01:31, 16.08s/it]                                                        38%|███▊      | 1230/3250 [5:47:03<9:01:31, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7425463199615479, 'eval_runtime': 2.4721, 'eval_samples_per_second': 4.854, 'eval_steps_per_second': 1.214, 'epoch': 0.38}
                                                        38%|███▊      | 1230/3250 [5:47:05<9:01:31, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1230
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1230the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1230
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5673, 'learning_rate': 6.864161105755915e-05, 'epoch': 0.38}
{'loss': 0.5381, 'learning_rate': 6.859672734489723e-05, 'epoch': 0.38}
{'loss': 0.5414, 'learning_rate': 6.855182623404033e-05, 'epoch': 0.38}
{'loss': 0.5235, 'learning_rate': 6.850690776699573e-05, 'epoch': 0.38}
{'loss': 0.5331, 'learning_rate': 6.846197198578695e-05, 'epoch': 0.38}
 38%|███▊      | 1231/3250 [5:47:22<9:31:27, 16.98s/it]                                                        38%|███▊      | 1231/3250 [5:47:22<9:31:27, 16.98s/it] 38%|███▊      | 1232/3250 [5:47:37<9:19:53, 16.65s/it]                                                        38%|███▊      | 1232/3250 [5:47:37<9:19:53, 16.65s/it] 38%|███▊      | 1233/3250 [5:47:53<9:11:42, 16.41s/it]                                                        38%|███▊      | 1233/3250 [5:47:53<9:11:42, 16.41s/it] 38%|███▊      | 1234/3250 [5:48:09<9:05:40, 16.24s/it]                                                        38%|███▊      | 1234/3250 [5:48:09<9:05:40, 16.24s/it] 38%|███▊      | 1235/3250 [5:48:25<9:01:27, 16.12s/it]                                                        38%|███▊      | 1235/3250 [5:48:25<9:01:27, 16.12s/it] 38%|███▊      | 1236/3250 [5:48:41<8:58:34, 16.04s/it]                                  {'loss': 0.5233, 'learning_rate': 6.841701893245374e-05, 'epoch': 0.38}
{'loss': 0.5513, 'learning_rate': 6.8372048649052e-05, 'epoch': 0.38}
{'loss': 0.5412, 'learning_rate': 6.832706117765375e-05, 'epoch': 0.38}
{'loss': 0.542, 'learning_rate': 6.828205656034706e-05, 'epoch': 0.38}
{'loss': 0.5318, 'learning_rate': 6.823703483923607e-05, 'epoch': 0.38}
                      38%|███▊      | 1236/3250 [5:48:41<8:58:34, 16.04s/it] 38%|███▊      | 1237/3250 [5:48:57<8:56:17, 15.99s/it]                                                        38%|███▊      | 1237/3250 [5:48:57<8:56:17, 15.99s/it] 38%|███▊      | 1238/3250 [5:49:13<8:54:35, 15.94s/it]                                                        38%|███▊      | 1238/3250 [5:49:13<8:54:35, 15.94s/it] 38%|███▊      | 1239/3250 [5:49:28<8:53:16, 15.91s/it]                                                        38%|███▊      | 1239/3250 [5:49:28<8:53:16, 15.91s/it] 38%|███▊      | 1240/3250 [5:49:44<8:52:18, 15.89s/it]                                                        38%|███▊      | 1240/3250 [5:49:44<8:52:18, 15.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7475914359092712, 'eval_runtime': 2.474, 'eval_samples_per_second': 4.85, 'eval_steps_per_second': 1.213, 'epoch': 0.38}
                                                        38%|███▊      | 1240/3250 [5:49:47<8:52:18, 15.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1240
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1240
 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1240
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5351, 'learning_rate': 6.819199605644094e-05, 'epoch': 0.38}
{'loss': 0.5018, 'learning_rate': 6.814694025409773e-05, 'epoch': 0.38}
{'loss': 0.5628, 'learning_rate': 6.810186747435849e-05, 'epoch': 0.38}
{'loss': 0.5095, 'learning_rate': 6.805677775939111e-05, 'epoch': 0.38}
{'loss': 0.5041, 'learning_rate': 6.801167115137934e-05, 'epoch': 0.38}
 38%|███▊      | 1241/3250 [5:50:03<9:23:48, 16.84s/it]                                                        38%|███▊      | 1241/3250 [5:50:03<9:23:48, 16.84s/it] 38%|███▊      | 1242/3250 [5:50:19<9:13:50, 16.55s/it]                                                        38%|███▊      | 1242/3250 [5:50:19<9:13:50, 16.55s/it] 38%|███▊      | 1243/3250 [5:50:35<9:07:04, 16.36s/it]                                                        38%|███▊      | 1243/3250 [5:50:35<9:07:04, 16.36s/it] 38%|███▊      | 1244/3250 [5:50:51<9:02:01, 16.21s/it]                                                        38%|███▊      | 1244/3250 [5:50:51<9:02:01, 16.21s/it] 38%|███▊      | 1245/3250 [5:51:07<8:58:15, 16.11s/it]                                                        38%|███▊      | 1245/3250 [5:51:07<8:58:15, 16.11s/it] 38%|███▊      | 1246/3250 [5:51:23<8:57:54, 16.10s/it]                                  {'loss': 0.5123, 'learning_rate': 6.796654769252274e-05, 'epoch': 0.38}
{'loss': 0.5256, 'learning_rate': 6.792140742503661e-05, 'epoch': 0.38}
{'loss': 0.524, 'learning_rate': 6.7876250391152e-05, 'epoch': 0.38}
{'loss': 0.5534, 'learning_rate': 6.783107663311565e-05, 'epoch': 0.38}
{'loss': 0.5315, 'learning_rate': 6.778588619318993e-05, 'epoch': 0.38}
                      38%|███▊      | 1246/3250 [5:51:23<8:57:54, 16.10s/it] 38%|███▊      | 1247/3250 [5:51:39<8:55:19, 16.04s/it]                                                        38%|███▊      | 1247/3250 [5:51:39<8:55:19, 16.04s/it] 38%|███▊      | 1248/3250 [5:51:55<8:53:18, 15.98s/it]                                                        38%|███▊      | 1248/3250 [5:51:55<8:53:18, 15.98s/it] 38%|███▊      | 1249/3250 [5:52:11<8:51:58, 15.95s/it]                                                        38%|███▊      | 1249/3250 [5:52:11<8:51:58, 15.95s/it] 38%|███▊      | 1250/3250 [5:52:26<8:50:49, 15.92s/it]                                                        38%|███▊      | 1250/3250 [5:52:26<8:50:49, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7411658763885498, 'eval_runtime': 2.48, 'eval_samples_per_second': 4.839, 'eval_steps_per_second': 1.21, 'epoch': 0.38}
                                                        38%|███▊      | 1250/3250 [5:52:29<8:50:49, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1250
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0384, 'learning_rate': 6.77406791136528e-05, 'epoch': 0.38}
{'loss': 0.5251, 'learning_rate': 6.769545543679785e-05, 'epoch': 0.39}
{'loss': 0.5493, 'learning_rate': 6.76502152049341e-05, 'epoch': 0.39}
{'loss': 0.5331, 'learning_rate': 6.760495846038614e-05, 'epoch': 0.39}
{'loss': 0.5274, 'learning_rate': 6.755968524549402e-05, 'epoch': 0.39}
 38%|███▊      | 1251/3250 [5:53:04<12:25:22, 22.37s/it]                                                         38%|███▊      | 1251/3250 [5:53:04<12:25:22, 22.37s/it] 39%|███▊      | 1252/3250 [5:53:20<11:20:07, 20.42s/it]                                                         39%|███▊      | 1252/3250 [5:53:20<11:20:07, 20.42s/it] 39%|███▊      | 1253/3250 [5:53:36<10:34:45, 19.07s/it]                                                         39%|███▊      | 1253/3250 [5:53:36<10:34:45, 19.07s/it] 39%|███▊      | 1254/3250 [5:53:51<10:02:22, 18.11s/it]                                                         39%|███▊      | 1254/3250 [5:53:51<10:02:22, 18.11s/it] 39%|███▊      | 1255/3250 [5:54:07<9:39:48, 17.44s/it]                                                         39%|███▊      | 1255/3250 [5:54:07<9:39:48, 17.44s/it] 39%|███▊      | 1256/3250 [5:54:23<9:23:45, 16.96s/it]                     {'loss': 0.5075, 'learning_rate': 6.75143956026131e-05, 'epoch': 0.39}
{'loss': 0.5771, 'learning_rate': 6.746908957411421e-05, 'epoch': 0.39}
{'loss': 0.5704, 'learning_rate': 6.742376720238346e-05, 'epoch': 0.39}
{'loss': 0.5097, 'learning_rate': 6.737842852982225e-05, 'epoch': 0.39}
{'loss': 0.513, 'learning_rate': 6.733307359884725e-05, 'epoch': 0.39}
                                   39%|███▊      | 1256/3250 [5:54:23<9:23:45, 16.96s/it] 39%|███▊      | 1257/3250 [5:54:39<9:12:28, 16.63s/it]                                                        39%|███▊      | 1257/3250 [5:54:39<9:12:28, 16.63s/it] 39%|███▊      | 1258/3250 [5:54:55<9:04:36, 16.40s/it]                                                        39%|███▊      | 1258/3250 [5:54:55<9:04:36, 16.40s/it] 39%|███▊      | 1259/3250 [5:55:11<8:59:06, 16.25s/it]                                                        39%|███▊      | 1259/3250 [5:55:11<8:59:06, 16.25s/it] 39%|███▉      | 1260/3250 [5:55:27<8:55:21, 16.14s/it]                                                        39%|███▉      | 1260/3250 [5:55:27<8:55:21, 16.14s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7410064339637756, 'eval_runtime': 2.7325, 'eval_samples_per_second': 4.392, 'eval_steps_per_second': 1.098, 'epoch': 0.39}
                                                        39%|███▉      | 1260/3250 [5:55:29<8:55:21, 16.14s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1260
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1260 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1260
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1260

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5427, 'learning_rate': 6.728770245189032e-05, 'epoch': 0.39}
{'loss': 0.5268, 'learning_rate': 6.724231513139852e-05, 'epoch': 0.39}
{'loss': 0.5448, 'learning_rate': 6.719691167983401e-05, 'epoch': 0.39}
{'loss': 0.5313, 'learning_rate': 6.715149213967407e-05, 'epoch': 0.39}
{'loss': 0.5353, 'learning_rate': 6.7106056553411e-05, 'epoch': 0.39}
 39%|███▉      | 1261/3250 [5:55:46<9:27:05, 17.11s/it]                                                        39%|███▉      | 1261/3250 [5:55:46<9:27:05, 17.11s/it] 39%|███▉      | 1262/3250 [5:56:02<9:14:35, 16.74s/it]                                                        39%|███▉      | 1262/3250 [5:56:02<9:14:35, 16.74s/it] 39%|███▉      | 1263/3250 [5:56:18<9:09:35, 16.60s/it]                                                        39%|███▉      | 1263/3250 [5:56:18<9:09:35, 16.60s/it] 39%|███▉      | 1264/3250 [5:56:34<9:02:10, 16.38s/it]                                                        39%|███▉      | 1264/3250 [5:56:34<9:02:10, 16.38s/it] 39%|███▉      | 1265/3250 [5:56:50<8:56:50, 16.23s/it]                                                        39%|███▉      | 1265/3250 [5:56:50<8:56:50, 16.23s/it] 39%|███▉      | 1266/3250 [5:57:06<8:52:54, 16.12s/it]                                  {'loss': 0.5249, 'learning_rate': 6.706060496355212e-05, 'epoch': 0.39}
{'loss': 0.5538, 'learning_rate': 6.701513741261976e-05, 'epoch': 0.39}
{'loss': 0.5304, 'learning_rate': 6.696965394315114e-05, 'epoch': 0.39}
{'loss': 0.5443, 'learning_rate': 6.692415459769836e-05, 'epoch': 0.39}
{'loss': 0.5427, 'learning_rate': 6.687863941882841e-05, 'epoch': 0.39}
                      39%|███▉      | 1266/3250 [5:57:06<8:52:54, 16.12s/it] 39%|███▉      | 1267/3250 [5:57:22<8:50:13, 16.04s/it]                                                        39%|███▉      | 1267/3250 [5:57:22<8:50:13, 16.04s/it] 39%|███▉      | 1268/3250 [5:57:38<8:48:09, 15.99s/it]                                                        39%|███▉      | 1268/3250 [5:57:38<8:48:09, 15.99s/it] 39%|███▉      | 1269/3250 [5:57:53<8:46:46, 15.95s/it]                                                        39%|███▉      | 1269/3250 [5:57:53<8:46:46, 15.95s/it] 39%|███▉      | 1270/3250 [5:58:09<8:45:36, 15.93s/it]                                                        39%|███▉      | 1270/3250 [5:58:09<8:45:36, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7483053207397461, 'eval_runtime': 2.4829, 'eval_samples_per_second': 4.833, 'eval_steps_per_second': 1.208, 'epoch': 0.39}
                                                        39%|███▉      | 1270/3250 [5:58:12<8:45:36, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1270
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1270/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1270/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5471, 'learning_rate': 6.683310844912311e-05, 'epoch': 0.39}
{'loss': 0.4924, 'learning_rate': 6.6787561731179e-05, 'epoch': 0.39}
{'loss': 0.5648, 'learning_rate': 6.674199930760738e-05, 'epoch': 0.39}
{'loss': 0.5197, 'learning_rate': 6.669642122103423e-05, 'epoch': 0.39}
{'loss': 0.5229, 'learning_rate': 6.665082751410023e-05, 'epoch': 0.39}
 39%|███▉      | 1271/3250 [5:58:28<9:17:14, 16.89s/it]                                                        39%|███▉      | 1271/3250 [5:58:28<9:17:14, 16.89s/it] 39%|███▉      | 1272/3250 [5:58:44<9:06:43, 16.58s/it]                                                        39%|███▉      | 1272/3250 [5:58:44<9:06:43, 16.58s/it] 39%|███▉      | 1273/3250 [5:59:00<8:59:24, 16.37s/it]                                                        39%|███▉      | 1273/3250 [5:59:00<8:59:24, 16.37s/it] 39%|███▉      | 1274/3250 [5:59:16<8:54:20, 16.23s/it]                                                        39%|███▉      | 1274/3250 [5:59:16<8:54:20, 16.23s/it] 39%|███▉      | 1275/3250 [5:59:32<8:50:29, 16.12s/it]                                                        39%|███▉      | 1275/3250 [5:59:32<8:50:29, 16.12s/it] 39%|███▉      | 1276/3250 [5:59:48<8:47:50, 16.04s/it]                                  {'loss': 0.5056, 'learning_rate': 6.66052182294606e-05, 'epoch': 0.39}
{'loss': 0.5161, 'learning_rate': 6.655959340978519e-05, 'epoch': 0.39}
{'loss': 0.5324, 'learning_rate': 6.651395309775837e-05, 'epoch': 0.39}
{'loss': 0.5263, 'learning_rate': 6.646829733607896e-05, 'epoch': 0.39}
{'loss': 0.5385, 'learning_rate': 6.642262616746034e-05, 'epoch': 0.39}
                      39%|███▉      | 1276/3250 [5:59:48<8:47:50, 16.04s/it] 39%|███▉      | 1277/3250 [6:00:04<8:45:48, 15.99s/it]                                                        39%|███▉      | 1277/3250 [6:00:04<8:45:48, 15.99s/it] 39%|███▉      | 1278/3250 [6:00:20<8:44:24, 15.96s/it]                                                        39%|███▉      | 1278/3250 [6:00:20<8:44:24, 15.96s/it] 39%|███▉      | 1279/3250 [6:00:36<8:45:53, 16.01s/it]                                                        39%|███▉      | 1279/3250 [6:00:36<8:45:53, 16.01s/it] 39%|███▉      | 1280/3250 [6:00:52<8:44:19, 15.97s/it]                                                        39%|███▉      | 1280/3250 [6:00:52<8:44:19, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7437509298324585, 'eval_runtime': 2.7034, 'eval_samples_per_second': 4.439, 'eval_steps_per_second': 1.11, 'epoch': 0.39}
                                                        39%|███▉      | 1280/3250 [6:00:54<8:44:19, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1280
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1280 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1280
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.023, 'learning_rate': 6.637693963463018e-05, 'epoch': 0.39}
{'loss': 0.5111, 'learning_rate': 6.633123778033061e-05, 'epoch': 0.39}
{'loss': 0.5431, 'learning_rate': 6.628552064731807e-05, 'epoch': 0.39}
{'loss': 0.5456, 'learning_rate': 6.623978827836327e-05, 'epoch': 0.4}
{'loss': 0.5385, 'learning_rate': 6.61940407162512e-05, 'epoch': 0.4}
 39%|███▉      | 1281/3250 [6:01:24<11:28:26, 20.98s/it]                                                         39%|███▉      | 1281/3250 [6:01:24<11:28:26, 20.98s/it] 39%|███▉      | 1282/3250 [6:01:40<10:38:07, 19.46s/it]                                                         39%|███▉      | 1282/3250 [6:01:40<10:38:07, 19.46s/it] 39%|███▉      | 1283/3250 [6:01:56<10:02:39, 18.38s/it]                                                         39%|███▉      | 1283/3250 [6:01:56<10:02:39, 18.38s/it] 40%|███▉      | 1284/3250 [6:02:12<9:38:00, 17.64s/it]                                                         40%|███▉      | 1284/3250 [6:02:12<9:38:00, 17.64s/it] 40%|███▉      | 1285/3250 [6:02:28<9:20:40, 17.12s/it]                                                        40%|███▉      | 1285/3250 [6:02:28<9:20:40, 17.12s/it] 40%|███▉      | 1286/3250 [6:02:44<9:08:20, 16.75s/it]                        {'loss': 0.5001, 'learning_rate': 6.614827800378108e-05, 'epoch': 0.4}
{'loss': 0.5236, 'learning_rate': 6.610250018376623e-05, 'epoch': 0.4}
{'loss': 0.5741, 'learning_rate': 6.60567072990342e-05, 'epoch': 0.4}
{'loss': 0.5298, 'learning_rate': 6.601089939242657e-05, 'epoch': 0.4}
{'loss': 0.5177, 'learning_rate': 6.5965076506799e-05, 'epoch': 0.4}
                                40%|███▉      | 1286/3250 [6:02:44<9:08:20, 16.75s/it] 40%|███▉      | 1287/3250 [6:03:00<8:59:39, 16.50s/it]                                                        40%|███▉      | 1287/3250 [6:03:00<8:59:39, 16.50s/it] 40%|███▉      | 1288/3250 [6:03:15<8:53:21, 16.31s/it]                                                        40%|███▉      | 1288/3250 [6:03:15<8:53:21, 16.31s/it] 40%|███▉      | 1289/3250 [6:03:31<8:48:54, 16.18s/it]                                                        40%|███▉      | 1289/3250 [6:03:31<8:48:54, 16.18s/it] 40%|███▉      | 1290/3250 [6:03:47<8:45:37, 16.09s/it]                                                        40%|███▉      | 1290/3250 [6:03:47<8:45:37, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7465847730636597, 'eval_runtime': 2.4647, 'eval_samples_per_second': 4.869, 'eval_steps_per_second': 1.217, 'epoch': 0.4}
                                                        40%|███▉      | 1290/3250 [6:03:50<8:45:37, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1290
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1290the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1290
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1290

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1290/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5316, 'learning_rate': 6.591923868502117e-05, 'epoch': 0.4}
{'loss': 0.5195, 'learning_rate': 6.58733859699767e-05, 'epoch': 0.4}
{'loss': 0.5401, 'learning_rate': 6.582751840456315e-05, 'epoch': 0.4}
{'loss': 0.5172, 'learning_rate': 6.578163603169202e-05, 'epoch': 0.4}
{'loss': 0.5232, 'learning_rate': 6.573573889428862e-05, 'epoch': 0.4}
 40%|███▉      | 1291/3250 [6:04:20<11:28:17, 21.08s/it]                                                         40%|███▉      | 1291/3250 [6:04:20<11:28:17, 21.08s/it] 40%|███▉      | 1292/3250 [6:04:36<10:37:20, 19.53s/it]                                                         40%|███▉      | 1292/3250 [6:04:36<10:37:20, 19.53s/it] 40%|███▉      | 1293/3250 [6:04:52<10:01:47, 18.45s/it]                                                         40%|███▉      | 1293/3250 [6:04:52<10:01:47, 18.45s/it] 40%|███▉      | 1294/3250 [6:05:08<9:36:50, 17.69s/it]                                                         40%|███▉      | 1294/3250 [6:05:08<9:36:50, 17.69s/it] 40%|███▉      | 1295/3250 [6:05:24<9:22:29, 17.26s/it]                                                        40%|███▉      | 1295/3250 [6:05:24<9:22:29, 17.26s/it] 40%|███▉      | 1296/3250 [6:05:40<9:09:16, 16.87s/it]                        {'loss': 0.5128, 'learning_rate': 6.568982703529206e-05, 'epoch': 0.4}
{'loss': 0.5271, 'learning_rate': 6.564390049765528e-05, 'epoch': 0.4}
{'loss': 0.5381, 'learning_rate': 6.55979593243449e-05, 'epoch': 0.4}
{'loss': 0.5263, 'learning_rate': 6.555200355834123e-05, 'epoch': 0.4}
{'loss': 0.5006, 'learning_rate': 6.55060332426383e-05, 'epoch': 0.4}
                                40%|███▉      | 1296/3250 [6:05:40<9:09:16, 16.87s/it] 40%|███▉      | 1297/3250 [6:05:56<8:59:44, 16.58s/it]                                                        40%|███▉      | 1297/3250 [6:05:56<8:59:44, 16.58s/it] 40%|███▉      | 1298/3250 [6:06:12<8:53:08, 16.39s/it]                                                        40%|███▉      | 1298/3250 [6:06:12<8:53:08, 16.39s/it] 40%|███▉      | 1299/3250 [6:06:28<8:48:22, 16.25s/it]                                                        40%|███▉      | 1299/3250 [6:06:28<8:48:22, 16.25s/it] 40%|████      | 1300/3250 [6:06:44<8:44:57, 16.15s/it]                                                        40%|████      | 1300/3250 [6:06:44<8:44:57, 16.15s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7544448375701904, 'eval_runtime': 2.466, 'eval_samples_per_second': 4.866, 'eval_steps_per_second': 1.217, 'epoch': 0.4}
                                                        40%|████      | 1300/3250 [6:06:46<8:44:57, 16.15s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1300
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1300

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1300
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1300

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1300
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5503, 'learning_rate': 6.546004842024369e-05, 'epoch': 0.4}
{'loss': 0.5074, 'learning_rate': 6.541404913417853e-05, 'epoch': 0.4}
{'loss': 0.5341, 'learning_rate': 6.536803542747756e-05, 'epoch': 0.4}
{'loss': 0.5175, 'learning_rate': 6.532200734318896e-05, 'epoch': 0.4}
{'loss': 0.5088, 'learning_rate': 6.527596492437436e-05, 'epoch': 0.4}
 40%|████      | 1301/3250 [6:07:17<11:34:06, 21.37s/it]                                                         40%|████      | 1301/3250 [6:07:17<11:34:06, 21.37s/it] 40%|████      | 1302/3250 [6:07:33<10:40:41, 19.73s/it]                                                         40%|████      | 1302/3250 [6:07:33<10:40:41, 19.73s/it] 40%|████      | 1303/3250 [6:07:49<10:03:12, 18.59s/it]                                                         40%|████      | 1303/3250 [6:07:49<10:03:12, 18.59s/it] 40%|████      | 1304/3250 [6:08:05<9:39:09, 17.86s/it]                                                         40%|████      | 1304/3250 [6:08:05<9:39:09, 17.86s/it] 40%|████      | 1305/3250 [6:08:21<9:20:04, 17.28s/it]                                                        40%|████      | 1305/3250 [6:08:21<9:20:04, 17.28s/it] 40%|████      | 1306/3250 [6:08:37<9:06:42, 16.87s/it]                        {'loss': 0.495, 'learning_rate': 6.52299082141088e-05, 'epoch': 0.4}
{'loss': 0.5063, 'learning_rate': 6.518383725548074e-05, 'epoch': 0.4}
{'loss': 0.5164, 'learning_rate': 6.51377520915919e-05, 'epoch': 0.4}
{'loss': 0.507, 'learning_rate': 6.509165276555734e-05, 'epoch': 0.4}
{'loss': 0.531, 'learning_rate': 6.504553932050534e-05, 'epoch': 0.4}
                                40%|████      | 1306/3250 [6:08:37<9:06:42, 16.87s/it] 40%|████      | 1307/3250 [6:08:53<8:57:15, 16.59s/it]                                                        40%|████      | 1307/3250 [6:08:53<8:57:15, 16.59s/it] 40%|████      | 1308/3250 [6:09:09<8:50:37, 16.39s/it]                                                        40%|████      | 1308/3250 [6:09:09<8:50:37, 16.39s/it] 40%|████      | 1309/3250 [6:09:25<8:45:49, 16.25s/it]                                                        40%|████      | 1309/3250 [6:09:25<8:45:49, 16.25s/it] 40%|████      | 1310/3250 [6:09:41<8:42:17, 16.15s/it]                                                        40%|████      | 1310/3250 [6:09:41<8:42:17, 16.15s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7475727200508118, 'eval_runtime': 2.4672, 'eval_samples_per_second': 4.864, 'eval_steps_per_second': 1.216, 'epoch': 0.4}
                                                        40%|████      | 1310/3250 [6:09:43<8:42:17, 16.15s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1310
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1310/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1310
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0264, 'learning_rate': 6.49994117995774e-05, 'epoch': 0.4}
{'loss': 0.5124, 'learning_rate': 6.495327024592817e-05, 'epoch': 0.4}
{'loss': 0.5034, 'learning_rate': 6.490711470272549e-05, 'epoch': 0.4}
{'loss': 0.5265, 'learning_rate': 6.486094521315022e-05, 'epoch': 0.4}
{'loss': 0.5344, 'learning_rate': 6.481476182039627e-05, 'epoch': 0.4}
 40%|████      | 1311/3250 [6:10:14<11:30:25, 21.36s/it]                                                         40%|████      | 1311/3250 [6:10:14<11:30:25, 21.36s/it] 40%|████      | 1312/3250 [6:10:31<10:41:51, 19.87s/it]                                                         40%|████      | 1312/3250 [6:10:31<10:41:51, 19.87s/it] 40%|████      | 1313/3250 [6:10:47<10:03:07, 18.68s/it]                                                         40%|████      | 1313/3250 [6:10:47<10:03:07, 18.68s/it] 40%|████      | 1314/3250 [6:11:02<9:36:14, 17.86s/it]                                                         40%|████      | 1314/3250 [6:11:02<9:36:14, 17.86s/it] 40%|████      | 1315/3250 [6:11:18<9:17:15, 17.28s/it]                                                        40%|████      | 1315/3250 [6:11:18<9:17:15, 17.28s/it] 40%|████      | 1316/3250 [6:11:34<9:03:54, 16.87s/it]                        {'loss': 0.5047, 'learning_rate': 6.476856456767064e-05, 'epoch': 0.4}
{'loss': 0.5155, 'learning_rate': 6.472235349819318e-05, 'epoch': 0.41}
{'loss': 0.5605, 'learning_rate': 6.467612865519674e-05, 'epoch': 0.41}
{'loss': 0.5257, 'learning_rate': 6.462989008192706e-05, 'epoch': 0.41}
{'loss': 0.5067, 'learning_rate': 6.458363782164266e-05, 'epoch': 0.41}
                                40%|████      | 1316/3250 [6:11:34<9:03:54, 16.87s/it] 41%|████      | 1317/3250 [6:11:50<8:54:27, 16.59s/it]                                                        41%|████      | 1317/3250 [6:11:50<8:54:27, 16.59s/it] 41%|████      | 1318/3250 [6:12:06<8:47:48, 16.39s/it]                                                        41%|████      | 1318/3250 [6:12:06<8:47:48, 16.39s/it] 41%|████      | 1319/3250 [6:12:22<8:43:14, 16.26s/it]                                                        41%|████      | 1319/3250 [6:12:22<8:43:14, 16.26s/it] 41%|████      | 1320/3250 [6:12:38<8:39:43, 16.16s/it]                                                        41%|████      | 1320/3250 [6:12:38<8:39:43, 16.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7493979930877686, 'eval_runtime': 2.47, 'eval_samples_per_second': 4.858, 'eval_steps_per_second': 1.215, 'epoch': 0.41}
                                                        41%|████      | 1320/3250 [6:12:41<8:39:43, 16.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1320
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1320/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1320

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1320

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4825, 'learning_rate': 6.453737191761493e-05, 'epoch': 0.41}
{'loss': 0.5367, 'learning_rate': 6.449109241312803e-05, 'epoch': 0.41}
{'loss': 0.519, 'learning_rate': 6.444479935147878e-05, 'epoch': 0.41}
{'loss': 0.5246, 'learning_rate': 6.439849277597671e-05, 'epoch': 0.41}
{'loss': 0.5029, 'learning_rate': 6.435217272994406e-05, 'epoch': 0.41}
 41%|████      | 1321/3250 [6:12:57<9:07:52, 17.04s/it]                                                        41%|████      | 1321/3250 [6:12:57<9:07:52, 17.04s/it] 41%|████      | 1322/3250 [6:13:13<8:56:51, 16.71s/it]                                                        41%|████      | 1322/3250 [6:13:13<8:56:51, 16.71s/it] 41%|████      | 1323/3250 [6:13:29<8:49:02, 16.47s/it]                                                        41%|████      | 1323/3250 [6:13:29<8:49:02, 16.47s/it] 41%|████      | 1324/3250 [6:13:45<8:43:32, 16.31s/it]                                                        41%|████      | 1324/3250 [6:13:45<8:43:32, 16.31s/it] 41%|████      | 1325/3250 [6:14:01<8:39:27, 16.19s/it]                                                        41%|████      | 1325/3250 [6:14:01<8:39:27, 16.19s/it] 41%|████      | 1326/3250 [6:14:17<8:36:37, 16.11s/it]                                  {'loss': 0.5292, 'learning_rate': 6.430583925671558e-05, 'epoch': 0.41}
{'loss': 0.518, 'learning_rate': 6.42594923996386e-05, 'epoch': 0.41}
{'loss': 0.5445, 'learning_rate': 6.421313220207304e-05, 'epoch': 0.41}
{'loss': 0.5189, 'learning_rate': 6.416675870739118e-05, 'epoch': 0.41}
{'loss': 0.5208, 'learning_rate': 6.412037195897785e-05, 'epoch': 0.41}
                      41%|████      | 1326/3250 [6:14:17<8:36:37, 16.11s/it] 41%|████      | 1327/3250 [6:14:33<8:34:34, 16.06s/it]                                                        41%|████      | 1327/3250 [6:14:33<8:34:34, 16.06s/it] 41%|████      | 1328/3250 [6:14:49<8:36:39, 16.13s/it]                                                        41%|████      | 1328/3250 [6:14:49<8:36:39, 16.13s/it] 41%|████      | 1329/3250 [6:15:05<8:34:25, 16.07s/it]                                                        41%|████      | 1329/3250 [6:15:05<8:34:25, 16.07s/it] 41%|████      | 1330/3250 [6:15:21<8:32:49, 16.03s/it]                                                        41%|████      | 1330/3250 [6:15:21<8:32:49, 16.03s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7528674602508545, 'eval_runtime': 2.4749, 'eval_samples_per_second': 4.849, 'eval_steps_per_second': 1.212, 'epoch': 0.41}
                                                        41%|████      | 1330/3250 [6:15:23<8:32:49, 16.03s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1330
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1330/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5222, 'learning_rate': 6.407397200023027e-05, 'epoch': 0.41}
{'loss': 0.5303, 'learning_rate': 6.402755887455792e-05, 'epoch': 0.41}
{'loss': 0.494, 'learning_rate': 6.398113262538272e-05, 'epoch': 0.41}
{'loss': 0.5463, 'learning_rate': 6.393469329613879e-05, 'epoch': 0.41}
{'loss': 0.4986, 'learning_rate': 6.388824093027253e-05, 'epoch': 0.41}
 41%|████      | 1331/3250 [6:15:40<9:02:13, 16.95s/it]                                                        41%|████      | 1331/3250 [6:15:40<9:02:13, 16.95s/it] 41%|████      | 1332/3250 [6:15:56<8:52:05, 16.65s/it]                                                        41%|████      | 1332/3250 [6:15:56<8:52:05, 16.65s/it] 41%|████      | 1333/3250 [6:16:12<8:44:49, 16.43s/it]                                                        41%|████      | 1333/3250 [6:16:12<8:44:49, 16.43s/it] 41%|████      | 1334/3250 [6:16:28<8:39:53, 16.28s/it]                                                        41%|████      | 1334/3250 [6:16:28<8:39:53, 16.28s/it] 41%|████      | 1335/3250 [6:16:44<8:36:25, 16.18s/it]                                                        41%|████      | 1335/3250 [6:16:44<8:36:25, 16.18s/it] 41%|████      | 1336/3250 [6:17:00<8:33:47, 16.11s/it]                                  {'loss': 0.4904, 'learning_rate': 6.384177557124247e-05, 'epoch': 0.41}
{'loss': 0.4998, 'learning_rate': 6.37952972625194e-05, 'epoch': 0.41}
{'loss': 0.5107, 'learning_rate': 6.374880604758615e-05, 'epoch': 0.41}
{'loss': 0.5123, 'learning_rate': 6.370230196993763e-05, 'epoch': 0.41}
{'loss': 0.529, 'learning_rate': 6.36557850730808e-05, 'epoch': 0.41}
                      41%|████      | 1336/3250 [6:17:00<8:33:47, 16.11s/it] 41%|████      | 1337/3250 [6:17:16<8:31:45, 16.05s/it]                                                        41%|████      | 1337/3250 [6:17:16<8:31:45, 16.05s/it] 41%|████      | 1338/3250 [6:17:31<8:30:29, 16.02s/it]                                                        41%|████      | 1338/3250 [6:17:31<8:30:29, 16.02s/it] 41%|████      | 1339/3250 [6:17:47<8:29:22, 15.99s/it]                                                        41%|████      | 1339/3250 [6:17:47<8:29:22, 15.99s/it] 41%|████      | 1340/3250 [6:18:03<8:28:38, 15.98s/it]                                                        41%|████      | 1340/3250 [6:18:03<8:28:38, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.746519148349762, 'eval_runtime': 2.6944, 'eval_samples_per_second': 4.454, 'eval_steps_per_second': 1.113, 'epoch': 0.41}
                                                        41%|████      | 1340/3250 [6:18:06<8:28:38, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1340
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1340/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5168, 'learning_rate': 6.360925540053463e-05, 'epoch': 0.41}
{'loss': 1.0118, 'learning_rate': 6.356271299582999e-05, 'epoch': 0.41}
{'loss': 0.5107, 'learning_rate': 6.351615790250973e-05, 'epoch': 0.41}
{'loss': 0.5258, 'learning_rate': 6.346959016412852e-05, 'epoch': 0.41}
{'loss': 0.5238, 'learning_rate': 6.342300982425284e-05, 'epoch': 0.41}
 41%|████▏     | 1341/3250 [6:18:23<9:01:06, 17.01s/it]                                                        41%|████▏     | 1341/3250 [6:18:23<9:01:06, 17.01s/it] 41%|████▏     | 1342/3250 [6:18:39<8:50:16, 16.68s/it]                                                        41%|████▏     | 1342/3250 [6:18:39<8:50:16, 16.68s/it] 41%|████▏     | 1343/3250 [6:18:55<8:42:52, 16.45s/it]                                                        41%|████▏     | 1343/3250 [6:18:55<8:42:52, 16.45s/it] 41%|████▏     | 1344/3250 [6:19:11<8:37:42, 16.30s/it]                                                        41%|████▏     | 1344/3250 [6:19:11<8:37:42, 16.30s/it] 41%|████▏     | 1345/3250 [6:19:27<8:36:28, 16.27s/it]                                                        41%|████▏     | 1345/3250 [6:19:27<8:36:28, 16.27s/it] 41%|████▏     | 1346/3250 [6:19:43<8:32:55, 16.16s/it]            {'loss': 0.5212, 'learning_rate': 6.337641692646106e-05, 'epoch': 0.41}
{'loss': 0.4961, 'learning_rate': 6.332981151434317e-05, 'epoch': 0.41}
{'loss': 0.5521, 'learning_rate': 6.328319363150095e-05, 'epoch': 0.41}
{'loss': 0.547, 'learning_rate': 6.323656332154786e-05, 'epoch': 0.42}
{'loss': 0.4979, 'learning_rate': 6.318992062810891e-05, 'epoch': 0.42}
                                            41%|████▏     | 1346/3250 [6:19:43<8:32:55, 16.16s/it] 41%|████▏     | 1347/3250 [6:19:59<8:30:21, 16.09s/it]                                                        41%|████▏     | 1347/3250 [6:19:59<8:30:21, 16.09s/it] 41%|████▏     | 1348/3250 [6:20:15<8:28:38, 16.05s/it]                                                        41%|████▏     | 1348/3250 [6:20:15<8:28:38, 16.05s/it] 42%|████▏     | 1349/3250 [6:20:30<8:27:13, 16.01s/it]                                                        42%|████▏     | 1349/3250 [6:20:30<8:27:13, 16.01s/it] 42%|████▏     | 1350/3250 [6:20:46<8:26:12, 15.99s/it]                                                        42%|████▏     | 1350/3250 [6:20:46<8:26:12, 15.99s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7517605423927307, 'eval_runtime': 2.4687, 'eval_samples_per_second': 4.861, 'eval_steps_per_second': 1.215, 'epoch': 0.42}
                                                        42%|████▏     | 1350/3250 [6:20:49<8:26:12, 15.99s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1350
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1350
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1350
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1350
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4972, 'learning_rate': 6.314326559482076e-05, 'epoch': 0.42}
{'loss': 0.5283, 'learning_rate': 6.30965982653316e-05, 'epoch': 0.42}
{'loss': 0.5132, 'learning_rate': 6.30499186833011e-05, 'epoch': 0.42}
{'loss': 0.526, 'learning_rate': 6.300322689240041e-05, 'epoch': 0.42}
{'loss': 0.5129, 'learning_rate': 6.295652293631212e-05, 'epoch': 0.42}
 42%|████▏     | 1351/3250 [6:21:20<11:10:14, 21.18s/it]                                                         42%|████▏     | 1351/3250 [6:21:20<11:10:14, 21.18s/it] 42%|████▏     | 1352/3250 [6:21:36<10:19:49, 19.59s/it]                                                         42%|████▏     | 1352/3250 [6:21:36<10:19:49, 19.59s/it] 42%|████▏     | 1353/3250 [6:21:51<9:44:23, 18.48s/it]                                                         42%|████▏     | 1353/3250 [6:21:51<9:44:23, 18.48s/it] 42%|████▏     | 1354/3250 [6:22:07<9:19:38, 17.71s/it]                                                        42%|████▏     | 1354/3250 [6:22:07<9:19:38, 17.71s/it] 42%|████▏     | 1355/3250 [6:22:23<9:02:08, 17.17s/it]                                                        42%|████▏     | 1355/3250 [6:22:23<9:02:08, 17.17s/it] 42%|████▏     | 1356/3250 [6:22:39<8:49:48, 16.78s/it]     {'loss': 0.5108, 'learning_rate': 6.290980685873017e-05, 'epoch': 0.42}
{'loss': 0.5126, 'learning_rate': 6.286307870335984e-05, 'epoch': 0.42}
{'loss': 0.5306, 'learning_rate': 6.281633851391777e-05, 'epoch': 0.42}
{'loss': 0.5129, 'learning_rate': 6.276958633413175e-05, 'epoch': 0.42}
{'loss': 0.5128, 'learning_rate': 6.272282220774091e-05, 'epoch': 0.42}
                                                   42%|████▏     | 1356/3250 [6:22:39<8:49:48, 16.78s/it] 42%|████▏     | 1357/3250 [6:22:55<8:41:01, 16.51s/it]                                                        42%|████▏     | 1357/3250 [6:22:55<8:41:01, 16.51s/it] 42%|████▏     | 1358/3250 [6:23:11<8:34:54, 16.33s/it]                                                        42%|████▏     | 1358/3250 [6:23:11<8:34:54, 16.33s/it] 42%|████▏     | 1359/3250 [6:23:32<9:16:23, 17.65s/it]                                                        42%|████▏     | 1359/3250 [6:23:32<9:16:23, 17.65s/it] 42%|████▏     | 1360/3250 [6:23:48<9:02:43, 17.23s/it]                                                        42%|████▏     | 1360/3250 [6:23:48<9:02:43, 17.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7510151267051697, 'eval_runtime': 3.4427, 'eval_samples_per_second': 3.486, 'eval_steps_per_second': 0.871, 'epoch': 0.42}
                                                        42%|████▏     | 1360/3250 [6:23:51<9:02:43, 17.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1360
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1360

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1360

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5132, 'learning_rate': 6.267604617849544e-05, 'epoch': 0.42}
{'loss': 0.527, 'learning_rate': 6.262925829015676e-05, 'epoch': 0.42}
{'loss': 0.4683, 'learning_rate': 6.258245858649731e-05, 'epoch': 0.42}
{'loss': 0.5375, 'learning_rate': 6.253564711130067e-05, 'epoch': 0.42}
{'loss': 0.4889, 'learning_rate': 6.248882390836135e-05, 'epoch': 0.42}
 42%|████▏     | 1361/3250 [6:24:09<9:38:08, 18.36s/it]                                                        42%|████▏     | 1361/3250 [6:24:09<9:38:08, 18.36s/it] 42%|████▏     | 1362/3250 [6:24:25<9:14:46, 17.63s/it]                                                        42%|████▏     | 1362/3250 [6:24:25<9:14:46, 17.63s/it] 42%|████▏     | 1363/3250 [6:24:41<8:58:20, 17.12s/it]                                                        42%|████▏     | 1363/3250 [6:24:41<8:58:20, 17.12s/it] 42%|████▏     | 1364/3250 [6:24:57<8:46:53, 16.76s/it]                                                        42%|████▏     | 1364/3250 [6:24:57<8:46:53, 16.76s/it] 42%|████▏     | 1365/3250 [6:25:13<8:38:50, 16.51s/it]                                                        42%|████▏     | 1365/3250 [6:25:13<8:38:50, 16.51s/it] 42%|████▏     | 1366/3250 [6:25:29<8:32:52, 16.33s/it]            {'loss': 0.4938, 'learning_rate': 6.244198902148486e-05, 'epoch': 0.42}
{'loss': 0.4846, 'learning_rate': 6.239514249448767e-05, 'epoch': 0.42}
{'loss': 0.4929, 'learning_rate': 6.234828437119709e-05, 'epoch': 0.42}
{'loss': 0.5115, 'learning_rate': 6.230141469545132e-05, 'epoch': 0.42}
{'loss': 0.4934, 'learning_rate': 6.225453351109934e-05, 'epoch': 0.42}
                                            42%|████▏     | 1366/3250 [6:25:29<8:32:52, 16.33s/it] 42%|████▏     | 1367/3250 [6:25:44<8:28:42, 16.21s/it]                                                        42%|████▏     | 1367/3250 [6:25:44<8:28:42, 16.21s/it] 42%|████▏     | 1368/3250 [6:26:00<8:25:44, 16.12s/it]                                                        42%|████▏     | 1368/3250 [6:26:00<8:25:44, 16.12s/it] 42%|████▏     | 1369/3250 [6:26:16<8:23:29, 16.06s/it]                                                        42%|████▏     | 1369/3250 [6:26:16<8:23:29, 16.06s/it] 42%|████▏     | 1370/3250 [6:26:32<8:22:01, 16.02s/it]                                                        42%|████▏     | 1370/3250 [6:26:32<8:22:01, 16.02s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7510020732879639, 'eval_runtime': 2.783, 'eval_samples_per_second': 4.312, 'eval_steps_per_second': 1.078, 'epoch': 0.42}
                                                        42%|████▏     | 1370/3250 [6:26:35<8:22:01, 16.02s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1370
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1370/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5247, 'learning_rate': 6.220764086200094e-05, 'epoch': 0.42}
{'loss': 1.0047, 'learning_rate': 6.216073679202656e-05, 'epoch': 0.42}
{'loss': 0.4934, 'learning_rate': 6.211382134505742e-05, 'epoch': 0.42}
{'loss': 0.5293, 'learning_rate': 6.206689456498529e-05, 'epoch': 0.42}
{'loss': 0.511, 'learning_rate': 6.20199564957126e-05, 'epoch': 0.42}
 42%|████▏     | 1371/3250 [6:26:52<8:53:22, 17.03s/it]                                                        42%|████▏     | 1371/3250 [6:26:52<8:53:22, 17.03s/it] 42%|████▏     | 1372/3250 [6:27:07<8:42:10, 16.68s/it]                                                        42%|████▏     | 1372/3250 [6:27:08<8:42:10, 16.68s/it] 42%|████▏     | 1373/3250 [6:27:23<8:34:34, 16.45s/it]                                                        42%|████▏     | 1373/3250 [6:27:23<8:34:34, 16.45s/it] 42%|████▏     | 1374/3250 [6:27:39<8:29:11, 16.29s/it]                                                        42%|████▏     | 1374/3250 [6:27:39<8:29:11, 16.29s/it] 42%|████▏     | 1375/3250 [6:27:55<8:25:25, 16.17s/it]                                                        42%|████▏     | 1375/3250 [6:27:55<8:25:25, 16.17s/it] 42%|████▏     | 1376/3250 [6:28:11<8:22:57, 16.10s/it]            {'loss': 0.526, 'learning_rate': 6.197300718115234e-05, 'epoch': 0.42}
{'loss': 0.484, 'learning_rate': 6.192604666522801e-05, 'epoch': 0.42}
{'loss': 0.507, 'learning_rate': 6.187907499187356e-05, 'epoch': 0.42}
{'loss': 0.5593, 'learning_rate': 6.183209220503343e-05, 'epoch': 0.42}
{'loss': 0.5105, 'learning_rate': 6.178509834866244e-05, 'epoch': 0.42}
                                            42%|████▏     | 1376/3250 [6:28:11<8:22:57, 16.10s/it] 42%|████▏     | 1377/3250 [6:28:27<8:23:45, 16.14s/it]                                                        42%|████▏     | 1377/3250 [6:28:27<8:23:45, 16.14s/it] 42%|████▏     | 1378/3250 [6:28:44<8:23:25, 16.14s/it]                                                        42%|████▏     | 1378/3250 [6:28:44<8:23:25, 16.14s/it] 42%|████▏     | 1379/3250 [6:28:59<8:21:21, 16.08s/it]                                                        42%|████▏     | 1379/3250 [6:28:59<8:21:21, 16.08s/it] 42%|████▏     | 1380/3250 [6:29:15<8:19:53, 16.04s/it]                                                        42%|████▏     | 1380/3250 [6:29:15<8:19:53, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.751875102519989, 'eval_runtime': 2.6184, 'eval_samples_per_second': 4.583, 'eval_steps_per_second': 1.146, 'epoch': 0.42}
                                                        42%|████▏     | 1380/3250 [6:29:18<8:19:53, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1380
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1380
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1380
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1380

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1380
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5029, 'learning_rate': 6.173809346672574e-05, 'epoch': 0.42}
{'loss': 0.5081, 'learning_rate': 6.169107760319884e-05, 'epoch': 0.43}
{'loss': 0.4968, 'learning_rate': 6.164405080206746e-05, 'epoch': 0.43}
{'loss': 0.5207, 'learning_rate': 6.159701310732763e-05, 'epoch': 0.43}
{'loss': 0.493, 'learning_rate': 6.154996456298552e-05, 'epoch': 0.43}
 42%|████▏     | 1381/3250 [6:29:48<10:54:14, 21.00s/it]                                                         42%|████▏     | 1381/3250 [6:29:48<10:54:14, 21.00s/it] 43%|████▎     | 1382/3250 [6:30:04<10:06:01, 19.47s/it]                                                         43%|████▎     | 1382/3250 [6:30:04<10:06:01, 19.47s/it] 43%|████▎     | 1383/3250 [6:30:20<9:32:08, 18.39s/it]                                                         43%|████▎     | 1383/3250 [6:30:20<9:32:08, 18.39s/it] 43%|████▎     | 1384/3250 [6:30:36<9:08:15, 17.63s/it]                                                        43%|████▎     | 1384/3250 [6:30:36<9:08:15, 17.63s/it] 43%|████▎     | 1385/3250 [6:30:51<8:51:33, 17.10s/it]                                                        43%|████▎     | 1385/3250 [6:30:51<8:51:33, 17.10s/it] 43%|████▎     | 1386/3250 [6:31:07<8:39:45, 16.73s/it]     {'loss': 0.5061, 'learning_rate': 6.150290521305746e-05, 'epoch': 0.43}
{'loss': 0.506, 'learning_rate': 6.145583510156989e-05, 'epoch': 0.43}
{'loss': 0.5078, 'learning_rate': 6.14087542725593e-05, 'epoch': 0.43}
{'loss': 0.518, 'learning_rate': 6.136166277007229e-05, 'epoch': 0.43}
{'loss': 0.5189, 'learning_rate': 6.13145606381653e-05, 'epoch': 0.43}
                                                   43%|████▎     | 1386/3250 [6:31:07<8:39:45, 16.73s/it] 43%|████▎     | 1387/3250 [6:31:23<8:31:29, 16.47s/it]                                                        43%|████▎     | 1387/3250 [6:31:23<8:31:29, 16.47s/it] 43%|████▎     | 1388/3250 [6:31:39<8:25:34, 16.29s/it]                                                        43%|████▎     | 1388/3250 [6:31:39<8:25:34, 16.29s/it] 43%|████▎     | 1389/3250 [6:31:55<8:21:26, 16.17s/it]                                                        43%|████▎     | 1389/3250 [6:31:55<8:21:26, 16.17s/it] 43%|████▎     | 1390/3250 [6:32:11<8:18:21, 16.08s/it]                                                        43%|████▎     | 1390/3250 [6:32:11<8:18:21, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7507227659225464, 'eval_runtime': 2.4632, 'eval_samples_per_second': 4.872, 'eval_steps_per_second': 1.218, 'epoch': 0.43}
                                                        43%|████▎     | 1390/3250 [6:32:13<8:18:21, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1390
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1390
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1390
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4971, 'learning_rate': 6.126744792090487e-05, 'epoch': 0.43}
{'loss': 0.5293, 'learning_rate': 6.122032466236733e-05, 'epoch': 0.43}
{'loss': 0.4868, 'learning_rate': 6.117319090663893e-05, 'epoch': 0.43}
{'loss': 0.5102, 'learning_rate': 6.112604669781572e-05, 'epoch': 0.43}
{'loss': 0.5015, 'learning_rate': 6.107889208000354e-05, 'epoch': 0.43}
 43%|████▎     | 1391/3250 [6:32:30<8:45:31, 16.96s/it]                                                        43%|████▎     | 1391/3250 [6:32:30<8:45:31, 16.96s/it] 43%|████▎     | 1392/3250 [6:32:46<8:34:57, 16.63s/it]                                                        43%|████▎     | 1392/3250 [6:32:46<8:34:57, 16.63s/it] 43%|████▎     | 1393/3250 [6:33:02<8:27:43, 16.40s/it]                                                        43%|████▎     | 1393/3250 [6:33:02<8:27:43, 16.40s/it] 43%|████▎     | 1394/3250 [6:33:18<8:31:40, 16.54s/it]                                                        43%|████▎     | 1394/3250 [6:33:18<8:31:40, 16.54s/it] 43%|████▎     | 1395/3250 [6:33:34<8:25:17, 16.34s/it]                                                        43%|████▎     | 1395/3250 [6:33:34<8:25:17, 16.34s/it] 43%|████▎     | 1396/3250 [6:33:50<8:20:45, 16.21s/it]            {'loss': 0.4914, 'learning_rate': 6.103172709731793e-05, 'epoch': 0.43}
{'loss': 0.4788, 'learning_rate': 6.098455179388417e-05, 'epoch': 0.43}
{'loss': 0.4867, 'learning_rate': 6.093736621383721e-05, 'epoch': 0.43}
{'loss': 0.5176, 'learning_rate': 6.089017040132155e-05, 'epoch': 0.43}
{'loss': 0.491, 'learning_rate': 6.084296440049132e-05, 'epoch': 0.43}
                                            43%|████▎     | 1396/3250 [6:33:50<8:20:45, 16.21s/it] 43%|████▎     | 1397/3250 [6:34:06<8:17:11, 16.10s/it]                                                        43%|████▎     | 1397/3250 [6:34:06<8:17:11, 16.10s/it] 43%|████▎     | 1398/3250 [6:34:22<8:14:45, 16.03s/it]                                                        43%|████▎     | 1398/3250 [6:34:22<8:14:45, 16.03s/it] 43%|████▎     | 1399/3250 [6:34:38<8:12:54, 15.98s/it]                                                        43%|████▎     | 1399/3250 [6:34:38<8:12:54, 15.98s/it] 43%|████▎     | 1400/3250 [6:34:54<8:11:38, 15.94s/it]                                                        43%|████▎     | 1400/3250 [6:34:54<8:11:38, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7477951049804688, 'eval_runtime': 2.9833, 'eval_samples_per_second': 4.022, 'eval_steps_per_second': 1.006, 'epoch': 0.43}
                                                        43%|████▎     | 1400/3250 [6:34:57<8:11:38, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1400
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1400/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5218, 'learning_rate': 6.079574825551017e-05, 'epoch': 0.43}
{'loss': 1.0071, 'learning_rate': 6.0748522010551215e-05, 'epoch': 0.43}
{'loss': 0.491, 'learning_rate': 6.070128570979703e-05, 'epoch': 0.43}
{'loss': 0.4985, 'learning_rate': 6.0654039397439635e-05, 'epoch': 0.43}
{'loss': 0.5111, 'learning_rate': 6.060678311768035e-05, 'epoch': 0.43}
 43%|████▎     | 1401/3250 [6:35:44<13:32:33, 26.37s/it]                                                         43%|████▎     | 1401/3250 [6:35:44<13:32:33, 26.37s/it] 43%|████▎     | 1402/3250 [6:36:00<11:54:54, 23.21s/it]                                                         43%|████▎     | 1402/3250 [6:36:00<11:54:54, 23.21s/it] 43%|████▎     | 1403/3250 [6:36:16<10:46:42, 21.01s/it]                                                         43%|████▎     | 1403/3250 [6:36:16<10:46:42, 21.01s/it] 43%|████▎     | 1404/3250 [6:36:32<9:58:47, 19.46s/it]                                                         43%|████▎     | 1404/3250 [6:36:32<9:58:47, 19.46s/it] 43%|████▎     | 1405/3250 [6:36:48<9:25:18, 18.38s/it]                                                        43%|████▎     | 1405/3250 [6:36:48<9:25:18, 18.38s/it] 43%|████▎     | 1406/3250 [6:37:04<9:01:46, 17.63s/it]  {'loss': 0.5109, 'learning_rate': 6.0559516914729886e-05, 'epoch': 0.43}
{'loss': 0.4876, 'learning_rate': 6.05122408328082e-05, 'epoch': 0.43}
{'loss': 0.4965, 'learning_rate': 6.0464954916144465e-05, 'epoch': 0.43}
{'loss': 0.5495, 'learning_rate': 6.0417659208977127e-05, 'epoch': 0.43}
{'loss': 0.5123, 'learning_rate': 6.0370353755553753e-05, 'epoch': 0.43}
                                                      43%|████▎     | 1406/3250 [6:37:04<9:01:46, 17.63s/it] 43%|████▎     | 1407/3250 [6:37:19<8:45:08, 17.10s/it]                                                        43%|████▎     | 1407/3250 [6:37:19<8:45:08, 17.10s/it] 43%|████▎     | 1408/3250 [6:37:35<8:34:50, 16.77s/it]                                                        43%|████▎     | 1408/3250 [6:37:35<8:34:50, 16.77s/it] 43%|████▎     | 1409/3250 [6:37:51<8:26:04, 16.49s/it]                                                        43%|████▎     | 1409/3250 [6:37:51<8:26:04, 16.49s/it] 43%|████▎     | 1410/3250 [6:38:08<8:26:07, 16.50s/it]                                                        43%|████▎     | 1410/3250 [6:38:08<8:26:07, 16.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7506009340286255, 'eval_runtime': 2.4672, 'eval_samples_per_second': 4.864, 'eval_steps_per_second': 1.216, 'epoch': 0.43}
                                                        43%|████▎     | 1410/3250 [6:38:10<8:26:07, 16.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1410
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1410
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1410

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4935, 'learning_rate': 6.0323038600131024e-05, 'epoch': 0.43}
{'loss': 0.4811, 'learning_rate': 6.027571378697468e-05, 'epoch': 0.43}
{'loss': 0.5224, 'learning_rate': 6.022837936035952e-05, 'epoch': 0.43}
{'loss': 0.4972, 'learning_rate': 6.018103536456936e-05, 'epoch': 0.44}
{'loss': 0.5043, 'learning_rate': 6.013368184389692e-05, 'epoch': 0.44}
 43%|████▎     | 1411/3250 [6:38:41<11:02:52, 21.63s/it]                                                         43%|████▎     | 1411/3250 [6:38:41<11:02:52, 21.63s/it] 43%|████▎     | 1412/3250 [6:38:57<10:09:45, 19.90s/it]                                                         43%|████▎     | 1412/3250 [6:38:57<10:09:45, 19.90s/it] 43%|████▎     | 1413/3250 [6:39:13<9:32:06, 18.69s/it]                                                         43%|████▎     | 1413/3250 [6:39:13<9:32:06, 18.69s/it] 44%|████▎     | 1414/3250 [6:39:29<9:05:43, 17.83s/it]                                                        44%|████▎     | 1414/3250 [6:39:29<9:05:43, 17.83s/it] 44%|████▎     | 1415/3250 [6:39:45<8:47:15, 17.24s/it]                                                        44%|████▎     | 1415/3250 [6:39:45<8:47:15, 17.24s/it] 44%|████▎     | 1416/3250 [6:40:01<8:34:17, 16.83s/it]     {'loss': 0.4823, 'learning_rate': 6.008631884264388e-05, 'epoch': 0.44}
{'loss': 0.5135, 'learning_rate': 6.003894640512073e-05, 'epoch': 0.44}
{'loss': 0.5062, 'learning_rate': 5.9991564575646855e-05, 'epoch': 0.44}
{'loss': 0.5266, 'learning_rate': 5.994417339855039e-05, 'epoch': 0.44}
{'loss': 0.5006, 'learning_rate': 5.989677291816818e-05, 'epoch': 0.44}
                                                   44%|████▎     | 1416/3250 [6:40:01<8:34:17, 16.83s/it] 44%|████▎     | 1417/3250 [6:40:17<8:25:08, 16.53s/it]                                                        44%|████▎     | 1417/3250 [6:40:17<8:25:08, 16.53s/it] 44%|████▎     | 1418/3250 [6:40:32<8:18:38, 16.33s/it]                                                        44%|████▎     | 1418/3250 [6:40:32<8:18:38, 16.33s/it] 44%|████▎     | 1419/3250 [6:40:48<8:13:58, 16.19s/it]                                                        44%|████▎     | 1419/3250 [6:40:48<8:13:58, 16.19s/it] 44%|████▎     | 1420/3250 [6:41:04<8:10:34, 16.08s/it]                                                        44%|████▎     | 1420/3250 [6:41:04<8:10:34, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7496804594993591, 'eval_runtime': 2.4762, 'eval_samples_per_second': 4.846, 'eval_steps_per_second': 1.212, 'epoch': 0.44}
                                                        44%|████▎     | 1420/3250 [6:41:07<8:10:34, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1420
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1420/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1420

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1420

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4988, 'learning_rate': 5.984936317884584e-05, 'epoch': 0.44}
{'loss': 0.5054, 'learning_rate': 5.9801944224937644e-05, 'epoch': 0.44}
{'loss': 0.5101, 'learning_rate': 5.9754516100806423e-05, 'epoch': 0.44}
{'loss': 0.4758, 'learning_rate': 5.970707885082364e-05, 'epoch': 0.44}
{'loss': 0.5371, 'learning_rate': 5.965963251936929e-05, 'epoch': 0.44}
 44%|████▎     | 1421/3250 [6:41:23<8:37:47, 16.99s/it]                                                        44%|████▎     | 1421/3250 [6:41:23<8:37:47, 16.99s/it] 44%|████▍     | 1422/3250 [6:41:39<8:27:20, 16.65s/it]                                                        44%|████▍     | 1422/3250 [6:41:39<8:27:20, 16.65s/it] 44%|████▍     | 1423/3250 [6:41:56<8:27:49, 16.68s/it]                                                        44%|████▍     | 1423/3250 [6:41:56<8:27:49, 16.68s/it] 44%|████▍     | 1424/3250 [6:42:12<8:22:00, 16.50s/it]                                                        44%|████▍     | 1424/3250 [6:42:12<8:22:00, 16.50s/it] 44%|████▍     | 1425/3250 [6:42:28<8:17:33, 16.36s/it]                                                        44%|████▍     | 1425/3250 [6:42:28<8:17:33, 16.36s/it] 44%|████▍     | 1426/3250 [6:42:44<8:12:56, 16.22s/it]            {'loss': 0.4897, 'learning_rate': 5.961217715083185e-05, 'epoch': 0.44}
{'loss': 0.4762, 'learning_rate': 5.9564712789608256e-05, 'epoch': 0.44}
{'loss': 0.4917, 'learning_rate': 5.951723948010388e-05, 'epoch': 0.44}
{'loss': 0.4993, 'learning_rate': 5.946975726673241e-05, 'epoch': 0.44}
{'loss': 0.484, 'learning_rate': 5.9422266193915924e-05, 'epoch': 0.44}
                                            44%|████▍     | 1426/3250 [6:42:44<8:12:56, 16.22s/it] 44%|████▍     | 1427/3250 [6:43:00<8:12:15, 16.20s/it]                                                        44%|████▍     | 1427/3250 [6:43:00<8:12:15, 16.20s/it] 44%|████▍     | 1428/3250 [6:43:16<8:08:59, 16.10s/it]                                                        44%|████▍     | 1428/3250 [6:43:16<8:08:59, 16.10s/it] 44%|████▍     | 1429/3250 [6:43:32<8:06:42, 16.04s/it]                                                        44%|████▍     | 1429/3250 [6:43:32<8:06:42, 16.04s/it] 44%|████▍     | 1430/3250 [6:43:48<8:05:56, 16.02s/it]                                                        44%|████▍     | 1430/3250 [6:43:48<8:05:56, 16.02s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7464601397514343, 'eval_runtime': 2.4792, 'eval_samples_per_second': 4.84, 'eval_steps_per_second': 1.21, 'epoch': 0.44}
                                                        44%|████▍     | 1430/3250 [6:43:50<8:05:56, 16.02s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1430
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1430
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1430
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5117, 'learning_rate': 5.937476630608475e-05, 'epoch': 0.44}
{'loss': 0.4995, 'learning_rate': 5.932725764767748e-05, 'epoch': 0.44}
{'loss': 0.9984, 'learning_rate': 5.927974026314091e-05, 'epoch': 0.44}
{'loss': 0.4956, 'learning_rate': 5.9232214196930014e-05, 'epoch': 0.44}
{'loss': 0.5175, 'learning_rate': 5.918467949350784e-05, 'epoch': 0.44}
 44%|████▍     | 1431/3250 [6:44:07<8:34:01, 16.96s/it]                                                        44%|████▍     | 1431/3250 [6:44:07<8:34:01, 16.96s/it] 44%|████▍     | 1432/3250 [6:44:23<8:23:41, 16.62s/it]                                                        44%|████▍     | 1432/3250 [6:44:23<8:23:41, 16.62s/it] 44%|████▍     | 1433/3250 [6:44:39<8:16:14, 16.39s/it]                                                        44%|████▍     | 1433/3250 [6:44:39<8:16:14, 16.39s/it] 44%|████▍     | 1434/3250 [6:44:54<8:11:01, 16.22s/it]                                                        44%|████▍     | 1434/3250 [6:44:54<8:11:01, 16.22s/it] 44%|████▍     | 1435/3250 [6:45:10<8:07:31, 16.12s/it]                                                        44%|████▍     | 1435/3250 [6:45:10<8:07:31, 16.12s/it] 44%|████▍     | 1436/3250 [6:45:26<8:04:47, 16.04s/it]            {'loss': 0.5142, 'learning_rate': 5.913713619734558e-05, 'epoch': 0.44}
{'loss': 0.5014, 'learning_rate': 5.908958435292241e-05, 'epoch': 0.44}
{'loss': 0.4835, 'learning_rate': 5.904202400472553e-05, 'epoch': 0.44}
{'loss': 0.535, 'learning_rate': 5.899445519725009e-05, 'epoch': 0.44}
{'loss': 0.5313, 'learning_rate': 5.894687797499916e-05, 'epoch': 0.44}
                                            44%|████▍     | 1436/3250 [6:45:26<8:04:47, 16.04s/it] 44%|████▍     | 1437/3250 [6:45:42<8:03:01, 15.99s/it]                                                        44%|████▍     | 1437/3250 [6:45:42<8:03:01, 15.99s/it] 44%|████▍     | 1438/3250 [6:45:58<8:01:34, 15.95s/it]                                                        44%|████▍     | 1438/3250 [6:45:58<8:01:34, 15.95s/it] 44%|████▍     | 1439/3250 [6:46:14<8:00:44, 15.93s/it]                                                        44%|████▍     | 1439/3250 [6:46:14<8:00:44, 15.93s/it] 44%|████▍     | 1440/3250 [6:46:30<7:59:57, 15.91s/it]                                                        44%|████▍     | 1440/3250 [6:46:30<7:59:57, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7489333748817444, 'eval_runtime': 2.4745, 'eval_samples_per_second': 4.849, 'eval_steps_per_second': 1.212, 'epoch': 0.44}
                                                        44%|████▍     | 1440/3250 [6:46:32<7:59:57, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1440
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1440/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1440the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4785, 'learning_rate': 5.889929238248368e-05, 'epoch': 0.44}
{'loss': 0.4839, 'learning_rate': 5.8851698464222416e-05, 'epoch': 0.44}
{'loss': 0.5116, 'learning_rate': 5.880409626474195e-05, 'epoch': 0.44}
{'loss': 0.4956, 'learning_rate': 5.8756485828576544e-05, 'epoch': 0.44}
{'loss': 0.5186, 'learning_rate': 5.870886720026825e-05, 'epoch': 0.44}
 44%|████▍     | 1441/3250 [6:46:49<8:29:14, 16.89s/it]                                                        44%|████▍     | 1441/3250 [6:46:49<8:29:14, 16.89s/it] 44%|████▍     | 1442/3250 [6:47:05<8:19:47, 16.59s/it]                                                        44%|████▍     | 1442/3250 [6:47:05<8:19:47, 16.59s/it] 44%|████▍     | 1443/3250 [6:47:21<8:20:42, 16.63s/it]                                                        44%|████▍     | 1443/3250 [6:47:21<8:20:42, 16.63s/it] 44%|████▍     | 1444/3250 [6:47:37<8:13:39, 16.40s/it]                                                        44%|████▍     | 1444/3250 [6:47:37<8:13:39, 16.40s/it] 44%|████▍     | 1445/3250 [6:47:53<8:08:37, 16.24s/it]                                                        44%|████▍     | 1445/3250 [6:47:53<8:08:37, 16.24s/it] 44%|████▍     | 1446/3250 [6:48:09<8:04:55, 16.13s/it]            {'loss': 0.4805, 'learning_rate': 5.8661240424366735e-05, 'epoch': 0.44}
{'loss': 0.4933, 'learning_rate': 5.861360554542927e-05, 'epoch': 0.45}
{'loss': 0.4981, 'learning_rate': 5.8565962608020765e-05, 'epoch': 0.45}
{'loss': 0.5197, 'learning_rate': 5.851831165671363e-05, 'epoch': 0.45}
{'loss': 0.502, 'learning_rate': 5.847065273608777e-05, 'epoch': 0.45}
                                            44%|████▍     | 1446/3250 [6:48:09<8:04:55, 16.13s/it] 45%|████▍     | 1447/3250 [6:48:25<8:02:28, 16.06s/it]                                                        45%|████▍     | 1447/3250 [6:48:25<8:02:28, 16.06s/it] 45%|████▍     | 1448/3250 [6:48:41<8:00:27, 16.00s/it]                                                        45%|████▍     | 1448/3250 [6:48:41<8:00:27, 16.00s/it] 45%|████▍     | 1449/3250 [6:48:57<7:58:54, 15.95s/it]                                                        45%|████▍     | 1449/3250 [6:48:57<7:58:54, 15.95s/it] 45%|████▍     | 1450/3250 [6:49:12<7:57:47, 15.93s/it]                                                        45%|████▍     | 1450/3250 [6:49:12<7:57:47, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7443435192108154, 'eval_runtime': 2.4769, 'eval_samples_per_second': 4.845, 'eval_steps_per_second': 1.211, 'epoch': 0.45}
                                                        45%|████▍     | 1450/3250 [6:49:15<7:57:47, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1450
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1450/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1450

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1450

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.492, 'learning_rate': 5.8422985890730576e-05, 'epoch': 0.45}
{'loss': 0.5077, 'learning_rate': 5.837531116523682e-05, 'epoch': 0.45}
{'loss': 0.5199, 'learning_rate': 5.832762860420868e-05, 'epoch': 0.45}
{'loss': 0.4502, 'learning_rate': 5.827993825225561e-05, 'epoch': 0.45}
{'loss': 0.5262, 'learning_rate': 5.823224015399442e-05, 'epoch': 0.45}
 45%|████▍     | 1451/3250 [6:49:32<8:30:49, 17.04s/it]                                                        45%|████▍     | 1451/3250 [6:49:32<8:30:49, 17.04s/it] 45%|████▍     | 1452/3250 [6:49:48<8:20:07, 16.69s/it]                                                        45%|████▍     | 1452/3250 [6:49:48<8:20:07, 16.69s/it] 45%|████▍     | 1453/3250 [6:50:04<8:12:29, 16.44s/it]                                                        45%|████▍     | 1453/3250 [6:50:04<8:12:29, 16.44s/it] 45%|████▍     | 1454/3250 [6:50:20<8:06:53, 16.27s/it]                                                        45%|████▍     | 1454/3250 [6:50:20<8:06:53, 16.27s/it] 45%|████▍     | 1455/3250 [6:50:36<8:03:06, 16.15s/it]                                                        45%|████▍     | 1455/3250 [6:50:36<8:03:06, 16.15s/it] 45%|████▍     | 1456/3250 [6:50:51<8:00:18, 16.06s/it]            {'loss': 0.4829, 'learning_rate': 5.8184534354049104e-05, 'epoch': 0.45}
{'loss': 0.487, 'learning_rate': 5.813682089705092e-05, 'epoch': 0.45}
{'loss': 0.4765, 'learning_rate': 5.808909982763825e-05, 'epoch': 0.45}
{'loss': 0.4859, 'learning_rate': 5.8041371190456595e-05, 'epoch': 0.45}
{'loss': 0.5005, 'learning_rate': 5.799363503015856e-05, 'epoch': 0.45}
                                            45%|████▍     | 1456/3250 [6:50:51<8:00:18, 16.06s/it] 45%|████▍     | 1457/3250 [6:51:07<7:58:15, 16.00s/it]                                                        45%|████▍     | 1457/3250 [6:51:07<7:58:15, 16.00s/it] 45%|████▍     | 1458/3250 [6:51:23<7:56:48, 15.96s/it]                                                        45%|████▍     | 1458/3250 [6:51:23<7:56:48, 15.96s/it] 45%|████▍     | 1459/3250 [6:51:39<7:57:51, 16.01s/it]                                                        45%|████▍     | 1459/3250 [6:51:39<7:57:51, 16.01s/it] 45%|████▍     | 1460/3250 [6:51:55<7:56:37, 15.98s/it]                                                        45%|████▍     | 1460/3250 [6:51:55<7:56:37, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7471845746040344, 'eval_runtime': 2.4694, 'eval_samples_per_second': 4.859, 'eval_steps_per_second': 1.215, 'epoch': 0.45}
                                                        45%|████▍     | 1460/3250 [6:51:58<7:56:37, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1460
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL the checkpoint model will be saved in 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1460
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1460
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1460

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4918, 'learning_rate': 5.794589139140381e-05, 'epoch': 0.45}
{'loss': 0.5069, 'learning_rate': 5.789814031885894e-05, 'epoch': 0.45}
{'loss': 0.9947, 'learning_rate': 5.7850381857197525e-05, 'epoch': 0.45}
{'loss': 0.4864, 'learning_rate': 5.7802616051100086e-05, 'epoch': 0.45}
{'loss': 0.5098, 'learning_rate': 5.775484294525399e-05, 'epoch': 0.45}
 45%|████▍     | 1461/3250 [6:52:14<8:24:10, 16.91s/it]                                                        45%|████▍     | 1461/3250 [6:52:14<8:24:10, 16.91s/it] 45%|████▍     | 1462/3250 [6:52:30<8:14:41, 16.60s/it]                                                        45%|████▍     | 1462/3250 [6:52:30<8:14:41, 16.60s/it] 45%|████▌     | 1463/3250 [6:52:46<8:07:37, 16.37s/it]                                                        45%|████▌     | 1463/3250 [6:52:46<8:07:37, 16.37s/it] 45%|████▌     | 1464/3250 [6:53:02<8:02:52, 16.22s/it]                                                        45%|████▌     | 1464/3250 [6:53:02<8:02:52, 16.22s/it] 45%|████▌     | 1465/3250 [6:53:18<7:59:28, 16.12s/it]                                                        45%|████▌     | 1465/3250 [6:53:18<7:59:28, 16.12s/it] 45%|████▌     | 1466/3250 [6:53:34<7:57:11, 16.05s/it]            {'loss': 0.5097, 'learning_rate': 5.770706258435342e-05, 'epoch': 0.45}
{'loss': 0.5124, 'learning_rate': 5.765927501309938e-05, 'epoch': 0.45}
{'loss': 0.4656, 'learning_rate': 5.761148027619958e-05, 'epoch': 0.45}
{'loss': 0.4944, 'learning_rate': 5.756367841836847e-05, 'epoch': 0.45}
{'loss': 0.5412, 'learning_rate': 5.7515869484327155e-05, 'epoch': 0.45}
                                            45%|████▌     | 1466/3250 [6:53:34<7:57:11, 16.05s/it] 45%|████▌     | 1467/3250 [6:53:49<7:55:28, 16.00s/it]                                                        45%|████▌     | 1467/3250 [6:53:49<7:55:28, 16.00s/it] 45%|████▌     | 1468/3250 [6:54:05<7:54:08, 15.96s/it]                                                        45%|████▌     | 1468/3250 [6:54:05<7:54:08, 15.96s/it] 45%|████▌     | 1469/3250 [6:54:21<7:53:03, 15.94s/it]                                                        45%|████▌     | 1469/3250 [6:54:21<7:53:03, 15.94s/it] 45%|████▌     | 1470/3250 [6:54:37<7:52:21, 15.92s/it]                                                        45%|████▌     | 1470/3250 [6:54:37<7:52:21, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7449784874916077, 'eval_runtime': 2.4783, 'eval_samples_per_second': 4.842, 'eval_steps_per_second': 1.211, 'epoch': 0.45}
                                                        45%|████▌     | 1470/3250 [6:54:40<7:52:21, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1470
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1470
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1470 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4945, 'learning_rate': 5.746805351880334e-05, 'epoch': 0.45}
{'loss': 0.4858, 'learning_rate': 5.742023056653131e-05, 'epoch': 0.45}
{'loss': 0.4926, 'learning_rate': 5.73724006722519e-05, 'epoch': 0.45}
{'loss': 0.4884, 'learning_rate': 5.732456388071247e-05, 'epoch': 0.45}
{'loss': 0.507, 'learning_rate': 5.7276720236666746e-05, 'epoch': 0.45}
 45%|████▌     | 1471/3250 [6:55:10<10:21:32, 20.96s/it]                                                         45%|████▌     | 1471/3250 [6:55:10<10:21:32, 20.96s/it] 45%|████▌     | 1472/3250 [6:55:26<9:36:03, 19.44s/it]                                                         45%|████▌     | 1472/3250 [6:55:26<9:36:03, 19.44s/it] 45%|████▌     | 1473/3250 [6:55:42<9:04:02, 18.37s/it]                                                        45%|████▌     | 1473/3250 [6:55:42<9:04:02, 18.37s/it] 45%|████▌     | 1474/3250 [6:55:57<8:41:38, 17.62s/it]                                                        45%|████▌     | 1474/3250 [6:55:57<8:41:38, 17.62s/it] 45%|████▌     | 1475/3250 [6:56:13<8:25:52, 17.10s/it]                                                        45%|████▌     | 1475/3250 [6:56:13<8:25:52, 17.10s/it] 45%|████▌     | 1476/3250 [6:56:30<8:20:32, 16.93s/it]        {'loss': 0.4762, 'learning_rate': 5.722886978487496e-05, 'epoch': 0.45}
{'loss': 0.4918, 'learning_rate': 5.7181012570103656e-05, 'epoch': 0.45}
{'loss': 0.4966, 'learning_rate': 5.713314863712571e-05, 'epoch': 0.45}
{'loss': 0.4902, 'learning_rate': 5.70852780307203e-05, 'epoch': 0.46}
{'loss': 0.5114, 'learning_rate': 5.703740079567286e-05, 'epoch': 0.46}
                                                45%|████▌     | 1476/3250 [6:56:30<8:20:32, 16.93s/it] 45%|████▌     | 1477/3250 [6:56:46<8:11:00, 16.62s/it]                                                        45%|████▌     | 1477/3250 [6:56:46<8:11:00, 16.62s/it] 45%|████▌     | 1478/3250 [6:57:02<8:04:13, 16.40s/it]                                                        45%|████▌     | 1478/3250 [6:57:02<8:04:13, 16.40s/it] 46%|████▌     | 1479/3250 [6:57:18<7:59:21, 16.24s/it]                                                        46%|████▌     | 1479/3250 [6:57:18<7:59:21, 16.24s/it] 46%|████▌     | 1480/3250 [6:57:33<7:55:50, 16.13s/it]                                                        46%|████▌     | 1480/3250 [6:57:33<7:55:50, 16.13s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7404250502586365, 'eval_runtime': 2.4763, 'eval_samples_per_second': 4.846, 'eval_steps_per_second': 1.212, 'epoch': 0.46}
                                                        46%|████▌     | 1480/3250 [6:57:36<7:55:50, 16.13s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1480
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1480/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1480


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1480

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5053, 'learning_rate': 5.698951697677498e-05, 'epoch': 0.46}
{'loss': 0.486, 'learning_rate': 5.694162661882444e-05, 'epoch': 0.46}
{'loss': 0.4998, 'learning_rate': 5.6893729766625146e-05, 'epoch': 0.46}
{'loss': 0.4686, 'learning_rate': 5.684582646498706e-05, 'epoch': 0.46}
{'loss': 0.5014, 'learning_rate': 5.679791675872619e-05, 'epoch': 0.46}
 46%|████▌     | 1481/3250 [6:57:52<8:21:21, 17.00s/it]                                                        46%|████▌     | 1481/3250 [6:57:52<8:21:21, 17.00s/it] 46%|████▌     | 1482/3250 [6:58:08<8:11:14, 16.67s/it]                                                        46%|████▌     | 1482/3250 [6:58:08<8:11:14, 16.67s/it] 46%|████▌     | 1483/3250 [6:58:24<8:03:55, 16.43s/it]                                                        46%|████▌     | 1483/3250 [6:58:24<8:03:55, 16.43s/it] 46%|████▌     | 1484/3250 [6:58:40<7:58:46, 16.27s/it]                                                        46%|████▌     | 1484/3250 [6:58:40<7:58:46, 16.27s/it] 46%|████▌     | 1485/3250 [6:58:56<7:55:05, 16.15s/it]                                                        46%|████▌     | 1485/3250 [6:58:56<7:55:05, 16.15s/it] 46%|████▌     | 1486/3250 [6:59:12<7:52:30, 16.07s/it]            {'loss': 0.4759, 'learning_rate': 5.675000069266451e-05, 'epoch': 0.46}
{'loss': 0.4686, 'learning_rate': 5.6702078311629995e-05, 'epoch': 0.46}
{'loss': 0.4684, 'learning_rate': 5.6654149660456455e-05, 'epoch': 0.46}
{'loss': 0.475, 'learning_rate': 5.660621478398367e-05, 'epoch': 0.46}
{'loss': 0.4952, 'learning_rate': 5.655827372705712e-05, 'epoch': 0.46}
                                            46%|████▌     | 1486/3250 [6:59:12<7:52:30, 16.07s/it] 46%|████▌     | 1487/3250 [6:59:28<7:50:31, 16.01s/it]                                                        46%|████▌     | 1487/3250 [6:59:28<7:50:31, 16.01s/it] 46%|████▌     | 1488/3250 [6:59:44<7:49:01, 15.97s/it]                                                        46%|████▌     | 1488/3250 [6:59:44<7:49:01, 15.97s/it] 46%|████▌     | 1489/3250 [7:00:00<7:48:03, 15.95s/it]                                                        46%|████▌     | 1489/3250 [7:00:00<7:48:03, 15.95s/it] 46%|████▌     | 1490/3250 [7:00:15<7:47:08, 15.93s/it]                                                        46%|████▌     | 1490/3250 [7:00:15<7:47:08, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7475932240486145, 'eval_runtime': 2.4793, 'eval_samples_per_second': 4.84, 'eval_steps_per_second': 1.21, 'epoch': 0.46}
                                                        46%|████▌     | 1490/3250 [7:00:18<7:47:08, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1490
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1490
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1490/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1490


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4778, 'learning_rate': 5.651032653452817e-05, 'epoch': 0.46}
{'loss': 0.5031, 'learning_rate': 5.6462373251253875e-05, 'epoch': 0.46}
{'loss': 0.9984, 'learning_rate': 5.641441392209699e-05, 'epoch': 0.46}
{'loss': 0.481, 'learning_rate': 5.636644859192594e-05, 'epoch': 0.46}
{'loss': 0.4909, 'learning_rate': 5.6318477305614756e-05, 'epoch': 0.46}
 46%|████▌     | 1491/3250 [7:00:34<8:14:29, 16.87s/it]                                                        46%|████▌     | 1491/3250 [7:00:34<8:14:29, 16.87s/it] 46%|████▌     | 1492/3250 [7:00:51<8:07:16, 16.63s/it]                                                        46%|████▌     | 1492/3250 [7:00:51<8:07:16, 16.63s/it] 46%|████▌     | 1493/3250 [7:01:06<7:59:47, 16.38s/it]                                                        46%|████▌     | 1493/3250 [7:01:06<7:59:47, 16.38s/it] 46%|████▌     | 1494/3250 [7:01:22<7:54:51, 16.23s/it]                                                        46%|████▌     | 1494/3250 [7:01:22<7:54:51, 16.23s/it] 46%|████▌     | 1495/3250 [7:01:38<7:51:17, 16.11s/it]                                                        46%|████▌     | 1495/3250 [7:01:38<7:51:17, 16.11s/it] 46%|████▌     | 1496/3250 [7:01:54<7:48:44, 16.03s/it]            {'loss': 0.495, 'learning_rate': 5.6270500108043046e-05, 'epoch': 0.46}
{'loss': 0.4932, 'learning_rate': 5.6222517044095945e-05, 'epoch': 0.46}
{'loss': 0.4794, 'learning_rate': 5.6174528158664096e-05, 'epoch': 0.46}
{'loss': 0.4827, 'learning_rate': 5.612653349664353e-05, 'epoch': 0.46}
{'loss': 0.5314, 'learning_rate': 5.6078533102935745e-05, 'epoch': 0.46}
                                            46%|████▌     | 1496/3250 [7:01:54<7:48:44, 16.03s/it] 46%|████▌     | 1497/3250 [7:02:10<7:46:51, 15.98s/it]                                                        46%|████▌     | 1497/3250 [7:02:10<7:46:51, 15.98s/it] 46%|████▌     | 1498/3250 [7:02:26<7:45:18, 15.94s/it]                                                        46%|████▌     | 1498/3250 [7:02:26<7:45:18, 15.94s/it] 46%|████▌     | 1499/3250 [7:02:41<7:44:11, 15.91s/it]                                                        46%|████▌     | 1499/3250 [7:02:41<7:44:11, 15.91s/it] 46%|████▌     | 1500/3250 [7:02:57<7:43:18, 15.89s/it]                                                        46%|████▌     | 1500/3250 [7:02:57<7:43:18, 15.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7450321316719055, 'eval_runtime': 2.4686, 'eval_samples_per_second': 4.861, 'eval_steps_per_second': 1.215, 'epoch': 0.46}
                                                        46%|████▌     | 1500/3250 [7:03:00<7:43:18, 15.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1500
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1500/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5052, 'learning_rate': 5.60305270224476e-05, 'epoch': 0.46}
{'loss': 0.4875, 'learning_rate': 5.598251530009121e-05, 'epoch': 0.46}
{'loss': 0.4617, 'learning_rate': 5.5934497980784054e-05, 'epoch': 0.46}
{'loss': 0.5082, 'learning_rate': 5.5886475109448765e-05, 'epoch': 0.46}
{'loss': 0.4958, 'learning_rate': 5.583844673101323e-05, 'epoch': 0.46}
 46%|████▌     | 1501/3250 [7:03:16<8:10:38, 16.83s/it]                                                        46%|████▌     | 1501/3250 [7:03:16<8:10:38, 16.83s/it] 46%|████▌     | 1502/3250 [7:03:32<8:02:01, 16.55s/it]                                                        46%|████▌     | 1502/3250 [7:03:32<8:02:01, 16.55s/it] 46%|████▌     | 1503/3250 [7:03:48<7:55:51, 16.34s/it]                                                        46%|████▌     | 1503/3250 [7:03:48<7:55:51, 16.34s/it] 46%|████▋     | 1504/3250 [7:04:04<7:51:33, 16.20s/it]                                                        46%|████▋     | 1504/3250 [7:04:04<7:51:33, 16.20s/it] 46%|████▋     | 1505/3250 [7:04:20<7:48:25, 16.11s/it]                                                        46%|████▋     | 1505/3250 [7:04:20<7:48:25, 16.11s/it] 46%|████▋     | 1506/3250 [7:04:36<7:46:13, 16.04s/it]            {'loss': 0.4963, 'learning_rate': 5.5790412890410446e-05, 'epoch': 0.46}
{'loss': 0.4781, 'learning_rate': 5.574237363257858e-05, 'epoch': 0.46}
{'loss': 0.4963, 'learning_rate': 5.56943290024608e-05, 'epoch': 0.46}
{'loss': 0.4932, 'learning_rate': 5.564627904500533e-05, 'epoch': 0.46}
{'loss': 0.5138, 'learning_rate': 5.559822380516539e-05, 'epoch': 0.46}
                                            46%|████▋     | 1506/3250 [7:04:36<7:46:13, 16.04s/it] 46%|████▋     | 1507/3250 [7:04:52<7:44:25, 15.99s/it]                                                        46%|████▋     | 1507/3250 [7:04:52<7:44:25, 15.99s/it] 46%|████▋     | 1508/3250 [7:05:07<7:43:13, 15.96s/it]                                                        46%|████▋     | 1508/3250 [7:05:07<7:43:13, 15.96s/it] 46%|████▋     | 1509/3250 [7:05:24<7:47:27, 16.11s/it]                                                        46%|████▋     | 1509/3250 [7:05:24<7:47:27, 16.11s/it] 46%|████▋     | 1510/3250 [7:05:40<7:45:11, 16.04s/it]                                                        46%|████▋     | 1510/3250 [7:05:40<7:45:11, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7417470216751099, 'eval_runtime': 2.4714, 'eval_samples_per_second': 4.856, 'eval_steps_per_second': 1.214, 'epoch': 0.46}
                                                        46%|████▋     | 1510/3250 [7:05:42<7:45:11, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1510
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1510
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1510
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1510

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1510/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1510/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.505, 'learning_rate': 5.5550163327899126e-05, 'epoch': 0.46}
{'loss': 0.4696, 'learning_rate': 5.550209765816958e-05, 'epoch': 0.47}
{'loss': 0.5022, 'learning_rate': 5.545402684094467e-05, 'epoch': 0.47}
{'loss': 0.4925, 'learning_rate': 5.540595092119709e-05, 'epoch': 0.47}
{'loss': 0.4558, 'learning_rate': 5.535786994390436e-05, 'epoch': 0.47}
 46%|████▋     | 1511/3250 [7:05:59<8:11:48, 16.97s/it]                                                        46%|████▋     | 1511/3250 [7:05:59<8:11:48, 16.97s/it] 47%|████▋     | 1512/3250 [7:06:15<8:02:06, 16.64s/it]                                                        47%|████▋     | 1512/3250 [7:06:15<8:02:06, 16.64s/it] 47%|████▋     | 1513/3250 [7:06:31<7:55:16, 16.42s/it]                                                        47%|████▋     | 1513/3250 [7:06:31<7:55:16, 16.42s/it] 47%|████▋     | 1514/3250 [7:06:47<7:50:13, 16.25s/it]                                                        47%|████▋     | 1514/3250 [7:06:47<7:50:13, 16.25s/it] 47%|████▋     | 1515/3250 [7:07:02<7:46:34, 16.14s/it]                                                        47%|████▋     | 1515/3250 [7:07:02<7:46:34, 16.14s/it] 47%|████▋     | 1516/3250 [7:07:18<7:43:59, 16.05s/it]            {'loss': 0.5157, 'learning_rate': 5.530978395404872e-05, 'epoch': 0.47}
{'loss': 0.4703, 'learning_rate': 5.526169299661705e-05, 'epoch': 0.47}
{'loss': 0.465, 'learning_rate': 5.521359711660094e-05, 'epoch': 0.47}
{'loss': 0.4728, 'learning_rate': 5.516549635899655e-05, 'epoch': 0.47}
{'loss': 0.4764, 'learning_rate': 5.511739076880461e-05, 'epoch': 0.47}
                                            47%|████▋     | 1516/3250 [7:07:18<7:43:59, 16.05s/it] 47%|████▋     | 1517/3250 [7:07:34<7:42:02, 16.00s/it]                                                        47%|████▋     | 1517/3250 [7:07:34<7:42:02, 16.00s/it] 47%|████▋     | 1518/3250 [7:07:50<7:40:35, 15.96s/it]                                                        47%|████▋     | 1518/3250 [7:07:50<7:40:35, 15.96s/it] 47%|████▋     | 1519/3250 [7:08:06<7:39:29, 15.93s/it]                                                        47%|████▋     | 1519/3250 [7:08:06<7:39:29, 15.93s/it] 47%|████▋     | 1520/3250 [7:08:22<7:38:51, 15.91s/it]                                                        47%|████▋     | 1520/3250 [7:08:22<7:38:51, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7514380812644958, 'eval_runtime': 2.475, 'eval_samples_per_second': 4.848, 'eval_steps_per_second': 1.212, 'epoch': 0.47}
                                                        47%|████▋     | 1520/3250 [7:08:24<7:38:51, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1520
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1520 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1520


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1520

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4918, 'learning_rate': 5.50692803910304e-05, 'epoch': 0.47}
{'loss': 0.503, 'learning_rate': 5.502116527068363e-05, 'epoch': 0.47}
{'loss': 0.5378, 'learning_rate': 5.497304545277846e-05, 'epoch': 0.47}
{'loss': 0.9493, 'learning_rate': 5.492492098233346e-05, 'epoch': 0.47}
{'loss': 0.4753, 'learning_rate': 5.487679190437158e-05, 'epoch': 0.47}
 47%|████▋     | 1521/3250 [7:08:41<8:05:39, 16.85s/it]                                                        47%|████▋     | 1521/3250 [7:08:41<8:05:39, 16.85s/it] 47%|████▋     | 1522/3250 [7:08:57<7:56:56, 16.56s/it]                                                        47%|████▋     | 1522/3250 [7:08:57<7:56:56, 16.56s/it] 47%|████▋     | 1523/3250 [7:09:13<7:50:43, 16.35s/it]                                                        47%|████▋     | 1523/3250 [7:09:13<7:50:43, 16.35s/it] 47%|████▋     | 1524/3250 [7:09:28<7:46:09, 16.21s/it]                                                        47%|████▋     | 1524/3250 [7:09:28<7:46:09, 16.21s/it] 47%|████▋     | 1525/3250 [7:09:45<7:45:02, 16.18s/it]                                                        47%|████▋     | 1525/3250 [7:09:45<7:45:02, 16.18s/it] 47%|████▋     | 1526/3250 [7:10:00<7:42:06, 16.08s/it]            {'loss': 0.495, 'learning_rate': 5.482865826392001e-05, 'epoch': 0.47}
{'loss': 0.4921, 'learning_rate': 5.4780520106010256e-05, 'epoch': 0.47}
{'loss': 0.4736, 'learning_rate': 5.473237747567805e-05, 'epoch': 0.47}
{'loss': 0.464, 'learning_rate': 5.468423041796331e-05, 'epoch': 0.47}
{'loss': 0.5197, 'learning_rate': 5.463607897791006e-05, 'epoch': 0.47}
                                            47%|████▋     | 1526/3250 [7:10:00<7:42:06, 16.08s/it] 47%|████▋     | 1527/3250 [7:10:16<7:39:56, 16.02s/it]                                                        47%|████▋     | 1527/3250 [7:10:16<7:39:56, 16.02s/it] 47%|████▋     | 1528/3250 [7:10:32<7:38:19, 15.97s/it]                                                        47%|████▋     | 1528/3250 [7:10:32<7:38:19, 15.97s/it] 47%|████▋     | 1529/3250 [7:10:48<7:37:04, 15.93s/it]                                                        47%|████▋     | 1529/3250 [7:10:48<7:37:04, 15.93s/it] 47%|████▋     | 1530/3250 [7:11:04<7:37:47, 15.97s/it]                                                        47%|████▋     | 1530/3250 [7:11:04<7:37:47, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7491931319236755, 'eval_runtime': 2.4737, 'eval_samples_per_second': 4.851, 'eval_steps_per_second': 1.213, 'epoch': 0.47}
                                                        47%|████▋     | 1530/3250 [7:11:06<7:37:47, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1530
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1530 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1530
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1530

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5041, 'learning_rate': 5.458792320056645e-05, 'epoch': 0.47}
{'loss': 0.4727, 'learning_rate': 5.45397631309847e-05, 'epoch': 0.47}
{'loss': 0.4703, 'learning_rate': 5.449159881422101e-05, 'epoch': 0.47}
{'loss': 0.4874, 'learning_rate': 5.444343029533562e-05, 'epoch': 0.47}
{'loss': 0.4797, 'learning_rate': 5.439525761939261e-05, 'epoch': 0.47}
 47%|████▋     | 1531/3250 [7:11:42<10:43:37, 22.47s/it]                                                         47%|████▋     | 1531/3250 [7:11:42<10:43:37, 22.47s/it] 47%|████▋     | 1532/3250 [7:11:58<9:46:58, 20.50s/it]                                                         47%|████▋     | 1532/3250 [7:11:58<9:46:58, 20.50s/it] 47%|████▋     | 1533/3250 [7:12:13<9:07:11, 19.12s/it]                                                        47%|████▋     | 1533/3250 [7:12:13<9:07:11, 19.12s/it] 47%|████▋     | 1534/3250 [7:12:29<8:39:11, 18.15s/it]                                                        47%|████▋     | 1534/3250 [7:12:29<8:39:11, 18.15s/it] 47%|████▋     | 1535/3250 [7:12:45<8:19:36, 17.48s/it]                                                        47%|████▋     | 1535/3250 [7:12:45<8:19:36, 17.48s/it] 47%|████▋     | 1536/3250 [7:13:01<8:05:52, 17.01s/it]        {'loss': 0.498, 'learning_rate': 5.4347080831460015e-05, 'epoch': 0.47}
{'loss': 0.4679, 'learning_rate': 5.4298899976609717e-05, 'epoch': 0.47}
{'loss': 0.472, 'learning_rate': 5.425071509991737e-05, 'epoch': 0.47}
{'loss': 0.5002, 'learning_rate': 5.420252624646238e-05, 'epoch': 0.47}
{'loss': 0.5056, 'learning_rate': 5.415433346132793e-05, 'epoch': 0.47}
                                                47%|████▋     | 1536/3250 [7:13:01<8:05:52, 17.01s/it] 47%|████▋     | 1537/3250 [7:13:17<7:56:16, 16.68s/it]                                                        47%|████▋     | 1537/3250 [7:13:17<7:56:16, 16.68s/it] 47%|████▋     | 1538/3250 [7:13:33<7:49:27, 16.45s/it]                                                        47%|████▋     | 1538/3250 [7:13:33<7:49:27, 16.45s/it] 47%|████▋     | 1539/3250 [7:13:49<7:44:42, 16.30s/it]                                                        47%|████▋     | 1539/3250 [7:13:49<7:44:42, 16.30s/it] 47%|████▋     | 1540/3250 [7:14:05<7:41:14, 16.18s/it]                                                        47%|████▋     | 1540/3250 [7:14:05<7:41:14, 16.18s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7429302930831909, 'eval_runtime': 2.4729, 'eval_samples_per_second': 4.853, 'eval_steps_per_second': 1.213, 'epoch': 0.47}
                                                        47%|████▋     | 1540/3250 [7:14:07<7:41:14, 16.18s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1540
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1540 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1540 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1540

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4828, 'learning_rate': 5.410613678960084e-05, 'epoch': 0.47}
{'loss': 0.491, 'learning_rate': 5.4057936276371565e-05, 'epoch': 0.47}
{'loss': 0.4858, 'learning_rate': 5.400973196673419e-05, 'epoch': 0.47}
{'loss': 0.4893, 'learning_rate': 5.3961523905786295e-05, 'epoch': 0.48}
{'loss': 0.436, 'learning_rate': 5.3913312138629014e-05, 'epoch': 0.48}
 47%|████▋     | 1541/3250 [7:14:37<10:01:19, 21.11s/it]                                                         47%|████▋     | 1541/3250 [7:14:37<10:01:19, 21.11s/it] 47%|████▋     | 1542/3250 [7:14:54<9:18:21, 19.61s/it]                                                         47%|████▋     | 1542/3250 [7:14:54<9:18:21, 19.61s/it] 47%|████▋     | 1543/3250 [7:15:10<8:48:40, 18.58s/it]                                                        47%|████▋     | 1543/3250 [7:15:10<8:48:40, 18.58s/it] 48%|████▊     | 1544/3250 [7:15:26<8:25:54, 17.79s/it]                                                        48%|████▊     | 1544/3250 [7:15:26<8:25:54, 17.79s/it] 48%|████▊     | 1545/3250 [7:15:42<8:10:00, 17.24s/it]                                                        48%|████▊     | 1545/3250 [7:15:42<8:10:00, 17.24s/it] 48%|████▊     | 1546/3250 [7:15:58<7:58:54, 16.86s/it]        {'loss': 0.5045, 'learning_rate': 5.386509671036695e-05, 'epoch': 0.48}
{'loss': 0.4674, 'learning_rate': 5.38168776661081e-05, 'epoch': 0.48}
{'loss': 0.4629, 'learning_rate': 5.376865505096385e-05, 'epoch': 0.48}
{'loss': 0.4552, 'learning_rate': 5.372042891004896e-05, 'epoch': 0.48}
{'loss': 0.4691, 'learning_rate': 5.367219928848145e-05, 'epoch': 0.48}
                                                48%|████▊     | 1546/3250 [7:15:58<7:58:54, 16.86s/it] 48%|████▊     | 1547/3250 [7:16:14<7:50:55, 16.59s/it]                                                        48%|████▊     | 1547/3250 [7:16:14<7:50:55, 16.59s/it] 48%|████▊     | 1548/3250 [7:16:30<7:45:03, 16.39s/it]                                                        48%|████▊     | 1548/3250 [7:16:30<7:45:03, 16.39s/it] 48%|████▊     | 1549/3250 [7:16:45<7:40:52, 16.26s/it]                                                        48%|████▊     | 1549/3250 [7:16:45<7:40:52, 16.26s/it] 48%|████▊     | 1550/3250 [7:17:01<7:38:08, 16.17s/it]                                                        48%|████▊     | 1550/3250 [7:17:01<7:38:08, 16.17s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7511623501777649, 'eval_runtime': 2.4818, 'eval_samples_per_second': 4.835, 'eval_steps_per_second': 1.209, 'epoch': 0.48}
                                                        48%|████▊     | 1550/3250 [7:17:04<7:38:08, 16.17s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1550
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1550
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1550 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1550


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4835, 'learning_rate': 5.3623966231382615e-05, 'epoch': 0.48}
{'loss': 0.4717, 'learning_rate': 5.357572978387697e-05, 'epoch': 0.48}
{'loss': 0.4961, 'learning_rate': 5.3527489991092186e-05, 'epoch': 0.48}
{'loss': 0.9812, 'learning_rate': 5.3479246898159063e-05, 'epoch': 0.48}
{'loss': 0.4673, 'learning_rate': 5.3431000550211505e-05, 'epoch': 0.48}
 48%|████▊     | 1551/3250 [7:17:35<10:04:15, 21.34s/it]                                                         48%|████▊     | 1551/3250 [7:17:35<10:04:15, 21.34s/it] 48%|████▊     | 1552/3250 [7:17:51<9:18:09, 19.72s/it]                                                         48%|████▊     | 1552/3250 [7:17:51<9:18:09, 19.72s/it] 48%|████▊     | 1553/3250 [7:18:07<8:45:50, 18.59s/it]                                                        48%|████▊     | 1553/3250 [7:18:07<8:45:50, 18.59s/it] 48%|████▊     | 1554/3250 [7:18:23<8:22:49, 17.79s/it]                                                        48%|████▊     | 1554/3250 [7:18:23<8:22:49, 17.79s/it] 48%|████▊     | 1555/3250 [7:18:39<8:06:58, 17.24s/it]                                                        48%|████▊     | 1555/3250 [7:18:39<8:06:58, 17.24s/it] 48%|████▊     | 1556/3250 [7:18:55<7:55:39, 16.85s/it]        {'loss': 0.493, 'learning_rate': 5.338275099238647e-05, 'epoch': 0.48}
{'loss': 0.4832, 'learning_rate': 5.333449826982385e-05, 'epoch': 0.48}
{'loss': 0.4964, 'learning_rate': 5.328624242766661e-05, 'epoch': 0.48}
{'loss': 0.4531, 'learning_rate': 5.323798351106052e-05, 'epoch': 0.48}
{'loss': 0.4863, 'learning_rate': 5.31897215651543e-05, 'epoch': 0.48}
                                                48%|████▊     | 1556/3250 [7:18:55<7:55:39, 16.85s/it] 48%|████▊     | 1557/3250 [7:19:11<7:47:50, 16.58s/it]                                                        48%|████▊     | 1557/3250 [7:19:11<7:47:50, 16.58s/it] 48%|████▊     | 1558/3250 [7:19:27<7:45:50, 16.52s/it]                                                        48%|████▊     | 1558/3250 [7:19:27<7:45:50, 16.52s/it] 48%|████▊     | 1559/3250 [7:19:43<7:40:43, 16.35s/it]                                                        48%|████▊     | 1559/3250 [7:19:43<7:40:43, 16.35s/it] 48%|████▊     | 1560/3250 [7:19:59<7:37:06, 16.23s/it]                                                        48%|████▊     | 1560/3250 [7:19:59<7:37:06, 16.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7459191083908081, 'eval_runtime': 2.4808, 'eval_samples_per_second': 4.837, 'eval_steps_per_second': 1.209, 'epoch': 0.48}
                                                        48%|████▊     | 1560/3250 [7:20:01<7:37:06, 16.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1560
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1560/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1560/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1560



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5262, 'learning_rate': 5.314145663509951e-05, 'epoch': 0.48}
{'loss': 0.4893, 'learning_rate': 5.309318876605043e-05, 'epoch': 0.48}
{'loss': 0.4646, 'learning_rate': 5.3044918003164156e-05, 'epoch': 0.48}
{'loss': 0.4882, 'learning_rate': 5.299664439160047e-05, 'epoch': 0.48}
{'loss': 0.4752, 'learning_rate': 5.294836797652182e-05, 'epoch': 0.48}
 48%|████▊     | 1561/3250 [7:20:18<8:01:09, 17.09s/it]                                                        48%|████▊     | 1561/3250 [7:20:18<8:01:09, 17.09s/it] 48%|████▊     | 1562/3250 [7:20:34<7:51:03, 16.74s/it]                                                        48%|████▊     | 1562/3250 [7:20:34<7:51:03, 16.74s/it] 48%|████▊     | 1563/3250 [7:20:50<7:44:01, 16.50s/it]                                                        48%|████▊     | 1563/3250 [7:20:50<7:44:01, 16.50s/it] 48%|████▊     | 1564/3250 [7:21:06<7:39:01, 16.34s/it]                                                        48%|████▊     | 1564/3250 [7:21:06<7:39:01, 16.34s/it] 48%|████▊     | 1565/3250 [7:21:22<7:35:23, 16.22s/it]                                                        48%|████▊     | 1565/3250 [7:21:22<7:35:23, 16.22s/it] 48%|████▊     | 1566/3250 [7:21:38<7:32:43, 16.13s/it]            {'loss': 0.5001, 'learning_rate': 5.290008880309326e-05, 'epoch': 0.48}
{'loss': 0.4738, 'learning_rate': 5.2851806916482464e-05, 'epoch': 0.48}
{'loss': 0.4869, 'learning_rate': 5.2803522361859594e-05, 'epoch': 0.48}
{'loss': 0.4716, 'learning_rate': 5.275523518439735e-05, 'epoch': 0.48}
{'loss': 0.4892, 'learning_rate': 5.270694542927088e-05, 'epoch': 0.48}
                                            48%|████▊     | 1566/3250 [7:21:38<7:32:43, 16.13s/it] 48%|████▊     | 1567/3250 [7:21:54<7:30:49, 16.07s/it]                                                        48%|████▊     | 1567/3250 [7:21:54<7:30:49, 16.07s/it] 48%|████▊     | 1568/3250 [7:22:09<7:29:13, 16.02s/it]                                                        48%|████▊     | 1568/3250 [7:22:09<7:29:13, 16.02s/it] 48%|████▊     | 1569/3250 [7:22:25<7:28:12, 16.00s/it]                                                        48%|████▊     | 1569/3250 [7:22:25<7:28:12, 16.00s/it] 48%|████▊     | 1570/3250 [7:22:41<7:27:36, 15.99s/it]                                                        48%|████▊     | 1570/3250 [7:22:41<7:27:36, 15.99s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.740923285484314, 'eval_runtime': 2.4779, 'eval_samples_per_second': 4.843, 'eval_steps_per_second': 1.211, 'epoch': 0.48}
                                                        48%|████▊     | 1570/3250 [7:22:44<7:27:36, 15.99s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1570
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1570/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1570

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1570

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1570/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1570/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4951, 'learning_rate': 5.265865314165771e-05, 'epoch': 0.48}
{'loss': 0.4912, 'learning_rate': 5.261035836673779e-05, 'epoch': 0.48}
{'loss': 0.4663, 'learning_rate': 5.256206114969333e-05, 'epoch': 0.48}
{'loss': 0.4905, 'learning_rate': 5.251376153570889e-05, 'epoch': 0.48}
{'loss': 0.4615, 'learning_rate': 5.2465459569971224e-05, 'epoch': 0.48}
 48%|████▊     | 1571/3250 [7:23:01<7:54:30, 16.96s/it]                                                        48%|████▊     | 1571/3250 [7:23:01<7:54:30, 16.96s/it] 48%|████▊     | 1572/3250 [7:23:16<7:45:35, 16.65s/it]                                                        48%|████▊     | 1572/3250 [7:23:16<7:45:35, 16.65s/it] 48%|████▊     | 1573/3250 [7:23:32<7:39:30, 16.44s/it]                                                        48%|████▊     | 1573/3250 [7:23:32<7:39:30, 16.44s/it] 48%|████▊     | 1574/3250 [7:23:49<7:37:48, 16.39s/it]                                                        48%|████▊     | 1574/3250 [7:23:49<7:37:48, 16.39s/it] 48%|████▊     | 1575/3250 [7:24:05<7:33:09, 16.23s/it]                                                        48%|████▊     | 1575/3250 [7:24:05<7:33:09, 16.23s/it] 48%|████▊     | 1576/3250 [7:24:20<7:29:48, 16.12s/it]            {'loss': 0.4897, 'learning_rate': 5.2417155297669326e-05, 'epoch': 0.48}
{'loss': 0.4713, 'learning_rate': 5.236884876399429e-05, 'epoch': 0.49}
{'loss': 0.4647, 'learning_rate': 5.232054001413941e-05, 'epoch': 0.49}
{'loss': 0.4594, 'learning_rate': 5.2272229093299985e-05, 'epoch': 0.49}
{'loss': 0.4489, 'learning_rate': 5.222391604667336e-05, 'epoch': 0.49}
                                            48%|████▊     | 1576/3250 [7:24:20<7:29:48, 16.12s/it] 49%|████▊     | 1577/3250 [7:24:36<7:27:29, 16.05s/it]                                                        49%|████▊     | 1577/3250 [7:24:36<7:27:29, 16.05s/it] 49%|████▊     | 1578/3250 [7:24:52<7:25:35, 15.99s/it]                                                        49%|████▊     | 1578/3250 [7:24:52<7:25:35, 15.99s/it] 49%|████▊     | 1579/3250 [7:25:08<7:24:16, 15.95s/it]                                                        49%|████▊     | 1579/3250 [7:25:08<7:24:16, 15.95s/it] 49%|████▊     | 1580/3250 [7:25:24<7:23:22, 15.93s/it]                                                        49%|████▊     | 1580/3250 [7:25:24<7:23:22, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7488523721694946, 'eval_runtime': 2.4699, 'eval_samples_per_second': 4.858, 'eval_steps_per_second': 1.215, 'epoch': 0.49}
                                                        49%|████▊     | 1580/3250 [7:25:26<7:23:22, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1580
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1580/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1580


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1580

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4769, 'learning_rate': 5.217560091945887e-05, 'epoch': 0.49}
{'loss': 0.4731, 'learning_rate': 5.212728375685781e-05, 'epoch': 0.49}
{'loss': 0.489, 'learning_rate': 5.20789646040734e-05, 'epoch': 0.49}
{'loss': 0.985, 'learning_rate': 5.203064350631064e-05, 'epoch': 0.49}
{'loss': 0.4807, 'learning_rate': 5.198232050877645e-05, 'epoch': 0.49}
 49%|████▊     | 1581/3250 [7:25:43<7:50:49, 16.93s/it]                                                        49%|████▊     | 1581/3250 [7:25:43<7:50:49, 16.93s/it] 49%|████▊     | 1582/3250 [7:25:59<7:41:58, 16.62s/it]                                                        49%|████▊     | 1582/3250 [7:25:59<7:41:58, 16.62s/it] 49%|████▊     | 1583/3250 [7:26:15<7:35:26, 16.39s/it]                                                        49%|████▊     | 1583/3250 [7:26:15<7:35:26, 16.39s/it] 49%|████▊     | 1584/3250 [7:26:31<7:30:14, 16.22s/it]                                                        49%|████▊     | 1584/3250 [7:26:31<7:30:14, 16.22s/it] 49%|████▉     | 1585/3250 [7:26:47<7:26:57, 16.11s/it]                                                        49%|████▉     | 1585/3250 [7:26:47<7:26:57, 16.11s/it] 49%|████▉     | 1586/3250 [7:27:03<7:26:16, 16.09s/it]            {'loss': 0.4829, 'learning_rate': 5.1933995656679444e-05, 'epoch': 0.49}
{'loss': 0.4871, 'learning_rate': 5.188566899523002e-05, 'epoch': 0.49}
{'loss': 0.4871, 'learning_rate': 5.183734056964027e-05, 'epoch': 0.49}
{'loss': 0.4608, 'learning_rate': 5.1789010425123894e-05, 'epoch': 0.49}
{'loss': 0.4646, 'learning_rate': 5.174067860689625e-05, 'epoch': 0.49}
                                            49%|████▉     | 1586/3250 [7:27:03<7:26:16, 16.09s/it] 49%|████▉     | 1587/3250 [7:27:18<7:24:02, 16.02s/it]                                                        49%|████▉     | 1587/3250 [7:27:18<7:24:02, 16.02s/it] 49%|████▉     | 1588/3250 [7:27:34<7:22:20, 15.97s/it]                                                        49%|████▉     | 1588/3250 [7:27:34<7:22:20, 15.97s/it] 49%|████▉     | 1589/3250 [7:27:50<7:21:16, 15.94s/it]                                                        49%|████▉     | 1589/3250 [7:27:50<7:21:16, 15.94s/it] 49%|████▉     | 1590/3250 [7:28:06<7:20:19, 15.92s/it]                                                        49%|████▉     | 1590/3250 [7:28:06<7:20:19, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7473979592323303, 'eval_runtime': 2.4746, 'eval_samples_per_second': 4.849, 'eval_steps_per_second': 1.212, 'epoch': 0.49}
                                                        49%|████▉     | 1590/3250 [7:28:09<7:20:19, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1590
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1590/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1590/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1590



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1590/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5236, 'learning_rate': 5.1692345160174225e-05, 'epoch': 0.49}
{'loss': 0.4934, 'learning_rate': 5.164401013017627e-05, 'epoch': 0.49}
{'loss': 0.4758, 'learning_rate': 5.159567356212226e-05, 'epoch': 0.49}
{'loss': 0.444, 'learning_rate': 5.1547335501233565e-05, 'epoch': 0.49}
{'loss': 0.4915, 'learning_rate': 5.149899599273291e-05, 'epoch': 0.49}
 49%|████▉     | 1591/3250 [7:28:40<9:46:00, 21.19s/it]                                                        49%|████▉     | 1591/3250 [7:28:40<9:46:00, 21.19s/it] 49%|████▉     | 1592/3250 [7:28:55<9:01:21, 19.59s/it]                                                        49%|████▉     | 1592/3250 [7:28:55<9:01:21, 19.59s/it] 49%|████▉     | 1593/3250 [7:29:13<8:44:36, 19.00s/it]                                                        49%|████▉     | 1593/3250 [7:29:13<8:44:36, 19.00s/it] 49%|████▉     | 1594/3250 [7:29:29<8:20:44, 18.14s/it]                                                        49%|████▉     | 1594/3250 [7:29:29<8:20:44, 18.14s/it] 49%|████▉     | 1595/3250 [7:29:45<8:01:08, 17.44s/it]                                                        49%|████▉     | 1595/3250 [7:29:45<8:01:08, 17.44s/it] 49%|████▉     | 1596/3250 [7:30:01<7:47:13, 16.95s/it]            {'loss': 0.4785, 'learning_rate': 5.14506550818444e-05, 'epoch': 0.49}
{'loss': 0.4741, 'learning_rate': 5.140231281379345e-05, 'epoch': 0.49}
{'loss': 0.4634, 'learning_rate': 5.135396923380673e-05, 'epoch': 0.49}
{'loss': 0.4882, 'learning_rate': 5.130562438711215e-05, 'epoch': 0.49}
{'loss': 0.4676, 'learning_rate': 5.1257278318938785e-05, 'epoch': 0.49}
                                            49%|████▉     | 1596/3250 [7:30:01<7:47:13, 16.95s/it] 49%|████▉     | 1597/3250 [7:30:17<7:37:33, 16.61s/it]                                                        49%|████▉     | 1597/3250 [7:30:17<7:37:33, 16.61s/it] 49%|████▉     | 1598/3250 [7:30:32<7:30:47, 16.37s/it]                                                        49%|████▉     | 1598/3250 [7:30:32<7:30:47, 16.37s/it] 49%|████▉     | 1599/3250 [7:30:48<7:25:55, 16.21s/it]                                                        49%|████▉     | 1599/3250 [7:30:48<7:25:55, 16.21s/it] 49%|████▉     | 1600/3250 [7:31:04<7:22:24, 16.09s/it]                                                        49%|████▉     | 1600/3250 [7:31:04<7:22:24, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7409207224845886, 'eval_runtime': 3.2417, 'eval_samples_per_second': 3.702, 'eval_steps_per_second': 0.925, 'epoch': 0.49}
                                                        49%|████▉     | 1600/3250 [7:31:07<7:22:24, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1600
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1600 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1600
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5005, 'learning_rate': 5.1208931074516885e-05, 'epoch': 0.49}
{'loss': 0.4783, 'learning_rate': 5.116058269907779e-05, 'epoch': 0.49}
{'loss': 0.4699, 'learning_rate': 5.111223323785387e-05, 'epoch': 0.49}
{'loss': 0.4798, 'learning_rate': 5.106388273607855e-05, 'epoch': 0.49}
{'loss': 0.482, 'learning_rate': 5.101553123898622e-05, 'epoch': 0.49}
 49%|████▉     | 1601/3250 [7:31:24<7:53:05, 17.21s/it]                                                        49%|████▉     | 1601/3250 [7:31:24<7:53:05, 17.21s/it] 49%|████▉     | 1602/3250 [7:31:40<7:41:12, 16.79s/it]                                                        49%|████▉     | 1602/3250 [7:31:40<7:41:12, 16.79s/it] 49%|████▉     | 1603/3250 [7:31:56<7:32:51, 16.50s/it]                                                        49%|████▉     | 1603/3250 [7:31:56<7:32:51, 16.50s/it] 49%|████▉     | 1604/3250 [7:32:11<7:26:56, 16.29s/it]                                                        49%|████▉     | 1604/3250 [7:32:11<7:26:56, 16.29s/it] 49%|████▉     | 1605/3250 [7:32:27<7:22:44, 16.15s/it]                                                        49%|████▉     | 1605/3250 [7:32:27<7:22:44, 16.15s/it] 49%|████▉     | 1606/3250 [7:32:43<7:19:44, 16.05s/it]            {'loss': 0.4463, 'learning_rate': 5.096717879181217e-05, 'epoch': 0.49}
{'loss': 0.4923, 'learning_rate': 5.0918825439792604e-05, 'epoch': 0.49}
{'loss': 0.4642, 'learning_rate': 5.087047122816458e-05, 'epoch': 0.49}
{'loss': 0.46, 'learning_rate': 5.082211620216595e-05, 'epoch': 0.5}
{'loss': 0.4571, 'learning_rate': 5.077376040703533e-05, 'epoch': 0.5}
                                            49%|████▉     | 1606/3250 [7:32:43<7:19:44, 16.05s/it] 49%|████▉     | 1607/3250 [7:33:00<7:24:27, 16.23s/it]                                                        49%|████▉     | 1607/3250 [7:33:00<7:24:27, 16.23s/it] 49%|████▉     | 1608/3250 [7:33:15<7:20:43, 16.10s/it]                                                        49%|████▉     | 1608/3250 [7:33:15<7:20:43, 16.10s/it] 50%|████▉     | 1609/3250 [7:33:31<7:19:26, 16.07s/it]                                                        50%|████▉     | 1609/3250 [7:33:32<7:19:26, 16.07s/it] 50%|████▉     | 1610/3250 [7:33:47<7:19:13, 16.07s/it]                                                        50%|████▉     | 1610/3250 [7:33:47<7:19:13, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7473222613334656, 'eval_runtime': 2.5444, 'eval_samples_per_second': 4.716, 'eval_steps_per_second': 1.179, 'epoch': 0.5}
                                                        50%|████▉     | 1610/3250 [7:33:50<7:19:13, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1610
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1610
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1610
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1610

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4774, 'learning_rate': 5.072540388801204e-05, 'epoch': 0.5}
{'loss': 0.4694, 'learning_rate': 5.0677046690336096e-05, 'epoch': 0.5}
{'loss': 0.4837, 'learning_rate': 5.0628688859248164e-05, 'epoch': 0.5}
{'loss': 0.5655, 'learning_rate': 5.0580330439989465e-05, 'epoch': 0.5}
{'loss': 0.8851, 'learning_rate': 5.0531971477801776e-05, 'epoch': 0.5}
 50%|████▉     | 1611/3250 [7:34:21<9:41:21, 21.28s/it]                                                        50%|████▉     | 1611/3250 [7:34:21<9:41:21, 21.28s/it] 50%|████▉     | 1612/3250 [7:34:37<8:56:18, 19.65s/it]                                                        50%|████▉     | 1612/3250 [7:34:37<8:56:18, 19.65s/it] 50%|████▉     | 1613/3250 [7:34:53<8:24:44, 18.50s/it]                                                        50%|████▉     | 1613/3250 [7:34:53<8:24:44, 18.50s/it] 50%|████▉     | 1614/3250 [7:35:08<8:02:35, 17.70s/it]                                                        50%|████▉     | 1614/3250 [7:35:08<8:02:35, 17.70s/it] 50%|████▉     | 1615/3250 [7:35:24<7:46:35, 17.12s/it]                                                        50%|████▉     | 1615/3250 [7:35:24<7:46:35, 17.12s/it] 50%|████▉     | 1616/3250 [7:35:40<7:35:54, 16.74s/it]            {'loss': 0.4656, 'learning_rate': 5.048361201792742e-05, 'epoch': 0.5}
{'loss': 0.4817, 'learning_rate': 5.043525210560912e-05, 'epoch': 0.5}
{'loss': 0.4848, 'learning_rate': 5.0386891786090105e-05, 'epoch': 0.5}
{'loss': 0.4785, 'learning_rate': 5.0338531104613926e-05, 'epoch': 0.5}
{'loss': 0.4573, 'learning_rate': 5.029017010642447e-05, 'epoch': 0.5}
                                            50%|████▉     | 1616/3250 [7:35:40<7:35:54, 16.74s/it] 50%|████▉     | 1617/3250 [7:35:56<7:28:23, 16.48s/it]                                                        50%|████▉     | 1617/3250 [7:35:56<7:28:23, 16.48s/it] 50%|████▉     | 1618/3250 [7:36:12<7:22:58, 16.29s/it]                                                        50%|████▉     | 1618/3250 [7:36:12<7:22:58, 16.29s/it] 50%|████▉     | 1619/3250 [7:36:28<7:19:38, 16.17s/it]                                                        50%|████▉     | 1619/3250 [7:36:28<7:19:38, 16.17s/it] 50%|████▉     | 1620/3250 [7:36:43<7:16:33, 16.07s/it]                                                        50%|████▉     | 1620/3250 [7:36:43<7:16:33, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7452312111854553, 'eval_runtime': 2.4651, 'eval_samples_per_second': 4.868, 'eval_steps_per_second': 1.217, 'epoch': 0.5}
                                                        50%|████▉     | 1620/3250 [7:36:46<7:16:33, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1620
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1620/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1620

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1620

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5087, 'learning_rate': 5.024180883676597e-05, 'epoch': 0.5}
{'loss': 0.4962, 'learning_rate': 5.019344734088287e-05, 'epoch': 0.5}
{'loss': 0.4592, 'learning_rate': 5.014508566401982e-05, 'epoch': 0.5}
{'loss': 0.4582, 'learning_rate': 5.009672385142167e-05, 'epoch': 0.5}
{'loss': 0.4811, 'learning_rate': 5.004836194833339e-05, 'epoch': 0.5}
 50%|████▉     | 1621/3250 [7:37:02<7:40:17, 16.95s/it]                                                        50%|████▉     | 1621/3250 [7:37:02<7:40:17, 16.95s/it] 50%|████▉     | 1622/3250 [7:37:18<7:31:02, 16.62s/it]                                                        50%|████▉     | 1622/3250 [7:37:18<7:31:02, 16.62s/it] 50%|████▉     | 1623/3250 [7:37:35<7:27:33, 16.51s/it]                                                        50%|████▉     | 1623/3250 [7:37:35<7:27:33, 16.51s/it] 50%|████▉     | 1624/3250 [7:37:50<7:22:02, 16.31s/it]                                                        50%|████▉     | 1624/3250 [7:37:50<7:22:02, 16.31s/it] 50%|█████     | 1625/3250 [7:38:06<7:17:56, 16.17s/it]                                                        50%|█████     | 1625/3250 [7:38:06<7:17:56, 16.17s/it] 50%|█████     | 1626/3250 [7:38:22<7:15:31, 16.09s/it]            {'loss': 0.4712, 'learning_rate': 5e-05, 'epoch': 0.5}
{'loss': 0.4868, 'learning_rate': 4.995163805166662e-05, 'epoch': 0.5}
{'loss': 0.4664, 'learning_rate': 4.990327614857834e-05, 'epoch': 0.5}
{'loss': 0.4693, 'learning_rate': 4.9854914335980193e-05, 'epoch': 0.5}
{'loss': 0.4693, 'learning_rate': 4.980655265911714e-05, 'epoch': 0.5}
                                            50%|█████     | 1626/3250 [7:38:22<7:15:31, 16.09s/it] 50%|█████     | 1627/3250 [7:38:38<7:13:07, 16.01s/it]                                                        50%|█████     | 1627/3250 [7:38:38<7:13:07, 16.01s/it] 50%|█████     | 1628/3250 [7:38:54<7:11:07, 15.95s/it]                                                        50%|█████     | 1628/3250 [7:38:54<7:11:07, 15.95s/it] 50%|█████     | 1629/3250 [7:39:10<7:09:45, 15.91s/it]                                                        50%|█████     | 1629/3250 [7:39:10<7:09:45, 15.91s/it] 50%|█████     | 1630/3250 [7:39:25<7:08:55, 15.89s/it]                                                        50%|█████     | 1630/3250 [7:39:25<7:08:55, 15.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7386166453361511, 'eval_runtime': 2.678, 'eval_samples_per_second': 4.481, 'eval_steps_per_second': 1.12, 'epoch': 0.5}
                                                        50%|█████     | 1630/3250 [7:39:28<7:08:55, 15.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1630
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1630/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.489, 'learning_rate': 4.975819116323403e-05, 'epoch': 0.5}
{'loss': 0.4725, 'learning_rate': 4.970982989357552e-05, 'epoch': 0.5}
{'loss': 0.4711, 'learning_rate': 4.966146889538608e-05, 'epoch': 0.5}
{'loss': 0.4756, 'learning_rate': 4.96131082139099e-05, 'epoch': 0.5}
{'loss': 0.4824, 'learning_rate': 4.9564747894390903e-05, 'epoch': 0.5}
 50%|█████     | 1631/3250 [7:39:45<7:36:13, 16.91s/it]                                                        50%|█████     | 1631/3250 [7:39:45<7:36:13, 16.91s/it] 50%|█████     | 1632/3250 [7:40:01<7:27:13, 16.58s/it]                                                        50%|█████     | 1632/3250 [7:40:01<7:27:13, 16.58s/it] 50%|█████     | 1633/3250 [7:40:16<7:20:58, 16.36s/it]                                                        50%|█████     | 1633/3250 [7:40:16<7:20:58, 16.36s/it] 50%|█████     | 1634/3250 [7:40:32<7:16:23, 16.20s/it]                                                        50%|█████     | 1634/3250 [7:40:32<7:16:23, 16.20s/it] 50%|█████     | 1635/3250 [7:40:48<7:13:01, 16.09s/it]                                                        50%|█████     | 1635/3250 [7:40:48<7:13:01, 16.09s/it] 50%|█████     | 1636/3250 [7:41:04<7:10:32, 16.01s/it]            {'loss': 0.4345, 'learning_rate': 4.951638798207261e-05, 'epoch': 0.5}
{'loss': 0.4969, 'learning_rate': 4.946802852219824e-05, 'epoch': 0.5}
{'loss': 0.4668, 'learning_rate': 4.941966956001056e-05, 'epoch': 0.5}
{'loss': 0.4587, 'learning_rate': 4.9371311140751854e-05, 'epoch': 0.5}
{'loss': 0.4496, 'learning_rate': 4.9322953309663916e-05, 'epoch': 0.5}
                                            50%|█████     | 1636/3250 [7:41:04<7:10:32, 16.01s/it] 50%|█████     | 1637/3250 [7:41:20<7:08:53, 15.95s/it]                                                        50%|█████     | 1637/3250 [7:41:20<7:08:53, 15.95s/it] 50%|█████     | 1638/3250 [7:41:36<7:07:41, 15.92s/it]                                                        50%|█████     | 1638/3250 [7:41:36<7:07:41, 15.92s/it] 50%|█████     | 1639/3250 [7:41:51<7:06:34, 15.89s/it]                                                        50%|█████     | 1639/3250 [7:41:51<7:06:34, 15.89s/it] 50%|█████     | 1640/3250 [7:42:08<7:15:25, 16.23s/it]                                                        50%|█████     | 1640/3250 [7:42:08<7:15:25, 16.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7525596022605896, 'eval_runtime': 2.8562, 'eval_samples_per_second': 4.201, 'eval_steps_per_second': 1.05, 'epoch': 0.5}
                                                        50%|█████     | 1640/3250 [7:42:11<7:15:25, 16.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1640
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1640
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1640

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1640
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4604, 'learning_rate': 4.9274596111987974e-05, 'epoch': 0.5}
{'loss': 0.4674, 'learning_rate': 4.922623959296468e-05, 'epoch': 0.51}
{'loss': 0.4573, 'learning_rate': 4.9177883797834064e-05, 'epoch': 0.51}
{'loss': 0.4795, 'learning_rate': 4.912952877183543e-05, 'epoch': 0.51}
{'loss': 0.9735, 'learning_rate': 4.908117456020741e-05, 'epoch': 0.51}
 50%|█████     | 1641/3250 [7:42:46<10:09:28, 22.73s/it]                                                         50%|█████     | 1641/3250 [7:42:46<10:09:28, 22.73s/it] 51%|█████     | 1642/3250 [7:43:02<9:13:30, 20.65s/it]                                                         51%|█████     | 1642/3250 [7:43:02<9:13:30, 20.65s/it] 51%|█████     | 1643/3250 [7:43:18<8:34:25, 19.21s/it]                                                        51%|█████     | 1643/3250 [7:43:18<8:34:25, 19.21s/it] 51%|█████     | 1644/3250 [7:43:34<8:07:14, 18.20s/it]                                                        51%|█████     | 1644/3250 [7:43:34<8:07:14, 18.20s/it] 51%|█████     | 1645/3250 [7:43:50<7:47:29, 17.48s/it]                                                        51%|█████     | 1645/3250 [7:43:50<7:47:29, 17.48s/it] 51%|█████     | 1646/3250 [7:44:05<7:33:53, 16.98s/it]        {'loss': 0.4604, 'learning_rate': 4.903282120818785e-05, 'epoch': 0.51}
{'loss': 0.4771, 'learning_rate': 4.898446876101379e-05, 'epoch': 0.51}
{'loss': 0.4873, 'learning_rate': 4.893611726392145e-05, 'epoch': 0.51}
{'loss': 0.4822, 'learning_rate': 4.8887766762146134e-05, 'epoch': 0.51}
{'loss': 0.4355, 'learning_rate': 4.8839417300922216e-05, 'epoch': 0.51}
                                                51%|█████     | 1646/3250 [7:44:05<7:33:53, 16.98s/it] 51%|█████     | 1647/3250 [7:44:21<7:24:15, 16.63s/it]                                                        51%|█████     | 1647/3250 [7:44:21<7:24:15, 16.63s/it] 51%|█████     | 1648/3250 [7:44:37<7:17:43, 16.39s/it]                                                        51%|█████     | 1648/3250 [7:44:37<7:17:43, 16.39s/it] 51%|█████     | 1649/3250 [7:44:53<7:12:47, 16.22s/it]                                                        51%|█████     | 1649/3250 [7:44:53<7:12:47, 16.22s/it] 51%|█████     | 1650/3250 [7:45:09<7:09:05, 16.09s/it]                                                        51%|█████     | 1650/3250 [7:45:09<7:09:05, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7475366592407227, 'eval_runtime': 2.4593, 'eval_samples_per_second': 4.88, 'eval_steps_per_second': 1.22, 'epoch': 0.51}
                                                        51%|█████     | 1650/3250 [7:45:11<7:09:05, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1650
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1650
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4729, 'learning_rate': 4.8791068925483106e-05, 'epoch': 0.51}
{'loss': 0.5148, 'learning_rate': 4.8742721681061226e-05, 'epoch': 0.51}
{'loss': 0.467, 'learning_rate': 4.869437561288788e-05, 'epoch': 0.51}
{'loss': 0.4559, 'learning_rate': 4.8646030766193285e-05, 'epoch': 0.51}
{'loss': 0.4681, 'learning_rate': 4.859768718620656e-05, 'epoch': 0.51}
 51%|█████     | 1651/3250 [7:45:28<7:33:06, 17.00s/it]                                                        51%|█████     | 1651/3250 [7:45:28<7:33:06, 17.00s/it] 51%|█████     | 1652/3250 [7:45:44<7:23:13, 16.64s/it]                                                        51%|█████     | 1652/3250 [7:45:44<7:23:13, 16.64s/it] 51%|█████     | 1653/3250 [7:45:59<7:16:26, 16.40s/it]                                                        51%|█████     | 1653/3250 [7:45:59<7:16:26, 16.40s/it] 51%|█████     | 1654/3250 [7:46:15<7:11:38, 16.23s/it]                                                        51%|█████     | 1654/3250 [7:46:15<7:11:38, 16.23s/it] 51%|█████     | 1655/3250 [7:46:31<7:08:28, 16.12s/it]                                                        51%|█████     | 1655/3250 [7:46:31<7:08:28, 16.12s/it] 51%|█████     | 1656/3250 [7:46:47<7:09:09, 16.15s/it]            {'loss': 0.4643, 'learning_rate': 4.854934491815561e-05, 'epoch': 0.51}
{'loss': 0.4816, 'learning_rate': 4.8501004007267095e-05, 'epoch': 0.51}
{'loss': 0.4574, 'learning_rate': 4.845266449876645e-05, 'epoch': 0.51}
{'loss': 0.464, 'learning_rate': 4.8404326437877746e-05, 'epoch': 0.51}
{'loss': 0.4752, 'learning_rate': 4.8355989869823737e-05, 'epoch': 0.51}
                                            51%|█████     | 1656/3250 [7:46:47<7:09:09, 16.15s/it] 51%|█████     | 1657/3250 [7:47:03<7:08:36, 16.14s/it]                                                        51%|█████     | 1657/3250 [7:47:04<7:08:36, 16.14s/it] 51%|█████     | 1658/3250 [7:47:20<7:08:01, 16.13s/it]                                                        51%|█████     | 1658/3250 [7:47:20<7:08:01, 16.13s/it] 51%|█████     | 1659/3250 [7:47:35<7:05:53, 16.06s/it]                                                        51%|█████     | 1659/3250 [7:47:35<7:05:53, 16.06s/it] 51%|█████     | 1660/3250 [7:47:51<7:04:16, 16.01s/it]                                                        51%|█████     | 1660/3250 [7:47:51<7:04:16, 16.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7395169138908386, 'eval_runtime': 2.5251, 'eval_samples_per_second': 4.752, 'eval_steps_per_second': 1.188, 'epoch': 0.51}
                                                        51%|█████     | 1660/3250 [7:47:54<7:04:16, 16.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1660
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1660

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1660
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1660

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1660
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4768, 'learning_rate': 4.830765483982578e-05, 'epoch': 0.51}
{'loss': 0.4848, 'learning_rate': 4.8259321393103754e-05, 'epoch': 0.51}
{'loss': 0.4918, 'learning_rate': 4.821098957487611e-05, 'epoch': 0.51}
{'loss': 0.4691, 'learning_rate': 4.816265943035975e-05, 'epoch': 0.51}
{'loss': 0.4702, 'learning_rate': 4.811433100476999e-05, 'epoch': 0.51}
 51%|█████     | 1661/3250 [7:48:10<7:28:58, 16.95s/it]                                                        51%|█████     | 1661/3250 [7:48:10<7:28:58, 16.95s/it] 51%|█████     | 1662/3250 [7:48:26<7:20:16, 16.64s/it]                                                        51%|█████     | 1662/3250 [7:48:27<7:20:16, 16.64s/it] 51%|█████     | 1663/3250 [7:48:43<7:22:25, 16.73s/it]                                                        51%|█████     | 1663/3250 [7:48:43<7:22:25, 16.73s/it] 51%|█████     | 1664/3250 [7:48:59<7:16:31, 16.51s/it]                                                        51%|█████     | 1664/3250 [7:48:59<7:16:31, 16.51s/it] 51%|█████     | 1665/3250 [7:49:15<7:11:06, 16.32s/it]                                                        51%|█████     | 1665/3250 [7:49:15<7:11:06, 16.32s/it] 51%|█████▏    | 1666/3250 [7:49:31<7:07:16, 16.18s/it]          {'loss': 0.4446, 'learning_rate': 4.806600434332056e-05, 'epoch': 0.51}
{'loss': 0.4879, 'learning_rate': 4.801767949122356e-05, 'epoch': 0.51}
{'loss': 0.458, 'learning_rate': 4.796935649368935e-05, 'epoch': 0.51}
{'loss': 0.4498, 'learning_rate': 4.7921035395926625e-05, 'epoch': 0.51}
{'loss': 0.4556, 'learning_rate': 4.7872716243142194e-05, 'epoch': 0.51}
                                              51%|█████▏    | 1666/3250 [7:49:31<7:07:16, 16.18s/it] 51%|█████▏    | 1667/3250 [7:49:47<7:04:29, 16.09s/it]                                                        51%|█████▏    | 1667/3250 [7:49:47<7:04:29, 16.09s/it] 51%|█████▏    | 1668/3250 [7:50:03<7:02:22, 16.02s/it]                                                        51%|█████▏    | 1668/3250 [7:50:03<7:02:22, 16.02s/it] 51%|█████▏    | 1669/3250 [7:50:19<7:00:58, 15.98s/it]                                                        51%|█████▏    | 1669/3250 [7:50:19<7:00:58, 15.98s/it] 51%|█████▏    | 1670/3250 [7:50:35<6:59:50, 15.94s/it]                                                        51%|█████▏    | 1670/3250 [7:50:35<6:59:50, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7506967186927795, 'eval_runtime': 2.4761, 'eval_samples_per_second': 4.846, 'eval_steps_per_second': 1.212, 'epoch': 0.51}
                                                        51%|█████▏    | 1670/3250 [7:50:37<6:59:50, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1670
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1670 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1670
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1670

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4401, 'learning_rate': 4.782439908054115e-05, 'epoch': 0.51}
{'loss': 0.475, 'learning_rate': 4.777608395332667e-05, 'epoch': 0.51}
{'loss': 0.4622, 'learning_rate': 4.7727770906700034e-05, 'epoch': 0.51}
{'loss': 0.4703, 'learning_rate': 4.76794599858606e-05, 'epoch': 0.52}
{'loss': 0.9734, 'learning_rate': 4.763115123600571e-05, 'epoch': 0.52}
 51%|█████▏    | 1671/3250 [7:50:54<7:24:28, 16.89s/it]                                                        51%|█████▏    | 1671/3250 [7:50:54<7:24:28, 16.89s/it] 51%|█████▏    | 1672/3250 [7:51:10<7:16:04, 16.58s/it]                                                        51%|█████▏    | 1672/3250 [7:51:10<7:16:04, 16.58s/it] 51%|█████▏    | 1673/3250 [7:51:26<7:15:11, 16.56s/it]                                                        51%|█████▏    | 1673/3250 [7:51:26<7:15:11, 16.56s/it] 52%|█████▏    | 1674/3250 [7:51:42<7:09:25, 16.35s/it]                                                        52%|█████▏    | 1674/3250 [7:51:42<7:09:25, 16.35s/it] 52%|█████▏    | 1675/3250 [7:51:58<7:05:04, 16.19s/it]                                                        52%|█████▏    | 1675/3250 [7:51:58<7:05:04, 16.19s/it] 52%|█████▏    | 1676/3250 [7:52:14<7:02:23, 1{'loss': 0.47, 'learning_rate': 4.7582844702330685e-05, 'epoch': 0.52}
{'loss': 0.454, 'learning_rate': 4.753454043002878e-05, 'epoch': 0.52}
{'loss': 0.4732, 'learning_rate': 4.748623846429112e-05, 'epoch': 0.52}
{'loss': 0.4706, 'learning_rate': 4.743793885030668e-05, 'epoch': 0.52}
{'loss': 0.4455, 'learning_rate': 4.7389641633262224e-05, 'epoch': 0.52}
6.10s/it]                                                        52%|█████▏    | 1676/3250 [7:52:14<7:02:23, 16.10s/it] 52%|█████▏    | 1677/3250 [7:52:30<7:05:40, 16.24s/it]                                                        52%|█████▏    | 1677/3250 [7:52:30<7:05:40, 16.24s/it] 52%|█████▏    | 1678/3250 [7:52:46<7:03:38, 16.17s/it]                                                        52%|█████▏    | 1678/3250 [7:52:46<7:03:38, 16.17s/it] 52%|█████▏    | 1679/3250 [7:53:02<7:00:53, 16.07s/it]                                                        52%|█████▏    | 1679/3250 [7:53:02<7:00:53, 16.07s/it] 52%|█████▏    | 1680/3250 [7:53:18<6:58:47, 16.00s/it]                                                        52%|█████▏    | 1680/3250 [7:53:18<6:58:47, 16.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7472890615463257, 'eval_runtime': 2.6284, 'eval_samples_per_second': 4.566, 'eval_steps_per_second': 1.141, 'epoch': 0.52}
                                                        52%|█████▏    | 1680/3250 [7:53:20<6:58:47, 16.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1680
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1680I AM HERE AND I AM SAVING THE MODEL 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1680

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1680

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1680/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1680/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4551, 'learning_rate': 4.7341346858342287e-05, 'epoch': 0.52}
{'loss': 0.5169, 'learning_rate': 4.729305457072913e-05, 'epoch': 0.52}
{'loss': 0.4776, 'learning_rate': 4.724476481560265e-05, 'epoch': 0.52}
{'loss': 0.4611, 'learning_rate': 4.7196477638140404e-05, 'epoch': 0.52}
{'loss': 0.44, 'learning_rate': 4.714819308351755e-05, 'epoch': 0.52}
 52%|█████▏    | 1681/3250 [7:53:37<7:24:38, 17.00s/it]                                                        52%|█████▏    | 1681/3250 [7:53:37<7:24:38, 17.00s/it] 52%|█████▏    | 1682/3250 [7:53:53<7:15:19, 16.66s/it]                                                        52%|█████▏    | 1682/3250 [7:53:53<7:15:19, 16.66s/it] 52%|█████▏    | 1683/3250 [7:54:09<7:09:11, 16.43s/it]                                                        52%|█████▏    | 1683/3250 [7:54:09<7:09:11, 16.43s/it] 52%|█████▏    | 1684/3250 [7:54:25<7:04:19, 16.26s/it]                                                        52%|█████▏    | 1684/3250 [7:54:25<7:04:19, 16.26s/it] 52%|█████▏    | 1685/3250 [7:54:41<7:00:54, 16.14s/it]                                                        52%|█████▏    | 1685/3250 [7:54:41<7:00:54, 16.14s/it] 52%|█████▏    | 1686/3250 [7:54:56<6:58:22, 1{'loss': 0.4782, 'learning_rate': 4.7099911196906764e-05, 'epoch': 0.52}
{'loss': 0.4711, 'learning_rate': 4.7051632023478204e-05, 'epoch': 0.52}
{'loss': 0.4588, 'learning_rate': 4.700335560839955e-05, 'epoch': 0.52}
{'loss': 0.4477, 'learning_rate': 4.695508199683586e-05, 'epoch': 0.52}
{'loss': 0.4717, 'learning_rate': 4.6906811233949585e-05, 'epoch': 0.52}
6.05s/it]                                                        52%|█████▏    | 1686/3250 [7:54:56<6:58:22, 16.05s/it] 52%|█████▏    | 1687/3250 [7:55:12<6:56:28, 15.99s/it]                                                        52%|█████▏    | 1687/3250 [7:55:12<6:56:28, 15.99s/it] 52%|█████▏    | 1688/3250 [7:55:28<6:55:05, 15.94s/it]                                                        52%|█████▏    | 1688/3250 [7:55:28<6:55:05, 15.94s/it] 52%|█████▏    | 1689/3250 [7:55:44<6:57:17, 16.04s/it]                                                        52%|█████▏    | 1689/3250 [7:55:44<6:57:17, 16.04s/it] 52%|█████▏    | 1690/3250 [7:56:00<6:55:31, 15.98s/it]                                                        52%|█████▏    | 1690/3250 [7:56:00<6:55:31, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.737601637840271, 'eval_runtime': 2.4724, 'eval_samples_per_second': 4.854, 'eval_steps_per_second': 1.213, 'epoch': 0.52}
                                                        52%|█████▏    | 1690/3250 [7:56:03<6:55:31, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1690
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1690/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4695, 'learning_rate': 4.68585433649005e-05, 'epoch': 0.52}
{'loss': 0.4833, 'learning_rate': 4.68102784348457e-05, 'epoch': 0.52}
{'loss': 0.4761, 'learning_rate': 4.676201648893949e-05, 'epoch': 0.52}
{'loss': 0.4554, 'learning_rate': 4.67137575723334e-05, 'epoch': 0.52}
{'loss': 0.4705, 'learning_rate': 4.666550173017615e-05, 'epoch': 0.52}
 52%|█████▏    | 1691/3250 [7:56:20<7:22:25, 17.03s/it]                                                        52%|█████▏    | 1691/3250 [7:56:20<7:22:25, 17.03s/it] 52%|█████▏    | 1692/3250 [7:56:36<7:13:00, 16.68s/it]                                                        52%|█████▏    | 1692/3250 [7:56:36<7:13:00, 16.68s/it] 52%|█████▏    | 1693/3250 [7:56:51<7:06:19, 16.43s/it]                                                        52%|█████▏    | 1693/3250 [7:56:51<7:06:19, 16.43s/it] 52%|█████▏    | 1694/3250 [7:57:07<7:01:48, 16.27s/it]                                                        52%|█████▏    | 1694/3250 [7:57:07<7:01:48, 16.27s/it] 52%|█████▏    | 1695/3250 [7:57:23<6:58:57, 16.17s/it]                                                        52%|█████▏    | 1695/3250 [7:57:23<6:58:57, 16.17s/it] 52%|█████▏    | 1696/3250 [7:57:39<6:56:23, 1{'loss': 0.4623, 'learning_rate': 4.6617249007613544e-05, 'epoch': 0.52}
{'loss': 0.4274, 'learning_rate': 4.65689994497885e-05, 'epoch': 0.52}
{'loss': 0.4843, 'learning_rate': 4.652075310184094e-05, 'epoch': 0.52}
{'loss': 0.4451, 'learning_rate': 4.647251000890782e-05, 'epoch': 0.52}
{'loss': 0.4433, 'learning_rate': 4.642427021612304e-05, 'epoch': 0.52}
6.08s/it]                                                        52%|█████▏    | 1696/3250 [7:57:39<6:56:23, 16.08s/it] 52%|█████▏    | 1697/3250 [7:57:55<6:54:30, 16.01s/it]                                                        52%|█████▏    | 1697/3250 [7:57:55<6:54:30, 16.01s/it] 52%|█████▏    | 1698/3250 [7:58:11<6:53:16, 15.98s/it]                                                        52%|█████▏    | 1698/3250 [7:58:11<6:53:16, 15.98s/it] 52%|█████▏    | 1699/3250 [7:58:27<6:52:10, 15.95s/it]                                                        52%|█████▏    | 1699/3250 [7:58:27<6:52:10, 15.95s/it] 52%|█████▏    | 1700/3250 [7:58:43<6:51:10, 15.92s/it]                                                        52%|█████▏    | 1700/3250 [7:58:43<6:51:10, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7550517320632935, 'eval_runtime': 2.4895, 'eval_samples_per_second': 4.82, 'eval_steps_per_second': 1.205, 'epoch': 0.52}
                                                        52%|█████▏    | 1700/3250 [7:58:45<6:51:10, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1700
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1700

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1700
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1700

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4408, 'learning_rate': 4.637603376861738e-05, 'epoch': 0.52}
{'loss': 0.4602, 'learning_rate': 4.6327800711518545e-05, 'epoch': 0.52}
{'loss': 0.4573, 'learning_rate': 4.6279571089951054e-05, 'epoch': 0.52}
{'loss': 0.475, 'learning_rate': 4.623134494903618e-05, 'epoch': 0.52}
{'loss': 0.6556, 'learning_rate': 4.6183122333891926e-05, 'epoch': 0.52}
 52%|█████▏    | 1701/3250 [7:59:02<7:17:08, 16.93s/it]                                                        52%|█████▏    | 1701/3250 [7:59:02<7:17:08, 16.93s/it] 52%|█████▏    | 1702/3250 [7:59:18<7:08:39, 16.61s/it]                                                        52%|█████▏    | 1702/3250 [7:59:18<7:08:39, 16.61s/it] 52%|█████▏    | 1703/3250 [7:59:34<7:02:41, 16.39s/it]                                                        52%|█████▏    | 1703/3250 [7:59:34<7:02:41, 16.39s/it] 52%|█████▏    | 1704/3250 [7:59:50<6:58:17, 16.23s/it]                                                        52%|█████▏    | 1704/3250 [7:59:50<6:58:17, 16.23s/it] 52%|█████▏    | 1705/3250 [8:00:06<6:57:25, 16.21s/it]                                                        52%|█████▏    | 1705/3250 [8:00:06<6:57:25, 16.21s/it] 52%|█████▏    | 1706/3250 [8:00:22<6:54:11, 1{'loss': 0.7751, 'learning_rate': 4.613490328963307e-05, 'epoch': 0.52}
{'loss': 0.4527, 'learning_rate': 4.6086687861371004e-05, 'epoch': 0.53}
{'loss': 0.4771, 'learning_rate': 4.6038476094213724e-05, 'epoch': 0.53}
{'loss': 0.4711, 'learning_rate': 4.599026803326583e-05, 'epoch': 0.53}
{'loss': 0.4608, 'learning_rate': 4.594206372362845e-05, 'epoch': 0.53}
6.10s/it]                                                        52%|█████▏    | 1706/3250 [8:00:22<6:54:11, 16.10s/it] 53%|█████▎    | 1707/3250 [8:00:37<6:51:54, 16.02s/it]                                                        53%|█████▎    | 1707/3250 [8:00:37<6:51:54, 16.02s/it] 53%|█████▎    | 1708/3250 [8:00:53<6:50:23, 15.97s/it]                                                        53%|█████▎    | 1708/3250 [8:00:53<6:50:23, 15.97s/it] 53%|█████▎    | 1709/3250 [8:01:09<6:49:07, 15.93s/it]                                                        53%|█████▎    | 1709/3250 [8:01:09<6:49:07, 15.93s/it] 53%|█████▎    | 1710/3250 [8:01:25<6:48:12, 15.90s/it]                                                        53%|█████▎    | 1710/3250 [8:01:25<6:48:12, 15.90s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7471568584442139, 'eval_runtime': 2.4611, 'eval_samples_per_second': 4.876, 'eval_steps_per_second': 1.219, 'epoch': 0.53}
                                                        53%|█████▎    | 1710/3250 [8:01:27<6:48:12, 15.90s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1710
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1710/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1710I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1710

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.446, 'learning_rate': 4.589386321039917e-05, 'epoch': 0.53}
{'loss': 0.5026, 'learning_rate': 4.5845666538672074e-05, 'epoch': 0.53}
{'loss': 0.4849, 'learning_rate': 4.579747375353763e-05, 'epoch': 0.53}
{'loss': 0.446, 'learning_rate': 4.574928490008264e-05, 'epoch': 0.53}
{'loss': 0.4441, 'learning_rate': 4.570110002339028e-05, 'epoch': 0.53}
 53%|█████▎    | 1711/3250 [8:01:44<7:12:40, 16.87s/it]                                                        53%|█████▎    | 1711/3250 [8:01:44<7:12:40, 16.87s/it] 53%|█████▎    | 1712/3250 [8:02:00<7:04:24, 16.56s/it]                                                        53%|█████▎    | 1712/3250 [8:02:00<7:04:24, 16.56s/it] 53%|█████▎    | 1713/3250 [8:02:16<6:58:41, 16.34s/it]                                                        53%|█████▎    | 1713/3250 [8:02:16<6:58:41, 16.34s/it] 53%|█████▎    | 1714/3250 [8:02:32<6:54:34, 16.19s/it]                                                        53%|█████▎    | 1714/3250 [8:02:32<6:54:34, 16.19s/it] 53%|█████▎    | 1715/3250 [8:02:47<6:51:39, 16.09s/it]                                                        53%|█████▎    | 1715/3250 [8:02:47<6:51:39, 16.09s/it] 53%|█████▎    | 1716/3250 [8:03:03<6:49:29, 1{'loss': 0.4731, 'learning_rate': 4.5652919168539976e-05, 'epoch': 0.53}
{'loss': 0.4608, 'learning_rate': 4.560474238060739e-05, 'epoch': 0.53}
{'loss': 0.4705, 'learning_rate': 4.5556569704664394e-05, 'epoch': 0.53}
{'loss': 0.4515, 'learning_rate': 4.5508401185778986e-05, 'epoch': 0.53}
{'loss': 0.4557, 'learning_rate': 4.546023686901533e-05, 'epoch': 0.53}
6.02s/it]                                                        53%|█████▎    | 1716/3250 [8:03:03<6:49:29, 16.02s/it] 53%|█████▎    | 1717/3250 [8:03:19<6:47:49, 15.96s/it]                                                        53%|█████▎    | 1717/3250 [8:03:19<6:47:49, 15.96s/it] 53%|█████▎    | 1718/3250 [8:03:35<6:46:36, 15.92s/it]                                                        53%|█████▎    | 1718/3250 [8:03:35<6:46:36, 15.92s/it] 53%|█████▎    | 1719/3250 [8:03:51<6:45:45, 15.90s/it]                                                        53%|█████▎    | 1719/3250 [8:03:51<6:45:45, 15.90s/it] 53%|█████▎    | 1720/3250 [8:04:07<6:45:07, 15.89s/it]                                                        53%|█████▎    | 1720/3250 [8:04:07<6:45:07, 15.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7383976578712463, 'eval_runtime': 2.47, 'eval_samples_per_second': 4.858, 'eval_steps_per_second': 1.215, 'epoch': 0.53}
                                                        53%|█████▎    | 1720/3250 [8:04:09<6:45:07, 15.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1720
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1720/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1720

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1720

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4534, 'learning_rate': 4.541207679943357e-05, 'epoch': 0.53}
{'loss': 0.4819, 'learning_rate': 4.5363921022089974e-05, 'epoch': 0.53}
{'loss': 0.4705, 'learning_rate': 4.531576958203671e-05, 'epoch': 0.53}
{'loss': 0.448, 'learning_rate': 4.526762252432195e-05, 'epoch': 0.53}
{'loss': 0.4695, 'learning_rate': 4.5219479893989756e-05, 'epoch': 0.53}
 53%|█████▎    | 1721/3250 [8:04:26<7:08:48, 16.83s/it]                                                        53%|█████▎    | 1721/3250 [8:04:26<7:08:48, 16.83s/it] 53%|█████▎    | 1722/3250 [8:04:42<7:06:00, 16.73s/it]                                                        53%|█████▎    | 1722/3250 [8:04:42<7:06:00, 16.73s/it] 53%|█████▎    | 1723/3250 [8:04:58<6:58:57, 16.46s/it]                                                        53%|█████▎    | 1723/3250 [8:04:58<6:58:57, 16.46s/it] 53%|█████▎    | 1724/3250 [8:05:14<6:53:52, 16.27s/it]                                                        53%|█████▎    | 1724/3250 [8:05:14<6:53:52, 16.27s/it] 53%|█████▎    | 1725/3250 [8:05:30<6:50:17, 16.14s/it]                                                        53%|█████▎    | 1725/3250 [8:05:30<6:50:17, 16.14s/it] 53%|█████▎    | 1726/3250 [8:05:45<6:47:40, 1{'loss': 0.4757, 'learning_rate': 4.5171341736080004e-05, 'epoch': 0.53}
{'loss': 0.4193, 'learning_rate': 4.5123208095628424e-05, 'epoch': 0.53}
{'loss': 0.4907, 'learning_rate': 4.5075079017666547e-05, 'epoch': 0.53}
{'loss': 0.4419, 'learning_rate': 4.502695454722156e-05, 'epoch': 0.53}
{'loss': 0.4483, 'learning_rate': 4.4978834729316384e-05, 'epoch': 0.53}
6.05s/it]                                                        53%|█████▎    | 1726/3250 [8:05:45<6:47:40, 16.05s/it] 53%|█████▎    | 1727/3250 [8:06:01<6:45:45, 15.99s/it]                                                        53%|█████▎    | 1727/3250 [8:06:01<6:45:45, 15.99s/it] 53%|█████▎    | 1728/3250 [8:06:17<6:44:21, 15.94s/it]                                                        53%|█████▎    | 1728/3250 [8:06:17<6:44:21, 15.94s/it] 53%|█████▎    | 1729/3250 [8:06:33<6:43:19, 15.91s/it]                                                        53%|█████▎    | 1729/3250 [8:06:33<6:43:19, 15.91s/it] 53%|█████▎    | 1730/3250 [8:06:49<6:43:41, 15.94s/it]                                                        53%|█████▎    | 1730/3250 [8:06:49<6:43:41, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7604173421859741, 'eval_runtime': 2.8212, 'eval_samples_per_second': 4.254, 'eval_steps_per_second': 1.063, 'epoch': 0.53}
                                                        53%|█████▎    | 1730/3250 [8:06:52<6:43:41, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1730
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1730/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1730

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1730

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4405, 'learning_rate': 4.493071960896961e-05, 'epoch': 0.53}
{'loss': 0.4463, 'learning_rate': 4.488260923119538e-05, 'epoch': 0.53}
{'loss': 0.465, 'learning_rate': 4.483450364100345e-05, 'epoch': 0.53}
{'loss': 0.4491, 'learning_rate': 4.478640288339907e-05, 'epoch': 0.53}
{'loss': 0.4683, 'learning_rate': 4.473830700338295e-05, 'epoch': 0.53}
 53%|█████▎    | 1731/3250 [8:07:08<7:10:15, 17.00s/it]                                                        53%|█████▎    | 1731/3250 [8:07:08<7:10:15, 17.00s/it] 53%|█████▎    | 1732/3250 [8:07:24<7:01:07, 16.65s/it]                                                        53%|█████▎    | 1732/3250 [8:07:24<7:01:07, 16.65s/it] 53%|█████▎    | 1733/3250 [8:07:40<6:54:33, 16.40s/it]                                                        53%|█████▎    | 1733/3250 [8:07:40<6:54:33, 16.40s/it] 53%|█████▎    | 1734/3250 [8:07:56<6:49:56, 16.22s/it]                                                        53%|█████▎    | 1734/3250 [8:07:56<6:49:56, 16.22s/it] 53%|█████▎    | 1735/3250 [8:08:12<6:46:37, 16.10s/it]                                                        53%|█████▎    | 1735/3250 [8:08:12<6:46:37, 16.10s/it] 53%|█████▎    | 1736/3250 [8:08:28<6:44:01, 1{'loss': 0.9663, 'learning_rate': 4.4690216045951305e-05, 'epoch': 0.53}
{'loss': 0.4474, 'learning_rate': 4.4642130056095644e-05, 'epoch': 0.53}
{'loss': 0.4723, 'learning_rate': 4.4594049078802925e-05, 'epoch': 0.53}
{'loss': 0.474, 'learning_rate': 4.454597315905535e-05, 'epoch': 0.54}
{'loss': 0.472, 'learning_rate': 4.449790234183044e-05, 'epoch': 0.54}
6.01s/it]                                                        53%|█████▎    | 1736/3250 [8:08:28<6:44:01, 16.01s/it] 53%|█████▎    | 1737/3250 [8:08:43<6:42:17, 15.95s/it]                                                        53%|█████▎    | 1737/3250 [8:08:43<6:42:17, 15.95s/it] 53%|█████▎    | 1738/3250 [8:08:59<6:42:53, 15.99s/it]                                                        53%|█████▎    | 1738/3250 [8:08:59<6:42:53, 15.99s/it] 54%|█████▎    | 1739/3250 [8:09:15<6:41:27, 15.94s/it]                                                        54%|█████▎    | 1739/3250 [8:09:15<6:41:27, 15.94s/it] 54%|█████▎    | 1740/3250 [8:09:31<6:40:20, 15.91s/it]                                                        54%|█████▎    | 1740/3250 [8:09:31<6:40:20, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.746547281742096, 'eval_runtime': 2.4939, 'eval_samples_per_second': 4.812, 'eval_steps_per_second': 1.203, 'epoch': 0.54}
                                                        54%|█████▎    | 1740/3250 [8:09:34<6:40:20, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1740
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1740/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1740

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1740

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4399, 'learning_rate': 4.4449836672100885e-05, 'epoch': 0.54}
{'loss': 0.4679, 'learning_rate': 4.4401776194834613e-05, 'epoch': 0.54}
{'loss': 0.5102, 'learning_rate': 4.435372095499468e-05, 'epoch': 0.54}
{'loss': 0.4621, 'learning_rate': 4.430567099753921e-05, 'epoch': 0.54}
{'loss': 0.4485, 'learning_rate': 4.425762636742143e-05, 'epoch': 0.54}
 54%|█████▎    | 1741/3250 [8:09:50<7:03:44, 16.85s/it]                                                        54%|█████▎    | 1741/3250 [8:09:50<7:03:44, 16.85s/it] 54%|█████▎    | 1742/3250 [8:10:06<6:55:48, 16.54s/it]                                                        54%|█████▎    | 1742/3250 [8:10:06<6:55:48, 16.54s/it] 54%|█████▎    | 1743/3250 [8:10:22<6:50:09, 16.33s/it]                                                        54%|█████▎    | 1743/3250 [8:10:22<6:50:09, 16.33s/it] 54%|█████▎    | 1744/3250 [8:10:38<6:46:06, 16.18s/it]                                                        54%|█████▎    | 1744/3250 [8:10:38<6:46:06, 16.18s/it] 54%|█████▎    | 1745/3250 [8:10:53<6:43:04, 16.07s/it]                                                        54%|█████▎    | 1745/3250 [8:10:53<6:43:04, 16.07s/it] 54%|█████▎    | 1746/3250 [8:11:09<6:40:58, 1{'loss': 0.4681, 'learning_rate': 4.420958710958956e-05, 'epoch': 0.54}
{'loss': 0.4568, 'learning_rate': 4.416155326898679e-05, 'epoch': 0.54}
{'loss': 0.476, 'learning_rate': 4.4113524890551246e-05, 'epoch': 0.54}
{'loss': 0.4452, 'learning_rate': 4.4065502019215965e-05, 'epoch': 0.54}
{'loss': 0.4633, 'learning_rate': 4.401748469990879e-05, 'epoch': 0.54}
6.00s/it]                                                        54%|█████▎    | 1746/3250 [8:11:09<6:40:58, 16.00s/it] 54%|█████▍    | 1747/3250 [8:11:25<6:39:27, 15.95s/it]                                                        54%|█████▍    | 1747/3250 [8:11:25<6:39:27, 15.95s/it] 54%|█████▍    | 1748/3250 [8:11:41<6:38:11, 15.91s/it]                                                        54%|█████▍    | 1748/3250 [8:11:41<6:38:11, 15.91s/it] 54%|█████▍    | 1749/3250 [8:11:57<6:37:17, 15.88s/it]                                                        54%|█████▍    | 1749/3250 [8:11:57<6:37:17, 15.88s/it] 54%|█████▍    | 1750/3250 [8:12:13<6:36:38, 15.87s/it]                                                        54%|█████▍    | 1750/3250 [8:12:13<6:36:38, 15.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7385082840919495, 'eval_runtime': 2.4663, 'eval_samples_per_second': 4.866, 'eval_steps_per_second': 1.216, 'epoch': 0.54}
                                                        54%|█████▍    | 1750/3250 [8:12:15<6:36:38, 15.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1750
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1750
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1750
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1750

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4595, 'learning_rate': 4.39694729775524e-05, 'epoch': 0.54}
{'loss': 0.4645, 'learning_rate': 4.392146689706425e-05, 'epoch': 0.54}
{'loss': 0.4748, 'learning_rate': 4.387346650335649e-05, 'epoch': 0.54}
{'loss': 0.4746, 'learning_rate': 4.382547184133593e-05, 'epoch': 0.54}
{'loss': 0.457, 'learning_rate': 4.377748295590407e-05, 'epoch': 0.54}
 54%|█████▍    | 1751/3250 [8:12:46<8:46:52, 21.09s/it]                                                        54%|█████▍    | 1751/3250 [8:12:46<8:46:52, 21.09s/it] 54%|█████▍    | 1752/3250 [8:13:02<8:07:12, 19.51s/it]                                                        54%|█████▍    | 1752/3250 [8:13:02<8:07:12, 19.51s/it] 54%|█████▍    | 1753/3250 [8:13:18<7:39:23, 18.41s/it]                                                        54%|█████▍    | 1753/3250 [8:13:18<7:39:23, 18.41s/it] 54%|█████▍    | 1754/3250 [8:13:34<7:24:28, 17.83s/it]                                                        54%|█████▍    | 1754/3250 [8:13:34<7:24:28, 17.83s/it] 54%|█████▍    | 1755/3250 [8:13:50<7:09:11, 17.22s/it]                                                        54%|█████▍    | 1755/3250 [8:13:50<7:09:11, 17.22s/it] 54%|█████▍    | 1756/3250 [8:14:06<6:58:30, 1{'loss': 0.4573, 'learning_rate': 4.372949989195697e-05, 'epoch': 0.54}
{'loss': 0.439, 'learning_rate': 4.3681522694385256e-05, 'epoch': 0.54}
{'loss': 0.4644, 'learning_rate': 4.3633551408074075e-05, 'epoch': 0.54}
{'loss': 0.4406, 'learning_rate': 4.358558607790303e-05, 'epoch': 0.54}
{'loss': 0.4291, 'learning_rate': 4.3537626748746143e-05, 'epoch': 0.54}
6.81s/it]                                                        54%|█████▍    | 1756/3250 [8:14:06<6:58:30, 16.81s/it] 54%|█████▍    | 1757/3250 [8:14:21<6:50:58, 16.52s/it]                                                        54%|█████▍    | 1757/3250 [8:14:21<6:50:58, 16.52s/it] 54%|█████▍    | 1758/3250 [8:14:37<6:45:31, 16.31s/it]                                                        54%|█████▍    | 1758/3250 [8:14:37<6:45:31, 16.31s/it] 54%|█████▍    | 1759/3250 [8:14:53<6:41:48, 16.17s/it]                                                        54%|█████▍    | 1759/3250 [8:14:53<6:41:48, 16.17s/it] 54%|█████▍    | 1760/3250 [8:15:09<6:39:00, 16.07s/it]                                                        54%|█████▍    | 1760/3250 [8:15:09<6:39:00, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7618040442466736, 'eval_runtime': 2.4878, 'eval_samples_per_second': 4.824, 'eval_steps_per_second': 1.206, 'epoch': 0.54}
                                                        54%|█████▍    | 1760/3250 [8:15:11<6:39:00, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1760
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1760 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1760

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4364, 'learning_rate': 4.348967346547185e-05, 'epoch': 0.54}
{'loss': 0.4316, 'learning_rate': 4.344172627294289e-05, 'epoch': 0.54}
{'loss': 0.4633, 'learning_rate': 4.339378521601635e-05, 'epoch': 0.54}
{'loss': 0.4496, 'learning_rate': 4.334585033954355e-05, 'epoch': 0.54}
{'loss': 0.4635, 'learning_rate': 4.329792168837002e-05, 'epoch': 0.54}
 54%|█████▍    | 1761/3250 [8:15:43<8:49:49, 21.35s/it]                                                        54%|█████▍    | 1761/3250 [8:15:43<8:49:49, 21.35s/it] 54%|█████▍    | 1762/3250 [8:15:58<8:08:34, 19.70s/it]                                                        54%|█████▍    | 1762/3250 [8:15:58<8:08:34, 19.70s/it] 54%|█████▍    | 1763/3250 [8:16:14<7:39:39, 18.55s/it]                                                        54%|█████▍    | 1763/3250 [8:16:14<7:39:39, 18.55s/it] 54%|█████▍    | 1764/3250 [8:16:30<7:19:19, 17.74s/it]                                                        54%|█████▍    | 1764/3250 [8:16:30<7:19:19, 17.74s/it] 54%|█████▍    | 1765/3250 [8:16:46<7:05:04, 17.17s/it]                                                        54%|█████▍    | 1765/3250 [8:16:46<7:05:04, 17.17s/it] 54%|█████▍    | 1766/3250 [8:17:02<6:54:42, 1{'loss': 0.9597, 'learning_rate': 4.3249999307335495e-05, 'epoch': 0.54}
{'loss': 0.4526, 'learning_rate': 4.320208324127383e-05, 'epoch': 0.54}
{'loss': 0.454, 'learning_rate': 4.3154173535012946e-05, 'epoch': 0.54}
{'loss': 0.4616, 'learning_rate': 4.3106270233374845e-05, 'epoch': 0.54}
{'loss': 0.4621, 'learning_rate': 4.3058373381175574e-05, 'epoch': 0.54}
6.77s/it]                                                        54%|█████▍    | 1766/3250 [8:17:02<6:54:42, 16.77s/it] 54%|█████▍    | 1767/3250 [8:17:18<6:47:40, 16.49s/it]                                                        54%|█████▍    | 1767/3250 [8:17:18<6:47:40, 16.49s/it] 54%|█████▍    | 1768/3250 [8:17:34<6:42:38, 16.30s/it]                                                        54%|█████▍    | 1768/3250 [8:17:34<6:42:38, 16.30s/it] 54%|█████▍    | 1769/3250 [8:17:49<6:39:02, 16.17s/it]                                                        54%|█████▍    | 1769/3250 [8:17:49<6:39:02, 16.17s/it] 54%|█████▍    | 1770/3250 [8:18:05<6:36:25, 16.07s/it]                                                        54%|█████▍    | 1770/3250 [8:18:05<6:36:25, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7437649369239807, 'eval_runtime': 2.4818, 'eval_samples_per_second': 4.835, 'eval_steps_per_second': 1.209, 'epoch': 0.54}
                                                        54%|█████▍    | 1770/3250 [8:18:08<6:36:25, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1770
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1770 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1770

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1770

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4398, 'learning_rate': 4.3010483023225045e-05, 'epoch': 0.54}
{'loss': 0.4463, 'learning_rate': 4.296259920432716e-05, 'epoch': 0.55}
{'loss': 0.4975, 'learning_rate': 4.2914721969279705e-05, 'epoch': 0.55}
{'loss': 0.4692, 'learning_rate': 4.28668513628743e-05, 'epoch': 0.55}
{'loss': 0.4526, 'learning_rate': 4.281898742989636e-05, 'epoch': 0.55}
 54%|█████▍    | 1771/3250 [8:18:25<7:00:25, 17.06s/it]                                                        54%|█████▍    | 1771/3250 [8:18:25<7:00:25, 17.06s/it] 55%|█████▍    | 1772/3250 [8:18:41<6:51:55, 16.72s/it]                                                        55%|█████▍    | 1772/3250 [8:18:41<6:51:55, 16.72s/it] 55%|█████▍    | 1773/3250 [8:18:57<6:45:54, 16.49s/it]                                                        55%|█████▍    | 1773/3250 [8:18:57<6:45:54, 16.49s/it] 55%|█████▍    | 1774/3250 [8:19:12<6:41:33, 16.32s/it]                                                        55%|█████▍    | 1774/3250 [8:19:12<6:41:33, 16.32s/it] 55%|█████▍    | 1775/3250 [8:19:28<6:38:24, 16.21s/it]                                                        55%|█████▍    | 1775/3250 [8:19:28<6:38:24, 16.21s/it] 55%|█████▍    | 1776/3250 [8:19:44<6:36:07, 1{'loss': 0.4175, 'learning_rate': 4.277113021512505e-05, 'epoch': 0.55}
{'loss': 0.4714, 'learning_rate': 4.2723279763333265e-05, 'epoch': 0.55}
{'loss': 0.465, 'learning_rate': 4.267543611928754e-05, 'epoch': 0.55}
{'loss': 0.4572, 'learning_rate': 4.2627599327748105e-05, 'epoch': 0.55}
{'loss': 0.4416, 'learning_rate': 4.2579769433468694e-05, 'epoch': 0.55}
6.12s/it]                                                        55%|█████▍    | 1776/3250 [8:19:44<6:36:07, 16.12s/it] 55%|█████▍    | 1777/3250 [8:20:00<6:34:25, 16.07s/it]                                                        55%|█████▍    | 1777/3250 [8:20:00<6:34:25, 16.07s/it] 55%|█████▍    | 1778/3250 [8:20:16<6:33:00, 16.02s/it]                                                        55%|█████▍    | 1778/3250 [8:20:16<6:33:00, 16.02s/it] 55%|█████▍    | 1779/3250 [8:20:32<6:32:02, 15.99s/it]                                                        55%|█████▍    | 1779/3250 [8:20:32<6:32:02, 15.99s/it] 55%|█████▍    | 1780/3250 [8:20:48<6:32:37, 16.03s/it]                                                        55%|█████▍    | 1780/3250 [8:20:48<6:32:37, 16.03s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7437397837638855, 'eval_runtime': 2.7127, 'eval_samples_per_second': 4.424, 'eval_steps_per_second': 1.106, 'epoch': 0.55}
                                                        55%|█████▍    | 1780/3250 [8:20:51<6:32:37, 16.03s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1780
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1780

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1780
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1780

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4625, 'learning_rate': 4.253194648119667e-05, 'epoch': 0.55}
{'loss': 0.4653, 'learning_rate': 4.2484130515672856e-05, 'epoch': 0.55}
{'loss': 0.4844, 'learning_rate': 4.243632158163152e-05, 'epoch': 0.55}
{'loss': 0.4647, 'learning_rate': 4.2388519723800415e-05, 'epoch': 0.55}
{'loss': 0.4517, 'learning_rate': 4.234072498690062e-05, 'epoch': 0.55}
 55%|█████▍    | 1781/3250 [8:21:08<6:57:38, 17.06s/it]                                                        55%|█████▍    | 1781/3250 [8:21:08<6:57:38, 17.06s/it] 55%|█████▍    | 1782/3250 [8:21:24<6:48:28, 16.70s/it]                                                        55%|█████▍    | 1782/3250 [8:21:24<6:48:28, 16.70s/it] 55%|█████▍    | 1783/3250 [8:21:39<6:41:55, 16.44s/it]                                                        55%|█████▍    | 1783/3250 [8:21:39<6:41:55, 16.44s/it] 55%|█████▍    | 1784/3250 [8:21:55<6:37:16, 16.26s/it]                                                        55%|█████▍    | 1784/3250 [8:21:55<6:37:16, 16.26s/it] 55%|█████▍    | 1785/3250 [8:22:11<6:33:54, 16.13s/it]                                                        55%|█████▍    | 1785/3250 [8:22:11<6:33:54, 16.13s/it] 55%|█████▍    | 1786/3250 [8:22:27<6:31:28, 1{'loss': 0.459, 'learning_rate': 4.229293741564658e-05, 'epoch': 0.55}
{'loss': 0.4582, 'learning_rate': 4.224515705474603e-05, 'epoch': 0.55}
{'loss': 0.4301, 'learning_rate': 4.2197383948899925e-05, 'epoch': 0.55}
{'loss': 0.4941, 'learning_rate': 4.2149618142802494e-05, 'epoch': 0.55}
{'loss': 0.4366, 'learning_rate': 4.210185968114109e-05, 'epoch': 0.55}
6.04s/it]                                                        55%|█████▍    | 1786/3250 [8:22:27<6:31:28, 16.04s/it] 55%|█████▍    | 1787/3250 [8:22:43<6:32:25, 16.09s/it]                                                        55%|█████▍    | 1787/3250 [8:22:43<6:32:25, 16.09s/it] 55%|█████▌    | 1788/3250 [8:22:59<6:30:13, 16.01s/it]                                                        55%|█████▌    | 1788/3250 [8:22:59<6:30:13, 16.01s/it] 55%|█████▌    | 1789/3250 [8:23:15<6:28:33, 15.96s/it]                                                        55%|█████▌    | 1789/3250 [8:23:15<6:28:33, 15.96s/it] 55%|█████▌    | 1790/3250 [8:23:31<6:27:24, 15.92s/it]                                                        55%|█████▌    | 1790/3250 [8:23:31<6:27:24, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7595436573028564, 'eval_runtime': 2.4661, 'eval_samples_per_second': 4.866, 'eval_steps_per_second': 1.216, 'epoch': 0.55}
                                                        55%|█████▌    | 1790/3250 [8:23:33<6:27:24, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1790
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1790 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1790

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1790

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4326, 'learning_rate': 4.20541086085962e-05, 'epoch': 0.55}
{'loss': 0.4329, 'learning_rate': 4.200636496984144e-05, 'epoch': 0.55}
{'loss': 0.4621, 'learning_rate': 4.1958628809543416e-05, 'epoch': 0.55}
{'loss': 0.452, 'learning_rate': 4.1910900172361764e-05, 'epoch': 0.55}
{'loss': 0.4566, 'learning_rate': 4.1863179102949094e-05, 'epoch': 0.55}
 55%|█████▌    | 1791/3250 [8:23:50<6:49:43, 16.85s/it]                                                        55%|█████▌    | 1791/3250 [8:23:50<6:49:43, 16.85s/it] 55%|█████▌    | 1792/3250 [8:24:05<6:42:05, 16.55s/it]                                                        55%|█████▌    | 1792/3250 [8:24:05<6:42:05, 16.55s/it] 55%|█████▌    | 1793/3250 [8:24:21<6:36:37, 16.33s/it]                                                        55%|█████▌    | 1793/3250 [8:24:21<6:36:37, 16.33s/it] 55%|█████▌    | 1794/3250 [8:24:37<6:32:39, 16.18s/it]                                                        55%|█████▌    | 1794/3250 [8:24:37<6:32:39, 16.18s/it] 55%|█████▌    | 1795/3250 [8:24:53<6:29:55, 16.08s/it]                                                        55%|█████▌    | 1795/3250 [8:24:53<6:29:55, 16.08s/it] 55%|█████▌    | 1796/3250 [8:25:09<6:27:44, 1{'loss': 0.6955, 'learning_rate': 4.18154656459509e-05, 'epoch': 0.55}
{'loss': 0.7304, 'learning_rate': 4.1767759846005596e-05, 'epoch': 0.55}
{'loss': 0.4359, 'learning_rate': 4.1720061747744396e-05, 'epoch': 0.55}
{'loss': 0.4676, 'learning_rate': 4.1672371395791335e-05, 'epoch': 0.55}
{'loss': 0.4623, 'learning_rate': 4.162468883476319e-05, 'epoch': 0.55}
6.00s/it]                                                        55%|█████▌    | 1796/3250 [8:25:09<6:27:44, 16.00s/it] 55%|█████▌    | 1797/3250 [8:25:25<6:26:02, 15.94s/it]                                                        55%|█████▌    | 1797/3250 [8:25:25<6:26:02, 15.94s/it] 55%|█████▌    | 1798/3250 [8:25:40<6:25:00, 15.91s/it]                                                        55%|█████▌    | 1798/3250 [8:25:40<6:25:00, 15.91s/it] 55%|█████▌    | 1799/3250 [8:25:56<6:24:13, 15.89s/it]                                                        55%|█████▌    | 1799/3250 [8:25:56<6:24:13, 15.89s/it] 55%|█████▌    | 1800/3250 [8:26:12<6:23:36, 15.87s/it]                                                        55%|█████▌    | 1800/3250 [8:26:12<6:23:36, 15.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7420924305915833, 'eval_runtime': 2.4623, 'eval_samples_per_second': 4.873, 'eval_steps_per_second': 1.218, 'epoch': 0.55}
                                                        55%|█████▌    | 1800/3250 [8:26:15<6:23:36, 15.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1800
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1800
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1800
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1800

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1800
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4418, 'learning_rate': 4.157701410926943e-05, 'epoch': 0.55}
{'loss': 0.4263, 'learning_rate': 4.152934726391223e-05, 'epoch': 0.55}
{'loss': 0.4915, 'learning_rate': 4.148168834328638e-05, 'epoch': 0.55}
{'loss': 0.4728, 'learning_rate': 4.1434037391979266e-05, 'epoch': 0.56}
{'loss': 0.4306, 'learning_rate': 4.1386394454570745e-05, 'epoch': 0.56}
 55%|█████▌    | 1801/3250 [8:26:50<9:00:26, 22.38s/it]                                                        55%|█████▌    | 1801/3250 [8:26:50<9:00:26, 22.38s/it] 55%|█████▌    | 1802/3250 [8:27:05<8:12:45, 20.42s/it]                                                        55%|█████▌    | 1802/3250 [8:27:05<8:12:45, 20.42s/it] 55%|█████▌    | 1803/3250 [8:27:21<7:39:15, 19.04s/it]                                                        55%|█████▌    | 1803/3250 [8:27:21<7:39:15, 19.04s/it] 56%|█████▌    | 1804/3250 [8:27:38<7:19:06, 18.22s/it]                                                        56%|█████▌    | 1804/3250 [8:27:38<7:19:06, 18.22s/it] 56%|█████▌    | 1805/3250 [8:27:53<7:01:38, 17.51s/it]                                                        56%|█████▌    | 1805/3250 [8:27:53<7:01:38, 17.51s/it] 56%|█████▌    | 1806/3250 [8:28:09<6:49:18, 1{'loss': 0.4348, 'learning_rate': 4.133875957563329e-05, 'epoch': 0.56}
{'loss': 0.4627, 'learning_rate': 4.129113279973177e-05, 'epoch': 0.56}
{'loss': 0.4564, 'learning_rate': 4.124351417142347e-05, 'epoch': 0.56}
{'loss': 0.4651, 'learning_rate': 4.1195903735258064e-05, 'epoch': 0.56}
{'loss': 0.4485, 'learning_rate': 4.114830153577759e-05, 'epoch': 0.56}
7.01s/it]                                                        56%|█████▌    | 1806/3250 [8:28:09<6:49:18, 17.01s/it] 56%|█████▌    | 1807/3250 [8:28:25<6:40:34, 16.66s/it]                                                        56%|█████▌    | 1807/3250 [8:28:25<6:40:34, 16.66s/it] 56%|█████▌    | 1808/3250 [8:28:41<6:34:26, 16.41s/it]                                                        56%|█████▌    | 1808/3250 [8:28:41<6:34:26, 16.41s/it] 56%|█████▌    | 1809/3250 [8:28:57<6:29:58, 16.24s/it]                                                        56%|█████▌    | 1809/3250 [8:28:57<6:29:58, 16.24s/it] 56%|█████▌    | 1810/3250 [8:29:13<6:26:41, 16.11s/it]                                                        56%|█████▌    | 1810/3250 [8:29:13<6:26:41, 16.11s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.746871292591095, 'eval_runtime': 2.4722, 'eval_samples_per_second': 4.854, 'eval_steps_per_second': 1.213, 'epoch': 0.56}
                                                        56%|█████▌    | 1810/3250 [8:29:15<6:26:41, 16.11s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1810
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in    /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1810/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1810/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1810



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4478, 'learning_rate': 4.110070761751633e-05, 'epoch': 0.56}
{'loss': 0.4549, 'learning_rate': 4.1053122025000843e-05, 'epoch': 0.56}
{'loss': 0.4724, 'learning_rate': 4.100554480274993e-05, 'epoch': 0.56}
{'loss': 0.4634, 'learning_rate': 4.095797599527449e-05, 'epoch': 0.56}
{'loss': 0.4473, 'learning_rate': 4.09104156470776e-05, 'epoch': 0.56}
 56%|█████▌    | 1811/3250 [8:29:32<6:49:07, 17.06s/it]                                                        56%|█████▌    | 1811/3250 [8:29:32<6:49:07, 17.06s/it] 56%|█████▌    | 1812/3250 [8:29:48<6:40:32, 16.71s/it]                                                        56%|█████▌    | 1812/3250 [8:29:48<6:40:32, 16.71s/it] 56%|█████▌    | 1813/3250 [8:30:04<6:34:28, 16.47s/it]                                                        56%|█████▌    | 1813/3250 [8:30:04<6:34:28, 16.47s/it] 56%|█████▌    | 1814/3250 [8:30:20<6:29:56, 16.29s/it]                                                        56%|█████▌    | 1814/3250 [8:30:20<6:29:56, 16.29s/it] 56%|█████▌    | 1815/3250 [8:30:35<6:26:48, 16.17s/it]                                                        56%|█████▌    | 1815/3250 [8:30:35<6:26:48, 16.17s/it] 56%|█████▌    | 1816/3250 [8:30:51<6:24:28, 1{'loss': 0.4534, 'learning_rate': 4.086286380265443e-05, 'epoch': 0.56}
{'loss': 0.4661, 'learning_rate': 4.081532050649216e-05, 'epoch': 0.56}
{'loss': 0.4103, 'learning_rate': 4.076778580306999e-05, 'epoch': 0.56}
{'loss': 0.4804, 'learning_rate': 4.072025973685908e-05, 'epoch': 0.56}
{'loss': 0.4367, 'learning_rate': 4.067274235232251e-05, 'epoch': 0.56}
6.09s/it]                                                        56%|█████▌    | 1816/3250 [8:30:51<6:24:28, 16.09s/it] 56%|█████▌    | 1817/3250 [8:31:07<6:22:48, 16.03s/it]                                                        56%|█████▌    | 1817/3250 [8:31:07<6:22:48, 16.03s/it] 56%|█████▌    | 1818/3250 [8:31:23<6:21:28, 15.98s/it]                                                        56%|█████▌    | 1818/3250 [8:31:23<6:21:28, 15.98s/it] 56%|█████▌    | 1819/3250 [8:31:39<6:20:36, 15.96s/it]                                                        56%|█████▌    | 1819/3250 [8:31:39<6:20:36, 15.96s/it] 56%|█████▌    | 1820/3250 [8:31:55<6:22:56, 16.07s/it]                                                        56%|█████▌    | 1820/3250 [8:31:55<6:22:56, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7597283124923706, 'eval_runtime': 2.4733, 'eval_samples_per_second': 4.852, 'eval_steps_per_second': 1.213, 'epoch': 0.56}
                                                        56%|█████▌    | 1820/3250 [8:31:58<6:22:56, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1820
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1820 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1820
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1820

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.44, 'learning_rate': 4.0625233693915264e-05, 'epoch': 0.56}
{'loss': 0.4223, 'learning_rate': 4.057773380608411e-05, 'epoch': 0.56}
{'loss': 0.4453, 'learning_rate': 4.053024273326761e-05, 'epoch': 0.56}
{'loss': 0.4503, 'learning_rate': 4.048276051989614e-05, 'epoch': 0.56}
{'loss': 0.437, 'learning_rate': 4.0435287210391756e-05, 'epoch': 0.56}
 56%|█████▌    | 1821/3250 [8:32:15<6:45:13, 17.01s/it]                                                        56%|█████▌    | 1821/3250 [8:32:15<6:45:13, 17.01s/it] 56%|█████▌    | 1822/3250 [8:32:30<6:37:00, 16.68s/it]                                                        56%|█████▌    | 1822/3250 [8:32:30<6:37:00, 16.68s/it] 56%|█████▌    | 1823/3250 [8:32:46<6:31:12, 16.45s/it]                                                        56%|█████▌    | 1823/3250 [8:32:46<6:31:12, 16.45s/it] 56%|█████▌    | 1824/3250 [8:33:02<6:27:04, 16.29s/it]                                                        56%|█████▌    | 1824/3250 [8:33:02<6:27:04, 16.29s/it] 56%|█████▌    | 1825/3250 [8:33:18<6:24:11, 16.18s/it]                                                        56%|█████▌    | 1825/3250 [8:33:18<6:24:11, 16.18s/it] 56%|█████▌    | 1826/3250 [8:33:34<6:21:54, 1{'loss': 0.4628, 'learning_rate': 4.038782284916816e-05, 'epoch': 0.56}
{'loss': 0.9617, 'learning_rate': 4.034036748063072e-05, 'epoch': 0.56}
{'loss': 0.4378, 'learning_rate': 4.029292114917638e-05, 'epoch': 0.56}
{'loss': 0.4635, 'learning_rate': 4.0245483899193595e-05, 'epoch': 0.56}
{'loss': 0.4633, 'learning_rate': 4.019805577506237e-05, 'epoch': 0.56}
6.09s/it]                                                        56%|█████▌    | 1826/3250 [8:33:34<6:21:54, 16.09s/it] 56%|█████▌    | 1827/3250 [8:33:50<6:20:10, 16.03s/it]                                                        56%|█████▌    | 1827/3250 [8:33:50<6:20:10, 16.03s/it] 56%|█████▌    | 1828/3250 [8:34:06<6:18:57, 15.99s/it]                                                        56%|█████▌    | 1828/3250 [8:34:06<6:18:57, 15.99s/it] 56%|█████▋    | 1829/3250 [8:34:22<6:18:09, 15.97s/it]                                                        56%|█████▋    | 1829/3250 [8:34:22<6:18:09, 15.97s/it] 56%|█████▋    | 1830/3250 [8:34:38<6:17:31, 15.95s/it]                                                        56%|█████▋    | 1830/3250 [8:34:38<6:17:31, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7404720187187195, 'eval_runtime': 2.4712, 'eval_samples_per_second': 4.856, 'eval_steps_per_second': 1.214, 'epoch': 0.56}
                                                        56%|█████▋    | 1830/3250 [8:34:40<6:17:31, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1830
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1830
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1830
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1830

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4627, 'learning_rate': 4.0150636821154166e-05, 'epoch': 0.56}
{'loss': 0.4291, 'learning_rate': 4.010322708183183e-05, 'epoch': 0.56}
{'loss': 0.4423, 'learning_rate': 4.005582660144963e-05, 'epoch': 0.56}
{'loss': 0.5146, 'learning_rate': 4.000843542435315e-05, 'epoch': 0.56}
{'loss': 0.4416, 'learning_rate': 3.9961053594879266e-05, 'epoch': 0.56}
 56%|█████▋    | 1831/3250 [8:35:43<12:07:26, 30.76s/it]                                                         56%|█████▋    | 1831/3250 [8:35:43<12:07:26, 30.76s/it] 56%|█████▋    | 1832/3250 [8:35:59<10:22:58, 26.36s/it]                                                         56%|█████▋    | 1832/3250 [8:35:59<10:22:58, 26.36s/it] 56%|█████▋    | 1833/3250 [8:36:15<9:07:58, 23.20s/it]                                                         56%|█████▋    | 1833/3250 [8:36:15<9:07:58, 23.20s/it] 56%|█████▋    | 1834/3250 [8:36:31<8:15:31, 21.00s/it]                                                        56%|█████▋    | 1834/3250 [8:36:31<8:15:31, 21.00s/it] 56%|█████▋    | 1835/3250 [8:36:47<7:38:57, 19.46s/it]                                                        56%|█████▋    | 1835/3250 [8:36:47<7:38:57, 19.46s/it] 56%|█████▋    | 1836/3250 [8:37:03<7:1{'loss': 0.441, 'learning_rate': 3.991368115735612e-05, 'epoch': 0.56}
{'loss': 0.4451, 'learning_rate': 3.986631815610308e-05, 'epoch': 0.57}
{'loss': 0.4452, 'learning_rate': 3.981896463543067e-05, 'epoch': 0.57}
{'loss': 0.4784, 'learning_rate': 3.977162063964049e-05, 'epoch': 0.57}
{'loss': 0.4404, 'learning_rate': 3.972428621302534e-05, 'epoch': 0.57}
3:04, 18.38s/it]                                                        56%|█████▋    | 1836/3250 [8:37:03<7:13:04, 18.38s/it] 57%|█████▋    | 1837/3250 [8:37:19<6:57:33, 17.73s/it]                                                        57%|█████▋    | 1837/3250 [8:37:19<6:57:33, 17.73s/it] 57%|█████▋    | 1838/3250 [8:37:35<6:44:00, 17.17s/it]                                                        57%|█████▋    | 1838/3250 [8:37:35<6:44:00, 17.17s/it] 57%|█████▋    | 1839/3250 [8:37:50<6:34:17, 16.77s/it]                                                        57%|█████▋    | 1839/3250 [8:37:50<6:34:17, 16.77s/it] 57%|█████▋    | 1840/3250 [8:38:09<6:49:36, 17.43s/it]                                                        57%|█████▋    | 1840/3250 [8:38:10<6:49:36, 17.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7503000497817993, 'eval_runtime': 3.3081, 'eval_samples_per_second': 3.627, 'eval_steps_per_second': 0.907, 'epoch': 0.57}
                                                        57%|█████▋    | 1840/3250 [8:38:13<6:49:36, 17.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1840
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1840
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4502, 'learning_rate': 3.9676961399869e-05, 'epoch': 0.57}
{'loss': 0.4407, 'learning_rate': 3.962964624444625e-05, 'epoch': 0.57}
{'loss': 0.4477, 'learning_rate': 3.958234079102288e-05, 'epoch': 0.57}
{'loss': 0.4722, 'learning_rate': 3.953504508385554e-05, 'epoch': 0.57}
{'loss': 0.4624, 'learning_rate': 3.9487759167191815e-05, 'epoch': 0.57}
 57%|█████▋    | 1841/3250 [8:38:50<9:33:14, 24.41s/it]                                                        57%|█████▋    | 1841/3250 [8:38:50<9:33:14, 24.41s/it] 57%|█████▋    | 1842/3250 [8:39:06<8:32:41, 21.85s/it]                                                        57%|█████▋    | 1842/3250 [8:39:06<8:32:41, 21.85s/it] 57%|█████▋    | 1843/3250 [8:39:22<7:50:23, 20.06s/it]                                                        57%|█████▋    | 1843/3250 [8:39:22<7:50:23, 20.06s/it] 57%|█████▋    | 1844/3250 [8:39:38<7:20:41, 18.81s/it]                                                        57%|█████▋    | 1844/3250 [8:39:38<7:20:41, 18.81s/it] 57%|█████▋    | 1845/3250 [8:39:54<6:59:55, 17.93s/it]                                                        57%|█████▋    | 1845/3250 [8:39:54<6:59:55, 17.93s/it] 57%|█████▋    | 1846/3250 [8:40:10<6:45:14, 1{'loss': 0.4525, 'learning_rate': 3.9440483085270126e-05, 'epoch': 0.57}
{'loss': 0.4488, 'learning_rate': 3.939321688231965e-05, 'epoch': 0.57}
{'loss': 0.4244, 'learning_rate': 3.934596060256037e-05, 'epoch': 0.57}
{'loss': 0.4555, 'learning_rate': 3.9298714290202977e-05, 'epoch': 0.57}
{'loss': 0.4366, 'learning_rate': 3.92514779894488e-05, 'epoch': 0.57}
7.32s/it]                                                        57%|█████▋    | 1846/3250 [8:40:10<6:45:14, 17.32s/it] 57%|█████▋    | 1847/3250 [8:40:25<6:34:56, 16.89s/it]                                                        57%|█████▋    | 1847/3250 [8:40:25<6:34:56, 16.89s/it] 57%|█████▋    | 1848/3250 [8:40:41<6:27:34, 16.59s/it]                                                        57%|█████▋    | 1848/3250 [8:40:41<6:27:34, 16.59s/it] 57%|█████▋    | 1849/3250 [8:40:57<6:22:22, 16.38s/it]                                                        57%|█████▋    | 1849/3250 [8:40:57<6:22:22, 16.38s/it] 57%|█████▋    | 1850/3250 [8:41:13<6:18:40, 16.23s/it]                                                        57%|█████▋    | 1850/3250 [8:41:13<6:18:40, 16.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7531269788742065, 'eval_runtime': 2.5437, 'eval_samples_per_second': 4.718, 'eval_steps_per_second': 1.179, 'epoch': 0.57}
                                                        57%|█████▋    | 1850/3250 [8:41:16<6:18:40, 16.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1850
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1850
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1850
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1850

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4175, 'learning_rate': 3.920425174448984e-05, 'epoch': 0.57}
{'loss': 0.4287, 'learning_rate': 3.9157035599508684e-05, 'epoch': 0.57}
{'loss': 0.4178, 'learning_rate': 3.910982959867845e-05, 'epoch': 0.57}
{'loss': 0.4515, 'learning_rate': 3.906263378616279e-05, 'epoch': 0.57}
{'loss': 0.4493, 'learning_rate': 3.901544820611584e-05, 'epoch': 0.57}
 57%|█████▋    | 1851/3250 [8:42:33<13:45:01, 35.38s/it]                                                         57%|█████▋    | 1851/3250 [8:42:33<13:45:01, 35.38s/it] 57%|█████▋    | 1852/3250 [8:42:49<11:28:16, 29.54s/it]                                                         57%|█████▋    | 1852/3250 [8:42:49<11:28:16, 29.54s/it] 57%|█████▋    | 1853/3250 [8:43:06<9:57:53, 25.68s/it]                                                         57%|█████▋    | 1853/3250 [8:43:06<9:57:53, 25.68s/it] 57%|█████▋    | 1854/3250 [8:43:22<8:49:11, 22.74s/it]                                                        57%|█████▋    | 1854/3250 [8:43:22<8:49:11, 22.74s/it] 57%|█████▋    | 1855/3250 [8:43:37<8:01:03, 20.69s/it]                                                        57%|█████▋    | 1855/3250 [8:43:37<8:01:03, 20.69s/it] 57%|█████▋    | 1856/3250 [8:43:53<7:2{'loss': 0.4598, 'learning_rate': 3.89682729026821e-05, 'epoch': 0.57}
{'loss': 0.9567, 'learning_rate': 3.892110791999649e-05, 'epoch': 0.57}
{'loss': 0.4545, 'learning_rate': 3.887395330218429e-05, 'epoch': 0.57}
{'loss': 0.4457, 'learning_rate': 3.882680909336108e-05, 'epoch': 0.57}
{'loss': 0.4619, 'learning_rate': 3.877967533763267e-05, 'epoch': 0.57}
7:24, 19.26s/it]                                                        57%|█████▋    | 1856/3250 [8:43:53<7:27:24, 19.26s/it] 57%|█████▋    | 1857/3250 [8:44:09<7:03:21, 18.24s/it]                                                        57%|█████▋    | 1857/3250 [8:44:09<7:03:21, 18.24s/it] 57%|█████▋    | 1858/3250 [8:44:25<6:46:49, 17.54s/it]                                                        57%|█████▋    | 1858/3250 [8:44:25<6:46:49, 17.54s/it] 57%|█████▋    | 1859/3250 [8:44:41<6:35:04, 17.04s/it]                                                        57%|█████▋    | 1859/3250 [8:44:41<6:35:04, 17.04s/it] 57%|█████▋    | 1860/3250 [8:44:57<6:26:50, 16.70s/it]                                                        57%|█████▋    | 1860/3250 [8:44:57<6:26:50, 16.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7407351732254028, 'eval_runtime': 2.4828, 'eval_samples_per_second': 4.833, 'eval_steps_per_second': 1.208, 'epoch': 0.57}
                                                        57%|█████▋    | 1860/3250 [8:44:59<6:26:50, 16.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1860
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1860/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4635, 'learning_rate': 3.873255207909514e-05, 'epoch': 0.57}
{'loss': 0.4302, 'learning_rate': 3.86854393618347e-05, 'epoch': 0.57}
{'loss': 0.4361, 'learning_rate': 3.863833722992774e-05, 'epoch': 0.57}
{'loss': 0.499, 'learning_rate': 3.859124572744071e-05, 'epoch': 0.57}
{'loss': 0.4628, 'learning_rate': 3.854416489843014e-05, 'epoch': 0.57}
 57%|█████▋    | 1861/3250 [8:45:16<6:44:17, 17.46s/it]                                                        57%|█████▋    | 1861/3250 [8:45:16<6:44:17, 17.46s/it] 57%|█████▋    | 1862/3250 [8:45:32<6:32:57, 16.99s/it]                                                        57%|█████▋    | 1862/3250 [8:45:32<6:32:57, 16.99s/it] 57%|█████▋    | 1863/3250 [8:45:48<6:25:01, 16.66s/it]                                                        57%|█████▋    | 1863/3250 [8:45:48<6:25:01, 16.66s/it] 57%|█████▋    | 1864/3250 [8:46:04<6:19:22, 16.42s/it]                                                        57%|█████▋    | 1864/3250 [8:46:04<6:19:22, 16.42s/it] 57%|█████▋    | 1865/3250 [8:46:20<6:15:25, 16.26s/it]                                                        57%|█████▋    | 1865/3250 [8:46:20<6:15:25, 16.26s/it] 57%|█████▋    | 1866/3250 [8:46:36<6:12:30, 1{'loss': 0.4406, 'learning_rate': 3.849709478694256e-05, 'epoch': 0.57}
{'loss': 0.4115, 'learning_rate': 3.8450035437014494e-05, 'epoch': 0.57}
{'loss': 0.4624, 'learning_rate': 3.8402986892672377e-05, 'epoch': 0.57}
{'loss': 0.4497, 'learning_rate': 3.8355949197932535e-05, 'epoch': 0.58}
{'loss': 0.4506, 'learning_rate': 3.830892239680117e-05, 'epoch': 0.58}
6.15s/it]                                                        57%|█████▋    | 1866/3250 [8:46:36<6:12:30, 16.15s/it] 57%|█████▋    | 1867/3250 [8:46:51<6:10:25, 16.07s/it]                                                        57%|█████▋    | 1867/3250 [8:46:51<6:10:25, 16.07s/it] 57%|█████▋    | 1868/3250 [8:47:07<6:09:36, 16.05s/it]                                                        57%|█████▋    | 1868/3250 [8:47:07<6:09:36, 16.05s/it] 58%|█████▊    | 1869/3250 [8:47:24<6:11:55, 16.16s/it]                                                        58%|█████▊    | 1869/3250 [8:47:24<6:11:55, 16.16s/it] 58%|█████▊    | 1870/3250 [8:47:40<6:09:46, 16.08s/it]                                                        58%|█████▊    | 1870/3250 [8:47:40<6:09:46, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7529036998748779, 'eval_runtime': 2.4705, 'eval_samples_per_second': 4.857, 'eval_steps_per_second': 1.214, 'epoch': 0.58}
                                                        58%|█████▊    | 1870/3250 [8:47:42<6:09:46, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1870
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1870

the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1870
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1870

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4495, 'learning_rate': 3.8261906533274254e-05, 'epoch': 0.58}
{'loss': 0.4676, 'learning_rate': 3.8214901651337574e-05, 'epoch': 0.58}
{'loss': 0.4538, 'learning_rate': 3.8167907794966574e-05, 'epoch': 0.58}
{'loss': 0.4683, 'learning_rate': 3.812092500812646e-05, 'epoch': 0.58}
{'loss': 0.4592, 'learning_rate': 3.807395333477201e-05, 'epoch': 0.58}
 58%|█████▊    | 1871/3250 [8:47:59<6:31:00, 17.01s/it]                                                        58%|█████▊    | 1871/3250 [8:47:59<6:31:00, 17.01s/it] 58%|█████▊    | 1872/3250 [8:48:15<6:22:58, 16.68s/it]                                                        58%|█████▊    | 1872/3250 [8:48:15<6:22:58, 16.68s/it] 58%|█████▊    | 1873/3250 [8:48:31<6:17:17, 16.44s/it]                                                        58%|█████▊    | 1873/3250 [8:48:31<6:17:17, 16.44s/it] 58%|█████▊    | 1874/3250 [8:48:47<6:13:10, 16.27s/it]                                                        58%|█████▊    | 1874/3250 [8:48:47<6:13:10, 16.27s/it] 58%|█████▊    | 1875/3250 [8:49:03<6:10:19, 16.16s/it]                                                        58%|█████▊    | 1875/3250 [8:49:03<6:10:19, 16.16s/it] 58%|█████▊    | 1876/3250 [8:49:18<6:08:15, 1{'loss': 0.4628, 'learning_rate': 3.802699281884767e-05, 'epoch': 0.58}
{'loss': 0.4609, 'learning_rate': 3.798004350428741e-05, 'epoch': 0.58}
{'loss': 0.4503, 'learning_rate': 3.793310543501473e-05, 'epoch': 0.58}
{'loss': 0.4258, 'learning_rate': 3.7886178654942595e-05, 'epoch': 0.58}
{'loss': 0.4713, 'learning_rate': 3.7839263207973444e-05, 'epoch': 0.58}
6.08s/it]                                                        58%|█████▊    | 1876/3250 [8:49:18<6:08:15, 16.08s/it] 58%|█████▊    | 1877/3250 [8:49:34<6:06:34, 16.02s/it]                                                        58%|█████▊    | 1877/3250 [8:49:34<6:06:34, 16.02s/it] 58%|█████▊    | 1878/3250 [8:49:50<6:05:26, 15.98s/it]                                                        58%|█████▊    | 1878/3250 [8:49:50<6:05:26, 15.98s/it] 58%|█████▊    | 1879/3250 [8:50:06<6:04:33, 15.95s/it]                                                        58%|█████▊    | 1879/3250 [8:50:06<6:04:33, 15.95s/it] 58%|█████▊    | 1880/3250 [8:50:22<6:03:52, 15.94s/it]                                                        58%|█████▊    | 1880/3250 [8:50:22<6:03:52, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7496693134307861, 'eval_runtime': 2.4765, 'eval_samples_per_second': 4.846, 'eval_steps_per_second': 1.211, 'epoch': 0.58}
                                                        58%|█████▊    | 1880/3250 [8:50:24<6:03:52, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1880
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1880
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1880
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1880

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1880
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1880/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1880/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4375, 'learning_rate': 3.7792359137999064e-05, 'epoch': 0.58}
{'loss': 0.4247, 'learning_rate': 3.774546648890066e-05, 'epoch': 0.58}
{'loss': 0.4309, 'learning_rate': 3.769858530454869e-05, 'epoch': 0.58}
{'loss': 0.4426, 'learning_rate': 3.7651715628802916e-05, 'epoch': 0.58}
{'loss': 0.4498, 'learning_rate': 3.7604857505512345e-05, 'epoch': 0.58}
 58%|█████▊    | 1881/3250 [8:50:56<8:04:50, 21.25s/it]                                                        58%|█████▊    | 1881/3250 [8:50:56<8:04:50, 21.25s/it] 58%|█████▊    | 1882/3250 [8:51:12<7:27:47, 19.64s/it]                                                        58%|█████▊    | 1882/3250 [8:51:12<7:27:47, 19.64s/it] 58%|█████▊    | 1883/3250 [8:51:27<7:02:07, 18.53s/it]                                                        58%|█████▊    | 1883/3250 [8:51:27<7:02:07, 18.53s/it] 58%|█████▊    | 1884/3250 [8:51:43<6:43:44, 17.73s/it]                                                        58%|█████▊    | 1884/3250 [8:51:43<6:43:44, 17.73s/it] 58%|█████▊    | 1885/3250 [8:51:59<6:30:52, 17.18s/it]                                                        58%|█████▊    | 1885/3250 [8:51:59<6:30:52, 17.18s/it] 58%|█████▊    | 1886/3250 [8:52:16<6:28:28, 1{'loss': 0.4505, 'learning_rate': 3.7558010978515143e-05, 'epoch': 0.58}
{'loss': 0.7874, 'learning_rate': 3.7511176091638653e-05, 'epoch': 0.58}
{'loss': 0.6055, 'learning_rate': 3.7464352888699333e-05, 'epoch': 0.58}
{'loss': 0.4364, 'learning_rate': 3.74175414135027e-05, 'epoch': 0.58}
{'loss': 0.466, 'learning_rate': 3.737074170984326e-05, 'epoch': 0.58}
7.09s/it]                                                        58%|█████▊    | 1886/3250 [8:52:16<6:28:28, 17.09s/it] 58%|█████▊    | 1887/3250 [8:52:32<6:19:53, 16.72s/it]                                                        58%|█████▊    | 1887/3250 [8:52:32<6:19:53, 16.72s/it] 58%|█████▊    | 1888/3250 [8:52:48<6:13:40, 16.46s/it]                                                        58%|█████▊    | 1888/3250 [8:52:48<6:13:40, 16.46s/it] 58%|█████▊    | 1889/3250 [8:53:04<6:09:25, 16.29s/it]                                                        58%|█████▊    | 1889/3250 [8:53:04<6:09:25, 16.29s/it] 58%|█████▊    | 1890/3250 [8:53:20<6:06:25, 16.17s/it]                                                        58%|█████▊    | 1890/3250 [8:53:20<6:06:25, 16.17s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7424951791763306, 'eval_runtime': 2.5187, 'eval_samples_per_second': 4.764, 'eval_steps_per_second': 1.191, 'epoch': 0.58}
                                                        58%|█████▊    | 1890/3250 [8:53:22<6:06:25, 16.17s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1890
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1890/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1890


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4583, 'learning_rate': 3.7323953821504576e-05, 'epoch': 0.58}
{'loss': 0.4484, 'learning_rate': 3.7277177792259114e-05, 'epoch': 0.58}
{'loss': 0.4307, 'learning_rate': 3.723041366586826e-05, 'epoch': 0.58}
{'loss': 0.4928, 'learning_rate': 3.7183661486082245e-05, 'epoch': 0.58}
{'loss': 0.4644, 'learning_rate': 3.7136921296640165e-05, 'epoch': 0.58}
 58%|█████▊    | 1891/3250 [8:53:39<6:27:10, 17.09s/it]                                                        58%|█████▊    | 1891/3250 [8:53:39<6:27:10, 17.09s/it] 58%|█████▊    | 1892/3250 [8:53:55<6:18:39, 16.73s/it]                                                        58%|█████▊    | 1892/3250 [8:53:55<6:18:39, 16.73s/it] 58%|█████▊    | 1893/3250 [8:54:11<6:12:29, 16.47s/it]                                                        58%|█████▊    | 1893/3250 [8:54:11<6:12:29, 16.47s/it] 58%|█████▊    | 1894/3250 [8:54:26<6:08:10, 16.29s/it]                                                        58%|█████▊    | 1894/3250 [8:54:26<6:08:10, 16.29s/it] 58%|█████▊    | 1895/3250 [8:54:42<6:04:59, 16.16s/it]                                                        58%|█████▊    | 1895/3250 [8:54:42<6:04:59, 16.16s/it] 58%|█████▊    | 1896/3250 [8:54:58<6:02:46, 1{'loss': 0.4287, 'learning_rate': 3.709019314126985e-05, 'epoch': 0.58}
{'loss': 0.4222, 'learning_rate': 3.704347706368789e-05, 'epoch': 0.58}
{'loss': 0.4607, 'learning_rate': 3.69967731075996e-05, 'epoch': 0.58}
{'loss': 0.4448, 'learning_rate': 3.695008131669891e-05, 'epoch': 0.58}
{'loss': 0.4436, 'learning_rate': 3.690340173466842e-05, 'epoch': 0.58}
6.08s/it]                                                        58%|█████▊    | 1896/3250 [8:54:58<6:02:46, 16.08s/it] 58%|█████▊    | 1897/3250 [8:55:14<6:01:04, 16.01s/it]                                                        58%|█████▊    | 1897/3250 [8:55:14<6:01:04, 16.01s/it] 58%|█████▊    | 1898/3250 [8:55:30<5:59:49, 15.97s/it]                                                        58%|█████▊    | 1898/3250 [8:55:30<5:59:49, 15.97s/it] 58%|█████▊    | 1899/3250 [8:55:47<6:03:37, 16.15s/it]                                                        58%|█████▊    | 1899/3250 [8:55:47<6:03:37, 16.15s/it] 58%|█████▊    | 1900/3250 [8:56:02<6:01:37, 16.07s/it]                                                        58%|█████▊    | 1900/3250 [8:56:02<6:01:37, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7533390522003174, 'eval_runtime': 2.4752, 'eval_samples_per_second': 4.848, 'eval_steps_per_second': 1.212, 'epoch': 0.58}
                                                        58%|█████▊    | 1900/3250 [8:56:05<6:01:37, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1900
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1900/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.436, 'learning_rate': 3.685673440517924e-05, 'epoch': 0.58}
{'loss': 0.4437, 'learning_rate': 3.6810079371891094e-05, 'epoch': 0.59}
{'loss': 0.4419, 'learning_rate': 3.6763436678452153e-05, 'epoch': 0.59}
{'loss': 0.4617, 'learning_rate': 3.6716806368499045e-05, 'epoch': 0.59}
{'loss': 0.4497, 'learning_rate': 3.667018848565683e-05, 'epoch': 0.59}
 58%|█████▊    | 1901/3250 [8:56:22<6:25:01, 17.12s/it]                                                        58%|█████▊    | 1901/3250 [8:56:22<6:25:01, 17.12s/it] 59%|█████▊    | 1902/3250 [8:56:39<6:23:49, 17.08s/it]                                                        59%|█████▊    | 1902/3250 [8:56:39<6:23:49, 17.08s/it] 59%|█████▊    | 1903/3250 [8:56:55<6:15:38, 16.73s/it]                                                        59%|█████▊    | 1903/3250 [8:56:55<6:15:38, 16.73s/it] 59%|█████▊    | 1904/3250 [8:57:11<6:09:53, 16.49s/it]                                                        59%|█████▊    | 1904/3250 [8:57:11<6:09:53, 16.49s/it] 59%|█████▊    | 1905/3250 [8:57:27<6:09:47, 16.50s/it]                                                        59%|█████▊    | 1905/3250 [8:57:27<6:09:47, 16.50s/it] 59%|█████▊    | 1906/3250 [8:57:43<6:05:48, 1{'loss': 0.4395, 'learning_rate': 3.6623583073538966e-05, 'epoch': 0.59}
{'loss': 0.4467, 'learning_rate': 3.657699017574717e-05, 'epoch': 0.59}
{'loss': 0.4589, 'learning_rate': 3.653040983587151e-05, 'epoch': 0.59}
{'loss': 0.4044, 'learning_rate': 3.6483842097490287e-05, 'epoch': 0.59}
{'loss': 0.4818, 'learning_rate': 3.643728700417002e-05, 'epoch': 0.59}
6.33s/it]                                                        59%|█████▊    | 1906/3250 [8:57:43<6:05:48, 16.33s/it] 59%|█████▊    | 1907/3250 [8:57:59<6:02:32, 16.20s/it]                                                        59%|█████▊    | 1907/3250 [8:57:59<6:02:32, 16.20s/it] 59%|█████▊    | 1908/3250 [8:58:15<6:00:16, 16.11s/it]                                                        59%|█████▊    | 1908/3250 [8:58:15<6:00:16, 16.11s/it] 59%|█████▊    | 1909/3250 [8:58:31<5:58:36, 16.05s/it]                                                        59%|█████▊    | 1909/3250 [8:58:31<5:58:36, 16.05s/it] 59%|█████▉    | 1910/3250 [8:58:47<5:57:15, 16.00s/it]                                                        59%|█████▉    | 1910/3250 [8:58:47<5:57:15, 16.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7485549449920654, 'eval_runtime': 3.1016, 'eval_samples_per_second': 3.869, 'eval_steps_per_second': 0.967, 'epoch': 0.59}
                                                        59%|█████▉    | 1910/3250 [8:58:50<5:57:15, 16.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1910
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1910
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1910

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1910
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1910/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4289, 'learning_rate': 3.639074459946539e-05, 'epoch': 0.59}
{'loss': 0.4321, 'learning_rate': 3.63442149269192e-05, 'epoch': 0.59}
{'loss': 0.4113, 'learning_rate': 3.629769803006239e-05, 'epoch': 0.59}
{'loss': 0.4335, 'learning_rate': 3.6251193952413865e-05, 'epoch': 0.59}
{'loss': 0.4387, 'learning_rate': 3.62047027374806e-05, 'epoch': 0.59}
 59%|█████▉    | 1911/3250 [8:59:21<7:58:58, 21.46s/it]                                                        59%|█████▉    | 1911/3250 [8:59:21<7:58:58, 21.46s/it] 59%|█████▉    | 1912/3250 [8:59:37<7:21:18, 19.79s/it]                                                        59%|█████▉    | 1912/3250 [8:59:37<7:21:18, 19.79s/it] 59%|█████▉    | 1913/3250 [8:59:53<6:54:51, 18.62s/it]                                                        59%|█████▉    | 1913/3250 [8:59:53<6:54:51, 18.62s/it] 59%|█████▉    | 1914/3250 [9:00:09<6:36:16, 17.80s/it]                                                        59%|█████▉    | 1914/3250 [9:00:09<6:36:16, 17.80s/it] 59%|█████▉    | 1915/3250 [9:00:25<6:23:12, 17.22s/it]                                                        59%|█████▉    | 1915/3250 [9:00:25<6:23:12, 17.22s/it] 59%|█████▉    | 1916/3250 [9:00:41<6:15:25, 1{'loss': 0.4297, 'learning_rate': 3.6158224428757535e-05, 'epoch': 0.59}
{'loss': 0.449, 'learning_rate': 3.611175906972749e-05, 'epoch': 0.59}
{'loss': 0.9532, 'learning_rate': 3.606530670386121e-05, 'epoch': 0.59}
{'loss': 0.4283, 'learning_rate': 3.601886737461729e-05, 'epoch': 0.59}
{'loss': 0.4495, 'learning_rate': 3.597244112544208e-05, 'epoch': 0.59}
6.89s/it]                                                        59%|█████▉    | 1916/3250 [9:00:41<6:15:25, 16.89s/it] 59%|█████▉    | 1917/3250 [9:00:57<6:08:28, 16.59s/it]                                                        59%|█████▉    | 1917/3250 [9:00:57<6:08:28, 16.59s/it] 59%|█████▉    | 1918/3250 [9:01:12<6:03:22, 16.37s/it]                                                        59%|█████▉    | 1918/3250 [9:01:12<6:03:22, 16.37s/it] 59%|█████▉    | 1919/3250 [9:01:29<6:02:05, 16.32s/it]                                                        59%|█████▉    | 1919/3250 [9:01:29<6:02:05, 16.32s/it] 59%|█████▉    | 1920/3250 [9:01:45<5:58:54, 16.19s/it]                                                        59%|█████▉    | 1920/3250 [9:01:45<5:58:54, 16.19s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7471479773521423, 'eval_runtime': 2.4996, 'eval_samples_per_second': 4.801, 'eval_steps_per_second': 1.2, 'epoch': 0.59}
                                                        59%|█████▉    | 1920/3250 [9:01:47<5:58:54, 16.19s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1920
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1920
 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1920
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4515, 'learning_rate': 3.5926027999769754e-05, 'epoch': 0.59}
{'loss': 0.4535, 'learning_rate': 3.587962804102214e-05, 'epoch': 0.59}
{'loss': 0.4219, 'learning_rate': 3.5833241292608846e-05, 'epoch': 0.59}
{'loss': 0.4401, 'learning_rate': 3.5786867797926996e-05, 'epoch': 0.59}
{'loss': 0.501, 'learning_rate': 3.5740507600361415e-05, 'epoch': 0.59}
 59%|█████▉    | 1921/3250 [9:02:04<6:18:23, 17.08s/it]                                                        59%|█████▉    | 1921/3250 [9:02:04<6:18:23, 17.08s/it] 59%|█████▉    | 1922/3250 [9:02:20<6:10:07, 16.72s/it]                                                        59%|█████▉    | 1922/3250 [9:02:20<6:10:07, 16.72s/it] 59%|█████▉    | 1923/3250 [9:02:35<6:04:20, 16.47s/it]                                                        59%|█████▉    | 1923/3250 [9:02:35<6:04:20, 16.47s/it] 59%|█████▉    | 1924/3250 [9:02:51<6:00:12, 16.30s/it]                                                        59%|█████▉    | 1924/3250 [9:02:51<6:00:12, 16.30s/it] 59%|█████▉    | 1925/3250 [9:03:07<5:57:11, 16.18s/it]                                                        59%|█████▉    | 1925/3250 [9:03:07<5:57:11, 16.18s/it] 59%|█████▉    | 1926/3250 [9:03:23<5:55:02, 1{'loss': 0.4261, 'learning_rate': 3.569416074328445e-05, 'epoch': 0.59}
{'loss': 0.4327, 'learning_rate': 3.5647827270055945e-05, 'epoch': 0.59}
{'loss': 0.4365, 'learning_rate': 3.560150722402329e-05, 'epoch': 0.59}
{'loss': 0.4318, 'learning_rate': 3.5555200648521236e-05, 'epoch': 0.59}
{'loss': 0.457, 'learning_rate': 3.550890758687199e-05, 'epoch': 0.59}
6.09s/it]                                                        59%|█████▉    | 1926/3250 [9:03:23<5:55:02, 16.09s/it] 59%|█████▉    | 1927/3250 [9:03:39<5:53:22, 16.03s/it]                                                        59%|█████▉    | 1927/3250 [9:03:39<5:53:22, 16.03s/it] 59%|█████▉    | 1928/3250 [9:03:55<5:52:07, 15.98s/it]                                                        59%|█████▉    | 1928/3250 [9:03:55<5:52:07, 15.98s/it] 59%|█████▉    | 1929/3250 [9:04:11<5:51:16, 15.95s/it]                                                        59%|█████▉    | 1929/3250 [9:04:11<5:51:16, 15.95s/it] 59%|█████▉    | 1930/3250 [9:04:27<5:50:26, 15.93s/it]                                                        59%|█████▉    | 1930/3250 [9:04:27<5:50:26, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7508760690689087, 'eval_runtime': 2.4869, 'eval_samples_per_second': 4.825, 'eval_steps_per_second': 1.206, 'epoch': 0.59}
                                                        59%|█████▉    | 1930/3250 [9:04:29<5:50:26, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1930
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1930

I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1930
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1930

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4383, 'learning_rate': 3.546262808238507e-05, 'epoch': 0.59}
{'loss': 0.4393, 'learning_rate': 3.5416362178357354e-05, 'epoch': 0.59}
{'loss': 0.4476, 'learning_rate': 3.537010991807296e-05, 'epoch': 0.59}
{'loss': 0.4505, 'learning_rate': 3.5323871344803263e-05, 'epoch': 0.6}
{'loss': 0.4605, 'learning_rate': 3.527764650180683e-05, 'epoch': 0.6}
 59%|█████▉    | 1931/3250 [9:04:46<6:11:53, 16.92s/it]                                                        59%|█████▉    | 1931/3250 [9:04:46<6:11:53, 16.92s/it] 59%|█████▉    | 1932/3250 [9:05:02<6:04:58, 16.61s/it]                                                        59%|█████▉    | 1932/3250 [9:05:02<6:04:58, 16.61s/it] 59%|█████▉    | 1933/3250 [9:05:18<5:59:51, 16.39s/it]                                                        59%|█████▉    | 1933/3250 [9:05:18<5:59:51, 16.39s/it] 60%|█████▉    | 1934/3250 [9:05:34<5:56:21, 16.25s/it]                                                        60%|█████▉    | 1934/3250 [9:05:34<5:56:21, 16.25s/it] 60%|█████▉    | 1935/3250 [9:05:50<5:58:00, 16.34s/it]                                                        60%|█████▉    | 1935/3250 [9:05:50<5:58:00, 16.34s/it] 60%|█████▉    | 1936/3250 [9:06:06<5:54:48, 1{'loss': 0.4593, 'learning_rate': 3.523143543232936e-05, 'epoch': 0.6}
{'loss': 0.4432, 'learning_rate': 3.518523817960372e-05, 'epoch': 0.6}
{'loss': 0.4493, 'learning_rate': 3.5139054786849784e-05, 'epoch': 0.6}
{'loss': 0.4269, 'learning_rate': 3.5092885297274524e-05, 'epoch': 0.6}
{'loss': 0.4592, 'learning_rate': 3.504672975407184e-05, 'epoch': 0.6}
6.20s/it]                                                        60%|█████▉    | 1936/3250 [9:06:06<5:54:48, 16.20s/it] 60%|█████▉    | 1937/3250 [9:06:22<5:52:30, 16.11s/it]                                                        60%|█████▉    | 1937/3250 [9:06:22<5:52:30, 16.11s/it] 60%|█████▉    | 1938/3250 [9:06:38<5:50:51, 16.05s/it]                                                        60%|█████▉    | 1938/3250 [9:06:38<5:50:51, 16.05s/it] 60%|█████▉    | 1939/3250 [9:06:54<5:49:33, 16.00s/it]                                                        60%|█████▉    | 1939/3250 [9:06:54<5:49:33, 16.00s/it] 60%|█████▉    | 1940/3250 [9:07:10<5:48:33, 15.96s/it]                                                        60%|█████▉    | 1940/3250 [9:07:10<5:48:33, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7468647360801697, 'eval_runtime': 2.4881, 'eval_samples_per_second': 4.823, 'eval_steps_per_second': 1.206, 'epoch': 0.6}
                                                        60%|█████▉    | 1940/3250 [9:07:12<5:48:33, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1940
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1940/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4422, 'learning_rate': 3.500058820042263e-05, 'epoch': 0.6}
{'loss': 0.4157, 'learning_rate': 3.4954460679494685e-05, 'epoch': 0.6}
{'loss': 0.4259, 'learning_rate': 3.490834723444268e-05, 'epoch': 0.6}
{'loss': 0.4135, 'learning_rate': 3.486224790840811e-05, 'epoch': 0.6}
{'loss': 0.4464, 'learning_rate': 3.4816162744519263e-05, 'epoch': 0.6}
 60%|█████▉    | 1941/3250 [9:07:29<6:09:24, 16.93s/it]                                                        60%|█████▉    | 1941/3250 [9:07:29<6:09:24, 16.93s/it] 60%|█████▉    | 1942/3250 [9:07:45<6:02:17, 16.62s/it]                                                        60%|█████▉    | 1942/3250 [9:07:45<6:02:17, 16.62s/it] 60%|█████▉    | 1943/3250 [9:08:01<5:57:19, 16.40s/it]                                                        60%|█████▉    | 1943/3250 [9:08:01<5:57:19, 16.40s/it] 60%|█████▉    | 1944/3250 [9:08:16<5:53:45, 16.25s/it]                                                        60%|█████▉    | 1944/3250 [9:08:16<5:53:45, 16.25s/it] 60%|█████▉    | 1945/3250 [9:08:32<5:51:06, 16.14s/it]                                                        60%|█████▉    | 1945/3250 [9:08:32<5:51:06, 16.14s/it] 60%|█████▉    | 1946/3250 [9:08:48<5:49:09, 1{'loss': 0.4398, 'learning_rate': 3.4770091785891205e-05, 'epoch': 0.6}
{'loss': 0.4504, 'learning_rate': 3.472403507562566e-05, 'epoch': 0.6}
{'loss': 0.9455, 'learning_rate': 3.467799265681105e-05, 'epoch': 0.6}
{'loss': 0.433, 'learning_rate': 3.463196457252245e-05, 'epoch': 0.6}
{'loss': 0.4418, 'learning_rate': 3.4585950865821473e-05, 'epoch': 0.6}
6.07s/it]                                                        60%|█████▉    | 1946/3250 [9:08:48<5:49:09, 16.07s/it] 60%|█████▉    | 1947/3250 [9:09:04<5:47:42, 16.01s/it]                                                        60%|█████▉    | 1947/3250 [9:09:04<5:47:42, 16.01s/it] 60%|█████▉    | 1948/3250 [9:09:20<5:46:20, 15.96s/it]                                                        60%|█████▉    | 1948/3250 [9:09:20<5:46:20, 15.96s/it] 60%|█████▉    | 1949/3250 [9:09:36<5:45:37, 15.94s/it]                                                        60%|█████▉    | 1949/3250 [9:09:36<5:45:37, 15.94s/it] 60%|██████    | 1950/3250 [9:09:52<5:45:04, 15.93s/it]                                                        60%|██████    | 1950/3250 [9:09:52<5:45:04, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7496007680892944, 'eval_runtime': 2.4818, 'eval_samples_per_second': 4.835, 'eval_steps_per_second': 1.209, 'epoch': 0.6}
                                                        60%|██████    | 1950/3250 [9:09:54<5:45:04, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1950
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1950
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1950 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1950

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4523, 'learning_rate': 3.4539951579756326e-05, 'epoch': 0.6}
{'loss': 0.4528, 'learning_rate': 3.449396675736171e-05, 'epoch': 0.6}
{'loss': 0.4327, 'learning_rate': 3.444799644165876e-05, 'epoch': 0.6}
{'loss': 0.425, 'learning_rate': 3.440204067565511e-05, 'epoch': 0.6}
{'loss': 0.5022, 'learning_rate': 3.435609950234473e-05, 'epoch': 0.6}
 60%|██████    | 1951/3250 [9:10:25<7:39:25, 21.22s/it]                                                        60%|██████    | 1951/3250 [9:10:25<7:39:25, 21.22s/it] 60%|██████    | 1952/3250 [9:10:41<7:04:26, 19.62s/it]                                                        60%|██████    | 1952/3250 [9:10:41<7:04:26, 19.62s/it] 60%|██████    | 1953/3250 [9:10:57<6:39:52, 18.50s/it]                                                        60%|██████    | 1953/3250 [9:10:57<6:39:52, 18.50s/it] 60%|██████    | 1954/3250 [9:11:13<6:22:38, 17.71s/it]                                                        60%|██████    | 1954/3250 [9:11:13<6:22:38, 17.71s/it] 60%|██████    | 1955/3250 [9:11:29<6:10:52, 17.18s/it]                                                        60%|██████    | 1955/3250 [9:11:29<6:10:52, 17.18s/it] 60%|██████    | 1956/3250 [9:11:45<6:02:18, 1{'loss': 0.4463, 'learning_rate': 3.431017296470797e-05, 'epoch': 0.6}
{'loss': 0.4366, 'learning_rate': 3.426426110571141e-05, 'epoch': 0.6}
{'loss': 0.4098, 'learning_rate': 3.4218363968308e-05, 'epoch': 0.6}
{'loss': 0.447, 'learning_rate': 3.417248159543687e-05, 'epoch': 0.6}
{'loss': 0.4461, 'learning_rate': 3.412661403002333e-05, 'epoch': 0.6}
6.80s/it]                                                        60%|██████    | 1956/3250 [9:11:45<6:02:18, 16.80s/it] 60%|██████    | 1957/3250 [9:12:01<5:56:17, 16.53s/it]                                                        60%|██████    | 1957/3250 [9:12:01<5:56:17, 16.53s/it] 60%|██████    | 1958/3250 [9:12:17<5:51:53, 16.34s/it]                                                        60%|██████    | 1958/3250 [9:12:17<5:51:53, 16.34s/it] 60%|██████    | 1959/3250 [9:12:33<5:48:52, 16.21s/it]                                                        60%|██████    | 1959/3250 [9:12:33<5:48:52, 16.21s/it] 60%|██████    | 1960/3250 [9:12:48<5:46:32, 16.12s/it]                                                        60%|██████    | 1960/3250 [9:12:48<5:46:32, 16.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7530356049537659, 'eval_runtime': 2.7622, 'eval_samples_per_second': 4.344, 'eval_steps_per_second': 1.086, 'epoch': 0.6}
                                                        60%|██████    | 1960/3250 [9:12:51<5:46:32, 16.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1960
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1960/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1960

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1960

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4388, 'learning_rate': 3.408076131497885e-05, 'epoch': 0.6}
{'loss': 0.4305, 'learning_rate': 3.403492349320101e-05, 'epoch': 0.6}
{'loss': 0.4493, 'learning_rate': 3.398910060757344e-05, 'epoch': 0.6}
{'loss': 0.4429, 'learning_rate': 3.3943292700965824e-05, 'epoch': 0.6}
{'loss': 0.4599, 'learning_rate': 3.389749981623379e-05, 'epoch': 0.6}
 60%|██████    | 1961/3250 [9:13:08<6:08:19, 17.14s/it]                                                        60%|██████    | 1961/3250 [9:13:08<6:08:19, 17.14s/it] 60%|██████    | 1962/3250 [9:13:24<5:59:58, 16.77s/it]                                                        60%|██████    | 1962/3250 [9:13:24<5:59:58, 16.77s/it] 60%|██████    | 1963/3250 [9:13:40<5:53:54, 16.50s/it]                                                        60%|██████    | 1963/3250 [9:13:40<5:53:54, 16.50s/it] 60%|██████    | 1964/3250 [9:13:56<5:49:42, 16.32s/it]                                                        60%|██████    | 1964/3250 [9:13:56<5:49:42, 16.32s/it] 60%|██████    | 1965/3250 [9:14:11<5:46:42, 16.19s/it]                                                        60%|██████    | 1965/3250 [9:14:11<5:46:42, 16.19s/it] 60%|██████    | 1966/3250 [9:14:27<5:44:29, 1{'loss': 0.4437, 'learning_rate': 3.3851721996218947e-05, 'epoch': 0.6}
{'loss': 0.4433, 'learning_rate': 3.380595928374881e-05, 'epoch': 0.61}
{'loss': 0.4438, 'learning_rate': 3.376021172163674e-05, 'epoch': 0.61}
{'loss': 0.429, 'learning_rate': 3.371447935268194e-05, 'epoch': 0.61}
{'loss': 0.4129, 'learning_rate': 3.366876221966939e-05, 'epoch': 0.61}
6.10s/it]                                                        60%|██████    | 1966/3250 [9:14:27<5:44:29, 16.10s/it] 61%|██████    | 1967/3250 [9:14:43<5:42:53, 16.04s/it]                                                        61%|██████    | 1967/3250 [9:14:43<5:42:53, 16.04s/it] 61%|██████    | 1968/3250 [9:15:00<5:46:42, 16.23s/it]                                                        61%|██████    | 1968/3250 [9:15:00<5:46:42, 16.23s/it] 61%|██████    | 1969/3250 [9:15:16<5:44:18, 16.13s/it]                                                        61%|██████    | 1969/3250 [9:15:16<5:44:18, 16.13s/it] 61%|██████    | 1970/3250 [9:15:32<5:42:27, 16.05s/it]                                                        61%|██████    | 1970/3250 [9:15:32<5:42:27, 16.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7483128309249878, 'eval_runtime': 2.4758, 'eval_samples_per_second': 4.847, 'eval_steps_per_second': 1.212, 'epoch': 0.61}
                                                        61%|██████    | 1970/3250 [9:15:34<5:42:27, 16.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1970
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1970

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1970

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1970
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4574, 'learning_rate': 3.362306036536982e-05, 'epoch': 0.61}
{'loss': 0.4084, 'learning_rate': 3.357737383253966e-05, 'epoch': 0.61}
{'loss': 0.4168, 'learning_rate': 3.353170266392104e-05, 'epoch': 0.61}
{'loss': 0.4203, 'learning_rate': 3.3486046902241664e-05, 'epoch': 0.61}
{'loss': 0.4376, 'learning_rate': 3.344040659021482e-05, 'epoch': 0.61}
 61%|██████    | 1971/3250 [9:16:10<8:01:14, 22.58s/it]                                                        61%|██████    | 1971/3250 [9:16:10<8:01:14, 22.58s/it] 61%|██████    | 1972/3250 [9:16:25<7:18:19, 20.58s/it]                                                        61%|██████    | 1972/3250 [9:16:25<7:18:19, 20.58s/it] 61%|██████    | 1973/3250 [9:16:41<6:47:59, 19.17s/it]                                                        61%|██████    | 1973/3250 [9:16:41<6:47:59, 19.17s/it] 61%|██████    | 1974/3250 [9:16:57<6:26:43, 18.18s/it]                                                        61%|██████    | 1974/3250 [9:16:57<6:26:43, 18.18s/it] 61%|██████    | 1975/3250 [9:17:13<6:11:48, 17.50s/it]                                                        61%|██████    | 1975/3250 [9:17:13<6:11:48, 17.50s/it] 61%|██████    | 1976/3250 [9:17:29<6:01:28, 1{'loss': 0.4309, 'learning_rate': 3.339478177053941e-05, 'epoch': 0.61}
{'loss': 0.4403, 'learning_rate': 3.3349172485899785e-05, 'epoch': 0.61}
{'loss': 0.8062, 'learning_rate': 3.330357877896577e-05, 'epoch': 0.61}
{'loss': 0.5635, 'learning_rate': 3.325800069239263e-05, 'epoch': 0.61}
{'loss': 0.4255, 'learning_rate': 3.321243826882101e-05, 'epoch': 0.61}
7.02s/it]                                                        61%|██████    | 1976/3250 [9:17:29<6:01:28, 17.02s/it] 61%|██████    | 1977/3250 [9:17:45<5:53:57, 16.68s/it]                                                        61%|██████    | 1977/3250 [9:17:45<5:53:57, 16.68s/it] 61%|██████    | 1978/3250 [9:18:01<5:48:23, 16.43s/it]                                                        61%|██████    | 1978/3250 [9:18:01<5:48:23, 16.43s/it] 61%|██████    | 1979/3250 [9:18:17<5:44:34, 16.27s/it]                                                        61%|██████    | 1979/3250 [9:18:17<5:44:34, 16.27s/it] 61%|██████    | 1980/3250 [9:18:32<5:41:47, 16.15s/it]                                                        61%|██████    | 1980/3250 [9:18:32<5:41:47, 16.15s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7521497011184692, 'eval_runtime': 2.4783, 'eval_samples_per_second': 4.842, 'eval_steps_per_second': 1.211, 'epoch': 0.61}
                                                        61%|██████    | 1980/3250 [9:18:35<5:41:47, 16.15s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1980
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1980
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1980
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1980

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4515, 'learning_rate': 3.31668915508769e-05, 'epoch': 0.61}
{'loss': 0.4482, 'learning_rate': 3.3121360581171594e-05, 'epoch': 0.61}
{'loss': 0.4358, 'learning_rate': 3.3075845402301655e-05, 'epoch': 0.61}
{'loss': 0.424, 'learning_rate': 3.303034605684888e-05, 'epoch': 0.61}
{'loss': 0.4859, 'learning_rate': 3.298486258738025e-05, 'epoch': 0.61}
 61%|██████    | 1981/3250 [9:19:05<7:23:43, 20.98s/it]                                                        61%|██████    | 1981/3250 [9:19:05<7:23:43, 20.98s/it] 61%|██████    | 1982/3250 [9:19:21<6:51:12, 19.46s/it]                                                        61%|██████    | 1982/3250 [9:19:21<6:51:12, 19.46s/it] 61%|██████    | 1983/3250 [9:19:37<6:28:18, 18.39s/it]                                                        61%|██████    | 1983/3250 [9:19:37<6:28:18, 18.39s/it] 61%|██████    | 1984/3250 [9:19:53<6:13:38, 17.71s/it]                                                        61%|██████    | 1984/3250 [9:19:53<6:13:38, 17.71s/it] 61%|██████    | 1985/3250 [9:20:09<6:01:50, 17.16s/it]                                                        61%|██████    | 1985/3250 [9:20:09<6:01:50, 17.16s/it] 61%|██████    | 1986/3250 [9:20:24<5:53:31, 1{'loss': 0.4562, 'learning_rate': 3.293939503644788e-05, 'epoch': 0.61}
{'loss': 0.419, 'learning_rate': 3.2893943446589005e-05, 'epoch': 0.61}
{'loss': 0.4194, 'learning_rate': 3.284850786032593e-05, 'epoch': 0.61}
{'loss': 0.4481, 'learning_rate': 3.2803088320165984e-05, 'epoch': 0.61}
{'loss': 0.44, 'learning_rate': 3.275768486860149e-05, 'epoch': 0.61}
6.78s/it]                                                        61%|██████    | 1986/3250 [9:20:24<5:53:31, 16.78s/it] 61%|██████    | 1987/3250 [9:20:40<5:47:42, 16.52s/it]                                                        61%|██████    | 1987/3250 [9:20:40<5:47:42, 16.52s/it] 61%|██████    | 1988/3250 [9:20:56<5:43:29, 16.33s/it]                                                        61%|██████    | 1988/3250 [9:20:56<5:43:29, 16.33s/it] 61%|██████    | 1989/3250 [9:21:12<5:40:28, 16.20s/it]                                                        61%|██████    | 1989/3250 [9:21:12<5:40:28, 16.20s/it] 61%|██████    | 1990/3250 [9:21:28<5:38:17, 16.11s/it]                                                        61%|██████    | 1990/3250 [9:21:28<5:38:17, 16.11s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7496580481529236, 'eval_runtime': 2.7276, 'eval_samples_per_second': 4.4, 'eval_steps_per_second': 1.1, 'epoch': 0.61}
                                                        61%|██████    | 1990/3250 [9:21:31<5:38:17, 16.11s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-1990
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1990the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1990 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1990

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-1990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4359, 'learning_rate': 3.271229754810969e-05, 'epoch': 0.61}
{'loss': 0.4291, 'learning_rate': 3.266692640115277e-05, 'epoch': 0.61}
{'loss': 0.4375, 'learning_rate': 3.262157147017777e-05, 'epoch': 0.61}
{'loss': 0.44, 'learning_rate': 3.257623279761656e-05, 'epoch': 0.61}
{'loss': 0.4615, 'learning_rate': 3.2530910425885806e-05, 'epoch': 0.61}
 61%|██████▏   | 1991/3250 [9:22:06<7:56:29, 22.71s/it]                                                        61%|██████▏   | 1991/3250 [9:22:06<7:56:29, 22.71s/it] 61%|██████▏   | 1992/3250 [9:22:22<7:13:18, 20.67s/it]                                                        61%|██████▏   | 1992/3250 [9:22:22<7:13:18, 20.67s/it] 61%|██████▏   | 1993/3250 [9:22:38<6:42:53, 19.23s/it]                                                        61%|██████▏   | 1993/3250 [9:22:38<6:42:53, 19.23s/it] 61%|██████▏   | 1994/3250 [9:22:54<6:21:39, 18.23s/it]                                                        61%|██████▏   | 1994/3250 [9:22:54<6:21:39, 18.23s/it] 61%|██████▏   | 1995/3250 [9:23:10<6:06:45, 17.53s/it]                                                        61%|██████▏   | 1995/3250 [9:23:10<6:06:45, 17.53s/it] 61%|██████▏   | 1996/32{'loss': 0.4411, 'learning_rate': 3.248560439738691e-05, 'epoch': 0.61}
{'loss': 0.4469, 'learning_rate': 3.244031475450599e-05, 'epoch': 0.61}
{'loss': 0.4371, 'learning_rate': 3.2395041539613855e-05, 'epoch': 0.61}
{'loss': 0.4531, 'learning_rate': 3.234978479506591e-05, 'epoch': 0.62}
{'loss': 0.3952, 'learning_rate': 3.2304544563202163e-05, 'epoch': 0.62}
50 [9:23:26<5:56:12, 17.04s/it]                                                        61%|██████▏   | 1996/3250 [9:23:26<5:56:12, 17.04s/it] 61%|██████▏   | 1997/3250 [9:23:42<5:48:47, 16.70s/it]                                                        61%|██████▏   | 1997/3250 [9:23:42<5:48:47, 16.70s/it] 61%|██████▏   | 1998/3250 [9:23:57<5:43:30, 16.46s/it]                                                        61%|██████▏   | 1998/3250 [9:23:57<5:43:30, 16.46s/it] 62%|██████▏   | 1999/3250 [9:24:13<5:39:41, 16.29s/it]                                                        62%|██████▏   | 1999/3250 [9:24:13<5:39:41, 16.29s/it] 62%|██████▏   | 2000/3250 [9:24:30<5:39:19, 16.29s/it]                                                        62%|██████▏   | 2000/3250 [9:24:30<5:39:19, 16.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7476748824119568, 'eval_runtime': 2.4857, 'eval_samples_per_second': 4.828, 'eval_steps_per_second': 1.207, 'epoch': 0.62}
                                                        62%|██████▏   | 2000/3250 [9:24:32<5:39:19, 16.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2000
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2000 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2000
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2000

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4655, 'learning_rate': 3.22593208863472e-05, 'epoch': 0.62}
{'loss': 0.4204, 'learning_rate': 3.221411380681007e-05, 'epoch': 0.62}
{'loss': 0.4223, 'learning_rate': 3.216892336688435e-05, 'epoch': 0.62}
{'loss': 0.411, 'learning_rate': 3.2123749608848e-05, 'epoch': 0.62}
{'loss': 0.4285, 'learning_rate': 3.207859257496339e-05, 'epoch': 0.62}
 62%|██████▏   | 2001/3250 [9:24:49<5:57:50, 17.19s/it]                                                        62%|██████▏   | 2001/3250 [9:24:49<5:57:50, 17.19s/it] 62%|██████▏   | 2002/3250 [9:25:05<5:49:29, 16.80s/it]                                                        62%|██████▏   | 2002/3250 [9:25:05<5:49:29, 16.80s/it] 62%|██████▏   | 2003/3250 [9:25:21<5:43:34, 16.53s/it]                                                        62%|██████▏   | 2003/3250 [9:25:21<5:43:34, 16.53s/it] 62%|██████▏   | 2004/3250 [9:25:37<5:39:19, 16.34s/it]                                                        62%|██████▏   | 2004/3250 [9:25:37<5:39:19, 16.34s/it] 62%|██████▏   | 2005/3250 [9:25:52<5:36:13, 16.20s/it]                                                        62%|██████▏   | 2005/3250 [9:25:52<5:36:13, 16.20s/it] 62%|██████▏   | 2006/32{'loss': 0.4408, 'learning_rate': 3.2033452307477276e-05, 'epoch': 0.62}
{'loss': 0.4202, 'learning_rate': 3.198832884862068e-05, 'epoch': 0.62}
{'loss': 0.4514, 'learning_rate': 3.194322224060891e-05, 'epoch': 0.62}
{'loss': 0.9371, 'learning_rate': 3.189813252564152e-05, 'epoch': 0.62}
{'loss': 0.4211, 'learning_rate': 3.1853059745902285e-05, 'epoch': 0.62}
50 [9:26:08<5:34:05, 16.11s/it]                                                        62%|██████▏   | 2006/3250 [9:26:08<5:34:05, 16.11s/it] 62%|██████▏   | 2007/3250 [9:26:24<5:32:31, 16.05s/it]                                                        62%|██████▏   | 2007/3250 [9:26:24<5:32:31, 16.05s/it] 62%|██████▏   | 2008/3250 [9:26:40<5:31:17, 16.00s/it]                                                        62%|██████▏   | 2008/3250 [9:26:40<5:31:17, 16.00s/it] 62%|██████▏   | 2009/3250 [9:26:56<5:30:13, 15.97s/it]                                                        62%|██████▏   | 2009/3250 [9:26:56<5:30:13, 15.97s/it] 62%|██████▏   | 2010/3250 [9:27:12<5:29:35, 15.95s/it]                                                        62%|██████▏   | 2010/3250 [9:27:12<5:29:35, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7524186968803406, 'eval_runtime': 2.4783, 'eval_samples_per_second': 4.842, 'eval_steps_per_second': 1.211, 'epoch': 0.62}
                                                        62%|██████▏   | 2010/3250 [9:27:14<5:29:35, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2010
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2010 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2010
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2010

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4393, 'learning_rate': 3.180800394355908e-05, 'epoch': 0.62}
{'loss': 0.4386, 'learning_rate': 3.176296516076394e-05, 'epoch': 0.62}
{'loss': 0.4413, 'learning_rate': 3.1717943439652954e-05, 'epoch': 0.62}
{'loss': 0.4208, 'learning_rate': 3.167293882234626e-05, 'epoch': 0.62}
{'loss': 0.428, 'learning_rate': 3.162795135094799e-05, 'epoch': 0.62}
 62%|██████▏   | 2011/3250 [9:27:31<5:49:17, 16.91s/it]                                                        62%|██████▏   | 2011/3250 [9:27:31<5:49:17, 16.91s/it] 62%|██████▏   | 2012/3250 [9:27:47<5:42:33, 16.60s/it]                                                        62%|██████▏   | 2012/3250 [9:27:47<5:42:33, 16.60s/it] 62%|██████▏   | 2013/3250 [9:28:03<5:37:41, 16.38s/it]                                                        62%|██████▏   | 2013/3250 [9:28:03<5:37:41, 16.38s/it] 62%|██████▏   | 2014/3250 [9:28:19<5:34:10, 16.22s/it]                                                        62%|██████▏   | 2014/3250 [9:28:19<5:34:10, 16.22s/it] 62%|██████▏   | 2015/3250 [9:28:35<5:31:50, 16.12s/it]                                                        62%|██████▏   | 2015/3250 [9:28:35<5:31:50, 16.12s/it] 62%|██████▏   | 2016/32{'loss': 0.5001, 'learning_rate': 3.158298106754626e-05, 'epoch': 0.62}
{'loss': 0.4179, 'learning_rate': 3.1538028014213055e-05, 'epoch': 0.62}
{'loss': 0.4271, 'learning_rate': 3.149309223300428e-05, 'epoch': 0.62}
{'loss': 0.4354, 'learning_rate': 3.144817376595968e-05, 'epoch': 0.62}
{'loss': 0.4311, 'learning_rate': 3.1403272655102764e-05, 'epoch': 0.62}
50 [9:28:50<5:30:01, 16.05s/it]                                                        62%|██████▏   | 2016/3250 [9:28:50<5:30:01, 16.05s/it] 62%|██████▏   | 2017/3250 [9:29:07<5:32:26, 16.18s/it]                                                        62%|██████▏   | 2017/3250 [9:29:07<5:32:26, 16.18s/it] 62%|██████▏   | 2018/3250 [9:29:23<5:31:28, 16.14s/it]                                                        62%|██████▏   | 2018/3250 [9:29:23<5:31:28, 16.14s/it] 62%|██████▏   | 2019/3250 [9:29:39<5:30:43, 16.12s/it]                                                        62%|██████▏   | 2019/3250 [9:29:39<5:30:43, 16.12s/it] 62%|██████▏   | 2020/3250 [9:29:55<5:28:57, 16.05s/it]                                                        62%|██████▏   | 2020/3250 [9:29:55<5:28:57, 16.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7514643669128418, 'eval_runtime': 2.4733, 'eval_samples_per_second': 4.852, 'eval_steps_per_second': 1.213, 'epoch': 0.62}
                                                        62%|██████▏   | 2020/3250 [9:29:57<5:28:57, 16.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2020
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2020
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2020 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2020

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4383, 'learning_rate': 3.135838894244086e-05, 'epoch': 0.62}
{'loss': 0.4273, 'learning_rate': 3.1313522669964976e-05, 'epoch': 0.62}
{'loss': 0.4209, 'learning_rate': 3.1268673879649815e-05, 'epoch': 0.62}
{'loss': 0.4417, 'learning_rate': 3.1223842613453745e-05, 'epoch': 0.62}
{'loss': 0.4439, 'learning_rate': 3.11790289133187e-05, 'epoch': 0.62}
 62%|██████▏   | 2021/3250 [9:30:14<5:47:11, 16.95s/it]                                                        62%|██████▏   | 2021/3250 [9:30:14<5:47:11, 16.95s/it] 62%|██████▏   | 2022/3250 [9:30:30<5:40:11, 16.62s/it]                                                        62%|██████▏   | 2022/3250 [9:30:30<5:40:11, 16.62s/it] 62%|██████▏   | 2023/3250 [9:30:46<5:35:18, 16.40s/it]                                                        62%|██████▏   | 2023/3250 [9:30:46<5:35:18, 16.40s/it] 62%|██████▏   | 2024/3250 [9:31:02<5:31:50, 16.24s/it]                                                        62%|██████▏   | 2024/3250 [9:31:02<5:31:50, 16.24s/it] 62%|██████▏   | 2025/3250 [9:31:18<5:29:35, 16.14s/it]                                                        62%|██████▏   | 2025/3250 [9:31:18<5:29:35, 16.14s/it] 62%|██████▏   | 2026/32{'loss': 0.4465, 'learning_rate': 3.11342328211702e-05, 'epoch': 0.62}
{'loss': 0.4487, 'learning_rate': 3.1089454378917304e-05, 'epoch': 0.62}
{'loss': 0.4369, 'learning_rate': 3.1044693628452557e-05, 'epoch': 0.62}
{'loss': 0.4347, 'learning_rate': 3.0999950611651915e-05, 'epoch': 0.62}
{'loss': 0.4098, 'learning_rate': 3.0955225370374805e-05, 'epoch': 0.62}
50 [9:31:33<5:27:44, 16.07s/it]                                                        62%|██████▏   | 2026/3250 [9:31:33<5:27:44, 16.07s/it] 62%|██████▏   | 2027/3250 [9:31:49<5:26:26, 16.01s/it]                                                        62%|██████▏   | 2027/3250 [9:31:49<5:26:26, 16.01s/it] 62%|██████▏   | 2028/3250 [9:32:05<5:25:26, 15.98s/it]                                                        62%|██████▏   | 2028/3250 [9:32:05<5:25:26, 15.98s/it] 62%|██████▏   | 2029/3250 [9:32:21<5:24:39, 15.95s/it]                                                        62%|██████▏   | 2029/3250 [9:32:21<5:24:39, 15.95s/it] 62%|██████▏   | 2030/3250 [9:32:37<5:24:05, 15.94s/it]                                                        62%|██████▏   | 2030/3250 [9:32:37<5:24:05, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.748298168182373, 'eval_runtime': 2.4783, 'eval_samples_per_second': 4.842, 'eval_steps_per_second': 1.21, 'epoch': 0.62}
                                                        62%|██████▏   | 2030/3250 [9:32:40<5:24:05, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2030
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2030/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2030

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2030

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4586, 'learning_rate': 3.091051794646398e-05, 'epoch': 0.62}
{'loss': 0.4316, 'learning_rate': 3.086582838174551e-05, 'epoch': 0.63}
{'loss': 0.4169, 'learning_rate': 3.082115671802882e-05, 'epoch': 0.63}
{'loss': 0.4184, 'learning_rate': 3.077650299710653e-05, 'epoch': 0.63}
{'loss': 0.412, 'learning_rate': 3.073186726075449e-05, 'epoch': 0.63}
 62%|██████▏   | 2031/3250 [9:33:10<7:10:29, 21.19s/it]                                                        62%|██████▏   | 2031/3250 [9:33:10<7:10:29, 21.19s/it] 63%|██████▎   | 2032/3250 [9:33:26<6:38:00, 19.61s/it]                                                        63%|██████▎   | 2032/3250 [9:33:26<6:38:00, 19.61s/it] 63%|██████▎   | 2033/3250 [9:33:43<6:16:36, 18.57s/it]                                                        63%|██████▎   | 2033/3250 [9:33:43<6:16:36, 18.57s/it] 63%|██████▎   | 2034/3250 [9:33:58<6:00:02, 17.77s/it]                                                        63%|██████▎   | 2034/3250 [9:33:58<6:00:02, 17.77s/it] 63%|██████▎   | 2035/3250 [9:34:14<5:48:23, 17.20s/it]                                                        63%|██████▎   | 2035/3250 [9:34:14<5:48:23, 17.20s/it] 63%|██████▎   | 2036/32{'loss': 0.4408, 'learning_rate': 3.068724955073172e-05, 'epoch': 0.63}
{'loss': 0.4217, 'learning_rate': 3.0642649908780413e-05, 'epoch': 0.63}
{'loss': 0.4463, 'learning_rate': 3.05980683766258e-05, 'epoch': 0.63}
{'loss': 0.9395, 'learning_rate': 3.05535049959762e-05, 'epoch': 0.63}
{'loss': 0.4304, 'learning_rate': 3.0508959808522974e-05, 'epoch': 0.63}
50 [9:34:30<5:40:08, 16.81s/it]                                                        63%|██████▎   | 2036/3250 [9:34:30<5:40:08, 16.81s/it] 63%|██████▎   | 2037/3250 [9:34:46<5:34:15, 16.53s/it]                                                        63%|██████▎   | 2037/3250 [9:34:46<5:34:15, 16.53s/it] 63%|██████▎   | 2038/3250 [9:35:02<5:30:04, 16.34s/it]                                                        63%|██████▎   | 2038/3250 [9:35:02<5:30:04, 16.34s/it] 63%|██████▎   | 2039/3250 [9:35:18<5:26:48, 16.19s/it]                                                        63%|██████▎   | 2039/3250 [9:35:18<5:26:48, 16.19s/it] 63%|██████▎   | 2040/3250 [9:35:34<5:24:46, 16.10s/it]                                                        63%|██████▎   | 2040/3250 [9:35:34<5:24:46, 16.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7534200549125671, 'eval_runtime': 2.4783, 'eval_samples_per_second': 4.842, 'eval_steps_per_second': 1.211, 'epoch': 0.63}
                                                        63%|██████▎   | 2040/3250 [9:35:36<5:24:46, 16.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2040
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2040
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4338, 'learning_rate': 3.0464432855940417e-05, 'epoch': 0.63}
{'loss': 0.4464, 'learning_rate': 3.0419924179885768e-05, 'epoch': 0.63}
{'loss': 0.4422, 'learning_rate': 3.03754338219992e-05, 'epoch': 0.63}
{'loss': 0.4252, 'learning_rate': 3.0330961823903735e-05, 'epoch': 0.63}
{'loss': 0.4211, 'learning_rate': 3.02865082272052e-05, 'epoch': 0.63}
 63%|██████▎   | 2041/3250 [9:36:35<9:58:06, 29.68s/it]                                                        63%|██████▎   | 2041/3250 [9:36:35<9:58:06, 29.68s/it] 63%|██████▎   | 2042/3250 [9:36:51<8:34:24, 25.55s/it]                                                        63%|██████▎   | 2042/3250 [9:36:51<8:34:24, 25.55s/it] 63%|██████▎   | 2043/3250 [9:37:07<7:35:41, 22.65s/it]                                                        63%|██████▎   | 2043/3250 [9:37:07<7:35:41, 22.65s/it] 63%|██████▎   | 2044/3250 [9:37:23<6:54:36, 20.63s/it]                                                        63%|██████▎   | 2044/3250 [9:37:23<6:54:36, 20.63s/it] 63%|██████▎   | 2045/3250 [9:37:39<6:25:46, 19.21s/it]                                                        63%|██████▎   | 2045/3250 [9:37:39<6:25:46, 19.21s/it] 63%|██████▎   | 2046/32{'loss': 0.4955, 'learning_rate': 3.024207307349224e-05, 'epoch': 0.63}
{'loss': 0.4382, 'learning_rate': 3.0197656404336206e-05, 'epoch': 0.63}
{'loss': 0.4293, 'learning_rate': 3.0153258261291195e-05, 'epoch': 0.63}
{'loss': 0.3982, 'learning_rate': 3.0108878685893947e-05, 'epoch': 0.63}
{'loss': 0.4467, 'learning_rate': 3.006451771966383e-05, 'epoch': 0.63}
50 [9:37:55<6:05:31, 18.22s/it]                                                        63%|██████▎   | 2046/3250 [9:37:55<6:05:31, 18.22s/it] 63%|██████▎   | 2047/3250 [9:38:11<5:51:19, 17.52s/it]                                                        63%|██████▎   | 2047/3250 [9:38:11<5:51:19, 17.52s/it] 63%|██████▎   | 2048/3250 [9:38:26<5:41:17, 17.04s/it]                                                        63%|██████▎   | 2048/3250 [9:38:26<5:41:17, 17.04s/it] 63%|██████▎   | 2049/3250 [9:38:42<5:34:10, 16.69s/it]                                                        63%|██████▎   | 2049/3250 [9:38:42<5:34:10, 16.69s/it] 63%|██████▎   | 2050/3250 [9:38:59<5:32:43, 16.64s/it]                                                        63%|██████▎   | 2050/3250 [9:38:59<5:32:43, 16.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7517184019088745, 'eval_runtime': 2.4791, 'eval_samples_per_second': 4.841, 'eval_steps_per_second': 1.21, 'epoch': 0.63}
                                                        63%|██████▎   | 2050/3250 [9:39:01<5:32:43, 16.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2050
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2050/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4321, 'learning_rate': 3.002017540410282e-05, 'epoch': 0.63}
{'loss': 0.4257, 'learning_rate': 2.9975851780695442e-05, 'epoch': 0.63}
{'loss': 0.42, 'learning_rate': 2.9931546890908697e-05, 'epoch': 0.63}
{'loss': 0.4388, 'learning_rate': 2.988726077619211e-05, 'epoch': 0.63}
{'loss': 0.4332, 'learning_rate': 2.9842993477977622e-05, 'epoch': 0.63}
 63%|██████▎   | 2051/3250 [9:39:46<8:33:06, 25.68s/it]                                                        63%|██████▎   | 2051/3250 [9:39:46<8:33:06, 25.68s/it] 63%|██████▎   | 2052/3250 [9:40:01<7:34:09, 22.75s/it]                                                        63%|██████▎   | 2052/3250 [9:40:01<7:34:09, 22.75s/it] 63%|██████▎   | 2053/3250 [9:40:17<6:52:44, 20.69s/it]                                                        63%|██████▎   | 2053/3250 [9:40:17<6:52:44, 20.69s/it] 63%|██████▎   | 2054/3250 [9:40:33<6:23:50, 19.26s/it]                                                        63%|██████▎   | 2054/3250 [9:40:33<6:23:50, 19.26s/it] 63%|██████▎   | 2055/3250 [9:40:49<6:03:23, 18.25s/it]                                                        63%|██████▎   | 2055/3250 [9:40:49<6:03:23, 18.25s/it] 63%|██████▎   | 2056/32{'loss': 0.4556, 'learning_rate': 2.9798745037679556e-05, 'epoch': 0.63}
{'loss': 0.437, 'learning_rate': 2.9754515496694603e-05, 'epoch': 0.63}
{'loss': 0.4311, 'learning_rate': 2.9710304896401802e-05, 'epoch': 0.63}
{'loss': 0.4414, 'learning_rate': 2.966611327816241e-05, 'epoch': 0.63}
{'loss': 0.4268, 'learning_rate': 2.962194068331996e-05, 'epoch': 0.63}
50 [9:41:05<5:49:03, 17.54s/it]                                                        63%|██████▎   | 2056/3250 [9:41:05<5:49:03, 17.54s/it] 63%|██████▎   | 2057/3250 [9:41:21<5:38:55, 17.05s/it]                                                        63%|██████▎   | 2057/3250 [9:41:21<5:38:55, 17.05s/it] 63%|██████▎   | 2058/3250 [9:41:37<5:31:49, 16.70s/it]                                                        63%|██████▎   | 2058/3250 [9:41:37<5:31:49, 16.70s/it] 63%|██████▎   | 2059/3250 [9:41:53<5:26:47, 16.46s/it]                                                        63%|██████▎   | 2059/3250 [9:41:53<5:26:47, 16.46s/it] 63%|██████▎   | 2060/3250 [9:42:09<5:23:11, 16.30s/it]                                                        63%|██████▎   | 2060/3250 [9:42:09<5:23:11, 16.30s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7486540079116821, 'eval_runtime': 2.4791, 'eval_samples_per_second': 4.84, 'eval_steps_per_second': 1.21, 'epoch': 0.63}
                                                        63%|██████▎   | 2060/3250 [9:42:11<5:23:11, 16.30s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2060
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2060/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2060

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2060

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4091, 'learning_rate': 2.9577787153200197e-05, 'epoch': 0.63}
{'loss': 0.4529, 'learning_rate': 2.9533652729111026e-05, 'epoch': 0.63}
{'loss': 0.4145, 'learning_rate': 2.948953745234246e-05, 'epoch': 0.63}
{'loss': 0.4157, 'learning_rate': 2.9445441364166616e-05, 'epoch': 0.64}
{'loss': 0.4163, 'learning_rate': 2.940136450583765e-05, 'epoch': 0.64}
 63%|██████▎   | 2061/3250 [9:42:28<5:40:03, 17.16s/it]                                                        63%|██████▎   | 2061/3250 [9:42:28<5:40:03, 17.16s/it] 63%|██████▎   | 2062/3250 [9:42:44<5:32:11, 16.78s/it]                                                        63%|██████▎   | 2062/3250 [9:42:44<5:32:11, 16.78s/it] 63%|██████▎   | 2063/3250 [9:43:00<5:26:54, 16.52s/it]                                                        63%|██████▎   | 2063/3250 [9:43:00<5:26:54, 16.52s/it] 64%|██████▎   | 2064/3250 [9:43:16<5:22:52, 16.33s/it]                                                        64%|██████▎   | 2064/3250 [9:43:16<5:22:52, 16.33s/it] 64%|██████▎   | 2065/3250 [9:43:31<5:19:50, 16.19s/it]                                                        64%|██████▎   | 2065/3250 [9:43:31<5:19:50, 16.19s/it] 64%|██████▎   | 2066/32{'loss': 0.4332, 'learning_rate': 2.935730691859172e-05, 'epoch': 0.64}
{'loss': 0.4311, 'learning_rate': 2.9313268643646986e-05, 'epoch': 0.64}
{'loss': 0.4424, 'learning_rate': 2.92692497222035e-05, 'epoch': 0.64}
{'loss': 0.8431, 'learning_rate': 2.9225250195443236e-05, 'epoch': 0.64}
{'loss': 0.5171, 'learning_rate': 2.9181270104530018e-05, 'epoch': 0.64}
50 [9:43:48<5:23:50, 16.41s/it]                                                        64%|██████▎   | 2066/3250 [9:43:48<5:23:50, 16.41s/it] 64%|██████▎   | 2067/3250 [9:44:04<5:20:22, 16.25s/it]                                                        64%|██████▎   | 2067/3250 [9:44:04<5:20:22, 16.25s/it] 64%|██████▎   | 2068/3250 [9:44:20<5:17:54, 16.14s/it]                                                        64%|██████▎   | 2068/3250 [9:44:20<5:17:54, 16.14s/it] 64%|██████▎   | 2069/3250 [9:44:36<5:15:56, 16.05s/it]                                                        64%|██████▎   | 2069/3250 [9:44:36<5:15:56, 16.05s/it] 64%|██████▎   | 2070/3250 [9:44:52<5:14:43, 16.00s/it]                                                        64%|██████▎   | 2070/3250 [9:44:52<5:14:43, 16.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7522280812263489, 'eval_runtime': 2.8326, 'eval_samples_per_second': 4.236, 'eval_steps_per_second': 1.059, 'epoch': 0.64}
                                                        64%|██████▎   | 2070/3250 [9:44:55<5:14:43, 16.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2070
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2070
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2070
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2070

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4203, 'learning_rate': 2.91373094906095e-05, 'epoch': 0.64}
{'loss': 0.4489, 'learning_rate': 2.909336839480905e-05, 'epoch': 0.64}
{'loss': 0.4376, 'learning_rate': 2.9049446858237854e-05, 'epoch': 0.64}
{'loss': 0.4335, 'learning_rate': 2.900554492198677e-05, 'epoch': 0.64}
{'loss': 0.4057, 'learning_rate': 2.8961662627128327e-05, 'epoch': 0.64}
 64%|██████▎   | 2071/3250 [9:45:11<5:34:42, 17.03s/it]                                                        64%|██████▎   | 2071/3250 [9:45:11<5:34:42, 17.03s/it] 64%|██████▍   | 2072/3250 [9:45:27<5:27:35, 16.69s/it]                                                        64%|██████▍   | 2072/3250 [9:45:27<5:27:35, 16.69s/it] 64%|██████▍   | 2073/3250 [9:45:43<5:22:36, 16.45s/it]                                                        64%|██████▍   | 2073/3250 [9:45:43<5:22:36, 16.45s/it] 64%|██████▍   | 2074/3250 [9:45:59<5:19:01, 16.28s/it]                                                        64%|██████▍   | 2074/3250 [9:45:59<5:19:01, 16.28s/it] 64%|██████▍   | 2075/3250 [9:46:15<5:16:25, 16.16s/it]                                                        64%|██████▍   | 2075/3250 [9:46:15<5:16:25, 16.16s/it] 64%|██████▍   | 2076/32{'loss': 0.4794, 'learning_rate': 2.8917800014716635e-05, 'epoch': 0.64}
{'loss': 0.444, 'learning_rate': 2.8873957125787443e-05, 'epoch': 0.64}
{'loss': 0.4151, 'learning_rate': 2.8830134001358055e-05, 'epoch': 0.64}
{'loss': 0.4147, 'learning_rate': 2.878633068242721e-05, 'epoch': 0.64}
{'loss': 0.4423, 'learning_rate': 2.8742547209975192e-05, 'epoch': 0.64}
50 [9:46:31<5:14:27, 16.07s/it]                                                        64%|██████▍   | 2076/3250 [9:46:31<5:14:27, 16.07s/it] 64%|██████▍   | 2077/3250 [9:46:47<5:12:55, 16.01s/it]                                                        64%|██████▍   | 2077/3250 [9:46:47<5:12:55, 16.01s/it] 64%|██████▍   | 2078/3250 [9:47:02<5:11:47, 15.96s/it]                                                        64%|██████▍   | 2078/3250 [9:47:02<5:11:47, 15.96s/it] 64%|██████▍   | 2079/3250 [9:47:18<5:10:55, 15.93s/it]                                                        64%|██████▍   | 2079/3250 [9:47:18<5:10:55, 15.93s/it] 64%|██████▍   | 2080/3250 [9:47:34<5:10:15, 15.91s/it]                                                        64%|██████▍   | 2080/3250 [9:47:34<5:10:15, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7515828013420105, 'eval_runtime': 2.4706, 'eval_samples_per_second': 4.857, 'eval_steps_per_second': 1.214, 'epoch': 0.64}
                                                        64%|██████▍   | 2080/3250 [9:47:37<5:10:15, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2080
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2080/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2080

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2080

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4295, 'learning_rate': 2.869878362496368e-05, 'epoch': 0.64}
{'loss': 0.428, 'learning_rate': 2.8655039968335773e-05, 'epoch': 0.64}
{'loss': 0.4187, 'learning_rate': 2.8611316281015908e-05, 'epoch': 0.64}
{'loss': 0.424, 'learning_rate': 2.8567612603909853e-05, 'epoch': 0.64}
{'loss': 0.4296, 'learning_rate': 2.8523928977904623e-05, 'epoch': 0.64}
 64%|██████▍   | 2081/3250 [9:47:53<5:29:21, 16.90s/it]                                                        64%|██████▍   | 2081/3250 [9:47:53<5:29:21, 16.90s/it] 64%|██████▍   | 2082/3250 [9:48:10<5:26:06, 16.75s/it]                                                        64%|██████▍   | 2082/3250 [9:48:10<5:26:06, 16.75s/it] 64%|██████▍   | 2083/3250 [9:48:26<5:20:43, 16.49s/it]                                                        64%|██████▍   | 2083/3250 [9:48:26<5:20:43, 16.49s/it] 64%|██████▍   | 2084/3250 [9:48:41<5:16:47, 16.30s/it]                                                        64%|██████▍   | 2084/3250 [9:48:41<5:16:47, 16.30s/it] 64%|██████▍   | 2085/3250 [9:48:57<5:13:54, 16.17s/it]                                                        64%|██████▍   | 2085/3250 [9:48:57<5:13:54, 16.17s/it] 64%|██████▍   | 2086/32{'loss': 0.4513, 'learning_rate': 2.848026544386851e-05, 'epoch': 0.64}
{'loss': 0.431, 'learning_rate': 2.843662204265099e-05, 'epoch': 0.64}
{'loss': 0.4383, 'learning_rate': 2.8392998815082717e-05, 'epoch': 0.64}
{'loss': 0.4297, 'learning_rate': 2.8349395801975453e-05, 'epoch': 0.64}
{'loss': 0.4325, 'learning_rate': 2.8305813044122097e-05, 'epoch': 0.64}
50 [9:49:13<5:11:56, 16.08s/it]                                                        64%|██████▍   | 2086/3250 [9:49:13<5:11:56, 16.08s/it] 64%|██████▍   | 2087/3250 [9:49:29<5:10:23, 16.01s/it]                                                        64%|██████▍   | 2087/3250 [9:49:29<5:10:23, 16.01s/it] 64%|██████▍   | 2088/3250 [9:49:45<5:09:20, 15.97s/it]                                                        64%|██████▍   | 2088/3250 [9:49:45<5:09:20, 15.97s/it] 64%|██████▍   | 2089/3250 [9:50:01<5:09:55, 16.02s/it]                                                        64%|██████▍   | 2089/3250 [9:50:01<5:09:55, 16.02s/it] 64%|██████▍   | 2090/3250 [9:50:17<5:08:53, 15.98s/it]                                                        64%|██████▍   | 2090/3250 [9:50:17<5:08:53, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7487420439720154, 'eval_runtime': 2.5644, 'eval_samples_per_second': 4.68, 'eval_steps_per_second': 1.17, 'epoch': 0.64}
                                                        64%|██████▍   | 2090/3250 [9:50:19<5:08:53, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2090
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2090
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2090the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2090

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3817, 'learning_rate': 2.826225058229651e-05, 'epoch': 0.64}
{'loss': 0.453, 'learning_rate': 2.821870845725366e-05, 'epoch': 0.64}
{'loss': 0.4061, 'learning_rate': 2.8175186709729397e-05, 'epoch': 0.64}
{'loss': 0.4158, 'learning_rate': 2.8131685380440585e-05, 'epoch': 0.64}
{'loss': 0.4005, 'learning_rate': 2.808820451008495e-05, 'epoch': 0.64}
 64%|██████▍   | 2091/3250 [9:50:36<5:26:55, 16.92s/it]                                                        64%|██████▍   | 2091/3250 [9:50:36<5:26:55, 16.92s/it] 64%|██████▍   | 2092/3250 [9:50:52<5:20:35, 16.61s/it]                                                        64%|██████▍   | 2092/3250 [9:50:52<5:20:35, 16.61s/it] 64%|██████▍   | 2093/3250 [9:51:08<5:16:02, 16.39s/it]                                                        64%|██████▍   | 2093/3250 [9:51:08<5:16:02, 16.39s/it] 64%|██████▍   | 2094/3250 [9:51:24<5:12:38, 16.23s/it]                                                        64%|██████▍   | 2094/3250 [9:51:24<5:12:38, 16.23s/it] 64%|██████▍   | 2095/3250 [9:51:40<5:10:17, 16.12s/it]                                                        64%|██████▍   | 2095/3250 [9:51:40<5:10:17, 16.12s/it] 64%|██████▍   | 2096/32{'loss': 0.415, 'learning_rate': 2.8044744139341094e-05, 'epoch': 0.64}
{'loss': 0.4328, 'learning_rate': 2.800130430886841e-05, 'epoch': 0.65}
{'loss': 0.4151, 'learning_rate': 2.79578850593071e-05, 'epoch': 0.65}
{'loss': 0.4415, 'learning_rate': 2.7914486431278098e-05, 'epoch': 0.65}
{'loss': 0.9323, 'learning_rate': 2.7871108465383066e-05, 'epoch': 0.65}
50 [9:51:55<5:08:29, 16.04s/it]                                                        64%|██████▍   | 2096/3250 [9:51:55<5:08:29, 16.04s/it] 65%|██████▍   | 2097/3250 [9:52:11<5:07:13, 15.99s/it]                                                        65%|██████▍   | 2097/3250 [9:52:11<5:07:13, 15.99s/it] 65%|██████▍   | 2098/3250 [9:52:27<5:06:59, 15.99s/it]                                                        65%|██████▍   | 2098/3250 [9:52:27<5:06:59, 15.99s/it] 65%|██████▍   | 2099/3250 [9:52:44<5:10:59, 16.21s/it]                                                        65%|██████▍   | 2099/3250 [9:52:44<5:10:59, 16.21s/it] 65%|██████▍   | 2100/3250 [9:53:00<5:08:32, 16.10s/it]                                                        65%|██████▍   | 2100/3250 [9:53:00<5:08:32, 16.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7579159736633301, 'eval_runtime': 2.4801, 'eval_samples_per_second': 4.839, 'eval_steps_per_second': 1.21, 'epoch': 0.65}
                                                        65%|██████▍   | 2100/3250 [9:53:02<5:08:32, 16.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2100
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2100/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2100 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2100



the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4156, 'learning_rate': 2.7827751202204323e-05, 'epoch': 0.65}
{'loss': 0.4372, 'learning_rate': 2.7784414682304832e-05, 'epoch': 0.65}
{'loss': 0.4339, 'learning_rate': 2.77410989462281e-05, 'epoch': 0.65}
{'loss': 0.4387, 'learning_rate': 2.769780403449824e-05, 'epoch': 0.65}
{'loss': 0.4088, 'learning_rate': 2.765452998761988e-05, 'epoch': 0.65}
 65%|██████▍   | 2101/3250 [9:53:40<7:26:01, 23.29s/it]                                                        65%|██████▍   | 2101/3250 [9:53:40<7:26:01, 23.29s/it] 65%|██████▍   | 2102/3250 [9:53:56<6:43:01, 21.06s/it]                                                        65%|██████▍   | 2102/3250 [9:53:56<6:43:01, 21.06s/it] 65%|██████▍   | 2103/3250 [9:54:12<6:12:53, 19.51s/it]                                                        65%|██████▍   | 2103/3250 [9:54:12<6:12:53, 19.51s/it] 65%|██████▍   | 2104/3250 [9:54:27<5:51:47, 18.42s/it]                                                        65%|██████▍   | 2104/3250 [9:54:27<5:51:47, 18.42s/it] 65%|██████▍   | 2105/3250 [9:54:43<5:36:58, 17.66s/it]                                                        65%|██████▍   | 2105/3250 [9:54:43<5:36:58, 17.66s/it] 65%|██████▍   | 2106/32{'loss': 0.4191, 'learning_rate': 2.761127684607811e-05, 'epoch': 0.65}
{'loss': 0.4877, 'learning_rate': 2.7568044650338464e-05, 'epoch': 0.65}
{'loss': 0.4204, 'learning_rate': 2.752483344084692e-05, 'epoch': 0.65}
{'loss': 0.4242, 'learning_rate': 2.7481643258029748e-05, 'epoch': 0.65}
{'loss': 0.4304, 'learning_rate': 2.743847414229358e-05, 'epoch': 0.65}
50 [9:54:59<5:26:27, 17.12s/it]                                                        65%|██████▍   | 2106/3250 [9:54:59<5:26:27, 17.12s/it] 65%|██████▍   | 2107/3250 [9:55:15<5:19:05, 16.75s/it]                                                        65%|██████▍   | 2107/3250 [9:55:15<5:19:05, 16.75s/it] 65%|██████▍   | 2108/3250 [9:55:31<5:13:41, 16.48s/it]                                                        65%|██████▍   | 2108/3250 [9:55:31<5:13:41, 16.48s/it] 65%|██████▍   | 2109/3250 [9:55:47<5:09:52, 16.30s/it]                                                        65%|██████▍   | 2109/3250 [9:55:47<5:09:52, 16.30s/it] 65%|██████▍   | 2110/3250 [9:56:03<5:07:05, 16.16s/it]                                                        65%|██████▍   | 2110/3250 [9:56:03<5:07:05, 16.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7509545683860779, 'eval_runtime': 2.4639, 'eval_samples_per_second': 4.87, 'eval_steps_per_second': 1.218, 'epoch': 0.65}
                                                        65%|██████▍   | 2110/3250 [9:56:05<5:07:05, 16.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2110
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2110

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2110
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2110
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4241, 'learning_rate': 2.7395326134025355e-05, 'epoch': 0.65}
{'loss': 0.4397, 'learning_rate': 2.7352199273592265e-05, 'epoch': 0.65}
{'loss': 0.4234, 'learning_rate': 2.7309093601341696e-05, 'epoch': 0.65}
{'loss': 0.4214, 'learning_rate': 2.7266009157601224e-05, 'epoch': 0.65}
{'loss': 0.4241, 'learning_rate': 2.722294598267859e-05, 'epoch': 0.65}
 65%|██████▍   | 2111/3250 [9:56:35<6:41:33, 21.15s/it]                                                        65%|██████▍   | 2111/3250 [9:56:35<6:41:33, 21.15s/it] 65%|██████▍   | 2112/3250 [9:56:51<6:11:04, 19.56s/it]                                                        65%|██████▍   | 2112/3250 [9:56:51<6:11:04, 19.56s/it] 65%|██████▌   | 2113/3250 [9:57:07<5:49:37, 18.45s/it]                                                        65%|██████▌   | 2113/3250 [9:57:07<5:49:37, 18.45s/it] 65%|██████▌   | 2114/3250 [9:57:23<5:34:32, 17.67s/it]                                                        65%|██████▌   | 2114/3250 [9:57:23<5:34:32, 17.67s/it] 65%|██████▌   | 2115/3250 [9:57:39<5:26:10, 17.24s/it]                                                        65%|██████▌   | 2115/3250 [9:57:39<5:26:10, 17.24s/it] 65%|██████▌   | 2116/32{'loss': 0.4485, 'learning_rate': 2.7179904116861556e-05, 'epoch': 0.65}
{'loss': 0.4464, 'learning_rate': 2.713688360041803e-05, 'epoch': 0.65}
{'loss': 0.4376, 'learning_rate': 2.7093884473595922e-05, 'epoch': 0.65}
{'loss': 0.4336, 'learning_rate': 2.705090677662311e-05, 'epoch': 0.65}
{'loss': 0.4256, 'learning_rate': 2.700795054970748e-05, 'epoch': 0.65}
50 [9:57:55<5:18:02, 16.83s/it]                                                        65%|██████▌   | 2116/3250 [9:57:55<5:18:02, 16.83s/it] 65%|██████▌   | 2117/3250 [9:58:11<5:12:15, 16.54s/it]                                                        65%|██████▌   | 2117/3250 [9:58:11<5:12:15, 16.54s/it] 65%|██████▌   | 2118/3250 [9:58:27<5:08:07, 16.33s/it]                                                        65%|██████▌   | 2118/3250 [9:58:27<5:08:07, 16.33s/it] 65%|██████▌   | 2119/3250 [9:58:43<5:05:09, 16.19s/it]                                                        65%|██████▌   | 2119/3250 [9:58:43<5:05:09, 16.19s/it] 65%|██████▌   | 2120/3250 [9:58:59<5:02:57, 16.09s/it]                                                        65%|██████▌   | 2120/3250 [9:58:59<5:02:57, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7496078014373779, 'eval_runtime': 2.4597, 'eval_samples_per_second': 4.879, 'eval_steps_per_second': 1.22, 'epoch': 0.65}
                                                        65%|██████▌   | 2120/3250 [9:59:01<5:02:57, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2120
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2120the checkpoint model will be saved in 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2120
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4115, 'learning_rate': 2.696501583303674e-05, 'epoch': 0.65}
{'loss': 0.4373, 'learning_rate': 2.692210266677855e-05, 'epoch': 0.65}
{'loss': 0.4214, 'learning_rate': 2.687921109108038e-05, 'epoch': 0.65}
{'loss': 0.4134, 'learning_rate': 2.6836341146069515e-05, 'epoch': 0.65}
{'loss': 0.4134, 'learning_rate': 2.6793492871852986e-05, 'epoch': 0.65}
 65%|██████▌   | 2121/3250 [9:59:18<5:19:50, 17.00s/it]                                                        65%|██████▌   | 2121/3250 [9:59:18<5:19:50, 17.00s/it] 65%|██████▌   | 2122/3250 [9:59:34<5:13:09, 16.66s/it]                                                        65%|██████▌   | 2122/3250 [9:59:34<5:13:09, 16.66s/it] 65%|██████▌   | 2123/3250 [9:59:49<5:08:25, 16.42s/it]                                                        65%|██████▌   | 2123/3250 [9:59:49<5:08:25, 16.42s/it] 65%|██████▌   | 2124/3250 [10:00:05<5:05:00, 16.25s/it]                                                         65%|██████▌   | 2124/3250 [10:00:05<5:05:00, 16.25s/it] 65%|██████▌   | 2125/3250 [10:00:21<5:02:28, 16.13s/it]                                                         65%|██████▌   | 2125/3250 [10:00:21<5:02:28, 16.13s/it] 65%|██████▌   | 2{'loss': 0.3938, 'learning_rate': 2.6750666308517573e-05, 'epoch': 0.65}
{'loss': 0.437, 'learning_rate': 2.670786149612972e-05, 'epoch': 0.65}
{'loss': 0.4264, 'learning_rate': 2.6665078474735505e-05, 'epoch': 0.65}
{'loss': 0.436, 'learning_rate': 2.6622317284360664e-05, 'epoch': 0.66}
{'loss': 0.933, 'learning_rate': 2.65795779650105e-05, 'epoch': 0.66}
126/3250 [10:00:37<5:00:54, 16.06s/it]                                                         65%|██████▌   | 2126/3250 [10:00:37<5:00:54, 16.06s/it] 65%|██████▌   | 2127/3250 [10:00:53<5:00:11, 16.04s/it]                                                         65%|██████▌   | 2127/3250 [10:00:53<5:00:11, 16.04s/it] 65%|██████▌   | 2128/3250 [10:01:09<4:58:58, 15.99s/it]                                                         65%|██████▌   | 2128/3250 [10:01:09<4:58:58, 15.99s/it] 66%|██████▌   | 2129/3250 [10:01:25<4:58:37, 15.98s/it]                                                         66%|██████▌   | 2129/3250 [10:01:25<4:58:37, 15.98s/it] 66%|██████▌   | 2130/3250 [10:01:41<4:57:33, 15.94s/it]                                                         66%|██████▌   | 2130/3250 [10:01:41<4:57:33, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7575654983520508, 'eval_runtime': 4.2144, 'eval_samples_per_second': 2.847, 'eval_steps_per_second': 0.712, 'epoch': 0.66}
                                                         66%|██████▌   | 2130/3250 [10:01:45<4:57:33, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2130
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2130/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4335, 'learning_rate': 2.653686055666983e-05, 'epoch': 0.66}
{'loss': 0.4201, 'learning_rate': 2.649416509930299e-05, 'epoch': 0.66}
{'loss': 0.436, 'learning_rate': 2.645149163285381e-05, 'epoch': 0.66}
{'loss': 0.4297, 'learning_rate': 2.6408840197245455e-05, 'epoch': 0.66}
{'loss': 0.4009, 'learning_rate': 2.6366210832380567e-05, 'epoch': 0.66}
 66%|██████▌   | 2131/3250 [10:02:01<5:24:22, 17.39s/it]                                                         66%|██████▌   | 2131/3250 [10:02:01<5:24:22, 17.39s/it] 66%|██████▌   | 2132/3250 [10:02:18<5:17:16, 17.03s/it]                                                         66%|██████▌   | 2132/3250 [10:02:18<5:17:16, 17.03s/it] 66%|██████▌   | 2133/3250 [10:02:35<5:17:41, 17.07s/it]                                                         66%|██████▌   | 2133/3250 [10:02:35<5:17:41, 17.07s/it] 66%|██████▌   | 2134/3250 [10:02:51<5:11:08, 16.73s/it]                                                         66%|██████▌   | 2134/3250 [10:02:51<5:11:08, 16.73s/it] 66%|██████▌   | 2135/3250 [10:03:07<5:06:04, 16.47s/it]                                                         66%|██████▌   | 2135/3250 [10:03:07<5:06:04, 16.47s/it] 66%|██████{'loss': 0.4092, 'learning_rate': 2.632360357814111e-05, 'epoch': 0.66}
{'loss': 0.481, 'learning_rate': 2.628101847438835e-05, 'epoch': 0.66}
{'loss': 0.4336, 'learning_rate': 2.6238455560962884e-05, 'epoch': 0.66}
{'loss': 0.4197, 'learning_rate': 2.619591487768444e-05, 'epoch': 0.66}
{'loss': 0.3956, 'learning_rate': 2.615339646435206e-05, 'epoch': 0.66}
▌   | 2136/3250 [10:03:23<5:04:20, 16.39s/it]                                                         66%|██████▌   | 2136/3250 [10:03:23<5:04:20, 16.39s/it] 66%|██████▌   | 2137/3250 [10:03:39<5:01:12, 16.24s/it]                                                         66%|██████▌   | 2137/3250 [10:03:39<5:01:12, 16.24s/it] 66%|██████▌   | 2138/3250 [10:03:55<4:58:57, 16.13s/it]                                                         66%|██████▌   | 2138/3250 [10:03:55<4:58:57, 16.13s/it] 66%|██████▌   | 2139/3250 [10:04:10<4:57:16, 16.05s/it]                                                         66%|██████▌   | 2139/3250 [10:04:10<4:57:16, 16.05s/it] 66%|██████▌   | 2140/3250 [10:04:27<4:57:05, 16.06s/it]                                                         66%|██████▌   | 2140/3250 [10:04:27<4:57:05, 16.06s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7538958787918091, 'eval_runtime': 2.788, 'eval_samples_per_second': 4.304, 'eval_steps_per_second': 1.076, 'epoch': 0.66}
                                                         66%|██████▌   | 2140/3250 [10:04:29<4:57:05, 16.06s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2140

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2140
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4343, 'learning_rate': 2.6110900360743896e-05, 'epoch': 0.66}
{'loss': 0.4323, 'learning_rate': 2.6068426606617252e-05, 'epoch': 0.66}
{'loss': 0.4223, 'learning_rate': 2.6025975241708478e-05, 'epoch': 0.66}
{'loss': 0.3995, 'learning_rate': 2.598354630573303e-05, 'epoch': 0.66}
{'loss': 0.436, 'learning_rate': 2.5941139838385383e-05, 'epoch': 0.66}
 66%|██████▌   | 2141/3250 [10:04:46<5:15:33, 17.07s/it]                                                         66%|██████▌   | 2141/3250 [10:04:46<5:15:33, 17.07s/it] 66%|██████▌   | 2142/3250 [10:05:02<5:08:38, 16.71s/it]                                                         66%|██████▌   | 2142/3250 [10:05:02<5:08:38, 16.71s/it] 66%|██████▌   | 2143/3250 [10:05:18<5:03:47, 16.47s/it]                                                         66%|██████▌   | 2143/3250 [10:05:18<5:03:47, 16.47s/it] 66%|██████▌   | 2144/3250 [10:05:34<5:00:11, 16.29s/it]                                                         66%|██████▌   | 2144/3250 [10:05:34<5:00:11, 16.29s/it] 66%|██████▌   | 2145/3250 [10:05:49<4:57:45, 16.17s/it]                                                         66%|██████▌   | 2145/3250 [10:05:50<4:57:45, 16.17s/it] 66%|██████{'loss': 0.4423, 'learning_rate': 2.589875587933892e-05, 'epoch': 0.66}
{'loss': 0.4448, 'learning_rate': 2.5856394468246036e-05, 'epoch': 0.66}
{'loss': 0.4277, 'learning_rate': 2.581405564473801e-05, 'epoch': 0.66}
{'loss': 0.4184, 'learning_rate': 2.5771739448425e-05, 'epoch': 0.66}
{'loss': 0.4344, 'learning_rate': 2.572944591889598e-05, 'epoch': 0.66}
▌   | 2146/3250 [10:06:05<4:55:54, 16.08s/it]                                                         66%|██████▌   | 2146/3250 [10:06:05<4:55:54, 16.08s/it] 66%|██████▌   | 2147/3250 [10:06:21<4:54:32, 16.02s/it]                                                         66%|██████▌   | 2147/3250 [10:06:21<4:54:32, 16.02s/it] 66%|██████▌   | 2148/3250 [10:06:38<4:55:56, 16.11s/it]                                                         66%|██████▌   | 2148/3250 [10:06:38<4:55:56, 16.11s/it] 66%|██████▌   | 2149/3250 [10:06:53<4:54:21, 16.04s/it]                                                         66%|██████▌   | 2149/3250 [10:06:53<4:54:21, 16.04s/it] 66%|██████▌   | 2150/3250 [10:07:09<4:53:12, 15.99s/it]                                                         66%|██████▌   | 2150/3250 [10:07:09<4:53:12, 15.99s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.750100314617157, 'eval_runtime': 2.7068, 'eval_samples_per_second': 4.433, 'eval_steps_per_second': 1.108, 'epoch': 0.66}
                                                         66%|██████▌   | 2150/3250 [10:07:12<4:53:12, 15.99s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2150
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2150/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2150

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4071, 'learning_rate': 2.5687175095718723e-05, 'epoch': 0.66}
{'loss': 0.4035, 'learning_rate': 2.5644927018439748e-05, 'epoch': 0.66}
{'loss': 0.4372, 'learning_rate': 2.56027017265843e-05, 'epoch': 0.66}
{'loss': 0.4017, 'learning_rate': 2.5560499259656325e-05, 'epoch': 0.66}
{'loss': 0.3991, 'learning_rate': 2.55183196571384e-05, 'epoch': 0.66}
 66%|██████▌   | 2151/3250 [10:07:29<5:11:24, 17.00s/it]                                                         66%|██████▌   | 2151/3250 [10:07:29<5:11:24, 17.00s/it] 66%|██████▌   | 2152/3250 [10:07:45<5:04:53, 16.66s/it]                                                         66%|██████▌   | 2152/3250 [10:07:45<5:04:53, 16.66s/it] 66%|██████▌   | 2153/3250 [10:08:00<5:00:13, 16.42s/it]                                                         66%|██████▌   | 2153/3250 [10:08:00<5:00:13, 16.42s/it] 66%|██████▋   | 2154/3250 [10:08:16<4:57:00, 16.26s/it]                                                         66%|██████▋   | 2154/3250 [10:08:16<4:57:00, 16.26s/it] 66%|██████▋   | 2155/3250 [10:08:32<4:54:38, 16.14s/it]                                                         66%|██████▋   | 2155/3250 [10:08:32<4:54:38, 16.14s/it] 66%|██████{'loss': 0.4006, 'learning_rate': 2.5476162958491727e-05, 'epoch': 0.66}
{'loss': 0.4276, 'learning_rate': 2.5434029203156035e-05, 'epoch': 0.66}
{'loss': 0.4203, 'learning_rate': 2.539191843054963e-05, 'epoch': 0.66}
{'loss': 0.4291, 'learning_rate': 2.5349830680069338e-05, 'epoch': 0.66}
{'loss': 0.9546, 'learning_rate': 2.530776599109036e-05, 'epoch': 0.66}
▋   | 2156/3250 [10:08:48<4:52:55, 16.07s/it]                                                         66%|██████▋   | 2156/3250 [10:08:48<4:52:55, 16.07s/it] 66%|██████▋   | 2157/3250 [10:09:04<4:51:36, 16.01s/it]                                                         66%|██████▋   | 2157/3250 [10:09:04<4:51:36, 16.01s/it] 66%|██████▋   | 2158/3250 [10:09:20<4:50:40, 15.97s/it]                                                         66%|██████▋   | 2158/3250 [10:09:20<4:50:40, 15.97s/it] 66%|██████▋   | 2159/3250 [10:09:36<4:49:54, 15.94s/it]                                                         66%|██████▋   | 2159/3250 [10:09:36<4:49:54, 15.94s/it] 66%|██████▋   | 2160/3250 [10:09:52<4:49:12, 15.92s/it]                                                         66%|██████▋   | 2160/3250 [10:09:52<4:49:12, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.757874608039856, 'eval_runtime': 2.4706, 'eval_samples_per_second': 4.857, 'eval_steps_per_second': 1.214, 'epoch': 0.66}
                                                         66%|██████▋   | 2160/3250 [10:09:54<4:49:12, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2160
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2160/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2160the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4021, 'learning_rate': 2.5265724402966407e-05, 'epoch': 0.66}
{'loss': 0.4098, 'learning_rate': 2.522370595502954e-05, 'epoch': 0.67}
{'loss': 0.4393, 'learning_rate': 2.51817106865902e-05, 'epoch': 0.67}
{'loss': 0.434, 'learning_rate': 2.5139738636937084e-05, 'epoch': 0.67}
{'loss': 0.422, 'learning_rate': 2.5097789845337223e-05, 'epoch': 0.67}
 66%|██████▋   | 2161/3250 [10:10:11<5:06:13, 16.87s/it]                                                         66%|██████▋   | 2161/3250 [10:10:11<5:06:13, 16.87s/it] 67%|██████▋   | 2162/3250 [10:10:27<5:00:32, 16.57s/it]                                                         67%|██████▋   | 2162/3250 [10:10:27<5:00:32, 16.57s/it] 67%|██████▋   | 2163/3250 [10:10:42<4:56:33, 16.37s/it]                                                         67%|██████▋   | 2163/3250 [10:10:42<4:56:33, 16.37s/it] 67%|██████▋   | 2164/3250 [10:10:59<4:54:47, 16.29s/it]                                                         67%|██████▋   | 2164/3250 [10:10:59<4:54:47, 16.29s/it] 67%|██████▋   | 2165/3250 [10:11:14<4:52:24, 16.17s/it]                                                         67%|██████▋   | 2165/3250 [10:11:14<4:52:24, 16.17s/it] 67%|██████{'loss': 0.4041, 'learning_rate': 2.5055864351035868e-05, 'epoch': 0.67}
{'loss': 0.4719, 'learning_rate': 2.5013962193256473e-05, 'epoch': 0.67}
{'loss': 0.4396, 'learning_rate': 2.497208341120067e-05, 'epoch': 0.67}
{'loss': 0.4102, 'learning_rate': 2.493022804404822e-05, 'epoch': 0.67}
{'loss': 0.4043, 'learning_rate': 2.4888396130956948e-05, 'epoch': 0.67}
▋   | 2166/3250 [10:11:30<4:50:32, 16.08s/it]                                                         67%|██████▋   | 2166/3250 [10:11:30<4:50:32, 16.08s/it] 67%|██████▋   | 2167/3250 [10:11:46<4:49:18, 16.03s/it]                                                         67%|██████▋   | 2167/3250 [10:11:46<4:49:18, 16.03s/it] 67%|██████▋   | 2168/3250 [10:12:02<4:48:23, 15.99s/it]                                                         67%|██████▋   | 2168/3250 [10:12:02<4:48:23, 15.99s/it] 67%|██████▋   | 2169/3250 [10:12:18<4:47:40, 15.97s/it]                                                         67%|██████▋   | 2169/3250 [10:12:18<4:47:40, 15.97s/it] 67%|██████▋   | 2170/3250 [10:12:34<4:47:03, 15.95s/it]                                                         67%|██████▋   | 2170/3250 [10:12:34<4:47:03, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7517349720001221, 'eval_runtime': 2.4849, 'eval_samples_per_second': 4.829, 'eval_steps_per_second': 1.207, 'epoch': 0.67}
                                                         67%|██████▋   | 2170/3250 [10:12:36<4:47:03, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2170
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2170
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2170
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4378, 'learning_rate': 2.4846587711062768e-05, 'epoch': 0.67}
{'loss': 0.4297, 'learning_rate': 2.4804802823479613e-05, 'epoch': 0.67}
{'loss': 0.4263, 'learning_rate': 2.4763041507299388e-05, 'epoch': 0.67}
{'loss': 0.4192, 'learning_rate': 2.472130380159199e-05, 'epoch': 0.67}
{'loss': 0.4239, 'learning_rate': 2.4679589745405124e-05, 'epoch': 0.67}
 67%|██████▋   | 2171/3250 [10:12:53<5:03:57, 16.90s/it]                                                         67%|██████▋   | 2171/3250 [10:12:53<5:03:57, 16.90s/it] 67%|██████▋   | 2172/3250 [10:13:09<4:58:13, 16.60s/it]                                                         67%|██████▋   | 2172/3250 [10:13:09<4:58:13, 16.60s/it] 67%|██████▋   | 2173/3250 [10:13:25<4:54:05, 16.38s/it]                                                         67%|██████▋   | 2173/3250 [10:13:25<4:54:05, 16.38s/it] 67%|██████▋   | 2174/3250 [10:13:41<4:51:07, 16.23s/it]                                                         67%|██████▋   | 2174/3250 [10:13:41<4:51:07, 16.23s/it] 67%|██████▋   | 2175/3250 [10:13:57<4:48:54, 16.12s/it]                                                         67%|██████▋   | 2175/3250 [10:13:57<4:48:54, 16.12s/it] 67%|██████{'loss': 0.4228, 'learning_rate': 2.4637899377764494e-05, 'epoch': 0.67}
{'loss': 0.439, 'learning_rate': 2.459623273767354e-05, 'epoch': 0.67}
{'loss': 0.4225, 'learning_rate': 2.4554589864113563e-05, 'epoch': 0.67}
{'loss': 0.427, 'learning_rate': 2.4512970796043616e-05, 'epoch': 0.67}
{'loss': 0.4301, 'learning_rate': 2.447137557240048e-05, 'epoch': 0.67}
▋   | 2176/3250 [10:14:12<4:47:16, 16.05s/it]                                                         67%|██████▋   | 2176/3250 [10:14:12<4:47:16, 16.05s/it] 67%|██████▋   | 2177/3250 [10:14:28<4:46:04, 16.00s/it]                                                         67%|██████▋   | 2177/3250 [10:14:28<4:46:04, 16.00s/it] 67%|██████▋   | 2178/3250 [10:14:44<4:45:12, 15.96s/it]                                                         67%|██████▋   | 2178/3250 [10:14:44<4:45:12, 15.96s/it] 67%|██████▋   | 2179/3250 [10:15:00<4:44:38, 15.95s/it]                                                         67%|██████▋   | 2179/3250 [10:15:00<4:44:38, 15.95s/it] 67%|██████▋   | 2180/3250 [10:15:16<4:44:02, 15.93s/it]                                                         67%|██████▋   | 2180/3250 [10:15:16<4:44:02, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7522906064987183, 'eval_runtime': 2.4824, 'eval_samples_per_second': 4.834, 'eval_steps_per_second': 1.209, 'epoch': 0.67}
                                                         67%|██████▋   | 2180/3250 [10:15:18<4:44:02, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2180
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2180
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2180 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2180

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2180/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.432, 'learning_rate': 2.442980423209864e-05, 'epoch': 0.67}
{'loss': 0.3834, 'learning_rate': 2.4388256814030187e-05, 'epoch': 0.67}
{'loss': 0.4517, 'learning_rate': 2.4346733357064888e-05, 'epoch': 0.67}
{'loss': 0.4129, 'learning_rate': 2.4305233900050074e-05, 'epoch': 0.67}
{'loss': 0.4178, 'learning_rate': 2.4263758481810617e-05, 'epoch': 0.67}
 67%|██████▋   | 2181/3250 [10:16:12<8:17:36, 27.93s/it]                                                         67%|██████▋   | 2181/3250 [10:16:12<8:17:36, 27.93s/it] 67%|██████▋   | 2182/3250 [10:16:28<7:13:01, 24.33s/it]                                                         67%|██████▋   | 2182/3250 [10:16:28<7:13:01, 24.33s/it] 67%|██████▋   | 2183/3250 [10:16:44<6:27:43, 21.80s/it]                                                         67%|██████▋   | 2183/3250 [10:16:44<6:27:43, 21.80s/it] 67%|██████▋   | 2184/3250 [10:17:00<5:55:57, 20.04s/it]                                                         67%|██████▋   | 2184/3250 [10:17:00<5:55:57, 20.04s/it] 67%|██████▋   | 2185/3250 [10:17:16<5:33:34, 18.79s/it]                                                         67%|██████▋   | 2185/3250 [10:17:16<5:33:34, 18.79s/it] 67%|██████{'loss': 0.4023, 'learning_rate': 2.422230714114891e-05, 'epoch': 0.67}
{'loss': 0.4093, 'learning_rate': 2.418087991684483e-05, 'epoch': 0.67}
{'loss': 0.4196, 'learning_rate': 2.4139476847655634e-05, 'epoch': 0.67}
{'loss': 0.4156, 'learning_rate': 2.4098097972316046e-05, 'epoch': 0.67}
{'loss': 0.4292, 'learning_rate': 2.4056743329538138e-05, 'epoch': 0.67}
▋   | 2186/3250 [10:17:31<5:17:59, 17.93s/it]                                                         67%|██████▋   | 2186/3250 [10:17:31<5:17:59, 17.93s/it] 67%|██████▋   | 2187/3250 [10:17:47<5:06:56, 17.32s/it]                                                         67%|██████▋   | 2187/3250 [10:17:47<5:06:56, 17.32s/it] 67%|██████▋   | 2188/3250 [10:18:03<4:59:08, 16.90s/it]                                                         67%|██████▋   | 2188/3250 [10:18:03<4:59:08, 16.90s/it] 67%|██████▋   | 2189/3250 [10:18:19<4:53:32, 16.60s/it]                                                         67%|██████▋   | 2189/3250 [10:18:19<4:53:32, 16.60s/it] 67%|██████▋   | 2190/3250 [10:18:35<4:49:35, 16.39s/it]                                                         67%|██████▋   | 2190/3250 [10:18:35<4:49:35, 16.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7596429586410522, 'eval_runtime': 2.4811, 'eval_samples_per_second': 4.837, 'eval_steps_per_second': 1.209, 'epoch': 0.67}
                                                         67%|██████▋   | 2190/3250 [10:18:38<4:49:35, 16.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2190
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2190/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9337, 'learning_rate': 2.4015412958011302e-05, 'epoch': 0.67}
{'loss': 0.4234, 'learning_rate': 2.3974106896402237e-05, 'epoch': 0.67}
{'loss': 0.4309, 'learning_rate': 2.393282518335486e-05, 'epoch': 0.67}
{'loss': 0.4315, 'learning_rate': 2.3891567857490372e-05, 'epoch': 0.68}
{'loss': 0.4303, 'learning_rate': 2.3850334957407087e-05, 'epoch': 0.68}
 67%|██████▋   | 2191/3250 [10:18:54<5:03:42, 17.21s/it]                                                         67%|██████▋   | 2191/3250 [10:18:54<5:03:42, 17.21s/it] 67%|██████▋   | 2192/3250 [10:19:10<4:56:25, 16.81s/it]                                                         67%|██████▋   | 2192/3250 [10:19:10<4:56:25, 16.81s/it] 67%|██████▋   | 2193/3250 [10:19:26<4:51:16, 16.53s/it]                                                         67%|██████▋   | 2193/3250 [10:19:26<4:51:16, 16.53s/it] 68%|██████▊   | 2194/3250 [10:19:42<4:47:33, 16.34s/it]                                                         68%|██████▊   | 2194/3250 [10:19:42<4:47:33, 16.34s/it] 68%|██████▊   | 2195/3250 [10:19:58<4:45:09, 16.22s/it]                                                         68%|██████▊   | 2195/3250 [10:19:58<4:45:09, 16.22s/it] 68%|██████{'loss': 0.4011, 'learning_rate': 2.3809126521680518e-05, 'epoch': 0.68}
{'loss': 0.4195, 'learning_rate': 2.3767942588863283e-05, 'epoch': 0.68}
{'loss': 0.4805, 'learning_rate': 2.372678319748507e-05, 'epoch': 0.68}
{'loss': 0.4155, 'learning_rate': 2.3685648386052617e-05, 'epoch': 0.68}
{'loss': 0.4118, 'learning_rate': 2.3644538193049625e-05, 'epoch': 0.68}
▊   | 2196/3250 [10:20:14<4:43:12, 16.12s/it]                                                         68%|██████▊   | 2196/3250 [10:20:14<4:43:12, 16.12s/it] 68%|██████▊   | 2197/3250 [10:20:30<4:42:53, 16.12s/it]                                                         68%|██████▊   | 2197/3250 [10:20:30<4:42:53, 16.12s/it] 68%|██████▊   | 2198/3250 [10:20:46<4:41:28, 16.05s/it]                                                         68%|██████▊   | 2198/3250 [10:20:46<4:41:28, 16.05s/it] 68%|██████▊   | 2199/3250 [10:21:02<4:40:27, 16.01s/it]                                                         68%|██████▊   | 2199/3250 [10:21:02<4:40:27, 16.01s/it] 68%|██████▊   | 2200/3250 [10:21:18<4:39:34, 15.98s/it]                                                         68%|██████▊   | 2200/3250 [10:21:18<4:39:34, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7532597184181213, 'eval_runtime': 2.4816, 'eval_samples_per_second': 4.836, 'eval_steps_per_second': 1.209, 'epoch': 0.68}
                                                         68%|██████▊   | 2200/3250 [10:21:20<4:39:34, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2200
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2200
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2200
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4292, 'learning_rate': 2.360345265693681e-05, 'epoch': 0.68}
{'loss': 0.4169, 'learning_rate': 2.356239181615181e-05, 'epoch': 0.68}
{'loss': 0.4236, 'learning_rate': 2.3521355709109138e-05, 'epoch': 0.68}
{'loss': 0.4145, 'learning_rate': 2.3480344374200203e-05, 'epoch': 0.68}
{'loss': 0.4183, 'learning_rate': 2.343935784979323e-05, 'epoch': 0.68}
 68%|██████▊   | 2201/3250 [10:21:55<6:32:45, 22.46s/it]                                                         68%|██████▊   | 2201/3250 [10:21:55<6:32:45, 22.46s/it] 68%|██████▊   | 2202/3250 [10:22:11<5:57:59, 20.50s/it]                                                         68%|██████▊   | 2202/3250 [10:22:11<5:57:59, 20.50s/it] 68%|██████▊   | 2203/3250 [10:22:27<5:33:40, 19.12s/it]                                                         68%|██████▊   | 2203/3250 [10:22:27<5:33:40, 19.12s/it] 68%|██████▊   | 2204/3250 [10:22:43<5:16:34, 18.16s/it]                                                         68%|██████▊   | 2204/3250 [10:22:43<5:16:34, 18.16s/it] 68%|██████▊   | 2205/3250 [10:22:59<5:04:26, 17.48s/it]                                                         68%|██████▊   | 2205/3250 [10:22:59<5:04:26, 17.48s/it] 68%|██████{'loss': 0.418, 'learning_rate': 2.3398396174233178e-05, 'epoch': 0.68}
{'loss': 0.4326, 'learning_rate': 2.3357459385841823e-05, 'epoch': 0.68}
{'loss': 0.4358, 'learning_rate': 2.3316547522917638e-05, 'epoch': 0.68}
{'loss': 0.4186, 'learning_rate': 2.3275660623735772e-05, 'epoch': 0.68}
{'loss': 0.4357, 'learning_rate': 2.3234798726548044e-05, 'epoch': 0.68}
▊   | 2206/3250 [10:23:15<4:55:55, 17.01s/it]                                                         68%|██████▊   | 2206/3250 [10:23:15<4:55:55, 17.01s/it] 68%|██████▊   | 2207/3250 [10:23:31<4:49:48, 16.67s/it]                                                         68%|██████▊   | 2207/3250 [10:23:31<4:49:48, 16.67s/it] 68%|██████▊   | 2208/3250 [10:23:46<4:45:30, 16.44s/it]                                                         68%|██████▊   | 2208/3250 [10:23:46<4:45:30, 16.44s/it] 68%|██████▊   | 2209/3250 [10:24:02<4:42:22, 16.27s/it]                                                         68%|██████▊   | 2209/3250 [10:24:02<4:42:22, 16.27s/it] 68%|██████▊   | 2210/3250 [10:24:18<4:40:05, 16.16s/it]                                                         68%|██████▊   | 2210/3250 [10:24:18<4:40:05, 16.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.750684916973114, 'eval_runtime': 2.4933, 'eval_samples_per_second': 4.813, 'eval_steps_per_second': 1.203, 'epoch': 0.68}
                                                         68%|██████▊   | 2210/3250 [10:24:21<4:40:05, 16.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2210
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2210 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2210
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4164, 'learning_rate': 2.3193961869582826e-05, 'epoch': 0.68}
{'loss': 0.4033, 'learning_rate': 2.3153150091045132e-05, 'epoch': 0.68}
{'loss': 0.43, 'learning_rate': 2.311236342911644e-05, 'epoch': 0.68}
{'loss': 0.4166, 'learning_rate': 2.3071601921954794e-05, 'epoch': 0.68}
{'loss': 0.4103, 'learning_rate': 2.3030865607694675e-05, 'epoch': 0.68}
 68%|██████▊   | 2211/3250 [10:24:38<4:57:09, 17.16s/it]                                                         68%|██████▊   | 2211/3250 [10:24:38<4:57:09, 17.16s/it] 68%|██████▊   | 2212/3250 [10:24:54<4:50:20, 16.78s/it]                                                         68%|██████▊   | 2212/3250 [10:24:54<4:50:20, 16.78s/it] 68%|██████▊   | 2213/3250 [10:25:10<4:45:26, 16.52s/it]                                                         68%|██████▊   | 2213/3250 [10:25:10<4:45:26, 16.52s/it] 68%|██████▊   | 2214/3250 [10:25:26<4:44:07, 16.45s/it]                                                         68%|██████▊   | 2214/3250 [10:25:26<4:44:07, 16.45s/it] 68%|██████▊   | 2215/3250 [10:25:42<4:41:00, 16.29s/it]                                                         68%|██████▊   | 2215/3250 [10:25:42<4:41:00, 16.29s/it] 68%|██████{'loss': 0.4076, 'learning_rate': 2.2990154524447005e-05, 'epoch': 0.68}
{'loss': 0.4008, 'learning_rate': 2.2949468710299116e-05, 'epoch': 0.68}
{'loss': 0.4267, 'learning_rate': 2.2908808203314635e-05, 'epoch': 0.68}
{'loss': 0.4166, 'learning_rate': 2.2868173041533585e-05, 'epoch': 0.68}
{'loss': 0.4373, 'learning_rate': 2.2827563262972244e-05, 'epoch': 0.68}
▊   | 2216/3250 [10:25:58<4:38:43, 16.17s/it]                                                         68%|██████▊   | 2216/3250 [10:25:58<4:38:43, 16.17s/it] 68%|██████▊   | 2217/3250 [10:26:14<4:37:01, 16.09s/it]                                                         68%|██████▊   | 2217/3250 [10:26:14<4:37:01, 16.09s/it] 68%|██████▊   | 2218/3250 [10:26:29<4:35:47, 16.03s/it]                                                         68%|██████▊   | 2218/3250 [10:26:29<4:35:47, 16.03s/it] 68%|██████▊   | 2219/3250 [10:26:45<4:34:54, 16.00s/it]                                                         68%|██████▊   | 2219/3250 [10:26:45<4:34:54, 16.00s/it] 68%|██████▊   | 2220/3250 [10:27:01<4:34:05, 15.97s/it]                                                         68%|██████▊   | 2220/3250 [10:27:01<4:34:05, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.757694661617279, 'eval_runtime': 2.4807, 'eval_samples_per_second': 4.837, 'eval_steps_per_second': 1.209, 'epoch': 0.68}
                                                         68%|██████▊   | 2220/3250 [10:27:04<4:34:05, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2220
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2220 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2220
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9254, 'learning_rate': 2.278697890562316e-05, 'epoch': 0.68}
{'loss': 0.4258, 'learning_rate': 2.274642000745507e-05, 'epoch': 0.68}
{'loss': 0.4145, 'learning_rate': 2.270588660641294e-05, 'epoch': 0.68}
{'loss': 0.4354, 'learning_rate': 2.266537874041781e-05, 'epoch': 0.68}
{'loss': 0.4344, 'learning_rate': 2.262489644736689e-05, 'epoch': 0.68}
 68%|██████▊   | 2221/3250 [10:27:20<4:49:46, 16.90s/it]                                                         68%|██████▊   | 2221/3250 [10:27:20<4:49:46, 16.90s/it] 68%|██████▊   | 2222/3250 [10:27:36<4:44:16, 16.59s/it]                                                         68%|██████▊   | 2222/3250 [10:27:36<4:44:16, 16.59s/it] 68%|██████▊   | 2223/3250 [10:27:52<4:40:24, 16.38s/it]                                                         68%|██████▊   | 2223/3250 [10:27:52<4:40:24, 16.38s/it] 68%|██████▊   | 2224/3250 [10:28:08<4:37:40, 16.24s/it]                                                         68%|██████▊   | 2224/3250 [10:28:08<4:37:40, 16.24s/it] 68%|██████▊   | 2225/3250 [10:28:24<4:35:37, 16.13s/it]                                                         68%|██████▊   | 2225/3250 [10:28:24<4:35:37, 16.13s/it] 68%|██████{'loss': 0.4133, 'learning_rate': 2.2584439765133454e-05, 'epoch': 0.68}
{'loss': 0.4085, 'learning_rate': 2.2544008731566817e-05, 'epoch': 0.69}
{'loss': 0.487, 'learning_rate': 2.250360338449226e-05, 'epoch': 0.69}
{'loss': 0.4222, 'learning_rate': 2.246322376171109e-05, 'epoch': 0.69}
{'loss': 0.4191, 'learning_rate': 2.242286990100052e-05, 'epoch': 0.69}
▊   | 2226/3250 [10:28:40<4:34:11, 16.07s/it]                                                         68%|██████▊   | 2226/3250 [10:28:40<4:34:11, 16.07s/it] 69%|██████▊   | 2227/3250 [10:28:56<4:33:02, 16.01s/it]                                                         69%|██████▊   | 2227/3250 [10:28:56<4:33:02, 16.01s/it] 69%|██████▊   | 2228/3250 [10:29:12<4:32:14, 15.98s/it]                                                         69%|██████▊   | 2228/3250 [10:29:12<4:32:14, 15.98s/it] 69%|██████▊   | 2229/3250 [10:29:27<4:31:32, 15.96s/it]                                                         69%|██████▊   | 2229/3250 [10:29:28<4:31:32, 15.96s/it] 69%|██████▊   | 2230/3250 [10:29:44<4:34:06, 16.12s/it]                                                         69%|██████▊   | 2230/3250 [10:29:44<4:34:06, 16.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7515109181404114, 'eval_runtime': 2.475, 'eval_samples_per_second': 4.848, 'eval_steps_per_second': 1.212, 'epoch': 0.69}
                                                         69%|██████▊   | 2230/3250 [10:29:46<4:34:06, 16.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2230
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2230/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3823, 'learning_rate': 2.238254184011364e-05, 'epoch': 0.69}
{'loss': 0.4375, 'learning_rate': 2.2342239616779442e-05, 'epoch': 0.69}
{'loss': 0.4284, 'learning_rate': 2.2301963268702723e-05, 'epoch': 0.69}
{'loss': 0.4214, 'learning_rate': 2.226171283356409e-05, 'epoch': 0.69}
{'loss': 0.4083, 'learning_rate': 2.2221488349019903e-05, 'epoch': 0.69}
 69%|██████▊   | 2231/3250 [10:30:03<4:49:59, 17.07s/it]                                                         69%|██████▊   | 2231/3250 [10:30:03<4:49:59, 17.07s/it] 69%|██████▊   | 2232/3250 [10:30:19<4:43:46, 16.73s/it]                                                         69%|██████▊   | 2232/3250 [10:30:19<4:43:46, 16.73s/it] 69%|██████▊   | 2233/3250 [10:30:35<4:39:14, 16.47s/it]                                                         69%|██████▊   | 2233/3250 [10:30:35<4:39:14, 16.47s/it] 69%|██████▊   | 2234/3250 [10:30:51<4:36:06, 16.31s/it]                                                         69%|██████▊   | 2234/3250 [10:30:51<4:36:06, 16.31s/it] 69%|██████▉   | 2235/3250 [10:31:07<4:33:42, 16.18s/it]                                                         69%|██████▉   | 2235/3250 [10:31:07<4:33:42, 16.18s/it] 69%|██████{'loss': 0.4195, 'learning_rate': 2.2181289852702204e-05, 'epoch': 0.69}
{'loss': 0.4214, 'learning_rate': 2.214111738221877e-05, 'epoch': 0.69}
{'loss': 0.4399, 'learning_rate': 2.210097097515301e-05, 'epoch': 0.69}
{'loss': 0.4231, 'learning_rate': 2.2060850669063963e-05, 'epoch': 0.69}
{'loss': 0.414, 'learning_rate': 2.2020756501486233e-05, 'epoch': 0.69}
▉   | 2236/3250 [10:31:23<4:32:03, 16.10s/it]                                                         69%|██████▉   | 2236/3250 [10:31:23<4:32:03, 16.10s/it] 69%|██████▉   | 2237/3250 [10:31:39<4:30:53, 16.04s/it]                                                         69%|██████▉   | 2237/3250 [10:31:39<4:30:53, 16.04s/it] 69%|██████▉   | 2238/3250 [10:31:55<4:29:49, 16.00s/it]                                                         69%|██████▉   | 2238/3250 [10:31:55<4:29:49, 16.00s/it] 69%|██████▉   | 2239/3250 [10:32:11<4:28:59, 15.96s/it]                                                         69%|██████▉   | 2239/3250 [10:32:11<4:28:59, 15.96s/it] 69%|██████▉   | 2240/3250 [10:32:26<4:28:19, 15.94s/it]                                                         69%|██████▉   | 2240/3250 [10:32:26<4:28:19, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7507701516151428, 'eval_runtime': 2.4797, 'eval_samples_per_second': 4.839, 'eval_steps_per_second': 1.21, 'epoch': 0.69}
                                                         69%|██████▉   | 2240/3250 [10:32:29<4:28:19, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2240
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2240I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2240
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2240/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2240/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4346, 'learning_rate': 2.1980688509929997e-05, 'epoch': 0.69}
{'loss': 0.4134, 'learning_rate': 2.194064673188089e-05, 'epoch': 0.69}
{'loss': 0.4046, 'learning_rate': 2.1900631204800054e-05, 'epoch': 0.69}
{'loss': 0.4459, 'learning_rate': 2.1860641966124114e-05, 'epoch': 0.69}
{'loss': 0.4121, 'learning_rate': 2.1820679053265e-05, 'epoch': 0.69}
 69%|██████▉   | 2241/3250 [10:33:03<6:12:39, 22.16s/it]                                                         69%|██████▉   | 2241/3250 [10:33:03<6:12:39, 22.16s/it] 69%|██████▉   | 2242/3250 [10:33:19<5:40:33, 20.27s/it]                                                         69%|██████▉   | 2242/3250 [10:33:19<5:40:33, 20.27s/it] 69%|██████▉   | 2243/3250 [10:33:35<5:18:07, 18.95s/it]                                                         69%|██████▉   | 2243/3250 [10:33:35<5:18:07, 18.95s/it] 69%|██████▉   | 2244/3250 [10:33:51<5:02:23, 18.04s/it]                                                         69%|██████▉   | 2244/3250 [10:33:51<5:02:23, 18.04s/it] 69%|██████▉   | 2245/3250 [10:34:07<4:51:19, 17.39s/it]                                                         69%|██████▉   | 2245/3250 [10:34:07<4:51:19, 17.39s/it] 69%|██████{'loss': 0.4014, 'learning_rate': 2.1780742503610118e-05, 'epoch': 0.69}
{'loss': 0.4047, 'learning_rate': 2.1740832354522145e-05, 'epoch': 0.69}
{'loss': 0.4252, 'learning_rate': 2.1700948643339103e-05, 'epoch': 0.69}
{'loss': 0.415, 'learning_rate': 2.1661091407374218e-05, 'epoch': 0.69}
{'loss': 0.4261, 'learning_rate': 2.1621260683916007e-05, 'epoch': 0.69}
▉   | 2246/3250 [10:34:23<4:44:41, 17.01s/it]                                                         69%|██████▉   | 2246/3250 [10:34:23<4:44:41, 17.01s/it] 69%|██████▉   | 2247/3250 [10:34:39<4:38:43, 16.67s/it]                                                         69%|██████▉   | 2247/3250 [10:34:39<4:38:43, 16.67s/it] 69%|██████▉   | 2248/3250 [10:34:54<4:34:32, 16.44s/it]                                                         69%|██████▉   | 2248/3250 [10:34:54<4:34:32, 16.44s/it] 69%|██████▉   | 2249/3250 [10:35:10<4:31:28, 16.27s/it]                                                         69%|██████▉   | 2249/3250 [10:35:10<4:31:28, 16.27s/it] 69%|██████▉   | 2250/3250 [10:35:26<4:29:17, 16.16s/it]                                                         69%|██████▉   | 2250/3250 [10:35:26<4:29:17, 16.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7572707533836365, 'eval_runtime': 2.4701, 'eval_samples_per_second': 4.858, 'eval_steps_per_second': 1.215, 'epoch': 0.69}
                                                         69%|██████▉   | 2250/3250 [10:35:29<4:29:17, 16.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2250
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2250 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2250

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9515, 'learning_rate': 2.1581456510228147e-05, 'epoch': 0.69}
{'loss': 0.3933, 'learning_rate': 2.1541678923549507e-05, 'epoch': 0.69}
{'loss': 0.4028, 'learning_rate': 2.1501927961094054e-05, 'epoch': 0.69}
{'loss': 0.4386, 'learning_rate': 2.1462203660050882e-05, 'epoch': 0.69}
{'loss': 0.4318, 'learning_rate': 2.1422506057584078e-05, 'epoch': 0.69}
 69%|██████▉   | 2251/3250 [10:35:59<5:53:15, 21.22s/it]                                                         69%|██████▉   | 2251/3250 [10:35:59<5:53:15, 21.22s/it] 69%|██████▉   | 2252/3250 [10:36:15<5:26:31, 19.63s/it]                                                         69%|██████▉   | 2252/3250 [10:36:15<5:26:31, 19.63s/it] 69%|██████▉   | 2253/3250 [10:36:31<5:07:49, 18.52s/it]                                                         69%|██████▉   | 2253/3250 [10:36:31<5:07:49, 18.52s/it] 69%|██████▉   | 2254/3250 [10:36:47<4:54:34, 17.75s/it]                                                         69%|██████▉   | 2254/3250 [10:36:47<4:54:34, 17.75s/it] 69%|██████▉   | 2255/3250 [10:37:03<4:46:50, 17.30s/it]                                                         69%|██████▉   | 2255/3250 [10:37:03<4:46:50, 17.30s/it] 69%|██████{'loss': 0.4151, 'learning_rate': 2.1382835190832813e-05, 'epoch': 0.69}
{'loss': 0.3979, 'learning_rate': 2.134319109691122e-05, 'epoch': 0.69}
{'loss': 0.4622, 'learning_rate': 2.1303573812908385e-05, 'epoch': 0.69}
{'loss': 0.4296, 'learning_rate': 2.126398337588834e-05, 'epoch': 0.7}
{'loss': 0.4026, 'learning_rate': 2.122441982288994e-05, 'epoch': 0.7}
▉   | 2256/3250 [10:37:19<4:39:31, 16.87s/it]                                                         69%|██████▉   | 2256/3250 [10:37:19<4:39:31, 16.87s/it] 69%|██████▉   | 2257/3250 [10:37:35<4:34:15, 16.57s/it]                                                         69%|██████▉   | 2257/3250 [10:37:35<4:34:15, 16.57s/it] 69%|██████▉   | 2258/3250 [10:37:51<4:31:28, 16.42s/it]                                                         69%|██████▉   | 2258/3250 [10:37:51<4:31:28, 16.42s/it] 70%|██████▉   | 2259/3250 [10:38:07<4:28:28, 16.25s/it]                                                         70%|██████▉   | 2259/3250 [10:38:07<4:28:28, 16.25s/it] 70%|██████▉   | 2260/3250 [10:38:23<4:26:22, 16.14s/it]                                                         70%|██████▉   | 2260/3250 [10:38:23<4:26:22, 16.14s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7497641444206238, 'eval_runtime': 2.4896, 'eval_samples_per_second': 4.82, 'eval_steps_per_second': 1.205, 'epoch': 0.7}
                                                         70%|██████▉   | 2260/3250 [10:38:25<4:26:22, 16.14s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2260
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2260
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2260

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3967, 'learning_rate': 2.1184883190926954e-05, 'epoch': 0.7}
{'loss': 0.4375, 'learning_rate': 2.11453735169879e-05, 'epoch': 0.7}
{'loss': 0.4146, 'learning_rate': 2.110589083803613e-05, 'epoch': 0.7}
{'loss': 0.4196, 'learning_rate': 2.1066435191009715e-05, 'epoch': 0.7}
{'loss': 0.4041, 'learning_rate': 2.1027006612821453e-05, 'epoch': 0.7}
 70%|██████▉   | 2261/3250 [10:38:42<4:41:39, 17.09s/it]                                                         70%|██████▉   | 2261/3250 [10:38:42<4:41:39, 17.09s/it] 70%|██████▉   | 2262/3250 [10:38:58<4:35:21, 16.72s/it]                                                         70%|██████▉   | 2262/3250 [10:38:58<4:35:21, 16.72s/it] 70%|██████▉   | 2263/3250 [10:39:15<4:33:54, 16.65s/it]                                                         70%|██████▉   | 2263/3250 [10:39:15<4:33:54, 16.65s/it] 70%|██████▉   | 2264/3250 [10:39:30<4:29:46, 16.42s/it]                                                         70%|██████▉   | 2264/3250 [10:39:30<4:29:46, 16.42s/it] 70%|██████▉   | 2265/3250 [10:39:46<4:26:44, 16.25s/it]                                                         70%|██████▉   | 2265/3250 [10:39:46<4:26:44, 16.25s/it] 70%|██████{'loss': 0.4201, 'learning_rate': 2.0987605140358824e-05, 'epoch': 0.7}
{'loss': 0.4289, 'learning_rate': 2.0948230810483888e-05, 'epoch': 0.7}
{'loss': 0.4388, 'learning_rate': 2.0908883660033374e-05, 'epoch': 0.7}
{'loss': 0.4173, 'learning_rate': 2.0869563725818575e-05, 'epoch': 0.7}
{'loss': 0.4331, 'learning_rate': 2.08302710446253e-05, 'epoch': 0.7}
▉   | 2266/3250 [10:40:02<4:24:34, 16.13s/it]                                                         70%|██████▉   | 2266/3250 [10:40:02<4:24:34, 16.13s/it] 70%|██████▉   | 2267/3250 [10:40:18<4:22:58, 16.05s/it]                                                         70%|██████▉   | 2267/3250 [10:40:18<4:22:58, 16.05s/it] 70%|██████▉   | 2268/3250 [10:40:34<4:21:47, 16.00s/it]                                                         70%|██████▉   | 2268/3250 [10:40:34<4:21:47, 16.00s/it] 70%|██████▉   | 2269/3250 [10:40:50<4:20:53, 15.96s/it]                                                         70%|██████▉   | 2269/3250 [10:40:50<4:20:53, 15.96s/it] 70%|██████▉   | 2270/3250 [10:41:06<4:20:06, 15.93s/it]                                                         70%|██████▉   | 2270/3250 [10:41:06<4:20:06, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7521719336509705, 'eval_runtime': 2.4685, 'eval_samples_per_second': 4.861, 'eval_steps_per_second': 1.215, 'epoch': 0.7}
                                                         70%|██████▉   | 2270/3250 [10:41:08<4:20:06, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2270
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2270I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2270
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2270

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4189, 'learning_rate': 2.0791005653213884e-05, 'epoch': 0.7}
{'loss': 0.4351, 'learning_rate': 2.075176758831913e-05, 'epoch': 0.7}
{'loss': 0.3745, 'learning_rate': 2.0712556886650235e-05, 'epoch': 0.7}
{'loss': 0.4553, 'learning_rate': 2.067337358489085e-05, 'epoch': 0.7}
{'loss': 0.4049, 'learning_rate': 2.0634217719698955e-05, 'epoch': 0.7}
 70%|██████▉   | 2271/3250 [10:41:52<6:47:21, 24.97s/it]                                                         70%|██████▉   | 2271/3250 [10:41:52<6:47:21, 24.97s/it] 70%|██████▉   | 2272/3250 [10:42:08<6:02:25, 22.23s/it]                                                         70%|██████▉   | 2272/3250 [10:42:08<6:02:25, 22.23s/it] 70%|██████▉   | 2273/3250 [10:42:23<5:30:52, 20.32s/it]                                                         70%|██████▉   | 2273/3250 [10:42:23<5:30:52, 20.32s/it] 70%|██████▉   | 2274/3250 [10:42:39<5:08:50, 18.99s/it]                                                         70%|██████▉   | 2274/3250 [10:42:39<5:08:50, 18.99s/it] 70%|███████   | 2275/3250 [10:42:55<4:53:22, 18.05s/it]                                                         70%|███████   | 2275/3250 [10:42:55<4:53:22, 18.05s/it] 70%|██████{'loss': 0.4173, 'learning_rate': 2.059508932770689e-05, 'epoch': 0.7}
{'loss': 0.3906, 'learning_rate': 2.055598844552129e-05, 'epoch': 0.7}
{'loss': 0.4104, 'learning_rate': 2.0516915109723e-05, 'epoch': 0.7}
{'loss': 0.4232, 'learning_rate': 2.0477869356867186e-05, 'epoch': 0.7}
{'loss': 0.414, 'learning_rate': 2.043885122348311e-05, 'epoch': 0.7}
█   | 2276/3250 [10:43:11<4:42:21, 17.39s/it]                                                         70%|███████   | 2276/3250 [10:43:11<4:42:21, 17.39s/it] 70%|███████   | 2277/3250 [10:43:27<4:34:40, 16.94s/it]                                                         70%|███████   | 2277/3250 [10:43:27<4:34:40, 16.94s/it] 70%|███████   | 2278/3250 [10:43:43<4:29:10, 16.62s/it]                                                         70%|███████   | 2278/3250 [10:43:43<4:29:10, 16.62s/it] 70%|███████   | 2279/3250 [10:43:59<4:26:30, 16.47s/it]                                                         70%|███████   | 2279/3250 [10:43:59<4:26:30, 16.47s/it] 70%|███████   | 2280/3250 [10:44:15<4:23:19, 16.29s/it]                                                         70%|███████   | 2280/3250 [10:44:15<4:23:19, 16.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7563730478286743, 'eval_runtime': 2.4709, 'eval_samples_per_second': 4.857, 'eval_steps_per_second': 1.214, 'epoch': 0.7}
                                                         70%|███████   | 2280/3250 [10:44:17<4:23:19, 16.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2280
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2280/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2280

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4184, 'learning_rate': 2.0399860746074262e-05, 'epoch': 0.7}
{'loss': 0.9279, 'learning_rate': 2.0360897961118248e-05, 'epoch': 0.7}
{'loss': 0.4071, 'learning_rate': 2.0321962905066748e-05, 'epoch': 0.7}
{'loss': 0.4312, 'learning_rate': 2.0283055614345532e-05, 'epoch': 0.7}
{'loss': 0.4143, 'learning_rate': 2.024417612535433e-05, 'epoch': 0.7}
 70%|███████   | 2281/3250 [10:44:34<4:36:31, 17.12s/it]                                                         70%|███████   | 2281/3250 [10:44:34<4:36:31, 17.12s/it] 70%|███████   | 2282/3250 [10:44:50<4:30:03, 16.74s/it]                                                         70%|███████   | 2282/3250 [10:44:50<4:30:03, 16.74s/it] 70%|███████   | 2283/3250 [10:45:05<4:25:33, 16.48s/it]                                                         70%|███████   | 2283/3250 [10:45:05<4:25:33, 16.48s/it] 70%|███████   | 2284/3250 [10:45:21<4:22:19, 16.29s/it]                                                         70%|███████   | 2284/3250 [10:45:21<4:22:19, 16.29s/it] 70%|███████   | 2285/3250 [10:45:37<4:20:03, 16.17s/it]                                                         70%|███████   | 2285/3250 [10:45:37<4:20:03, 16.17s/it] 70%|██████{'loss': 0.4205, 'learning_rate': 2.020532447446693e-05, 'epoch': 0.7}
{'loss': 0.3919, 'learning_rate': 2.016650069803105e-05, 'epoch': 0.7}
{'loss': 0.4644, 'learning_rate': 2.012770483236832e-05, 'epoch': 0.7}
{'loss': 0.4367, 'learning_rate': 2.008893691377428e-05, 'epoch': 0.7}
{'loss': 0.4068, 'learning_rate': 2.005019697851832e-05, 'epoch': 0.7}
█   | 2286/3250 [10:45:53<4:18:20, 16.08s/it]                                                         70%|███████   | 2286/3250 [10:45:53<4:18:20, 16.08s/it] 70%|███████   | 2287/3250 [10:46:09<4:17:02, 16.01s/it]                                                         70%|███████   | 2287/3250 [10:46:09<4:17:02, 16.01s/it] 70%|███████   | 2288/3250 [10:46:25<4:16:03, 15.97s/it]                                                         70%|███████   | 2288/3250 [10:46:25<4:16:03, 15.97s/it] 70%|███████   | 2289/3250 [10:46:41<4:15:16, 15.94s/it]                                                         70%|███████   | 2289/3250 [10:46:41<4:15:16, 15.94s/it] 70%|███████   | 2290/3250 [10:46:57<4:14:40, 15.92s/it]                                                         70%|███████   | 2290/3250 [10:46:57<4:14:40, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7516514658927917, 'eval_runtime': 2.7186, 'eval_samples_per_second': 4.414, 'eval_steps_per_second': 1.104, 'epoch': 0.7}
                                                         70%|███████   | 2290/3250 [10:46:59<4:14:40, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2290
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2290
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2290
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2290

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4093, 'learning_rate': 2.001148506284361e-05, 'epoch': 0.7}
{'loss': 0.4213, 'learning_rate': 1.9972801202967162e-05, 'epoch': 0.71}
{'loss': 0.411, 'learning_rate': 1.9934145435079702e-05, 'epoch': 0.71}
{'loss': 0.4136, 'learning_rate': 1.989551779534571e-05, 'epoch': 0.71}
{'loss': 0.4073, 'learning_rate': 1.985691831990333e-05, 'epoch': 0.71}
 70%|███████   | 2291/3250 [10:47:16<4:30:35, 16.93s/it]                                                         70%|███████   | 2291/3250 [10:47:16<4:30:35, 16.93s/it] 71%|███████   | 2292/3250 [10:47:32<4:25:09, 16.61s/it]                                                         71%|███████   | 2292/3250 [10:47:32<4:25:09, 16.61s/it] 71%|███████   | 2293/3250 [10:47:48<4:21:26, 16.39s/it]                                                         71%|███████   | 2293/3250 [10:47:48<4:21:26, 16.39s/it] 71%|███████   | 2294/3250 [10:48:03<4:18:39, 16.23s/it]                                                         71%|███████   | 2294/3250 [10:48:03<4:18:39, 16.23s/it] 71%|███████   | 2295/3250 [10:48:19<4:16:52, 16.14s/it]                                                         71%|███████   | 2295/3250 [10:48:19<4:16:52, 16.14s/it] 71%|██████{'loss': 0.4047, 'learning_rate': 1.9818347044864328e-05, 'epoch': 0.71}
{'loss': 0.4185, 'learning_rate': 1.9779804006314147e-05, 'epoch': 0.71}
{'loss': 0.4358, 'learning_rate': 1.9741289240311755e-05, 'epoch': 0.71}
{'loss': 0.4238, 'learning_rate': 1.9702802782889706e-05, 'epoch': 0.71}
{'loss': 0.4211, 'learning_rate': 1.9664344670054067e-05, 'epoch': 0.71}
█   | 2296/3250 [10:48:36<4:17:08, 16.17s/it]                                                         71%|███████   | 2296/3250 [10:48:36<4:17:08, 16.17s/it] 71%|███████   | 2297/3250 [10:48:51<4:15:26, 16.08s/it]                                                         71%|███████   | 2297/3250 [10:48:51<4:15:26, 16.08s/it] 71%|███████   | 2298/3250 [10:49:07<4:14:12, 16.02s/it]                                                         71%|███████   | 2298/3250 [10:49:07<4:14:12, 16.02s/it] 71%|███████   | 2299/3250 [10:49:23<4:13:11, 15.97s/it]                                                         71%|███████   | 2299/3250 [10:49:23<4:13:11, 15.97s/it] 71%|███████   | 2300/3250 [10:49:39<4:12:27, 15.94s/it]                                                         71%|███████   | 2300/3250 [10:49:39<4:12:27, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7528572082519531, 'eval_runtime': 2.4758, 'eval_samples_per_second': 4.847, 'eval_steps_per_second': 1.212, 'epoch': 0.71}
                                                         71%|███████   | 2300/3250 [10:49:42<4:12:27, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2300
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2300
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2300
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2300

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2300/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2300/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2300/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4184, 'learning_rate': 1.962591493778438e-05, 'epoch': 0.71}
{'loss': 0.4219, 'learning_rate': 1.9587513622033648e-05, 'epoch': 0.71}
{'loss': 0.3912, 'learning_rate': 1.9549140758728246e-05, 'epoch': 0.71}
{'loss': 0.4235, 'learning_rate': 1.951079638376798e-05, 'epoch': 0.71}
{'loss': 0.4107, 'learning_rate': 1.9472480533025984e-05, 'epoch': 0.71}
 71%|███████   | 2301/3250 [10:49:58<4:27:06, 16.89s/it]                                                         71%|███████   | 2301/3250 [10:49:58<4:27:06, 16.89s/it] 71%|███████   | 2302/3250 [10:50:14<4:21:58, 16.58s/it]                                                         71%|███████   | 2302/3250 [10:50:14<4:21:58, 16.58s/it] 71%|███████   | 2303/3250 [10:50:30<4:18:10, 16.36s/it]                                                         71%|███████   | 2303/3250 [10:50:30<4:18:10, 16.36s/it] 71%|███████   | 2304/3250 [10:50:46<4:15:29, 16.20s/it]                                                         71%|███████   | 2304/3250 [10:50:46<4:15:29, 16.20s/it] 71%|███████   | 2305/3250 [10:51:02<4:13:39, 16.11s/it]                                                         71%|███████   | 2305/3250 [10:51:02<4:13:39, 16.11s/it] 71%|██████{'loss': 0.4037, 'learning_rate': 1.9434193242348708e-05, 'epoch': 0.71}
{'loss': 0.4052, 'learning_rate': 1.9395934547555878e-05, 'epoch': 0.71}
{'loss': 0.3895, 'learning_rate': 1.9357704484440498e-05, 'epoch': 0.71}
{'loss': 0.4211, 'learning_rate': 1.931950308876871e-05, 'epoch': 0.71}
{'loss': 0.4189, 'learning_rate': 1.9281330396279912e-05, 'epoch': 0.71}
█   | 2306/3250 [10:51:17<4:12:08, 16.03s/it]                                                         71%|███████   | 2306/3250 [10:51:17<4:12:08, 16.03s/it] 71%|███████   | 2307/3250 [10:51:33<4:11:03, 15.97s/it]                                                         71%|███████   | 2307/3250 [10:51:33<4:11:03, 15.97s/it] 71%|███████   | 2308/3250 [10:51:49<4:10:14, 15.94s/it]                                                         71%|███████   | 2308/3250 [10:51:49<4:10:14, 15.94s/it] 71%|███████   | 2309/3250 [10:52:05<4:09:35, 15.91s/it]                                                         71%|███████   | 2309/3250 [10:52:05<4:09:35, 15.91s/it] 71%|███████   | 2310/3250 [10:52:21<4:09:05, 15.90s/it]                                                         71%|███████   | 2310/3250 [10:52:21<4:09:05, 15.90s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7588144540786743, 'eval_runtime': 2.4589, 'eval_samples_per_second': 4.88, 'eval_steps_per_second': 1.22, 'epoch': 0.71}
                                                         71%|███████   | 2310/3250 [10:52:23<4:09:05, 15.90s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2310
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2310
the checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2310
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2310

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4222, 'learning_rate': 1.9243186442686615e-05, 'epoch': 0.71}
{'loss': 0.9258, 'learning_rate': 1.920507126367448e-05, 'epoch': 0.71}
{'loss': 0.4272, 'learning_rate': 1.916698489490216e-05, 'epoch': 0.71}
{'loss': 0.4055, 'learning_rate': 1.9128927372001454e-05, 'epoch': 0.71}
{'loss': 0.4335, 'learning_rate': 1.9090898730577133e-05, 'epoch': 0.71}
 71%|███████   | 2311/3250 [10:52:40<4:24:34, 16.91s/it]                                                         71%|███████   | 2311/3250 [10:52:40<4:24:34, 16.91s/it] 71%|███████   | 2312/3250 [10:52:57<4:22:14, 16.77s/it]                                                         71%|███████   | 2312/3250 [10:52:57<4:22:14, 16.77s/it] 71%|███████   | 2313/3250 [10:53:13<4:18:11, 16.53s/it]                                                         71%|███████   | 2313/3250 [10:53:13<4:18:11, 16.53s/it] 71%|███████   | 2314/3250 [10:53:29<4:15:01, 16.35s/it]                                                         71%|███████   | 2314/3250 [10:53:29<4:15:01, 16.35s/it] 71%|███████   | 2315/3250 [10:53:44<4:12:28, 16.20s/it]                                                         71%|███████   | 2315/3250 [10:53:44<4:12:28, 16.20s/it] 71%|██████{'loss': 0.427, 'learning_rate': 1.905289900620692e-05, 'epoch': 0.71}
{'loss': 0.4031, 'learning_rate': 1.9014928234441525e-05, 'epoch': 0.71}
{'loss': 0.4053, 'learning_rate': 1.897698645080456e-05, 'epoch': 0.71}
{'loss': 0.4771, 'learning_rate': 1.893907369079252e-05, 'epoch': 0.71}
{'loss': 0.4252, 'learning_rate': 1.8901189989874745e-05, 'epoch': 0.71}
█▏  | 2316/3250 [10:54:00<4:10:40, 16.10s/it]                                                         71%|███████▏  | 2316/3250 [10:54:00<4:10:40, 16.10s/it] 71%|███████▏  | 2317/3250 [10:54:16<4:09:15, 16.03s/it]                                                         71%|███████▏  | 2317/3250 [10:54:16<4:09:15, 16.03s/it] 71%|███████▏  | 2318/3250 [10:54:32<4:08:10, 15.98s/it]                                                         71%|███████▏  | 2318/3250 [10:54:32<4:08:10, 15.98s/it] 71%|███████▏  | 2319/3250 [10:54:48<4:07:21, 15.94s/it]                                                         71%|███████▏  | 2319/3250 [10:54:48<4:07:21, 15.94s/it] 71%|███████▏  | 2320/3250 [10:55:04<4:06:43, 15.92s/it]                                                         71%|███████▏  | 2320/3250 [10:55:04<4:06:43, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7515496015548706, 'eval_runtime': 2.5841, 'eval_samples_per_second': 4.644, 'eval_steps_per_second': 1.161, 'epoch': 0.71}
                                                         71%|███████▏  | 2320/3250 [10:55:06<4:06:43, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2320
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2320
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2320
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2320

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.412, 'learning_rate': 1.886333538349337e-05, 'epoch': 0.71}
{'loss': 0.379, 'learning_rate': 1.8825509907063327e-05, 'epoch': 0.71}
{'loss': 0.4309, 'learning_rate': 1.8787713595972306e-05, 'epoch': 0.71}
{'loss': 0.4234, 'learning_rate': 1.8749946485580693e-05, 'epoch': 0.72}
{'loss': 0.4099, 'learning_rate': 1.8712208611221572e-05, 'epoch': 0.72}
 71%|███████▏  | 2321/3250 [10:55:38<5:29:38, 21.29s/it]                                                         71%|███████▏  | 2321/3250 [10:55:38<5:29:38, 21.29s/it] 71%|███████▏  | 2322/3250 [10:55:53<5:04:05, 19.66s/it]                                                         71%|███████▏  | 2322/3250 [10:55:53<5:04:05, 19.66s/it] 71%|███████▏  | 2323/3250 [10:56:09<4:46:12, 18.53s/it]                                                         71%|███████▏  | 2323/3250 [10:56:09<4:46:12, 18.53s/it] 72%|███████▏  | 2324/3250 [10:56:25<4:33:34, 17.73s/it]                                                         72%|███████▏  | 2324/3250 [10:56:25<4:33:34, 17.73s/it] 72%|███████▏  | 2325/3250 [10:56:41<4:24:39, 17.17s/it]                                                         72%|███████▏  | 2325/3250 [10:56:41<4:24:39, 17.17s/it] 72{'loss': 0.3998, 'learning_rate': 1.8674500008200674e-05, 'epoch': 0.72}
{'loss': 0.4096, 'learning_rate': 1.8636820711796306e-05, 'epoch': 0.72}
{'loss': 0.4186, 'learning_rate': 1.8599170757259406e-05, 'epoch': 0.72}
{'loss': 0.4373, 'learning_rate': 1.856155017981345e-05, 'epoch': 0.72}
{'loss': 0.4135, 'learning_rate': 1.8523959014654407e-05, 'epoch': 0.72}
%|███████▏  | 2326/3250 [10:56:57<4:18:22, 16.78s/it]                                                         72%|███████▏  | 2326/3250 [10:56:57<4:18:22, 16.78s/it] 72%|███████▏  | 2327/3250 [10:57:13<4:13:55, 16.51s/it]                                                         72%|███████▏  | 2327/3250 [10:57:13<4:13:55, 16.51s/it] 72%|███████▏  | 2328/3250 [10:57:29<4:12:31, 16.43s/it]                                                         72%|███████▏  | 2328/3250 [10:57:29<4:12:31, 16.43s/it] 72%|███████▏  | 2329/3250 [10:57:45<4:10:24, 16.31s/it]                                                         72%|███████▏  | 2329/3250 [10:57:45<4:10:24, 16.31s/it] 72%|███████▏  | 2330/3250 [10:58:01<4:08:08, 16.18s/it]                                                         72%|███████▏  | 2330/3250 [10:58:01<4:08:08, 16.18s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7536894083023071, 'eval_runtime': 2.4733, 'eval_samples_per_second': 4.852, 'eval_steps_per_second': 1.213, 'epoch': 0.72}
                                                         72%|███████▏  | 2330/3250 [10:58:03<4:08:08, 16.18s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2330
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2330
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2330
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2330

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4059, 'learning_rate': 1.8486397296950745e-05, 'epoch': 0.72}
{'loss': 0.4325, 'learning_rate': 1.8448865061843397e-05, 'epoch': 0.72}
{'loss': 0.4142, 'learning_rate': 1.8411362344445708e-05, 'epoch': 0.72}
{'loss': 0.4098, 'learning_rate': 1.8373889179843374e-05, 'epoch': 0.72}
{'loss': 0.4273, 'learning_rate': 1.833644560309447e-05, 'epoch': 0.72}
 72%|███████▏  | 2331/3250 [10:58:39<5:46:48, 22.64s/it]                                                         72%|███████▏  | 2331/3250 [10:58:39<5:46:48, 22.64s/it] 72%|███████▏  | 2332/3250 [10:58:54<5:15:19, 20.61s/it]                                                         72%|███████▏  | 2332/3250 [10:58:54<5:15:19, 20.61s/it] 72%|███████▏  | 2333/3250 [10:59:10<4:53:07, 19.18s/it]                                                         72%|███████▏  | 2333/3250 [10:59:10<4:53:07, 19.18s/it] 72%|███████▏  | 2334/3250 [10:59:26<4:37:34, 18.18s/it]                                                         72%|███████▏  | 2334/3250 [10:59:26<4:37:34, 18.18s/it] 72%|███████▏  | 2335/3250 [10:59:42<4:26:39, 17.49s/it]                                                         72%|███████▏  | 2335/3250 [10:59:42<4:26:39, 17.49s/it] 72{'loss': 0.3957, 'learning_rate': 1.8299031649229402e-05, 'epoch': 0.72}
{'loss': 0.3944, 'learning_rate': 1.8261647353250842e-05, 'epoch': 0.72}
{'loss': 0.3975, 'learning_rate': 1.8224292750133743e-05, 'epoch': 0.72}
{'loss': 0.409, 'learning_rate': 1.8186967874825217e-05, 'epoch': 0.72}
{'loss': 0.4211, 'learning_rate': 1.8149672762244624e-05, 'epoch': 0.72}
%|███████▏  | 2336/3250 [10:59:58<4:18:55, 17.00s/it]                                                         72%|███████▏  | 2336/3250 [10:59:58<4:18:55, 17.00s/it] 72%|███████▏  | 2337/3250 [11:00:14<4:13:22, 16.65s/it]                                                         72%|███████▏  | 2337/3250 [11:00:14<4:13:22, 16.65s/it] 72%|███████▏  | 2338/3250 [11:00:30<4:09:25, 16.41s/it]                                                         72%|███████▏  | 2338/3250 [11:00:30<4:09:25, 16.41s/it] 72%|███████▏  | 2339/3250 [11:00:45<4:06:36, 16.24s/it]                                                         72%|███████▏  | 2339/3250 [11:00:45<4:06:36, 16.24s/it] 72%|███████▏  | 2340/3250 [11:01:01<4:04:36, 16.13s/it]                                                         72%|███████▏  | 2340/3250 [11:01:01<4:04:36, 16.13s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7583176493644714, 'eval_runtime': 2.4827, 'eval_samples_per_second': 4.833, 'eval_steps_per_second': 1.208, 'epoch': 0.72}
                                                         72%|███████▏  | 2340/3250 [11:01:04<4:04:36, 16.13s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2340
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2340/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2340

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.422, 'learning_rate': 1.8112407447283465e-05, 'epoch': 0.72}
{'loss': 0.9336, 'learning_rate': 1.8075171964805354e-05, 'epoch': 0.72}
{'loss': 0.3898, 'learning_rate': 1.8037966349646e-05, 'epoch': 0.72}
{'loss': 0.4026, 'learning_rate': 1.8000790636613197e-05, 'epoch': 0.72}
{'loss': 0.4309, 'learning_rate': 1.7963644860486706e-05, 'epoch': 0.72}
 72%|███████▏  | 2341/3250 [11:01:20<4:17:42, 17.01s/it]                                                         72%|███████▏  | 2341/3250 [11:01:20<4:17:42, 17.01s/it] 72%|███████▏  | 2342/3250 [11:01:36<4:12:07, 16.66s/it]                                                         72%|███████▏  | 2342/3250 [11:01:36<4:12:07, 16.66s/it] 72%|███████▏  | 2343/3250 [11:01:52<4:08:14, 16.42s/it]                                                         72%|███████▏  | 2343/3250 [11:01:52<4:08:14, 16.42s/it] 72%|███████▏  | 2344/3250 [11:02:08<4:05:24, 16.25s/it]                                                         72%|███████▏  | 2344/3250 [11:02:08<4:05:24, 16.25s/it] 72%|███████▏  | 2345/3250 [11:02:25<4:07:21, 16.40s/it]                                                         72%|███████▏  | 2345/3250 [11:02:25<4:07:21, 16.40s/it] 72{'loss': 0.4315, 'learning_rate': 1.7926529056018298e-05, 'epoch': 0.72}
{'loss': 0.4046, 'learning_rate': 1.7889443257931737e-05, 'epoch': 0.72}
{'loss': 0.4149, 'learning_rate': 1.785238750092269e-05, 'epoch': 0.72}
{'loss': 0.4605, 'learning_rate': 1.7815361819658732e-05, 'epoch': 0.72}
{'loss': 0.4299, 'learning_rate': 1.777836624877929e-05, 'epoch': 0.72}
%|███████▏  | 2346/3250 [11:02:41<4:04:42, 16.24s/it]                                                         72%|███████▏  | 2346/3250 [11:02:41<4:04:42, 16.24s/it] 72%|███████▏  | 2347/3250 [11:02:56<4:02:42, 16.13s/it]                                                         72%|███████▏  | 2347/3250 [11:02:56<4:02:42, 16.13s/it] 72%|███████▏  | 2348/3250 [11:03:12<4:01:12, 16.05s/it]                                                         72%|███████▏  | 2348/3250 [11:03:12<4:01:12, 16.05s/it] 72%|███████▏  | 2349/3250 [11:03:28<4:00:05, 15.99s/it]                                                         72%|███████▏  | 2349/3250 [11:03:28<4:00:05, 15.99s/it] 72%|███████▏  | 2350/3250 [11:03:44<3:59:13, 15.95s/it]                                                         72%|███████▏  | 2350/3250 [11:03:44<3:59:13, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.75152987241745, 'eval_runtime': 2.4855, 'eval_samples_per_second': 4.828, 'eval_steps_per_second': 1.207, 'epoch': 0.72}
                                                         72%|███████▏  | 2350/3250 [11:03:46<3:59:13, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2350
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2350 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2350
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2350

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4033, 'learning_rate': 1.774140082289563e-05, 'epoch': 0.72}
{'loss': 0.3947, 'learning_rate': 1.770446557659079e-05, 'epoch': 0.72}
{'loss': 0.4392, 'learning_rate': 1.76675605444196e-05, 'epoch': 0.72}
{'loss': 0.4222, 'learning_rate': 1.7630685760908622e-05, 'epoch': 0.72}
{'loss': 0.4228, 'learning_rate': 1.7593841260556103e-05, 'epoch': 0.72}
 72%|███████▏  | 2351/3250 [11:04:03<4:13:42, 16.93s/it]                                                         72%|███████▏  | 2351/3250 [11:04:03<4:13:42, 16.93s/it] 72%|███████▏  | 2352/3250 [11:04:19<4:08:39, 16.61s/it]                                                         72%|███████▏  | 2352/3250 [11:04:19<4:08:39, 16.61s/it] 72%|███████▏  | 2353/3250 [11:04:35<4:05:04, 16.39s/it]                                                         72%|███████▏  | 2353/3250 [11:04:35<4:05:04, 16.39s/it] 72%|███████▏  | 2354/3250 [11:04:51<4:02:25, 16.23s/it]                                                         72%|███████▏  | 2354/3250 [11:04:51<4:02:25, 16.23s/it] 72%|███████▏  | 2355/3250 [11:05:07<4:00:32, 16.13s/it]                                                         72%|███████▏  | 2355/3250 [11:05:07<4:00:32, 16.13s/it] 72{'loss': 0.4079, 'learning_rate': 1.7557027077832e-05, 'epoch': 0.72}
{'loss': 0.4117, 'learning_rate': 1.7520243247177824e-05, 'epoch': 0.73}
{'loss': 0.4217, 'learning_rate': 1.7483489803006776e-05, 'epoch': 0.73}
{'loss': 0.4393, 'learning_rate': 1.7446766779703576e-05, 'epoch': 0.73}
{'loss': 0.4213, 'learning_rate': 1.7410074211624518e-05, 'epoch': 0.73}
%|███████▏  | 2356/3250 [11:05:23<3:59:02, 16.04s/it]                                                         72%|███████▏  | 2356/3250 [11:05:23<3:59:02, 16.04s/it] 73%|███████▎  | 2357/3250 [11:05:38<3:57:58, 15.99s/it]                                                         73%|███████▎  | 2357/3250 [11:05:38<3:57:58, 15.99s/it] 73%|███████▎  | 2358/3250 [11:05:54<3:57:08, 15.95s/it]                                                         73%|███████▎  | 2358/3250 [11:05:54<3:57:08, 15.95s/it] 73%|███████▎  | 2359/3250 [11:06:10<3:56:28, 15.92s/it]                                                         73%|███████▎  | 2359/3250 [11:06:10<3:56:28, 15.92s/it] 73%|███████▎  | 2360/3250 [11:06:26<3:55:55, 15.90s/it]                                                         73%|███████▎  | 2360/3250 [11:06:26<3:55:55, 15.90s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7528186440467834, 'eval_runtime': 2.4666, 'eval_samples_per_second': 4.865, 'eval_steps_per_second': 1.216, 'epoch': 0.73}
                                                         73%|███████▎  | 2360/3250 [11:06:28<3:55:55, 15.90s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2360
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2360
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2360
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2360

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4253, 'learning_rate': 1.7373412133097372e-05, 'epoch': 0.73}
{'loss': 0.4112, 'learning_rate': 1.733678057842142e-05, 'epoch': 0.73}
{'loss': 0.42, 'learning_rate': 1.730017958186735e-05, 'epoch': 0.73}
{'loss': 0.3782, 'learning_rate': 1.726360917767726e-05, 'epoch': 0.73}
{'loss': 0.4388, 'learning_rate': 1.722706940006466e-05, 'epoch': 0.73}
 73%|███████▎  | 2361/3250 [11:06:48<4:21:19, 17.64s/it]                                                         73%|███████▎  | 2361/3250 [11:06:48<4:21:19, 17.64s/it] 73%|███████▎  | 2362/3250 [11:07:05<4:18:23, 17.46s/it]                                                         73%|███████▎  | 2362/3250 [11:07:05<4:18:23, 17.46s/it] 73%|███████▎  | 2363/3250 [11:07:21<4:12:15, 17.06s/it]                                                         73%|███████▎  | 2363/3250 [11:07:21<4:12:15, 17.06s/it] 73%|███████▎  | 2364/3250 [11:07:37<4:07:02, 16.73s/it]                                                         73%|███████▎  | 2364/3250 [11:07:37<4:07:02, 16.73s/it] 73%|███████▎  | 2365/3250 [11:07:53<4:03:13, 16.49s/it]                                                         73%|███████▎  | 2365/3250 [11:07:53<4:03:13, 16.49s/it] 73{'loss': 0.3898, 'learning_rate': 1.7190560283214395e-05, 'epoch': 0.73}
{'loss': 0.4002, 'learning_rate': 1.7154081861282617e-05, 'epoch': 0.73}
{'loss': 0.3871, 'learning_rate': 1.7117634168396774e-05, 'epoch': 0.73}
{'loss': 0.3971, 'learning_rate': 1.7081217238655563e-05, 'epoch': 0.73}
{'loss': 0.4104, 'learning_rate': 1.7044831106128866e-05, 'epoch': 0.73}
%|███████▎  | 2366/3250 [11:08:09<4:00:32, 16.33s/it]                                                         73%|███████▎  | 2366/3250 [11:08:09<4:00:32, 16.33s/it] 73%|███████▎  | 2367/3250 [11:08:26<4:05:55, 16.71s/it]                                                         73%|███████▎  | 2367/3250 [11:08:26<4:05:55, 16.71s/it] 73%|███████▎  | 2368/3250 [11:08:42<4:02:31, 16.50s/it]                                                         73%|███████▎  | 2368/3250 [11:08:42<4:02:31, 16.50s/it] 73%|███████▎  | 2369/3250 [11:08:58<3:59:36, 16.32s/it]                                                         73%|███████▎  | 2369/3250 [11:08:58<3:59:36, 16.32s/it] 73%|███████▎  | 2370/3250 [11:09:14<3:57:32, 16.20s/it]                                                         73%|███████▎  | 2370/3250 [11:09:14<3:57:32, 16.20s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7582586407661438, 'eval_runtime': 3.8104, 'eval_samples_per_second': 3.149, 'eval_steps_per_second': 0.787, 'epoch': 0.73}
                                                         73%|███████▎  | 2370/3250 [11:09:18<3:57:32, 16.20s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2370
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2370the checkpoint model will be saved in 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2370
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2370

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4048, 'learning_rate': 1.7008475804857804e-05, 'epoch': 0.73}
{'loss': 0.4177, 'learning_rate': 1.697215136885462e-05, 'epoch': 0.73}
{'loss': 0.9156, 'learning_rate': 1.69358578321027e-05, 'epoch': 0.73}
{'loss': 0.4054, 'learning_rate': 1.689959522855652e-05, 'epoch': 0.73}
{'loss': 0.4399, 'learning_rate': 1.6863363592141618e-05, 'epoch': 0.73}
 73%|███████▎  | 2371/3250 [11:09:35<4:17:02, 17.55s/it]                                                         73%|███████▎  | 2371/3250 [11:09:35<4:17:02, 17.55s/it] 73%|███████▎  | 2372/3250 [11:09:51<4:09:32, 17.05s/it]                                                         73%|███████▎  | 2372/3250 [11:09:51<4:09:32, 17.05s/it] 73%|███████▎  | 2373/3250 [11:10:07<4:07:44, 16.95s/it]                                                         73%|███████▎  | 2373/3250 [11:10:07<4:07:44, 16.95s/it] 73%|███████▎  | 2374/3250 [11:10:23<4:03:42, 16.69s/it]                                                         73%|███████▎  | 2374/3250 [11:10:23<4:03:42, 16.69s/it] 73%|███████▎  | 2375/3250 [11:10:39<3:59:57, 16.45s/it]                                                         73%|███████▎  | 2375/3250 [11:10:39<3:59:57, 16.45s/it] 73{'loss': 0.4059, 'learning_rate': 1.6827162956754522e-05, 'epoch': 0.73}
{'loss': 0.4173, 'learning_rate': 1.6790993356262803e-05, 'epoch': 0.73}
{'loss': 0.3959, 'learning_rate': 1.675485482450499e-05, 'epoch': 0.73}
{'loss': 0.4626, 'learning_rate': 1.6718747395290552e-05, 'epoch': 0.73}
{'loss': 0.4297, 'learning_rate': 1.6682671102399805e-05, 'epoch': 0.73}
%|███████▎  | 2376/3250 [11:10:55<3:57:11, 16.28s/it]                                                         73%|███████▎  | 2376/3250 [11:10:55<3:57:11, 16.28s/it] 73%|███████▎  | 2377/3250 [11:11:11<3:55:12, 16.17s/it]                                                         73%|███████▎  | 2377/3250 [11:11:11<3:55:12, 16.17s/it] 73%|███████▎  | 2378/3250 [11:11:28<3:56:52, 16.30s/it]                                                         73%|███████▎  | 2378/3250 [11:11:28<3:56:52, 16.30s/it] 73%|███████▎  | 2379/3250 [11:11:44<3:54:51, 16.18s/it]                                                         73%|███████▎  | 2379/3250 [11:11:44<3:54:51, 16.18s/it] 73%|███████▎  | 2380/3250 [11:12:00<3:53:21, 16.09s/it]                                                         73%|███████▎  | 2380/3250 [11:12:00<3:53:21, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7524853348731995, 'eval_runtime': 2.4922, 'eval_samples_per_second': 4.815, 'eval_steps_per_second': 1.204, 'epoch': 0.73}
                                                         73%|███████▎  | 2380/3250 [11:12:02<3:53:21, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2380
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2380 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2380
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2380

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4034, 'learning_rate': 1.6646625979584024e-05, 'epoch': 0.73}
{'loss': 0.403, 'learning_rate': 1.6610612060565234e-05, 'epoch': 0.73}
{'loss': 0.4141, 'learning_rate': 1.6574629379036322e-05, 'epoch': 0.73}
{'loss': 0.4134, 'learning_rate': 1.6538677968660948e-05, 'epoch': 0.73}
{'loss': 0.423, 'learning_rate': 1.6502757863073498e-05, 'epoch': 0.73}
 73%|███████▎  | 2381/3250 [11:12:19<4:06:27, 17.02s/it]                                                         73%|███████▎  | 2381/3250 [11:12:19<4:06:27, 17.02s/it] 73%|███████▎  | 2382/3250 [11:12:35<4:01:17, 16.68s/it]                                                         73%|███████▎  | 2382/3250 [11:12:35<4:01:17, 16.68s/it] 73%|███████▎  | 2383/3250 [11:12:50<3:57:32, 16.44s/it]                                                         73%|███████▎  | 2383/3250 [11:12:50<3:57:32, 16.44s/it] 73%|███████▎  | 2384/3250 [11:13:06<3:54:52, 16.27s/it]                                                         73%|███████▎  | 2384/3250 [11:13:06<3:54:52, 16.27s/it] 73%|███████▎  | 2385/3250 [11:13:22<3:52:52, 16.15s/it]                                                         73%|███████▎  | 2385/3250 [11:13:22<3:52:52, 16.15s/it] 73{'loss': 0.4103, 'learning_rate': 1.646686909587908e-05, 'epoch': 0.73}
{'loss': 0.3963, 'learning_rate': 1.6431011700653493e-05, 'epoch': 0.73}
{'loss': 0.4229, 'learning_rate': 1.639518571094315e-05, 'epoch': 0.73}
{'loss': 0.4319, 'learning_rate': 1.6359391160265125e-05, 'epoch': 0.74}
{'loss': 0.423, 'learning_rate': 1.632362808210705e-05, 'epoch': 0.74}
%|███████▎  | 2386/3250 [11:13:38<3:52:17, 16.13s/it]                                                         73%|███████▎  | 2386/3250 [11:13:38<3:52:17, 16.13s/it] 73%|███████▎  | 2387/3250 [11:13:54<3:50:52, 16.05s/it]                                                         73%|███████▎  | 2387/3250 [11:13:54<3:50:52, 16.05s/it] 73%|███████▎  | 2388/3250 [11:14:10<3:49:51, 16.00s/it]                                                         73%|███████▎  | 2388/3250 [11:14:10<3:49:51, 16.00s/it] 74%|███████▎  | 2389/3250 [11:14:26<3:49:03, 15.96s/it]                                                         74%|███████▎  | 2389/3250 [11:14:26<3:49:03, 15.96s/it] 74%|███████▎  | 2390/3250 [11:14:42<3:48:25, 15.94s/it]                                                         74%|███████▎  | 2390/3250 [11:14:42<3:48:25, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7538974285125732, 'eval_runtime': 2.4738, 'eval_samples_per_second': 4.851, 'eval_steps_per_second': 1.213, 'epoch': 0.74}
                                                         74%|███████▎  | 2390/3250 [11:14:44<3:48:25, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2390
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2390/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2390

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.417, 'learning_rate': 1.6287896509927135e-05, 'epoch': 0.74}
{'loss': 0.4207, 'learning_rate': 1.6252196477154095e-05, 'epoch': 0.74}
{'loss': 0.4154, 'learning_rate': 1.6216528017187176e-05, 'epoch': 0.74}
{'loss': 0.3924, 'learning_rate': 1.618089116339601e-05, 'epoch': 0.74}
{'loss': 0.4393, 'learning_rate': 1.6145285949120738e-05, 'epoch': 0.74}
 74%|███████▎  | 2391/3250 [11:15:01<4:01:34, 16.87s/it]                                                         74%|███████▎  | 2391/3250 [11:15:01<4:01:34, 16.87s/it] 74%|███████▎  | 2392/3250 [11:15:17<3:56:59, 16.57s/it]                                                         74%|███████▎  | 2392/3250 [11:15:17<3:56:59, 16.57s/it] 74%|███████▎  | 2393/3250 [11:15:33<3:53:43, 16.36s/it]                                                         74%|███████▎  | 2393/3250 [11:15:33<3:53:43, 16.36s/it] 74%|███████▎  | 2394/3250 [11:15:49<3:52:59, 16.33s/it]                                                         74%|███████▎  | 2394/3250 [11:15:49<3:52:59, 16.33s/it] 74%|███████▎  | 2395/3250 [11:16:05<3:50:46, 16.19s/it]                                                         74%|███████▎  | 2395/3250 [11:16:05<3:50:46, 16.19s/it] 74{'loss': 0.4166, 'learning_rate': 1.6109712407671867e-05, 'epoch': 0.74}
{'loss': 0.4013, 'learning_rate': 1.6074170572330256e-05, 'epoch': 0.74}
{'loss': 0.3953, 'learning_rate': 1.6038660476347135e-05, 'epoch': 0.74}
{'loss': 0.3913, 'learning_rate': 1.600318215294402e-05, 'epoch': 0.74}
{'loss': 0.4236, 'learning_rate': 1.596773563531273e-05, 'epoch': 0.74}
%|███████▎  | 2396/3250 [11:16:21<3:49:08, 16.10s/it]                                                         74%|███████▎  | 2396/3250 [11:16:21<3:49:08, 16.10s/it] 74%|███████▍  | 2397/3250 [11:16:37<3:47:54, 16.03s/it]                                                         74%|███████▍  | 2397/3250 [11:16:37<3:47:54, 16.03s/it] 74%|███████▍  | 2398/3250 [11:16:52<3:47:01, 15.99s/it]                                                         74%|███████▍  | 2398/3250 [11:16:52<3:47:01, 15.99s/it] 74%|███████▍  | 2399/3250 [11:17:08<3:46:17, 15.95s/it]                                                         74%|███████▍  | 2399/3250 [11:17:08<3:46:17, 15.95s/it] 74%|███████▍  | 2400/3250 [11:17:24<3:45:43, 15.93s/it]                                                         74%|███████▍  | 2400/3250 [11:17:24<3:45:43, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7562982439994812, 'eval_runtime': 2.731, 'eval_samples_per_second': 4.394, 'eval_steps_per_second': 1.099, 'epoch': 0.74}
                                                         74%|███████▍  | 2400/3250 [11:17:27<3:45:43, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2400
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2400/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2400

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4085, 'learning_rate': 1.5932320956615265e-05, 'epoch': 0.74}
{'loss': 0.4221, 'learning_rate': 1.5896938149983908e-05, 'epoch': 0.74}
{'loss': 0.9245, 'learning_rate': 1.586158724852108e-05, 'epoch': 0.74}
{'loss': 0.4199, 'learning_rate': 1.582626828529938e-05, 'epoch': 0.74}
{'loss': 0.4001, 'learning_rate': 1.5790981293361516e-05, 'epoch': 0.74}
 74%|███████▍  | 2401/3250 [11:17:44<4:00:32, 17.00s/it]                                                         74%|███████▍  | 2401/3250 [11:17:44<4:00:32, 17.00s/it] 74%|███████▍  | 2402/3250 [11:18:00<3:55:31, 16.66s/it]                                                         74%|███████▍  | 2402/3250 [11:18:00<3:55:31, 16.66s/it] 74%|███████▍  | 2403/3250 [11:18:15<3:51:47, 16.42s/it]                                                         74%|███████▍  | 2403/3250 [11:18:15<3:51:47, 16.42s/it] 74%|███████▍  | 2404/3250 [11:18:31<3:49:36, 16.28s/it]                                                         74%|███████▍  | 2404/3250 [11:18:31<3:49:36, 16.28s/it] 74%|███████▍  | 2405/3250 [11:18:47<3:47:36, 16.16s/it]                                                         74%|███████▍  | 2405/3250 [11:18:47<3:47:36, 16.16s/it] 74{'loss': 0.4205, 'learning_rate': 1.5755726305720266e-05, 'epoch': 0.74}
{'loss': 0.4144, 'learning_rate': 1.5720503355358495e-05, 'epoch': 0.74}
{'loss': 0.3881, 'learning_rate': 1.5685312475229085e-05, 'epoch': 0.74}
{'loss': 0.3918, 'learning_rate': 1.5650153698254916e-05, 'epoch': 0.74}
{'loss': 0.465, 'learning_rate': 1.561502705732883e-05, 'epoch': 0.74}
%|███████▍  | 2406/3250 [11:19:03<3:46:08, 16.08s/it]                                                         74%|███████▍  | 2406/3250 [11:19:03<3:46:08, 16.08s/it] 74%|███████▍  | 2407/3250 [11:19:19<3:45:07, 16.02s/it]                                                         74%|███████▍  | 2407/3250 [11:19:19<3:45:07, 16.02s/it] 74%|███████▍  | 2408/3250 [11:19:35<3:44:11, 15.98s/it]                                                         74%|███████▍  | 2408/3250 [11:19:35<3:44:11, 15.98s/it] 74%|███████▍  | 2409/3250 [11:19:51<3:43:28, 15.94s/it]                                                         74%|███████▍  | 2409/3250 [11:19:51<3:43:28, 15.94s/it] 74%|███████▍  | 2410/3250 [11:20:07<3:43:56, 16.00s/it]                                                         74%|███████▍  | 2410/3250 [11:20:07<3:43:56, 16.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.754966676235199, 'eval_runtime': 2.471, 'eval_samples_per_second': 4.856, 'eval_steps_per_second': 1.214, 'epoch': 0.74}
                                                         74%|███████▍  | 2410/3250 [11:20:09<3:43:56, 16.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2410
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2410
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2410 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2410


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2410
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4168, 'learning_rate': 1.557993258531362e-05, 'epoch': 0.74}
{'loss': 0.41, 'learning_rate': 1.5544870315041943e-05, 'epoch': 0.74}
{'loss': 0.3805, 'learning_rate': 1.550984027931639e-05, 'epoch': 0.74}
{'loss': 0.4207, 'learning_rate': 1.547484251090932e-05, 'epoch': 0.74}
{'loss': 0.4193, 'learning_rate': 1.5439877042562973e-05, 'epoch': 0.74}
 74%|███████▍  | 2411/3250 [11:20:45<5:15:13, 22.54s/it]                                                         74%|███████▍  | 2411/3250 [11:20:45<5:15:13, 22.54s/it] 74%|███████▍  | 2412/3250 [11:21:01<4:46:56, 20.54s/it]                                                         74%|███████▍  | 2412/3250 [11:21:01<4:46:56, 20.54s/it] 74%|███████▍  | 2413/3250 [11:21:16<4:27:02, 19.14s/it]                                                         74%|███████▍  | 2413/3250 [11:21:16<4:27:02, 19.14s/it] 74%|███████▍  | 2414/3250 [11:21:32<4:13:07, 18.17s/it]                                                         74%|███████▍  | 2414/3250 [11:21:32<4:13:07, 18.17s/it] 74%|███████▍  | 2415/3250 [11:21:48<4:03:19, 17.48s/it]                                                         74%|███████▍  | 2415/3250 [11:21:48<4:03:19, 17.48s/it] 74{'loss': 0.4046, 'learning_rate': 1.5404943906989334e-05, 'epoch': 0.74}
{'loss': 0.4089, 'learning_rate': 1.5370043136870148e-05, 'epoch': 0.74}
{'loss': 0.4131, 'learning_rate': 1.5335174764856908e-05, 'epoch': 0.74}
{'loss': 0.4142, 'learning_rate': 1.5300338823570725e-05, 'epoch': 0.74}
{'loss': 0.4338, 'learning_rate': 1.526553534560244e-05, 'epoch': 0.74}
%|███████▍  | 2416/3250 [11:22:04<3:56:22, 17.01s/it]                                                         74%|███████▍  | 2416/3250 [11:22:04<3:56:22, 17.01s/it] 74%|███████▍  | 2417/3250 [11:22:20<3:51:27, 16.67s/it]                                                         74%|███████▍  | 2417/3250 [11:22:20<3:51:27, 16.67s/it] 74%|███████▍  | 2418/3250 [11:22:36<3:47:49, 16.43s/it]                                                         74%|███████▍  | 2418/3250 [11:22:36<3:47:49, 16.43s/it] 74%|███████▍  | 2419/3250 [11:22:52<3:45:19, 16.27s/it]                                                         74%|███████▍  | 2419/3250 [11:22:52<3:45:19, 16.27s/it] 74%|███████▍  | 2420/3250 [11:23:08<3:43:28, 16.15s/it]                                                         74%|███████▍  | 2420/3250 [11:23:08<3:43:28, 16.15s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7562891244888306, 'eval_runtime': 2.4728, 'eval_samples_per_second': 4.853, 'eval_steps_per_second': 1.213, 'epoch': 0.74}
                                                         74%|███████▍  | 2420/3250 [11:23:10<3:43:28, 16.15s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2420
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2420the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2420
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2420

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.417, 'learning_rate': 1.5230764363512485e-05, 'epoch': 0.74}
{'loss': 0.4036, 'learning_rate': 1.5196025909830913e-05, 'epoch': 0.75}
{'loss': 0.4298, 'learning_rate': 1.5161320017057346e-05, 'epoch': 0.75}
{'loss': 0.4052, 'learning_rate': 1.5126646717660897e-05, 'epoch': 0.75}
{'loss': 0.4222, 'learning_rate': 1.5092006044080242e-05, 'epoch': 0.75}
 74%|███████▍  | 2421/3250 [11:23:41<4:52:30, 21.17s/it]                                                         74%|███████▍  | 2421/3250 [11:23:41<4:52:30, 21.17s/it] 75%|███████▍  | 2422/3250 [11:23:56<4:30:13, 19.58s/it]                                                         75%|███████▍  | 2422/3250 [11:23:56<4:30:13, 19.58s/it] 75%|███████▍  | 2423/3250 [11:24:12<4:14:36, 18.47s/it]                                                         75%|███████▍  | 2423/3250 [11:24:12<4:14:36, 18.47s/it] 75%|███████▍  | 2424/3250 [11:24:28<4:03:36, 17.70s/it]                                                         75%|███████▍  | 2424/3250 [11:24:28<4:03:36, 17.70s/it] 75%|███████▍  | 2425/3250 [11:24:44<3:55:46, 17.15s/it]                                                         75%|███████▍  | 2425/3250 [11:24:44<3:55:46, 17.15s/it] 75{'loss': 0.4093, 'learning_rate': 1.5057398028723513e-05, 'epoch': 0.75}
{'loss': 0.3943, 'learning_rate': 1.5022822703968281e-05, 'epoch': 0.75}
{'loss': 0.3892, 'learning_rate': 1.498828010216155e-05, 'epoch': 0.75}
{'loss': 0.3991, 'learning_rate': 1.4953770255619714e-05, 'epoch': 0.75}
{'loss': 0.4033, 'learning_rate': 1.4919293196628492e-05, 'epoch': 0.75}
%|███████▍  | 2426/3250 [11:25:00<3:50:15, 16.77s/it]                                                         75%|███████▍  | 2426/3250 [11:25:00<3:50:15, 16.77s/it] 75%|███████▍  | 2427/3250 [11:25:16<3:49:01, 16.70s/it]                                                         75%|███████▍  | 2427/3250 [11:25:16<3:49:01, 16.70s/it] 75%|███████▍  | 2428/3250 [11:25:32<3:45:22, 16.45s/it]                                                         75%|███████▍  | 2428/3250 [11:25:32<3:45:22, 16.45s/it] 75%|███████▍  | 2429/3250 [11:25:48<3:42:48, 16.28s/it]                                                         75%|███████▍  | 2429/3250 [11:25:48<3:42:48, 16.28s/it] 75%|███████▍  | 2430/3250 [11:26:04<3:40:53, 16.16s/it]                                                         75%|███████▍  | 2430/3250 [11:26:04<3:40:53, 16.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7584102749824524, 'eval_runtime': 2.4918, 'eval_samples_per_second': 4.816, 'eval_steps_per_second': 1.204, 'epoch': 0.75}
                                                         75%|███████▍  | 2430/3250 [11:26:07<3:40:53, 16.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2430
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2430/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2430

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4137, 'learning_rate': 1.4884848957442931e-05, 'epoch': 0.75}
{'loss': 0.4141, 'learning_rate': 1.4850437570287406e-05, 'epoch': 0.75}
{'loss': 0.9303, 'learning_rate': 1.4816059067355536e-05, 'epoch': 0.75}
{'loss': 0.3933, 'learning_rate': 1.4781713480810184e-05, 'epoch': 0.75}
{'loss': 0.3879, 'learning_rate': 1.4747400842783404e-05, 'epoch': 0.75}
 75%|███████▍  | 2431/3250 [11:26:23<3:53:38, 17.12s/it]                                                         75%|███████▍  | 2431/3250 [11:26:23<3:53:38, 17.12s/it] 75%|███████▍  | 2432/3250 [11:26:39<3:48:24, 16.75s/it]                                                         75%|███████▍  | 2432/3250 [11:26:39<3:48:24, 16.75s/it] 75%|███████▍  | 2433/3250 [11:26:55<3:44:28, 16.49s/it]                                                         75%|███████▍  | 2433/3250 [11:26:55<3:44:28, 16.49s/it] 75%|███████▍  | 2434/3250 [11:27:11<3:41:47, 16.31s/it]                                                         75%|███████▍  | 2434/3250 [11:27:11<3:41:47, 16.31s/it] 75%|███████▍  | 2435/3250 [11:27:27<3:39:52, 16.19s/it]                                                         75%|███████▍  | 2435/3250 [11:27:27<3:39:52, 16.19s/it] 75{'loss': 0.4265, 'learning_rate': 1.4713121185376461e-05, 'epoch': 0.75}
{'loss': 0.4326, 'learning_rate': 1.4678874540659694e-05, 'epoch': 0.75}
{'loss': 0.4088, 'learning_rate': 1.4644660940672627e-05, 'epoch': 0.75}
{'loss': 0.4021, 'learning_rate': 1.4610480417423839e-05, 'epoch': 0.75}
{'loss': 0.4426, 'learning_rate': 1.4576333002890969e-05, 'epoch': 0.75}
%|███████▍  | 2436/3250 [11:27:43<3:38:28, 16.10s/it]                                                         75%|███████▍  | 2436/3250 [11:27:43<3:38:28, 16.10s/it] 75%|███████▍  | 2437/3250 [11:27:59<3:37:23, 16.04s/it]                                                         75%|███████▍  | 2437/3250 [11:27:59<3:37:23, 16.04s/it] 75%|███████▌  | 2438/3250 [11:28:15<3:36:31, 16.00s/it]                                                         75%|███████▌  | 2438/3250 [11:28:15<3:36:31, 16.00s/it] 75%|███████▌  | 2439/3250 [11:28:31<3:35:50, 15.97s/it]                                                         75%|███████▌  | 2439/3250 [11:28:31<3:35:50, 15.97s/it] 75%|███████▌  | 2440/3250 [11:28:46<3:35:19, 15.95s/it]                                                         75%|███████▌  | 2440/3250 [11:28:46<3:35:19, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7540023326873779, 'eval_runtime': 2.4894, 'eval_samples_per_second': 4.821, 'eval_steps_per_second': 1.205, 'epoch': 0.75}
                                                         75%|███████▌  | 2440/3250 [11:28:49<3:35:19, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2440
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2440/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2440
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2440

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4305, 'learning_rate': 1.454221872902069e-05, 'epoch': 0.75}
{'loss': 0.3979, 'learning_rate': 1.450813762772863e-05, 'epoch': 0.75}
{'loss': 0.391, 'learning_rate': 1.4474089730899437e-05, 'epoch': 0.75}
{'loss': 0.4322, 'learning_rate': 1.444007507038666e-05, 'epoch': 0.75}
{'loss': 0.4186, 'learning_rate': 1.4406093678012766e-05, 'epoch': 0.75}
 75%|███████▌  | 2441/3250 [11:29:06<3:47:45, 16.89s/it]                                                         75%|███████▌  | 2441/3250 [11:29:06<3:47:45, 16.89s/it] 75%|███████▌  | 2442/3250 [11:29:21<3:43:31, 16.60s/it]                                                         75%|███████▌  | 2442/3250 [11:29:22<3:43:31, 16.60s/it] 75%|███████▌  | 2443/3250 [11:29:38<3:41:52, 16.50s/it]                                                         75%|███████▌  | 2443/3250 [11:29:38<3:41:52, 16.50s/it] 75%|███████▌  | 2444/3250 [11:29:54<3:39:14, 16.32s/it]                                                         75%|███████▌  | 2444/3250 [11:29:54<3:39:14, 16.32s/it] 75%|███████▌  | 2445/3250 [11:30:10<3:37:18, 16.20s/it]                                                         75%|███████▌  | 2445/3250 [11:30:10<3:37:18, 16.20s/it] 75{'loss': 0.4199, 'learning_rate': 1.4372145585569097e-05, 'epoch': 0.75}
{'loss': 0.4008, 'learning_rate': 1.4338230824815852e-05, 'epoch': 0.75}
{'loss': 0.4065, 'learning_rate': 1.4304349427482028e-05, 'epoch': 0.75}
{'loss': 0.41, 'learning_rate': 1.4270501425265386e-05, 'epoch': 0.75}
{'loss': 0.4252, 'learning_rate': 1.4236686849832498e-05, 'epoch': 0.75}
%|███████▌  | 2446/3250 [11:30:25<3:35:51, 16.11s/it]                                                         75%|███████▌  | 2446/3250 [11:30:25<3:35:51, 16.11s/it] 75%|███████▌  | 2447/3250 [11:30:41<3:34:47, 16.05s/it]                                                         75%|███████▌  | 2447/3250 [11:30:41<3:34:47, 16.05s/it] 75%|███████▌  | 2448/3250 [11:30:57<3:33:57, 16.01s/it]                                                         75%|███████▌  | 2448/3250 [11:30:57<3:33:57, 16.01s/it] 75%|███████▌  | 2449/3250 [11:31:13<3:33:21, 15.98s/it]                                                         75%|███████▌  | 2449/3250 [11:31:13<3:33:21, 15.98s/it] 75%|███████▌  | 2450/3250 [11:31:29<3:32:47, 15.96s/it]                                                         75%|███████▌  | 2450/3250 [11:31:29<3:32:47, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7533724308013916, 'eval_runtime': 2.4785, 'eval_samples_per_second': 4.842, 'eval_steps_per_second': 1.21, 'epoch': 0.75}
                                                         75%|███████▌  | 2450/3250 [11:31:32<3:32:47, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2450
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2450/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2450

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2450

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2450/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4163, 'learning_rate': 1.4202905732818633e-05, 'epoch': 0.75}
{'loss': 0.4248, 'learning_rate': 1.4169158105827768e-05, 'epoch': 0.75}
{'loss': 0.4092, 'learning_rate': 1.4135444000432541e-05, 'epoch': 0.75}
{'loss': 0.4165, 'learning_rate': 1.410176344817425e-05, 'epoch': 0.76}
{'loss': 0.376, 'learning_rate': 1.4068116480562754e-05, 'epoch': 0.76}
 75%|███████▌  | 2451/3250 [11:31:48<3:45:36, 16.94s/it]                                                         75%|███████▌  | 2451/3250 [11:31:48<3:45:36, 16.94s/it] 75%|███████▌  | 2452/3250 [11:32:04<3:41:12, 16.63s/it]                                                         75%|███████▌  | 2452/3250 [11:32:04<3:41:12, 16.63s/it] 75%|███████▌  | 2453/3250 [11:32:20<3:38:01, 16.41s/it]                                                         75%|███████▌  | 2453/3250 [11:32:20<3:38:01, 16.41s/it] 76%|███████▌  | 2454/3250 [11:32:36<3:35:45, 16.26s/it]                                                         76%|███████▌  | 2454/3250 [11:32:36<3:35:45, 16.26s/it] 76%|███████▌  | 2455/3250 [11:32:52<3:34:01, 16.15s/it]                                                         76%|███████▌  | 2455/3250 [11:32:52<3:34:01, 16.15s/it] 76{'loss': 0.4291, 'learning_rate': 1.4034503129076531e-05, 'epoch': 0.76}
{'loss': 0.391, 'learning_rate': 1.4000923425162604e-05, 'epoch': 0.76}
{'loss': 0.3972, 'learning_rate': 1.3967377400236515e-05, 'epoch': 0.76}
{'loss': 0.3856, 'learning_rate': 1.3933865085682312e-05, 'epoch': 0.76}
{'loss': 0.3954, 'learning_rate': 1.3900386512852454e-05, 'epoch': 0.76}
%|███████▌  | 2456/3250 [11:33:08<3:32:43, 16.08s/it]                                                         76%|███████▌  | 2456/3250 [11:33:08<3:32:43, 16.08s/it] 76%|███████▌  | 2457/3250 [11:33:24<3:31:44, 16.02s/it]                                                         76%|███████▌  | 2457/3250 [11:33:24<3:31:44, 16.02s/it] 76%|███████▌  | 2458/3250 [11:33:40<3:30:57, 15.98s/it]                                                         76%|███████▌  | 2458/3250 [11:33:40<3:30:57, 15.98s/it] 76%|███████▌  | 2459/3250 [11:33:56<3:32:13, 16.10s/it]                                                         76%|███████▌  | 2459/3250 [11:33:56<3:32:13, 16.10s/it] 76%|███████▌  | 2460/3250 [11:34:12<3:31:11, 16.04s/it]                                                         76%|███████▌  | 2460/3250 [11:34:12<3:31:11, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7571738362312317, 'eval_runtime': 2.4848, 'eval_samples_per_second': 4.829, 'eval_steps_per_second': 1.207, 'epoch': 0.76}
                                                         76%|███████▌  | 2460/3250 [11:34:14<3:31:11, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2460
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2460/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2460the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2460

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4117, 'learning_rate': 1.3866941713067888e-05, 'epoch': 0.76}
{'loss': 0.4171, 'learning_rate': 1.3833530717617937e-05, 'epoch': 0.76}
{'loss': 0.4131, 'learning_rate': 1.3800153557760315e-05, 'epoch': 0.76}
{'loss': 0.9221, 'learning_rate': 1.376681026472108e-05, 'epoch': 0.76}
{'loss': 0.403, 'learning_rate': 1.3733500869694572e-05, 'epoch': 0.76}
 76%|███████▌  | 2461/3250 [11:34:31<3:43:42, 17.01s/it]                                                         76%|███████▌  | 2461/3250 [11:34:31<3:43:42, 17.01s/it] 76%|███████▌  | 2462/3250 [11:34:47<3:39:03, 16.68s/it]                                                         76%|███████▌  | 2462/3250 [11:34:47<3:39:03, 16.68s/it] 76%|███████▌  | 2463/3250 [11:35:03<3:35:44, 16.45s/it]                                                         76%|███████▌  | 2463/3250 [11:35:03<3:35:44, 16.45s/it] 76%|███████▌  | 2464/3250 [11:35:19<3:33:05, 16.27s/it]                                                         76%|███████▌  | 2464/3250 [11:35:19<3:33:05, 16.27s/it] 76%|███████▌  | 2465/3250 [11:35:35<3:31:24, 16.16s/it]                                                         76%|███████▌  | 2465/3250 [11:35:35<3:31:24, 16.16s/it] 76{'loss': 0.4346, 'learning_rate': 1.3700225403843469e-05, 'epoch': 0.76}
{'loss': 0.4125, 'learning_rate': 1.3666983898298657e-05, 'epoch': 0.76}
{'loss': 0.4161, 'learning_rate': 1.3633776384159285e-05, 'epoch': 0.76}
{'loss': 0.389, 'learning_rate': 1.3600602892492693e-05, 'epoch': 0.76}
{'loss': 0.466, 'learning_rate': 1.3567463454334389e-05, 'epoch': 0.76}
%|███████▌  | 2466/3250 [11:35:51<3:30:05, 16.08s/it]                                                         76%|███████▌  | 2466/3250 [11:35:51<3:30:05, 16.08s/it] 76%|███████▌  | 2467/3250 [11:36:07<3:29:06, 16.02s/it]                                                         76%|███████▌  | 2467/3250 [11:36:07<3:29:06, 16.02s/it] 76%|███████▌  | 2468/3250 [11:36:22<3:28:19, 15.98s/it]                                                         76%|███████▌  | 2468/3250 [11:36:22<3:28:19, 15.98s/it] 76%|███████▌  | 2469/3250 [11:36:38<3:27:44, 15.96s/it]                                                         76%|███████▌  | 2469/3250 [11:36:38<3:27:44, 15.96s/it] 76%|███████▌  | 2470/3250 [11:36:54<3:27:15, 15.94s/it]                                                         76%|███████▌  | 2470/3250 [11:36:54<3:27:15, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7546772956848145, 'eval_runtime': 2.4778, 'eval_samples_per_second': 4.843, 'eval_steps_per_second': 1.211, 'epoch': 0.76}
                                                         76%|███████▌  | 2470/3250 [11:36:57<3:27:15, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2470
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2470/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2470

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2470

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4345, 'learning_rate': 1.3534358100688016e-05, 'epoch': 0.76}
{'loss': 0.4001, 'learning_rate': 1.3501286862525358e-05, 'epoch': 0.76}
{'loss': 0.4037, 'learning_rate': 1.3468249770786223e-05, 'epoch': 0.76}
{'loss': 0.4077, 'learning_rate': 1.3435246856378525e-05, 'epoch': 0.76}
{'loss': 0.4086, 'learning_rate': 1.34022781501782e-05, 'epoch': 0.76}
 76%|███████▌  | 2471/3250 [11:37:13<3:39:18, 16.89s/it]                                                         76%|███████▌  | 2471/3250 [11:37:13<3:39:18, 16.89s/it] 76%|███████▌  | 2472/3250 [11:37:29<3:35:08, 16.59s/it]                                                         76%|███████▌  | 2472/3250 [11:37:29<3:35:08, 16.59s/it] 76%|███████▌  | 2473/3250 [11:37:45<3:32:11, 16.39s/it]                                                         76%|███████▌  | 2473/3250 [11:37:45<3:32:11, 16.39s/it] 76%|███████▌  | 2474/3250 [11:38:01<3:30:00, 16.24s/it]                                                         76%|███████▌  | 2474/3250 [11:38:01<3:30:00, 16.24s/it] 76%|███████▌  | 2475/3250 [11:38:17<3:28:23, 16.13s/it]                                                         76%|███████▌  | 2475/3250 [11:38:17<3:28:23, 16.13s/it] 76{'loss': 0.4153, 'learning_rate': 1.3369343683029151e-05, 'epoch': 0.76}
{'loss': 0.4139, 'learning_rate': 1.3336443485743294e-05, 'epoch': 0.76}
{'loss': 0.416, 'learning_rate': 1.3303577589100418e-05, 'epoch': 0.76}
{'loss': 0.4178, 'learning_rate': 1.327074602384828e-05, 'epoch': 0.76}
{'loss': 0.4457, 'learning_rate': 1.3237948820702495e-05, 'epoch': 0.76}
%|███████▌  | 2476/3250 [11:38:33<3:28:33, 16.17s/it]                                                         76%|███████▌  | 2476/3250 [11:38:33<3:28:33, 16.17s/it] 76%|███████▌  | 2477/3250 [11:38:49<3:27:14, 16.09s/it]                                                         76%|███████▌  | 2477/3250 [11:38:49<3:27:14, 16.09s/it] 76%|███████▌  | 2478/3250 [11:39:05<3:26:16, 16.03s/it]                                                         76%|███████▌  | 2478/3250 [11:39:05<3:26:16, 16.03s/it] 76%|███████▋  | 2479/3250 [11:39:21<3:25:28, 15.99s/it]                                                         76%|███████▋  | 2479/3250 [11:39:21<3:25:28, 15.99s/it] 76%|███████▋  | 2480/3250 [11:39:37<3:24:49, 15.96s/it]                                                         76%|███████▋  | 2480/3250 [11:39:37<3:24:49, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7533372640609741, 'eval_runtime': 2.4762, 'eval_samples_per_second': 4.846, 'eval_steps_per_second': 1.212, 'epoch': 0.76}
                                                         76%|███████▋  | 2480/3250 [11:39:39<3:24:49, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2480
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2480
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2480
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2480

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2480/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2480/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4048, 'learning_rate': 1.3205186010346548e-05, 'epoch': 0.76}
{'loss': 0.4265, 'learning_rate': 1.3172457623431706e-05, 'epoch': 0.76}
{'loss': 0.4237, 'learning_rate': 1.3139763690577073e-05, 'epoch': 0.76}
{'loss': 0.4143, 'learning_rate': 1.3107104242369517e-05, 'epoch': 0.76}
{'loss': 0.3889, 'learning_rate': 1.3074479309363608e-05, 'epoch': 0.76}
 76%|███████▋  | 2481/3250 [11:39:56<3:37:28, 16.97s/it]                                                         76%|███████▋  | 2481/3250 [11:39:56<3:37:28, 16.97s/it] 76%|███████▋  | 2482/3250 [11:40:12<3:33:04, 16.65s/it]                                                         76%|███████▋  | 2482/3250 [11:40:12<3:33:04, 16.65s/it] 76%|███████▋  | 2483/3250 [11:40:28<3:29:54, 16.42s/it]                                                         76%|███████▋  | 2483/3250 [11:40:28<3:29:54, 16.42s/it] 76%|███████▋  | 2484/3250 [11:40:44<3:27:36, 16.26s/it]                                                         76%|███████▋  | 2484/3250 [11:40:44<3:27:36, 16.26s/it] 76%|███████▋  | 2485/3250 [11:41:00<3:25:54, 16.15s/it]                                                         76%|███████▋  | 2485/3250 [11:41:00<3:25:54, 16.15s/it] 76{'loss': 0.4438, 'learning_rate': 1.3041888922081657e-05, 'epoch': 0.76}
{'loss': 0.4132, 'learning_rate': 1.300933311101365e-05, 'epoch': 0.77}
{'loss': 0.3963, 'learning_rate': 1.2976811906617225e-05, 'epoch': 0.77}
{'loss': 0.3935, 'learning_rate': 1.2944325339317637e-05, 'epoch': 0.77}
{'loss': 0.3888, 'learning_rate': 1.2911873439507765e-05, 'epoch': 0.77}
%|███████▋  | 2486/3250 [11:41:16<3:24:40, 16.07s/it]                                                         76%|███████▋  | 2486/3250 [11:41:16<3:24:40, 16.07s/it] 77%|███████▋  | 2487/3250 [11:41:31<3:23:41, 16.02s/it]                                                         77%|███████▋  | 2487/3250 [11:41:31<3:23:41, 16.02s/it] 77%|███████▋  | 2488/3250 [11:41:47<3:22:55, 15.98s/it]                                                         77%|███████▋  | 2488/3250 [11:41:47<3:22:55, 15.98s/it] 77%|███████▋  | 2489/3250 [11:42:03<3:22:18, 15.95s/it]                                                         77%|███████▋  | 2489/3250 [11:42:03<3:22:18, 15.95s/it] 77%|███████▋  | 2490/3250 [11:42:19<3:21:45, 15.93s/it]                                                         77%|███████▋  | 2490/3250 [11:42:19<3:21:45, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7557233572006226, 'eval_runtime': 2.4788, 'eval_samples_per_second': 4.841, 'eval_steps_per_second': 1.21, 'epoch': 0.77}
                                                         77%|███████▋  | 2490/3250 [11:42:22<3:21:45, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2490
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2490/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2490

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2490

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4209, 'learning_rate': 1.2879456237547988e-05, 'epoch': 0.77}
{'loss': 0.4072, 'learning_rate': 1.2847073763766287e-05, 'epoch': 0.77}
{'loss': 0.4118, 'learning_rate': 1.2814726048458137e-05, 'epoch': 0.77}
{'loss': 0.9209, 'learning_rate': 1.2782413121886483e-05, 'epoch': 0.77}
{'loss': 0.4055, 'learning_rate': 1.2750135014281729e-05, 'epoch': 0.77}
 77%|███████▋  | 2491/3250 [11:42:38<3:33:44, 16.90s/it]                                                         77%|███████▋  | 2491/3250 [11:42:38<3:33:44, 16.90s/it] 77%|███████▋  | 2492/3250 [11:42:54<3:30:33, 16.67s/it]                                                         77%|███████▋  | 2492/3250 [11:42:54<3:30:33, 16.67s/it] 77%|███████▋  | 2493/3250 [11:43:10<3:28:03, 16.49s/it]                                                         77%|███████▋  | 2493/3250 [11:43:10<3:28:03, 16.49s/it] 77%|███████▋  | 2494/3250 [11:43:26<3:25:22, 16.30s/it]                                                         77%|███████▋  | 2494/3250 [11:43:26<3:25:22, 16.30s/it] 77%|███████▋  | 2495/3250 [11:43:42<3:23:36, 16.18s/it]                                                         77%|███████▋  | 2495/3250 [11:43:42<3:23:36, 16.18s/it] 77{'loss': 0.4065, 'learning_rate': 1.2717891755841722e-05, 'epoch': 0.77}
{'loss': 0.4332, 'learning_rate': 1.268568337673166e-05, 'epoch': 0.77}
{'loss': 0.417, 'learning_rate': 1.2653509907084171e-05, 'epoch': 0.77}
{'loss': 0.4063, 'learning_rate': 1.2621371376999152e-05, 'epoch': 0.77}
{'loss': 0.4012, 'learning_rate': 1.2589267816543876e-05, 'epoch': 0.77}
%|███████▋  | 2496/3250 [11:43:58<3:23:03, 16.16s/it]                                                         77%|███████▋  | 2496/3250 [11:43:58<3:23:03, 16.16s/it] 77%|███████▋  | 2497/3250 [11:44:14<3:21:52, 16.09s/it]                                                         77%|███████▋  | 2497/3250 [11:44:14<3:21:52, 16.09s/it] 77%|███████▋  | 2498/3250 [11:44:30<3:20:53, 16.03s/it]                                                         77%|███████▋  | 2498/3250 [11:44:30<3:20:53, 16.03s/it] 77%|███████▋  | 2499/3250 [11:44:46<3:20:12, 15.99s/it]                                                         77%|███████▋  | 2499/3250 [11:44:46<3:20:12, 15.99s/it] 77%|███████▋  | 2500/3250 [11:45:02<3:19:34, 15.97s/it]                                                         77%|███████▋  | 2500/3250 [11:45:02<3:19:34, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7558869123458862, 'eval_runtime': 2.4871, 'eval_samples_per_second': 4.825, 'eval_steps_per_second': 1.206, 'epoch': 0.77}
                                                         77%|███████▋  | 2500/3250 [11:45:04<3:19:34, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2500
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2500/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2500

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4636, 'learning_rate': 1.2557199255752867e-05, 'epoch': 0.77}
{'loss': 0.4155, 'learning_rate': 1.2525165724627936e-05, 'epoch': 0.77}
{'loss': 0.4045, 'learning_rate': 1.249316725313806e-05, 'epoch': 0.77}
{'loss': 0.373, 'learning_rate': 1.2461203871219473e-05, 'epoch': 0.77}
{'loss': 0.4298, 'learning_rate': 1.242927560877557e-05, 'epoch': 0.77}
 77%|███████▋  | 2501/3250 [11:45:21<3:31:20, 16.93s/it]                                                         77%|███████▋  | 2501/3250 [11:45:21<3:31:20, 16.93s/it] 77%|███████▋  | 2502/3250 [11:45:37<3:27:14, 16.62s/it]                                                         77%|███████▋  | 2502/3250 [11:45:37<3:27:14, 16.62s/it] 77%|███████▋  | 2503/3250 [11:45:53<3:24:17, 16.41s/it]                                                         77%|███████▋  | 2503/3250 [11:45:53<3:24:17, 16.41s/it] 77%|███████▋  | 2504/3250 [11:46:09<3:22:03, 16.25s/it]                                                         77%|███████▋  | 2504/3250 [11:46:09<3:22:03, 16.25s/it] 77%|███████▋  | 2505/3250 [11:46:25<3:20:31, 16.15s/it]                                                         77%|███████▋  | 2505/3250 [11:46:25<3:20:31, 16.15s/it] 77{'loss': 0.4201, 'learning_rate': 1.2397382495676874e-05, 'epoch': 0.77}
{'loss': 0.4007, 'learning_rate': 1.2365524561761039e-05, 'epoch': 0.77}
{'loss': 0.4025, 'learning_rate': 1.2333701836832812e-05, 'epoch': 0.77}
{'loss': 0.4071, 'learning_rate': 1.2301914350663957e-05, 'epoch': 0.77}
{'loss': 0.4089, 'learning_rate': 1.2270162132993323e-05, 'epoch': 0.77}
%|███████▋  | 2506/3250 [11:46:41<3:19:20, 16.08s/it]                                                         77%|███████▋  | 2506/3250 [11:46:41<3:19:20, 16.08s/it] 77%|███████▋  | 2507/3250 [11:46:57<3:18:27, 16.03s/it]                                                         77%|███████▋  | 2507/3250 [11:46:57<3:18:27, 16.03s/it] 77%|███████▋  | 2508/3250 [11:47:12<3:17:41, 15.99s/it]                                                         77%|███████▋  | 2508/3250 [11:47:12<3:17:41, 15.99s/it] 77%|███████▋  | 2509/3250 [11:47:29<3:19:19, 16.14s/it]                                                         77%|███████▋  | 2509/3250 [11:47:29<3:19:19, 16.14s/it] 77%|███████▋  | 2510/3250 [11:47:45<3:18:10, 16.07s/it]                                                         77%|███████▋  | 2510/3250 [11:47:45<3:18:10, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7543942928314209, 'eval_runtime': 2.4855, 'eval_samples_per_second': 4.828, 'eval_steps_per_second': 1.207, 'epoch': 0.77}
                                                         77%|███████▋  | 2510/3250 [11:47:47<3:18:10, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2510
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2510/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2510

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2510

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4319, 'learning_rate': 1.223844521352674e-05, 'epoch': 0.77}
{'loss': 0.4077, 'learning_rate': 1.2206763621937023e-05, 'epoch': 0.77}
{'loss': 0.4049, 'learning_rate': 1.2175117387863916e-05, 'epoch': 0.77}
{'loss': 0.4259, 'learning_rate': 1.2143506540914128e-05, 'epoch': 0.77}
{'loss': 0.4018, 'learning_rate': 1.2111931110661212e-05, 'epoch': 0.77}
 77%|███████▋  | 2511/3250 [11:48:04<3:30:13, 17.07s/it]                                                         77%|███████▋  | 2511/3250 [11:48:04<3:30:13, 17.07s/it] 77%|███████▋  | 2512/3250 [11:48:20<3:25:31, 16.71s/it]                                                         77%|███████▋  | 2512/3250 [11:48:20<3:25:31, 16.71s/it] 77%|███████▋  | 2513/3250 [11:48:36<3:22:14, 16.47s/it]                                                         77%|███████▋  | 2513/3250 [11:48:36<3:22:14, 16.47s/it] 77%|███████▋  | 2514/3250 [11:48:52<3:19:55, 16.30s/it]                                                         77%|███████▋  | 2514/3250 [11:48:52<3:19:55, 16.30s/it] 77%|███████▋  | 2515/3250 [11:49:08<3:18:12, 16.18s/it]                                                         77%|███████▋  | 2515/3250 [11:49:08<3:18:12, 16.18s/it] 77{'loss': 0.4235, 'learning_rate': 1.2080391126645596e-05, 'epoch': 0.77}
{'loss': 0.4057, 'learning_rate': 1.2048886618374566e-05, 'epoch': 0.77}
{'loss': 0.396, 'learning_rate': 1.2017417615322219e-05, 'epoch': 0.77}
{'loss': 0.3838, 'learning_rate': 1.1985984146929413e-05, 'epoch': 0.78}
{'loss': 0.3934, 'learning_rate': 1.1954586242603783e-05, 'epoch': 0.78}
%|███████▋  | 2516/3250 [11:49:24<3:16:51, 16.09s/it]                                                         77%|███████▋  | 2516/3250 [11:49:24<3:16:51, 16.09s/it] 77%|███████▋  | 2517/3250 [11:49:40<3:15:54, 16.04s/it]                                                         77%|███████▋  | 2517/3250 [11:49:40<3:15:54, 16.04s/it] 77%|███████▋  | 2518/3250 [11:49:56<3:15:06, 15.99s/it]                                                         77%|███████▋  | 2518/3250 [11:49:56<3:15:06, 15.99s/it] 78%|███████▊  | 2519/3250 [11:50:11<3:14:26, 15.96s/it]                                                         78%|███████▊  | 2519/3250 [11:50:11<3:14:26, 15.96s/it] 78%|███████▊  | 2520/3250 [11:50:27<3:13:56, 15.94s/it]                                                         78%|███████▊  | 2520/3250 [11:50:27<3:13:56, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7573827505111694, 'eval_runtime': 2.4925, 'eval_samples_per_second': 4.814, 'eval_steps_per_second': 1.204, 'epoch': 0.78}
                                                         78%|███████▊  | 2520/3250 [11:50:30<3:13:56, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2520
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2520/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2520

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2520

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2520/pytorch_model.binthe pytorch model path is 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3987, 'learning_rate': 1.1923223931719695e-05, 'epoch': 0.78}
{'loss': 0.4108, 'learning_rate': 1.1891897243618182e-05, 'epoch': 0.78}
{'loss': 0.4135, 'learning_rate': 1.1860606207606978e-05, 'epoch': 0.78}
{'loss': 0.9344, 'learning_rate': 1.1829350852960464e-05, 'epoch': 0.78}
{'loss': 0.3909, 'learning_rate': 1.1798131208919627e-05, 'epoch': 0.78}
 78%|███████▊  | 2521/3250 [11:50:46<3:25:22, 16.90s/it]                                                         78%|███████▊  | 2521/3250 [11:50:46<3:25:22, 16.90s/it] 78%|███████▊  | 2522/3250 [11:51:02<3:21:27, 16.60s/it]                                                         78%|███████▊  | 2522/3250 [11:51:02<3:21:27, 16.60s/it] 78%|███████▊  | 2523/3250 [11:51:18<3:18:36, 16.39s/it]                                                         78%|███████▊  | 2523/3250 [11:51:18<3:18:36, 16.39s/it] 78%|███████▊  | 2524/3250 [11:51:34<3:16:26, 16.24s/it]                                                         78%|███████▊  | 2524/3250 [11:51:34<3:16:26, 16.24s/it] 78%|███████▊  | 2525/3250 [11:51:50<3:15:44, 16.20s/it]                                                         78%|███████▊  | 2525/3250 [11:51:50<3:15:44, 16.20s/it] 78{'loss': 0.3842, 'learning_rate': 1.176694730469206e-05, 'epoch': 0.78}
{'loss': 0.4241, 'learning_rate': 1.1735799169451888e-05, 'epoch': 0.78}
{'loss': 0.4252, 'learning_rate': 1.1704686832339812e-05, 'epoch': 0.78}
{'loss': 0.4066, 'learning_rate': 1.1673610322463014e-05, 'epoch': 0.78}
{'loss': 0.4073, 'learning_rate': 1.164256966889517e-05, 'epoch': 0.78}
%|███████▊  | 2526/3250 [11:52:06<3:14:15, 16.10s/it]                                                         78%|███████▊  | 2526/3250 [11:52:06<3:14:15, 16.10s/it] 78%|███████▊  | 2527/3250 [11:52:22<3:13:09, 16.03s/it]                                                         78%|███████▊  | 2527/3250 [11:52:22<3:13:09, 16.03s/it] 78%|███████▊  | 2528/3250 [11:52:38<3:12:22, 15.99s/it]                                                         78%|███████▊  | 2528/3250 [11:52:38<3:12:22, 15.99s/it] 78%|███████▊  | 2529/3250 [11:52:54<3:11:44, 15.96s/it]                                                         78%|███████▊  | 2529/3250 [11:52:54<3:11:44, 15.96s/it] 78%|███████▊  | 2530/3250 [11:53:10<3:11:08, 15.93s/it]                                                         78%|███████▊  | 2530/3250 [11:53:10<3:11:08, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7559078931808472, 'eval_runtime': 2.4794, 'eval_samples_per_second': 4.84, 'eval_steps_per_second': 1.21, 'epoch': 0.78}
                                                         78%|███████▊  | 2530/3250 [11:53:12<3:11:08, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2530
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2530  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2530/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2530


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2530
/u/bzd2/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.
  warnings.warn("Repo card metadata block was not found. Setting CardData to empty.")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4445, 'learning_rate': 1.1611564900676408e-05, 'epoch': 0.78}
{'loss': 0.413, 'learning_rate': 1.1580596046813302e-05, 'epoch': 0.78}
{'loss': 0.3928, 'learning_rate': 1.1549663136278788e-05, 'epoch': 0.78}
{'loss': 0.3807, 'learning_rate': 1.1518766198012187e-05, 'epoch': 0.78}
{'loss': 0.4203, 'learning_rate': 1.148790526091918e-05, 'epoch': 0.78}
 78%|███████▊  | 2531/3250 [11:53:29<3:22:59, 16.94s/it]                                                         78%|███████▊  | 2531/3250 [11:53:29<3:22:59, 16.94s/it] 78%|███████▊  | 2532/3250 [11:53:45<3:18:56, 16.62s/it]                                                         78%|███████▊  | 2532/3250 [11:53:45<3:18:56, 16.62s/it] 78%|███████▊  | 2533/3250 [11:54:01<3:16:04, 16.41s/it]                                                         78%|███████▊  | 2533/3250 [11:54:01<3:16:04, 16.41s/it] 78%|███████▊  | 2534/3250 [11:54:17<3:13:58, 16.25s/it]                                                         78%|███████▊  | 2534/3250 [11:54:17<3:13:58, 16.25s/it] 78%|███████▊  | 2535/3250 [11:54:32<3:12:26, 16.15s/it]                                                         78%|███████▊  | 2535/3250 [11:54:32<3:12:26, 16.15s/it] 78{'loss': 0.4141, 'learning_rate': 1.1457080353871769e-05, 'epoch': 0.78}
{'loss': 0.4146, 'learning_rate': 1.1426291505708236e-05, 'epoch': 0.78}
{'loss': 0.3993, 'learning_rate': 1.1395538745233131e-05, 'epoch': 0.78}
{'loss': 0.414, 'learning_rate': 1.136482210121726e-05, 'epoch': 0.78}
{'loss': 0.4217, 'learning_rate': 1.1334141602397597e-05, 'epoch': 0.78}
%|███████▊  | 2536/3250 [11:54:48<3:11:16, 16.07s/it]                                                         78%|███████▊  | 2536/3250 [11:54:48<3:11:16, 16.07s/it] 78%|███████▊  | 2537/3250 [11:55:04<3:10:21, 16.02s/it]                                                         78%|███████▊  | 2537/3250 [11:55:04<3:10:21, 16.02s/it] 78%|███████▊  | 2538/3250 [11:55:20<3:09:34, 15.97s/it]                                                         78%|███████▊  | 2538/3250 [11:55:20<3:09:34, 15.97s/it] 78%|███████▊  | 2539/3250 [11:55:36<3:08:55, 15.94s/it]                                                         78%|███████▊  | 2539/3250 [11:55:36<3:08:55, 15.94s/it] 78%|███████▊  | 2540/3250 [11:55:52<3:08:21, 15.92s/it]                                                         78%|███████▊  | 2540/3250 [11:55:52<3:08:21, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7552170157432556, 'eval_runtime': 2.4955, 'eval_samples_per_second': 4.809, 'eval_steps_per_second': 1.202, 'epoch': 0.78}
                                                         78%|███████▊  | 2540/3250 [11:55:54<3:08:21, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2540
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2540/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2540the checkpoint model will be saved in 

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2540

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4343, 'learning_rate': 1.1303497277477337e-05, 'epoch': 0.78}
{'loss': 0.4104, 'learning_rate': 1.127288915512582e-05, 'epoch': 0.78}
{'loss': 0.4182, 'learning_rate': 1.1242317263978525e-05, 'epoch': 0.78}
{'loss': 0.4095, 'learning_rate': 1.1211781632637041e-05, 'epoch': 0.78}
{'loss': 0.4294, 'learning_rate': 1.1181282289668993e-05, 'epoch': 0.78}
 78%|███████▊  | 2541/3250 [11:56:12<3:22:09, 17.11s/it]                                                         78%|███████▊  | 2541/3250 [11:56:12<3:22:09, 17.11s/it] 78%|███████▊  | 2542/3250 [11:56:28<3:17:27, 16.73s/it]                                                         78%|███████▊  | 2542/3250 [11:56:28<3:17:27, 16.73s/it] 78%|███████▊  | 2543/3250 [11:56:44<3:14:11, 16.48s/it]                                                         78%|███████▊  | 2543/3250 [11:56:44<3:14:11, 16.48s/it] 78%|███████▊  | 2544/3250 [11:56:59<3:11:41, 16.29s/it]                                                         78%|███████▊  | 2544/3250 [11:56:59<3:11:41, 16.29s/it] 78%|███████▊  | 2545/3250 [11:57:15<3:09:59, 16.17s/it]                                                         78%|███████▊  | 2545/3250 [11:57:15<3:09:59, 16.17s/it] 78{'loss': 0.383, 'learning_rate': 1.1150819263608097e-05, 'epoch': 0.78}
{'loss': 0.4343, 'learning_rate': 1.112039258295408e-05, 'epoch': 0.78}
{'loss': 0.393, 'learning_rate': 1.109000227617269e-05, 'epoch': 0.78}
{'loss': 0.3996, 'learning_rate': 1.1059648371695585e-05, 'epoch': 0.78}
{'loss': 0.3894, 'learning_rate': 1.102933089792042e-05, 'epoch': 0.78}
%|███████▊  | 2546/3250 [11:57:31<3:09:36, 16.16s/it]                                                         78%|███████▊  | 2546/3250 [11:57:31<3:09:36, 16.16s/it] 78%|███████▊  | 2547/3250 [11:57:47<3:08:24, 16.08s/it]                                                         78%|███████▊  | 2547/3250 [11:57:47<3:08:24, 16.08s/it] 78%|███████▊  | 2548/3250 [11:58:03<3:07:30, 16.03s/it]                                                         78%|███████▊  | 2548/3250 [11:58:03<3:07:30, 16.03s/it] 78%|███████▊  | 2549/3250 [11:58:19<3:06:43, 15.98s/it]                                                         78%|███████▊  | 2549/3250 [11:58:19<3:06:43, 15.98s/it] 78%|███████▊  | 2550/3250 [11:58:35<3:06:11, 15.96s/it]                                                         78%|███████▊  | 2550/3250 [11:58:35<3:06:11, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7558994889259338, 'eval_runtime': 2.5729, 'eval_samples_per_second': 4.664, 'eval_steps_per_second': 1.166, 'epoch': 0.78}
                                                         78%|███████▊  | 2550/3250 [11:58:38<3:06:11, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2550
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2550 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2550
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2550

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3946, 'learning_rate': 1.0999049883210771e-05, 'epoch': 0.78}
{'loss': 0.4102, 'learning_rate': 1.0968805355896044e-05, 'epoch': 0.79}
{'loss': 0.4106, 'learning_rate': 1.0938597344271579e-05, 'epoch': 0.79}
{'loss': 0.4157, 'learning_rate': 1.090842587659851e-05, 'epoch': 0.79}
{'loss': 0.9116, 'learning_rate': 1.087829098110381e-05, 'epoch': 0.79}
 78%|███████▊  | 2551/3250 [11:58:54<3:17:09, 16.92s/it]                                                         78%|███████▊  | 2551/3250 [11:58:54<3:17:09, 16.92s/it] 79%|███████▊  | 2552/3250 [11:59:10<3:13:17, 16.61s/it]                                                         79%|███████▊  | 2552/3250 [11:59:10<3:13:17, 16.61s/it] 79%|███████▊  | 2553/3250 [11:59:26<3:10:26, 16.39s/it]                                                         79%|███████▊  | 2553/3250 [11:59:26<3:10:26, 16.39s/it] 79%|███████▊  | 2554/3250 [11:59:42<3:08:21, 16.24s/it]                                                         79%|███████▊  | 2554/3250 [11:59:42<3:08:21, 16.24s/it] 79%|███████▊  | 2555/3250 [11:59:58<3:06:44, 16.12s/it]                                                         79%|███████▊  | 2555/3250 [11:59:58<3:06:44, 16.12s/it] 79{'loss': 0.3974, 'learning_rate': 1.0848192685980219e-05, 'epoch': 0.79}
{'loss': 0.4251, 'learning_rate': 1.0818131019386252e-05, 'epoch': 0.79}
{'loss': 0.4058, 'learning_rate': 1.0788106009446119e-05, 'epoch': 0.79}
{'loss': 0.4141, 'learning_rate': 1.0758117684249769e-05, 'epoch': 0.79}
{'loss': 0.3951, 'learning_rate': 1.0728166071852835e-05, 'epoch': 0.79}
%|███████▊  | 2556/3250 [12:00:14<3:05:37, 16.05s/it]                                                         79%|███████▊  | 2556/3250 [12:00:14<3:05:37, 16.05s/it] 79%|███████▊  | 2557/3250 [12:00:29<3:04:45, 16.00s/it]                                                         79%|███████▊  | 2557/3250 [12:00:29<3:04:45, 16.00s/it] 79%|███████▊  | 2558/3250 [12:00:46<3:04:53, 16.03s/it]                                                         79%|███████▊  | 2558/3250 [12:00:46<3:04:53, 16.03s/it] 79%|███████▊  | 2559/3250 [12:01:01<3:04:03, 15.98s/it]                                                         79%|███████▊  | 2559/3250 [12:01:01<3:04:03, 15.98s/it] 79%|███████▉  | 2560/3250 [12:01:17<3:03:26, 15.95s/it]                                                         79%|███████▉  | 2560/3250 [12:01:17<3:03:26, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7575450539588928, 'eval_runtime': 2.4871, 'eval_samples_per_second': 4.825, 'eval_steps_per_second': 1.206, 'epoch': 0.79}
                                                         79%|███████▉  | 2560/3250 [12:01:20<3:03:26, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2560
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2560

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2560
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2560

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.455, 'learning_rate': 1.0698251200276583e-05, 'epoch': 0.79}
{'loss': 0.4334, 'learning_rate': 1.0668373097507923e-05, 'epoch': 0.79}
{'loss': 0.3962, 'learning_rate': 1.063853179149934e-05, 'epoch': 0.79}
{'loss': 0.407, 'learning_rate': 1.060872731016892e-05, 'epoch': 0.79}
{'loss': 0.4115, 'learning_rate': 1.0578959681400297e-05, 'epoch': 0.79}
 79%|███████▉  | 2561/3250 [12:01:51<4:03:03, 21.17s/it]                                                         79%|███████▉  | 2561/3250 [12:01:51<4:03:03, 21.17s/it] 79%|███████▉  | 2562/3250 [12:02:06<3:44:29, 19.58s/it]                                                         79%|███████▉  | 2562/3250 [12:02:06<3:44:29, 19.58s/it] 79%|███████▉  | 2563/3250 [12:02:22<3:31:27, 18.47s/it]                                                         79%|███████▉  | 2563/3250 [12:02:22<3:31:27, 18.47s/it] 79%|███████▉  | 2564/3250 [12:02:39<3:24:58, 17.93s/it]                                                         79%|███████▉  | 2564/3250 [12:02:39<3:24:58, 17.93s/it] 79%|███████▉  | 2565/3250 [12:02:55<3:17:38, 17.31s/it]                                                         79%|███████▉  | 2565/3250 [12:02:55<3:17:38, 17.31s/it] 79{'loss': 0.4091, 'learning_rate': 1.05492289330426e-05, 'epoch': 0.79}
{'loss': 0.4117, 'learning_rate': 1.0519535092910482e-05, 'epoch': 0.79}
{'loss': 0.406, 'learning_rate': 1.0489878188784063e-05, 'epoch': 0.79}
{'loss': 0.402, 'learning_rate': 1.0460258248408911e-05, 'epoch': 0.79}
{'loss': 0.4106, 'learning_rate': 1.0430675299495973e-05, 'epoch': 0.79}
%|███████▉  | 2566/3250 [12:03:11<3:12:26, 16.88s/it]                                                         79%|███████▉  | 2566/3250 [12:03:11<3:12:26, 16.88s/it] 79%|███████▉  | 2567/3250 [12:03:27<3:08:41, 16.58s/it]                                                         79%|███████▉  | 2567/3250 [12:03:27<3:08:41, 16.58s/it] 79%|███████▉  | 2568/3250 [12:03:42<3:05:59, 16.36s/it]                                                         79%|███████▉  | 2568/3250 [12:03:42<3:05:59, 16.36s/it] 79%|███████▉  | 2569/3250 [12:03:58<3:04:03, 16.22s/it]                                                         79%|███████▉  | 2569/3250 [12:03:58<3:04:03, 16.22s/it] 79%|███████▉  | 2570/3250 [12:04:14<3:02:37, 16.11s/it]                                                         79%|███████▉  | 2570/3250 [12:04:14<3:02:37, 16.11s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7540550231933594, 'eval_runtime': 2.4907, 'eval_samples_per_second': 4.818, 'eval_steps_per_second': 1.204, 'epoch': 0.79}
                                                         79%|███████▉  | 2570/3250 [12:04:17<3:02:37, 16.11s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2570
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2570/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2570

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2570

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4354, 'learning_rate': 1.040112936972164e-05, 'epoch': 0.79}
{'loss': 0.4051, 'learning_rate': 1.0371620486727651e-05, 'epoch': 0.79}
{'loss': 0.4145, 'learning_rate': 1.0342148678121073e-05, 'epoch': 0.79}
{'loss': 0.4161, 'learning_rate': 1.0312713971474307e-05, 'epoch': 0.79}
{'loss': 0.4028, 'learning_rate': 1.0283316394325054e-05, 'epoch': 0.79}
 79%|███████▉  | 2571/3250 [12:04:34<3:13:12, 17.07s/it]                                                         79%|███████▉  | 2571/3250 [12:04:34<3:13:12, 17.07s/it] 79%|███████▉  | 2572/3250 [12:04:49<3:08:56, 16.72s/it]                                                         79%|███████▉  | 2572/3250 [12:04:49<3:08:56, 16.72s/it] 79%|███████▉  | 2573/3250 [12:05:05<3:05:46, 16.47s/it]                                                         79%|███████▉  | 2573/3250 [12:05:05<3:05:46, 16.47s/it] 79%|███████▉  | 2574/3250 [12:05:22<3:05:10, 16.44s/it]                                                         79%|███████▉  | 2574/3250 [12:05:22<3:05:10, 16.44s/it] 79%|███████▉  | 2575/3250 [12:05:38<3:03:03, 16.27s/it]                                                         79%|███████▉  | 2575/3250 [12:05:38<3:03:03, 16.27s/it] 79{'loss': 0.3783, 'learning_rate': 1.0253955974176215e-05, 'epoch': 0.79}
{'loss': 0.4264, 'learning_rate': 1.0224632738496003e-05, 'epoch': 0.79}
{'loss': 0.3897, 'learning_rate': 1.0195346714717813e-05, 'epoch': 0.79}
{'loss': 0.3806, 'learning_rate': 1.0166097930240215e-05, 'epoch': 0.79}
{'loss': 0.3828, 'learning_rate': 1.0136886412426972e-05, 'epoch': 0.79}
%|███████▉  | 2576/3250 [12:05:53<3:01:29, 16.16s/it]                                                         79%|███████▉  | 2576/3250 [12:05:53<3:01:29, 16.16s/it] 79%|███████▉  | 2577/3250 [12:06:09<3:00:17, 16.07s/it]                                                         79%|███████▉  | 2577/3250 [12:06:09<3:00:17, 16.07s/it] 79%|███████▉  | 2578/3250 [12:06:25<2:59:20, 16.01s/it]                                                         79%|███████▉  | 2578/3250 [12:06:25<2:59:20, 16.01s/it] 79%|███████▉  | 2579/3250 [12:06:41<2:58:38, 15.97s/it]                                                         79%|███████▉  | 2579/3250 [12:06:41<2:58:38, 15.97s/it] 79%|███████▉  | 2580/3250 [12:06:57<2:58:01, 15.94s/it]                                                         79%|███████▉  | 2580/3250 [12:06:57<2:58:01, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7573749423027039, 'eval_runtime': 2.4863, 'eval_samples_per_second': 4.826, 'eval_steps_per_second': 1.207, 'epoch': 0.79}
                                                         79%|███████▉  | 2580/3250 [12:06:59<2:58:01, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2580
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2580/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2580

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2580

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2580/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2580/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2580/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3911, 'learning_rate': 1.0107712188606933e-05, 'epoch': 0.79}
{'loss': 0.4142, 'learning_rate': 1.0078575286074116e-05, 'epoch': 0.79}
{'loss': 0.3991, 'learning_rate': 1.004947573208756e-05, 'epoch': 0.79}
{'loss': 0.4043, 'learning_rate': 1.002041355387141e-05, 'epoch': 0.8}
{'loss': 0.9124, 'learning_rate': 9.991388778614824e-06, 'epoch': 0.8}
 79%|███████▉  | 2581/3250 [12:07:16<3:08:17, 16.89s/it]                                                         79%|███████▉  | 2581/3250 [12:07:16<3:08:17, 16.89s/it] 79%|███████▉  | 2582/3250 [12:07:32<3:04:41, 16.59s/it]                                                         79%|███████▉  | 2582/3250 [12:07:32<3:04:41, 16.59s/it] 79%|███████▉  | 2583/3250 [12:07:48<3:02:00, 16.37s/it]                                                         79%|███████▉  | 2583/3250 [12:07:48<3:02:00, 16.37s/it] 80%|███████▉  | 2584/3250 [12:08:04<3:00:04, 16.22s/it]                                                         80%|███████▉  | 2584/3250 [12:08:04<3:00:04, 16.22s/it] 80%|███████▉  | 2585/3250 [12:08:20<2:58:33, 16.11s/it]                                                         80%|███████▉  | 2585/3250 [12:08:20<2:58:33, 16.11s/it] 80{'loss': 0.4013, 'learning_rate': 9.962401433471985e-06, 'epoch': 0.8}
{'loss': 0.4048, 'learning_rate': 9.933451545562044e-06, 'epoch': 0.8}
{'loss': 0.4199, 'learning_rate': 9.904539141969093e-06, 'epoch': 0.8}
{'loss': 0.4129, 'learning_rate': 9.875664249742183e-06, 'epoch': 0.8}
{'loss': 0.395, 'learning_rate': 9.84682689589526e-06, 'epoch': 0.8}
%|███████▉  | 2586/3250 [12:08:35<2:57:29, 16.04s/it]                                                         80%|███████▉  | 2586/3250 [12:08:35<2:57:29, 16.04s/it] 80%|███████▉  | 2587/3250 [12:08:51<2:56:44, 15.99s/it]                                                         80%|███████▉  | 2587/3250 [12:08:51<2:56:44, 15.99s/it] 80%|███████▉  | 2588/3250 [12:09:07<2:56:07, 15.96s/it]                                                         80%|███████▉  | 2588/3250 [12:09:07<2:56:07, 15.96s/it] 80%|███████▉  | 2589/3250 [12:09:23<2:55:35, 15.94s/it]                                                         80%|███████▉  | 2589/3250 [12:09:23<2:55:35, 15.94s/it] 80%|███████▉  | 2590/3250 [12:09:39<2:55:06, 15.92s/it]                                                         80%|███████▉  | 2590/3250 [12:09:39<2:55:06, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7587092518806458, 'eval_runtime': 2.4988, 'eval_samples_per_second': 4.802, 'eval_steps_per_second': 1.201, 'epoch': 0.8}
                                                         80%|███████▉  | 2590/3250 [12:09:41<2:55:06, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2590
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2590 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2590
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2590

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4011, 'learning_rate': 9.818027107407151e-06, 'epoch': 0.8}
{'loss': 0.4648, 'learning_rate': 9.789264911221546e-06, 'epoch': 0.8}
{'loss': 0.4158, 'learning_rate': 9.760540334246965e-06, 'epoch': 0.8}
{'loss': 0.4067, 'learning_rate': 9.731853403356705e-06, 'epoch': 0.8}
{'loss': 0.3719, 'learning_rate': 9.703204145388879e-06, 'epoch': 0.8}
 80%|███████▉  | 2591/3250 [12:09:59<3:08:46, 17.19s/it]                                                         80%|███████▉  | 2591/3250 [12:09:59<3:08:46, 17.19s/it] 80%|███████▉  | 2592/3250 [12:10:15<3:04:10, 16.79s/it]                                                         80%|███████▉  | 2592/3250 [12:10:15<3:04:10, 16.79s/it] 80%|███████▉  | 2593/3250 [12:10:31<3:00:53, 16.52s/it]                                                         80%|███████▉  | 2593/3250 [12:10:31<3:00:53, 16.52s/it] 80%|███████▉  | 2594/3250 [12:10:47<2:58:31, 16.33s/it]                                                         80%|███████▉  | 2594/3250 [12:10:47<2:58:31, 16.33s/it] 80%|███████▉  | 2595/3250 [12:11:03<2:56:49, 16.20s/it]                                                         80%|███████▉  | 2595/3250 [12:11:03<2:56:49, 16.20s/it] 80{'loss': 0.4192, 'learning_rate': 9.674592587146336e-06, 'epoch': 0.8}
{'loss': 0.4228, 'learning_rate': 9.646018755396664e-06, 'epoch': 0.8}
{'loss': 0.3996, 'learning_rate': 9.617482676872164e-06, 'epoch': 0.8}
{'loss': 0.3974, 'learning_rate': 9.588984378269783e-06, 'epoch': 0.8}
{'loss': 0.4095, 'learning_rate': 9.560523886251171e-06, 'epoch': 0.8}
%|███████▉  | 2596/3250 [12:11:18<2:55:29, 16.10s/it]                                                         80%|███████▉  | 2596/3250 [12:11:18<2:55:29, 16.10s/it] 80%|███████▉  | 2597/3250 [12:11:34<2:54:32, 16.04s/it]                                                         80%|███████▉  | 2597/3250 [12:11:34<2:54:32, 16.04s/it] 80%|███████▉  | 2598/3250 [12:11:50<2:53:45, 15.99s/it]                                                         80%|███████▉  | 2598/3250 [12:11:50<2:53:45, 15.99s/it] 80%|███████▉  | 2599/3250 [12:12:07<2:57:09, 16.33s/it]                                                         80%|███████▉  | 2599/3250 [12:12:07<2:57:09, 16.33s/it] 80%|████████  | 2600/3250 [12:12:23<2:55:45, 16.22s/it]                                                         80%|████████  | 2600/3250 [12:12:23<2:55:45, 16.22s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7533024549484253, 'eval_runtime': 2.6126, 'eval_samples_per_second': 4.593, 'eval_steps_per_second': 1.148, 'epoch': 0.8}
                                                         80%|████████  | 2600/3250 [12:12:26<2:55:45, 16.22s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2600
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2600/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2600

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4102, 'learning_rate': 9.532101227442552e-06, 'epoch': 0.8}
{'loss': 0.4363, 'learning_rate': 9.5037164284348e-06, 'epoch': 0.8}
{'loss': 0.406, 'learning_rate': 9.475369515783355e-06, 'epoch': 0.8}
{'loss': 0.4165, 'learning_rate': 9.447060516008211e-06, 'epoch': 0.8}
{'loss': 0.4241, 'learning_rate': 9.418789455593906e-06, 'epoch': 0.8}
 80%|████████  | 2601/3250 [12:12:43<3:05:53, 17.19s/it]                                                         80%|████████  | 2601/3250 [12:12:43<3:05:53, 17.19s/it] 80%|████████  | 2602/3250 [12:12:59<3:01:26, 16.80s/it]                                                         80%|████████  | 2602/3250 [12:12:59<3:01:26, 16.80s/it] 80%|████████  | 2603/3250 [12:13:15<2:58:16, 16.53s/it]                                                         80%|████████  | 2603/3250 [12:13:15<2:58:16, 16.53s/it] 80%|████████  | 2604/3250 [12:13:30<2:55:57, 16.34s/it]                                                         80%|████████  | 2604/3250 [12:13:30<2:55:57, 16.34s/it] 80%|████████  | 2605/3250 [12:13:46<2:54:16, 16.21s/it]                                                         80%|████████  | 2605/3250 [12:13:46<2:54:16, 16.21s/it] 80{'loss': 0.3966, 'learning_rate': 9.39055636098945e-06, 'epoch': 0.8}
{'loss': 0.4131, 'learning_rate': 9.362361258608365e-06, 'epoch': 0.8}
{'loss': 0.4072, 'learning_rate': 9.334204174828614e-06, 'epoch': 0.8}
{'loss': 0.385, 'learning_rate': 9.30608513599261e-06, 'epoch': 0.8}
{'loss': 0.3841, 'learning_rate': 9.27800416840715e-06, 'epoch': 0.8}
%|████████  | 2606/3250 [12:14:02<2:53:02, 16.12s/it]                                                         80%|████████  | 2606/3250 [12:14:02<2:53:02, 16.12s/it] 80%|████████  | 2607/3250 [12:14:19<2:53:23, 16.18s/it]                                                         80%|████████  | 2607/3250 [12:14:19<2:53:23, 16.18s/it] 80%|████████  | 2608/3250 [12:14:35<2:52:14, 16.10s/it]                                                         80%|████████  | 2608/3250 [12:14:35<2:52:14, 16.10s/it] 80%|████████  | 2609/3250 [12:14:50<2:51:20, 16.04s/it]                                                         80%|████████  | 2609/3250 [12:14:50<2:51:20, 16.04s/it] 80%|████████  | 2610/3250 [12:15:08<2:56:03, 16.50s/it]                                                         80%|████████  | 2610/3250 [12:15:08<2:56:03, 16.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7570690512657166, 'eval_runtime': 3.9, 'eval_samples_per_second': 3.077, 'eval_steps_per_second': 0.769, 'epoch': 0.8}
                                                         80%|████████  | 2610/3250 [12:15:12<2:56:03, 16.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2610
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2610

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2610

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2610/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3892, 'learning_rate': 9.249961298343435e-06, 'epoch': 0.8}
{'loss': 0.4113, 'learning_rate': 9.221956552036992e-06, 'epoch': 0.8}
{'loss': 0.4084, 'learning_rate': 9.193989955687715e-06, 'epoch': 0.8}
{'loss': 0.4161, 'learning_rate': 9.166061535459798e-06, 'epoch': 0.8}
{'loss': 0.9294, 'learning_rate': 9.138171317481697e-06, 'epoch': 0.8}
 80%|████████  | 2611/3250 [12:15:30<3:12:32, 18.08s/it]                                                         80%|████████  | 2611/3250 [12:15:30<3:12:32, 18.08s/it] 80%|████████  | 2612/3250 [12:15:46<3:05:26, 17.44s/it]                                                         80%|████████  | 2612/3250 [12:15:46<3:05:26, 17.44s/it] 80%|████████  | 2613/3250 [12:16:02<3:00:17, 16.98s/it]                                                         80%|████████  | 2613/3250 [12:16:02<3:00:17, 16.98s/it] 80%|████████  | 2614/3250 [12:16:18<2:56:36, 16.66s/it]                                                         80%|████████  | 2614/3250 [12:16:18<2:56:36, 16.66s/it] 80%|████████  | 2615/3250 [12:16:33<2:53:50, 16.43s/it]                                                         80%|████████  | 2615/3250 [12:16:33<2:53:50, 16.43s/it] 80{'loss': 0.3827, 'learning_rate': 9.11031932784618e-06, 'epoch': 0.8}
{'loss': 0.3869, 'learning_rate': 9.082505592610174e-06, 'epoch': 0.81}
{'loss': 0.4145, 'learning_rate': 9.054730137794886e-06, 'epoch': 0.81}
{'loss': 0.4203, 'learning_rate': 9.026992989385669e-06, 'epoch': 0.81}
{'loss': 0.404, 'learning_rate': 8.999294173332058e-06, 'epoch': 0.81}
%|████████  | 2616/3250 [12:16:50<2:52:31, 16.33s/it]                                                         80%|████████  | 2616/3250 [12:16:50<2:52:31, 16.33s/it] 81%|████████  | 2617/3250 [12:17:05<2:50:55, 16.20s/it]                                                         81%|████████  | 2617/3250 [12:17:05<2:50:55, 16.20s/it] 81%|████████  | 2618/3250 [12:17:21<2:49:43, 16.11s/it]                                                         81%|████████  | 2618/3250 [12:17:21<2:49:43, 16.11s/it] 81%|████████  | 2619/3250 [12:17:37<2:48:48, 16.05s/it]                                                         81%|████████  | 2619/3250 [12:17:37<2:48:48, 16.05s/it] 81%|████████  | 2620/3250 [12:17:53<2:48:03, 16.01s/it]                                                         81%|████████  | 2620/3250 [12:17:53<2:48:03, 16.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7597811818122864, 'eval_runtime': 2.727, 'eval_samples_per_second': 4.401, 'eval_steps_per_second': 1.1, 'epoch': 0.81}
                                                         81%|████████  | 2620/3250 [12:17:56<2:48:03, 16.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2620
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2620
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2620
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2620

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2620
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2620/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2620/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4062, 'learning_rate': 8.971633715547717e-06, 'epoch': 0.81}
{'loss': 0.44, 'learning_rate': 8.944011641910432e-06, 'epoch': 0.81}
{'loss': 0.4223, 'learning_rate': 8.916427978262082e-06, 'epoch': 0.81}
{'loss': 0.3949, 'learning_rate': 8.888882750408578e-06, 'epoch': 0.81}
{'loss': 0.3786, 'learning_rate': 8.861375984119918e-06, 'epoch': 0.81}
 81%|████████  | 2621/3250 [12:18:31<3:56:13, 22.53s/it]                                                         81%|████████  | 2621/3250 [12:18:31<3:56:13, 22.53s/it] 81%|████████  | 2622/3250 [12:18:47<3:35:05, 20.55s/it]                                                         81%|████████  | 2622/3250 [12:18:47<3:35:05, 20.55s/it] 81%|████████  | 2623/3250 [12:19:03<3:21:45, 19.31s/it]                                                         81%|████████  | 2623/3250 [12:19:03<3:21:45, 19.31s/it] 81%|████████  | 2624/3250 [12:19:19<3:10:56, 18.30s/it]                                                         81%|████████  | 2624/3250 [12:19:19<3:10:56, 18.30s/it] 81%|████████  | 2625/3250 [12:19:35<3:03:19, 17.60s/it]                                                         81%|████████  | 2625/3250 [12:19:35<3:03:19, 17.60s/it] 81{'loss': 0.4249, 'learning_rate': 8.83390770513009e-06, 'epoch': 0.81}
{'loss': 0.4058, 'learning_rate': 8.80647793913708e-06, 'epoch': 0.81}
{'loss': 0.4022, 'learning_rate': 8.779086711802847e-06, 'epoch': 0.81}
{'loss': 0.392, 'learning_rate': 8.751734048753306e-06, 'epoch': 0.81}
{'loss': 0.4069, 'learning_rate': 8.724419975578257e-06, 'epoch': 0.81}
%|████████  | 2626/3250 [12:19:51<2:58:32, 17.17s/it]                                                         81%|████████  | 2626/3250 [12:19:51<2:58:32, 17.17s/it] 81%|████████  | 2627/3250 [12:20:07<2:54:12, 16.78s/it]                                                         81%|████████  | 2627/3250 [12:20:07<2:54:12, 16.78s/it] 81%|████████  | 2628/3250 [12:20:23<2:51:07, 16.51s/it]                                                         81%|████████  | 2628/3250 [12:20:23<2:51:07, 16.51s/it] 81%|████████  | 2629/3250 [12:20:39<2:48:55, 16.32s/it]                                                         81%|████████  | 2629/3250 [12:20:39<2:48:55, 16.32s/it] 81%|████████  | 2630/3250 [12:20:55<2:47:15, 16.19s/it]                                                         81%|████████  | 2630/3250 [12:20:55<2:47:15, 16.19s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7557579278945923, 'eval_runtime': 2.4697, 'eval_samples_per_second': 4.859, 'eval_steps_per_second': 1.215, 'epoch': 0.81}
                                                         81%|████████  | 2630/3250 [12:20:57<2:47:15, 16.19s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2630
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2630/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2630

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4139, 'learning_rate': 8.697144517831435e-06, 'epoch': 0.81}
{'loss': 0.4342, 'learning_rate': 8.669907701030428e-06, 'epoch': 0.81}
{'loss': 0.4012, 'learning_rate': 8.64270955065669e-06, 'epoch': 0.81}
{'loss': 0.424, 'learning_rate': 8.615550092155478e-06, 'epoch': 0.81}
{'loss': 0.4043, 'learning_rate': 8.588429350935857e-06, 'epoch': 0.81}
 81%|████████  | 2631/3250 [12:21:14<2:55:52, 17.05s/it]                                                         81%|████████  | 2631/3250 [12:21:14<2:55:52, 17.05s/it] 81%|████████  | 2632/3250 [12:21:30<2:51:58, 16.70s/it]                                                         81%|████████  | 2632/3250 [12:21:30<2:51:58, 16.70s/it] 81%|████████  | 2633/3250 [12:21:46<2:49:07, 16.45s/it]                                                         81%|████████  | 2633/3250 [12:21:46<2:49:07, 16.45s/it] 81%|████████  | 2634/3250 [12:22:01<2:47:04, 16.27s/it]                                                         81%|████████  | 2634/3250 [12:22:01<2:47:04, 16.27s/it] 81%|████████  | 2635/3250 [12:22:17<2:45:33, 16.15s/it]                                                         81%|████████  | 2635/3250 [12:22:17<2:45:33, 16.15s/it] 81{'loss': 0.4175, 'learning_rate': 8.561347352370703e-06, 'epoch': 0.81}
{'loss': 0.3786, 'learning_rate': 8.534304121796582e-06, 'epoch': 0.81}
{'loss': 0.4337, 'learning_rate': 8.507299684513848e-06, 'epoch': 0.81}
{'loss': 0.3971, 'learning_rate': 8.480334065786532e-06, 'epoch': 0.81}
{'loss': 0.3998, 'learning_rate': 8.45340729084237e-06, 'epoch': 0.81}
%|████████  | 2636/3250 [12:22:33<2:44:39, 16.09s/it]                                                         81%|████████  | 2636/3250 [12:22:33<2:44:39, 16.09s/it] 81%|████████  | 2637/3250 [12:22:49<2:43:53, 16.04s/it]                                                         81%|████████  | 2637/3250 [12:22:49<2:43:53, 16.04s/it] 81%|████████  | 2638/3250 [12:23:05<2:43:14, 16.00s/it]                                                         81%|████████  | 2638/3250 [12:23:05<2:43:14, 16.00s/it] 81%|████████  | 2639/3250 [12:23:21<2:42:48, 15.99s/it]                                                         81%|████████  | 2639/3250 [12:23:21<2:42:48, 15.99s/it] 81%|████████  | 2640/3250 [12:23:37<2:43:03, 16.04s/it]                                                         81%|████████  | 2640/3250 [12:23:37<2:43:03, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.757011353969574, 'eval_runtime': 2.7197, 'eval_samples_per_second': 4.412, 'eval_steps_per_second': 1.103, 'epoch': 0.81}
                                                         81%|████████  | 2640/3250 [12:23:40<2:43:03, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2640
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2640/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2640


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2640

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3891, 'learning_rate': 8.426519384872733e-06, 'epoch': 0.81}
{'loss': 0.3879, 'learning_rate': 8.399670373032664e-06, 'epoch': 0.81}
{'loss': 0.3964, 'learning_rate': 8.372860280440759e-06, 'epoch': 0.81}
{'loss': 0.418, 'learning_rate': 8.346089132179257e-06, 'epoch': 0.81}
{'loss': 0.3988, 'learning_rate': 8.319356953293944e-06, 'epoch': 0.81}
 81%|████████▏ | 2641/3250 [12:23:57<2:53:06, 17.05s/it]                                                         81%|████████▏ | 2641/3250 [12:23:57<2:53:06, 17.05s/it] 81%|████████▏ | 2642/3250 [12:24:13<2:49:22, 16.71s/it]                                                         81%|████████▏ | 2642/3250 [12:24:13<2:49:22, 16.71s/it] 81%|████████▏ | 2643/3250 [12:24:29<2:46:37, 16.47s/it]                                                         81%|████████▏ | 2643/3250 [12:24:29<2:46:37, 16.47s/it] 81%|████████▏ | 2644/3250 [12:24:44<2:44:41, 16.31s/it]                                                         81%|████████▏ | 2644/3250 [12:24:44<2:44:41, 16.31s/it] 81%|████████▏ | 2645/3250 [12:25:00<2:43:15, 16.19s/it]                                                         81%|████████▏ | 2645/3250 [12:25:00<2:4{'loss': 0.9101, 'learning_rate': 8.292663768794145e-06, 'epoch': 0.81}
{'loss': 0.3994, 'learning_rate': 8.266009603652724e-06, 'epoch': 0.81}
{'loss': 0.4326, 'learning_rate': 8.239394482805996e-06, 'epoch': 0.81}
{'loss': 0.4117, 'learning_rate': 8.21281843115379e-06, 'epoch': 0.82}
{'loss': 0.4072, 'learning_rate': 8.186281473559381e-06, 'epoch': 0.82}
3:15, 16.19s/it] 81%|████████▏ | 2646/3250 [12:25:16<2:42:01, 16.09s/it]                                                         81%|████████▏ | 2646/3250 [12:25:16<2:42:01, 16.09s/it] 81%|████████▏ | 2647/3250 [12:25:32<2:41:10, 16.04s/it]                                                         81%|████████▏ | 2647/3250 [12:25:32<2:41:10, 16.04s/it] 81%|████████▏ | 2648/3250 [12:25:48<2:40:27, 15.99s/it]                                                         81%|████████▏ | 2648/3250 [12:25:48<2:40:27, 15.99s/it] 82%|████████▏ | 2649/3250 [12:26:04<2:39:55, 15.97s/it]                                                         82%|████████▏ | 2649/3250 [12:26:04<2:39:55, 15.97s/it] 82%|████████▏ | 2650/3250 [12:26:20<2:39:22, 15.94s/it]                                                         82%|████████▏ | 2650/3250 [12:26:20<2:39:22, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7594833374023438, 'eval_runtime': 2.489, 'eval_samples_per_second': 4.821, 'eval_steps_per_second': 1.205, 'epoch': 0.82}
                                                         82%|████████▏ | 2650/3250 [12:26:22<2:39:22, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2650
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2650/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2650

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3962, 'learning_rate': 8.159783634849427e-06, 'epoch': 0.82}
{'loss': 0.4483, 'learning_rate': 8.13332493981404e-06, 'epoch': 0.82}
{'loss': 0.4301, 'learning_rate': 8.106905413206689e-06, 'epoch': 0.82}
{'loss': 0.3908, 'learning_rate': 8.080525079744212e-06, 'epoch': 0.82}
{'loss': 0.4028, 'learning_rate': 8.054183964106738e-06, 'epoch': 0.82}
 82%|████████▏ | 2651/3250 [12:26:39<2:48:37, 16.89s/it]                                                         82%|████████▏ | 2651/3250 [12:26:39<2:48:37, 16.89s/it] 82%|████████▏ | 2652/3250 [12:26:55<2:45:17, 16.58s/it]                                                         82%|████████▏ | 2652/3250 [12:26:55<2:45:17, 16.58s/it] 82%|████████▏ | 2653/3250 [12:27:11<2:42:51, 16.37s/it]                                                         82%|████████▏ | 2653/3250 [12:27:11<2:42:51, 16.37s/it] 82%|████████▏ | 2654/3250 [12:27:27<2:41:08, 16.22s/it]                                                         82%|████████▏ | 2654/3250 [12:27:27<2:41:08, 16.22s/it] 82%|████████▏ | 2655/3250 [12:27:42<2:39:48, 16.12s/it]                                                         82%|████████▏ | 2655/3250 [12:27:42<2:3{'loss': 0.4126, 'learning_rate': 8.02788209093775e-06, 'epoch': 0.82}
{'loss': 0.3998, 'learning_rate': 8.001619484844009e-06, 'epoch': 0.82}
{'loss': 0.4123, 'learning_rate': 7.975396170395521e-06, 'epoch': 0.82}
{'loss': 0.3879, 'learning_rate': 7.949212172125565e-06, 'epoch': 0.82}
{'loss': 0.3987, 'learning_rate': 7.923067514530613e-06, 'epoch': 0.82}
9:48, 16.12s/it] 82%|████████▏ | 2656/3250 [12:27:59<2:39:54, 16.15s/it]                                                         82%|████████▏ | 2656/3250 [12:27:59<2:39:54, 16.15s/it] 82%|████████▏ | 2657/3250 [12:28:15<2:38:51, 16.07s/it]                                                         82%|████████▏ | 2657/3250 [12:28:15<2:38:51, 16.07s/it] 82%|████████▏ | 2658/3250 [12:28:30<2:38:00, 16.01s/it]                                                         82%|████████▏ | 2658/3250 [12:28:30<2:38:00, 16.01s/it] 82%|████████▏ | 2659/3250 [12:28:46<2:37:23, 15.98s/it]                                                         82%|████████▏ | 2659/3250 [12:28:46<2:37:23, 15.98s/it] 82%|████████▏ | 2660/3250 [12:29:02<2:36:49, 15.95s/it]                                                         82%|████████▏ | 2660/3250 [12:29:02<2:36:49, 15.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7549155354499817, 'eval_runtime': 2.482, 'eval_samples_per_second': 4.835, 'eval_steps_per_second': 1.209, 'epoch': 0.82}
                                                         82%|████████▏ | 2660/3250 [12:29:05<2:36:49, 15.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2660
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2660
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2660
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2660

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2660/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4056, 'learning_rate': 7.89696222207032e-06, 'epoch': 0.82}
{'loss': 0.4398, 'learning_rate': 7.870896319167548e-06, 'epoch': 0.82}
{'loss': 0.4013, 'learning_rate': 7.844869830208273e-06, 'epoch': 0.82}
{'loss': 0.4092, 'learning_rate': 7.818882779541631e-06, 'epoch': 0.82}
{'loss': 0.4175, 'learning_rate': 7.79293519147985e-06, 'epoch': 0.82}
 82%|████████▏ | 2661/3250 [12:29:21<2:46:07, 16.92s/it]                                                         82%|████████▏ | 2661/3250 [12:29:21<2:46:07, 16.92s/it] 82%|████████▏ | 2662/3250 [12:29:37<2:42:44, 16.61s/it]                                                         82%|████████▏ | 2662/3250 [12:29:37<2:42:44, 16.61s/it] 82%|████████▏ | 2663/3250 [12:29:53<2:40:20, 16.39s/it]                                                         82%|████████▏ | 2663/3250 [12:29:53<2:40:20, 16.39s/it] 82%|████████▏ | 2664/3250 [12:30:09<2:38:33, 16.23s/it]                                                         82%|████████▏ | 2664/3250 [12:30:09<2:38:33, 16.23s/it] 82%|████████▏ | 2665/3250 [12:30:25<2:37:15, 16.13s/it]                                                         82%|████████▏ | 2665/3250 [12:30:25<2:3{'loss': 0.4074, 'learning_rate': 7.767027090298207e-06, 'epoch': 0.82}
{'loss': 0.3777, 'learning_rate': 7.74115850023509e-06, 'epoch': 0.82}
{'loss': 0.4294, 'learning_rate': 7.715329445491876e-06, 'epoch': 0.82}
{'loss': 0.3979, 'learning_rate': 7.689539950232977e-06, 'epoch': 0.82}
{'loss': 0.39, 'learning_rate': 7.663790038585793e-06, 'epoch': 0.82}
7:15, 16.13s/it] 82%|████████▏ | 2666/3250 [12:30:41<2:36:16, 16.06s/it]                                                         82%|████████▏ | 2666/3250 [12:30:41<2:36:16, 16.06s/it] 82%|████████▏ | 2667/3250 [12:30:57<2:35:28, 16.00s/it]                                                         82%|████████▏ | 2667/3250 [12:30:57<2:35:28, 16.00s/it] 82%|████████▏ | 2668/3250 [12:31:12<2:34:48, 15.96s/it]                                                         82%|████████▏ | 2668/3250 [12:31:12<2:34:48, 15.96s/it] 82%|████████▏ | 2669/3250 [12:31:28<2:34:17, 15.93s/it]                                                         82%|████████▏ | 2669/3250 [12:31:28<2:34:17, 15.93s/it] 82%|████████▏ | 2670/3250 [12:31:44<2:33:52, 15.92s/it]                                                         82%|████████▏ | 2670/3250 [12:31:44<2:33:52, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.755723237991333, 'eval_runtime': 2.4895, 'eval_samples_per_second': 4.82, 'eval_steps_per_second': 1.205, 'epoch': 0.82}
                                                         82%|████████▏ | 2670/3250 [12:31:47<2:33:52, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2670
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2670 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2670/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2670


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3966, 'learning_rate': 7.638079734640703e-06, 'epoch': 0.82}
{'loss': 0.3923, 'learning_rate': 7.612409062451015e-06, 'epoch': 0.82}
{'loss': 0.4047, 'learning_rate': 7.58677804603295e-06, 'epoch': 0.82}
{'loss': 0.402, 'learning_rate': 7.561186709365653e-06, 'epoch': 0.82}
{'loss': 0.4115, 'learning_rate': 7.5356350763911345e-06, 'epoch': 0.82}
 82%|████████▏ | 2671/3250 [12:32:03<2:42:54, 16.88s/it]                                                         82%|████████▏ | 2671/3250 [12:32:03<2:42:54, 16.88s/it] 82%|████████▏ | 2672/3250 [12:32:19<2:39:43, 16.58s/it]                                                         82%|████████▏ | 2672/3250 [12:32:19<2:39:43, 16.58s/it] 82%|████████▏ | 2673/3250 [12:32:36<2:38:57, 16.53s/it]                                                         82%|████████▏ | 2673/3250 [12:32:36<2:38:57, 16.53s/it] 82%|████████▏ | 2674/3250 [12:32:52<2:36:50, 16.34s/it]                                                         82%|████████▏ | 2674/3250 [12:32:52<2:36:50, 16.34s/it] 82%|████████▏ | 2675/3250 [12:33:07<2:35:17, 16.20s/it]                                                         82%|████████▏ | 2675/3250 [12:33:07<2:3{'loss': 0.9186, 'learning_rate': 7.510123171014255e-06, 'epoch': 0.82}
{'loss': 0.4034, 'learning_rate': 7.484651017102728e-06, 'epoch': 0.82}
{'loss': 0.3998, 'learning_rate': 7.459218638487064e-06, 'epoch': 0.82}
{'loss': 0.4261, 'learning_rate': 7.4338260589605415e-06, 'epoch': 0.82}
{'loss': 0.4082, 'learning_rate': 7.408473302279234e-06, 'epoch': 0.82}
5:17, 16.20s/it] 82%|████████▏ | 2676/3250 [12:33:23<2:33:57, 16.09s/it]                                                         82%|████████▏ | 2676/3250 [12:33:23<2:33:57, 16.09s/it] 82%|████████▏ | 2677/3250 [12:33:39<2:33:03, 16.03s/it]                                                         82%|████████▏ | 2677/3250 [12:33:39<2:33:03, 16.03s/it] 82%|████████▏ | 2678/3250 [12:33:55<2:32:21, 15.98s/it]                                                         82%|████████▏ | 2678/3250 [12:33:55<2:32:21, 15.98s/it] 82%|████████▏ | 2679/3250 [12:34:11<2:31:51, 15.96s/it]                                                         82%|████████▏ | 2679/3250 [12:34:11<2:31:51, 15.96s/it] 82%|████████▏ | 2680/3250 [12:34:27<2:31:22, 15.93s/it]                                                         82%|████████▏ | 2680/3250 [12:34:27<2:31:22, 15.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7591450214385986, 'eval_runtime': 2.4813, 'eval_samples_per_second': 4.836, 'eval_steps_per_second': 1.209, 'epoch': 0.82}
                                                         82%|████████▏ | 2680/3250 [12:34:29<2:31:22, 15.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2680
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2680
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2680
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2680

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3951, 'learning_rate': 7.383160392161953e-06, 'epoch': 0.82}
{'loss': 0.395, 'learning_rate': 7.357887352290227e-06, 'epoch': 0.83}
{'loss': 0.4524, 'learning_rate': 7.332654206308298e-06, 'epoch': 0.83}
{'loss': 0.4117, 'learning_rate': 7.307460977823044e-06, 'epoch': 0.83}
{'loss': 0.3984, 'learning_rate': 7.282307690404055e-06, 'epoch': 0.83}
 82%|████████▏ | 2681/3250 [12:34:46<2:40:08, 16.89s/it]                                                         82%|████████▏ | 2681/3250 [12:34:46<2:40:08, 16.89s/it] 83%|████████▎ | 2682/3250 [12:35:02<2:37:05, 16.60s/it]                                                         83%|████████▎ | 2682/3250 [12:35:02<2:37:05, 16.60s/it] 83%|████████▎ | 2683/3250 [12:35:18<2:34:54, 16.39s/it]                                                         83%|████████▎ | 2683/3250 [12:35:18<2:34:54, 16.39s/it] 83%|████████▎ | 2684/3250 [12:35:34<2:33:17, 16.25s/it]                                                         83%|████████▎ | 2684/3250 [12:35:34<2:33:17, 16.25s/it] 83%|████████▎ | 2685/3250 [12:35:50<2:32:14, 16.17s/it]                                                         83%|████████▎ | 2685/3250 [12:35:50<2:3{'loss': 0.3727, 'learning_rate': 7.257194367583503e-06, 'epoch': 0.83}
{'loss': 0.4195, 'learning_rate': 7.232121032856193e-06, 'epoch': 0.83}
{'loss': 0.4199, 'learning_rate': 7.207087709679533e-06, 'epoch': 0.83}
{'loss': 0.3918, 'learning_rate': 7.182094421473479e-06, 'epoch': 0.83}
{'loss': 0.3876, 'learning_rate': 7.157141191620548e-06, 'epoch': 0.83}
2:14, 16.17s/it] 83%|████████▎ | 2686/3250 [12:36:06<2:31:19, 16.10s/it]                                                         83%|████████▎ | 2686/3250 [12:36:06<2:31:19, 16.10s/it] 83%|████████▎ | 2687/3250 [12:36:21<2:30:30, 16.04s/it]                                                         83%|████████▎ | 2687/3250 [12:36:21<2:30:30, 16.04s/it] 83%|████████▎ | 2688/3250 [12:36:37<2:29:53, 16.00s/it]                                                         83%|████████▎ | 2688/3250 [12:36:37<2:29:53, 16.00s/it] 83%|████████▎ | 2689/3250 [12:36:54<2:30:23, 16.09s/it]                                                         83%|████████▎ | 2689/3250 [12:36:54<2:30:23, 16.09s/it] 83%|████████▎ | 2690/3250 [12:37:10<2:29:35, 16.03s/it]                                                         83%|████████▎ | 2690/3250 [12:37:10<2:29:35, 16.03s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7574536800384521, 'eval_runtime': 2.4879, 'eval_samples_per_second': 4.823, 'eval_steps_per_second': 1.206, 'epoch': 0.83}
                                                         83%|████████▎ | 2690/3250 [12:37:12<2:29:35, 16.03s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2690the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2690

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.415, 'learning_rate': 7.13222804346575e-06, 'epoch': 0.83}
{'loss': 0.4009, 'learning_rate': 7.107355000316624e-06, 'epoch': 0.83}
{'loss': 0.4308, 'learning_rate': 7.082522085443183e-06, 'epoch': 0.83}
{'loss': 0.4079, 'learning_rate': 7.0577293220778996e-06, 'epoch': 0.83}
{'loss': 0.4098, 'learning_rate': 7.032976733415675e-06, 'epoch': 0.83}
 83%|████████▎ | 2691/3250 [12:37:29<2:38:31, 17.02s/it]                                                         83%|████████▎ | 2691/3250 [12:37:29<2:38:31, 17.02s/it] 83%|████████▎ | 2692/3250 [12:37:45<2:35:11, 16.69s/it]                                                         83%|████████▎ | 2692/3250 [12:37:45<2:35:11, 16.69s/it] 83%|████████▎ | 2693/3250 [12:38:01<2:32:45, 16.46s/it]                                                         83%|████████▎ | 2693/3250 [12:38:01<2:32:45, 16.46s/it] 83%|████████▎ | 2694/3250 [12:38:17<2:30:58, 16.29s/it]                                                         83%|████████▎ | 2694/3250 [12:38:17<2:30:58, 16.29s/it] 83%|████████▎ | 2695/3250 [12:38:33<2:29:35, 16.17s/it]                                                         83%|████████▎ | 2695/3250 [12:38:33<2:2{'loss': 0.4118, 'learning_rate': 7.0082643426138405e-06, 'epoch': 0.83}
{'loss': 0.3843, 'learning_rate': 6.983592172792086e-06, 'epoch': 0.83}
{'loss': 0.4091, 'learning_rate': 6.958960247032514e-06, 'epoch': 0.83}
{'loss': 0.3965, 'learning_rate': 6.934368588379553e-06, 'epoch': 0.83}
{'loss': 0.3749, 'learning_rate': 6.909817219839959e-06, 'epoch': 0.83}
9:35, 16.17s/it] 83%|████████▎ | 2696/3250 [12:38:48<2:28:34, 16.09s/it]                                                         83%|████████▎ | 2696/3250 [12:38:48<2:28:34, 16.09s/it] 83%|████████▎ | 2697/3250 [12:39:04<2:27:46, 16.03s/it]                                                         83%|████████▎ | 2697/3250 [12:39:04<2:27:46, 16.03s/it] 83%|████████▎ | 2698/3250 [12:39:20<2:27:06, 15.99s/it]                                                         83%|████████▎ | 2698/3250 [12:39:20<2:27:06, 15.99s/it] 83%|████████▎ | 2699/3250 [12:39:36<2:26:31, 15.96s/it]                                                         83%|████████▎ | 2699/3250 [12:39:36<2:26:31, 15.96s/it] 83%|████████▎ | 2700/3250 [12:39:52<2:26:06, 15.94s/it]                                                         83%|████████▎ | 2700/3250 [12:39:52<2:26:06, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7571251392364502, 'eval_runtime': 2.4899, 'eval_samples_per_second': 4.819, 'eval_steps_per_second': 1.205, 'epoch': 0.83}
                                                         83%|████████▎ | 2700/3250 [12:39:54<2:26:06, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2700
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2700/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2700

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2700

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3843, 'learning_rate': 6.8853061643828156e-06, 'epoch': 0.83}
{'loss': 0.3847, 'learning_rate': 6.860835444939456e-06, 'epoch': 0.83}
{'loss': 0.3972, 'learning_rate': 6.8364050844035245e-06, 'epoch': 0.83}
{'loss': 0.4067, 'learning_rate': 6.812015105630842e-06, 'epoch': 0.83}
{'loss': 0.4152, 'learning_rate': 6.787665531439513e-06, 'epoch': 0.83}
 83%|████████▎ | 2701/3250 [12:40:11<2:34:33, 16.89s/it]                                                         83%|████████▎ | 2701/3250 [12:40:11<2:34:33, 16.89s/it] 83%|████████▎ | 2702/3250 [12:40:27<2:31:31, 16.59s/it]                                                         83%|████████▎ | 2702/3250 [12:40:27<2:31:31, 16.59s/it] 83%|████████▎ | 2703/3250 [12:40:43<2:29:21, 16.38s/it]                                                         83%|████████▎ | 2703/3250 [12:40:43<2:29:21, 16.38s/it] 83%|████████▎ | 2704/3250 [12:40:59<2:27:50, 16.25s/it]                                                         83%|████████▎ | 2704/3250 [12:40:59<2:27:50, 16.25s/it] 83%|████████▎ | 2705/3250 [12:41:15<2:27:49, 16.27s/it]                                                         83%|████████▎ | 2705/3250 [12:41:15<2:2{'loss': 0.9248, 'learning_rate': 6.763356384609809e-06, 'epoch': 0.83}
{'loss': 0.3799, 'learning_rate': 6.739087687884188e-06, 'epoch': 0.83}
{'loss': 0.3865, 'learning_rate': 6.714859463967283e-06, 'epoch': 0.83}
{'loss': 0.4138, 'learning_rate': 6.690671735525811e-06, 'epoch': 0.83}
{'loss': 0.4084, 'learning_rate': 6.666524525188656e-06, 'epoch': 0.83}
7:49, 16.27s/it] 83%|████████▎ | 2706/3250 [12:41:31<2:26:29, 16.16s/it]                                                         83%|████████▎ | 2706/3250 [12:41:31<2:26:29, 16.16s/it] 83%|████████▎ | 2707/3250 [12:41:47<2:25:32, 16.08s/it]                                                         83%|████████▎ | 2707/3250 [12:41:47<2:25:32, 16.08s/it] 83%|████████▎ | 2708/3250 [12:42:03<2:24:47, 16.03s/it]                                                         83%|████████▎ | 2708/3250 [12:42:03<2:24:47, 16.03s/it] 83%|████████▎ | 2709/3250 [12:42:19<2:24:07, 15.99s/it]                                                         83%|████████▎ | 2709/3250 [12:42:19<2:24:07, 15.99s/it] 83%|████████▎ | 2710/3250 [12:42:35<2:23:36, 15.96s/it]                                                         83%|████████▎ | 2710/3250 [12:42:35<2:23:36, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7611628770828247, 'eval_runtime': 2.476, 'eval_samples_per_second': 4.847, 'eval_steps_per_second': 1.212, 'epoch': 0.83}
                                                         83%|████████▎ | 2710/3250 [12:42:37<2:23:36, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2710
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2710
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2710

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2710

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2710
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.397, 'learning_rate': 6.642417855546768e-06, 'epoch': 0.83}
{'loss': 0.4039, 'learning_rate': 6.6183517491531844e-06, 'epoch': 0.83}
{'loss': 0.4377, 'learning_rate': 6.594326228522979e-06, 'epoch': 0.83}
{'loss': 0.4159, 'learning_rate': 6.570341316133271e-06, 'epoch': 0.84}
{'loss': 0.4036, 'learning_rate': 6.546397034423163e-06, 'epoch': 0.84}
 83%|████████▎ | 2711/3250 [12:43:07<3:07:32, 20.88s/it]                                                         83%|████████▎ | 2711/3250 [12:43:07<3:07:32, 20.88s/it] 83%|████████▎ | 2712/3250 [12:43:23<2:53:44, 19.38s/it]                                                         83%|████████▎ | 2712/3250 [12:43:23<2:53:44, 19.38s/it] 83%|████████▎ | 2713/3250 [12:43:39<2:44:04, 18.33s/it]                                                         83%|████████▎ | 2713/3250 [12:43:39<2:44:04, 18.33s/it] 84%|████████▎ | 2714/3250 [12:43:55<2:37:11, 17.60s/it]                                                         84%|████████▎ | 2714/3250 [12:43:55<2:37:11, 17.60s/it] 84%|████████▎ | 2715/3250 [12:44:11<2:32:19, 17.08s/it]                                                         84%|████████▎ | 2715/3250 [12:44:11<2:3{'loss': 0.378, 'learning_rate': 6.522493405793778e-06, 'epoch': 0.84}
{'loss': 0.4235, 'learning_rate': 6.498630452608179e-06, 'epoch': 0.84}
{'loss': 0.412, 'learning_rate': 6.474808197191401e-06, 'epoch': 0.84}
{'loss': 0.4135, 'learning_rate': 6.4510266618303724e-06, 'epoch': 0.84}
{'loss': 0.3905, 'learning_rate': 6.427285868773947e-06, 'epoch': 0.84}
2:19, 17.08s/it] 84%|████████▎ | 2716/3250 [12:44:26<2:28:46, 16.72s/it]                                                         84%|████████▎ | 2716/3250 [12:44:26<2:28:46, 16.72s/it] 84%|████████▎ | 2717/3250 [12:44:42<2:26:14, 16.46s/it]                                                         84%|████████▎ | 2717/3250 [12:44:42<2:26:14, 16.46s/it] 84%|████████▎ | 2718/3250 [12:44:58<2:24:28, 16.29s/it]                                                         84%|████████▎ | 2718/3250 [12:44:58<2:24:28, 16.29s/it] 84%|████████▎ | 2719/3250 [12:45:14<2:23:06, 16.17s/it]                                                         84%|████████▎ | 2719/3250 [12:45:14<2:23:06, 16.17s/it] 84%|████████▎ | 2720/3250 [12:45:30<2:22:02, 16.08s/it]                                                         84%|████████▎ | 2720/3250 [12:45:30<2:22:02, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7561364769935608, 'eval_runtime': 2.4742, 'eval_samples_per_second': 4.85, 'eval_steps_per_second': 1.213, 'epoch': 0.84}
                                                         84%|████████▎ | 2720/3250 [12:45:32<2:22:02, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2720
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2720/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2720

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2720

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4, 'learning_rate': 6.403585840232873e-06, 'epoch': 0.84}
{'loss': 0.4112, 'learning_rate': 6.379926598379726e-06, 'epoch': 0.84}
{'loss': 0.4366, 'learning_rate': 6.356308165348951e-06, 'epoch': 0.84}
{'loss': 0.4084, 'learning_rate': 6.332730563236805e-06, 'epoch': 0.84}
{'loss': 0.4006, 'learning_rate': 6.3091938141013495e-06, 'epoch': 0.84}
 84%|████████▎ | 2721/3250 [12:45:49<2:29:36, 16.97s/it]                                                         84%|████████▎ | 2721/3250 [12:45:49<2:29:36, 16.97s/it] 84%|████████▍ | 2722/3250 [12:46:05<2:27:25, 16.75s/it]                                                         84%|████████▍ | 2722/3250 [12:46:05<2:27:25, 16.75s/it] 84%|████████▍ | 2723/3250 [12:46:21<2:24:48, 16.49s/it]                                                         84%|████████▍ | 2723/3250 [12:46:21<2:24:48, 16.49s/it] 84%|████████▍ | 2724/3250 [12:46:37<2:23:32, 16.37s/it]                                                         84%|████████▍ | 2724/3250 [12:46:37<2:23:32, 16.37s/it] 84%|████████▍ | 2725/3250 [12:46:53<2:21:59, 16.23s/it]                                                         84%|████████▍ | 2725/3250 [12:46:53<2:2{'loss': 0.4055, 'learning_rate': 6.285697939962437e-06, 'epoch': 0.84}
{'loss': 0.4179, 'learning_rate': 6.262242962801645e-06, 'epoch': 0.84}
{'loss': 0.3748, 'learning_rate': 6.238828904562316e-06, 'epoch': 0.84}
{'loss': 0.4198, 'learning_rate': 6.2154557871495156e-06, 'epoch': 0.84}
{'loss': 0.3903, 'learning_rate': 6.192123632429986e-06, 'epoch': 0.84}
1:59, 16.23s/it] 84%|████████▍ | 2726/3250 [12:47:09<2:20:49, 16.13s/it]                                                         84%|████████▍ | 2726/3250 [12:47:09<2:20:49, 16.13s/it] 84%|████████▍ | 2727/3250 [12:47:25<2:19:55, 16.05s/it]                                                         84%|████████▍ | 2727/3250 [12:47:25<2:19:55, 16.05s/it] 84%|████████▍ | 2728/3250 [12:47:41<2:19:12, 16.00s/it]                                                         84%|████████▍ | 2728/3250 [12:47:41<2:19:12, 16.00s/it] 84%|████████▍ | 2729/3250 [12:47:57<2:18:37, 15.96s/it]                                                         84%|████████▍ | 2729/3250 [12:47:57<2:18:37, 15.96s/it] 84%|████████▍ | 2730/3250 [12:48:13<2:18:39, 16.00s/it]                                                         84%|████████▍ | 2730/3250 [12:48:13<2:18:39, 16.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7567647695541382, 'eval_runtime': 2.4921, 'eval_samples_per_second': 4.815, 'eval_steps_per_second': 1.204, 'epoch': 0.84}
                                                         84%|████████▍ | 2730/3250 [12:48:15<2:18:39, 16.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2730
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2730
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2730
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2730

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2730/pytorch_model.bin
{'loss': 0.3965, 'learning_rate': 6.168832462232171e-06, 'epoch': 0.84}
{'loss': 0.3814, 'learning_rate': 6.145582298346153e-06, 'epoch': 0.84}
{'loss': 0.393, 'learning_rate': 6.1223731625236534e-06, 'epoch': 0.84}
{'loss': 0.4081, 'learning_rate': 6.099205076478004e-06, 'epoch': 0.84}
{'loss': 0.4263, 'learning_rate': 6.076078061884166e-06, 'epoch': 0.84}
 84%|████████▍ | 2731/3250 [12:48:32<2:26:51, 16.98s/it]                                                         84%|████████▍ | 2731/3250 [12:48:32<2:26:51, 16.98s/it] 84%|████████▍ | 2732/3250 [12:48:48<2:23:46, 16.65s/it]                                                         84%|████████▍ | 2732/3250 [12:48:48<2:23:46, 16.65s/it] 84%|████████▍ | 2733/3250 [12:49:04<2:21:31, 16.42s/it]                                                         84%|████████▍ | 2733/3250 [12:49:04<2:21:31, 16.42s/it] 84%|████████▍ | 2734/3250 [12:49:20<2:19:51, 16.26s/it]                                                         84%|████████▍ | 2734/3250 [12:49:20<2:19:51, 16.26s/it] 84%|████████▍ | 2735/3250 [12:49:36<2:19:08, 16.21s/it]                                                         84%|████████▍ | 2735/3250 [12:49:36<2:1{'loss': 0.3973, 'learning_rate': 6.052992140378627e-06, 'epoch': 0.84}
{'loss': 0.9107, 'learning_rate': 6.02994733355946e-06, 'epoch': 0.84}
{'loss': 0.3936, 'learning_rate': 6.006943662986275e-06, 'epoch': 0.84}
{'loss': 0.4193, 'learning_rate': 5.98398115018019e-06, 'epoch': 0.84}
{'loss': 0.4007, 'learning_rate': 5.961059816623799e-06, 'epoch': 0.84}
9:08, 16.21s/it] 84%|████████▍ | 2736/3250 [12:49:52<2:18:00, 16.11s/it]                                                         84%|████████▍ | 2736/3250 [12:49:52<2:18:00, 16.11s/it] 84%|████████▍ | 2737/3250 [12:50:07<2:17:06, 16.04s/it]                                                         84%|████████▍ | 2737/3250 [12:50:07<2:17:06, 16.04s/it] 84%|████████▍ | 2738/3250 [12:50:24<2:17:06, 16.07s/it]                                                         84%|████████▍ | 2738/3250 [12:50:24<2:17:06, 16.07s/it] 84%|████████▍ | 2739/3250 [12:50:39<2:16:23, 16.01s/it]                                                         84%|████████▍ | 2739/3250 [12:50:39<2:16:23, 16.01s/it] 84%|████████▍ | 2740/3250 [12:50:55<2:15:47, 15.98s/it]                                                         84%|████████▍ | 2740/3250 [12:50:55<2:15:47, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7609851956367493, 'eval_runtime': 2.4865, 'eval_samples_per_second': 4.826, 'eval_steps_per_second': 1.207, 'epoch': 0.84}
                                                         84%|████████▍ | 2740/3250 [12:50:58<2:15:47, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2740
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2740
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2740
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2740

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4017, 'learning_rate': 5.9381796837612025e-06, 'epoch': 0.84}
{'loss': 0.3767, 'learning_rate': 5.91534077299794e-06, 'epoch': 0.84}
{'loss': 0.448, 'learning_rate': 5.892543105700987e-06, 'epoch': 0.84}
{'loss': 0.4153, 'learning_rate': 5.8697867031987485e-06, 'epoch': 0.84}
{'loss': 0.3908, 'learning_rate': 5.8470715867809774e-06, 'epoch': 0.84}
 84%|████████▍ | 2741/3250 [12:51:29<3:00:16, 21.25s/it]                                                         84%|████████▍ | 2741/3250 [12:51:29<3:00:16, 21.25s/it] 84%|████████▍ | 2742/3250 [12:51:45<2:46:15, 19.64s/it]                                                         84%|████████▍ | 2742/3250 [12:51:45<2:46:15, 19.64s/it] 84%|████████▍ | 2743/3250 [12:52:01<2:36:23, 18.51s/it]                                                         84%|████████▍ | 2743/3250 [12:52:01<2:36:23, 18.51s/it] 84%|████████▍ | 2744/3250 [12:52:17<2:29:24, 17.72s/it]                                                         84%|████████▍ | 2744/3250 [12:52:17<2:29:24, 17.72s/it] 84%|████████▍ | 2745/3250 [12:52:32<2:24:30, 17.17s/it]                                                         84%|████████▍ | 2745/3250 [12:52:32<2:2{'loss': 0.3998, 'learning_rate': 5.824397777698859e-06, 'epoch': 0.84}
{'loss': 0.4024, 'learning_rate': 5.801765297164891e-06, 'epoch': 0.85}
{'loss': 0.3931, 'learning_rate': 5.779174166352935e-06, 'epoch': 0.85}
{'loss': 0.4142, 'learning_rate': 5.756624406398159e-06, 'epoch': 0.85}
{'loss': 0.3901, 'learning_rate': 5.734116038397019e-06, 'epoch': 0.85}
4:30, 17.17s/it] 84%|████████▍ | 2746/3250 [12:52:48<2:20:59, 16.78s/it]                                                         84%|████████▍ | 2746/3250 [12:52:48<2:20:59, 16.78s/it] 85%|████████▍ | 2747/3250 [12:53:04<2:18:25, 16.51s/it]                                                         85%|████████▍ | 2747/3250 [12:53:04<2:18:25, 16.51s/it] 85%|████████▍ | 2748/3250 [12:53:20<2:16:34, 16.32s/it]                                                         85%|████████▍ | 2748/3250 [12:53:20<2:16:34, 16.32s/it] 85%|████████▍ | 2749/3250 [12:53:36<2:15:10, 16.19s/it]                                                         85%|████████▍ | 2749/3250 [12:53:36<2:15:10, 16.19s/it] 85%|████████▍ | 2750/3250 [12:53:52<2:14:06, 16.09s/it]                                                         85%|████████▍ | 2750/3250 [12:53:52<2:14:06, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7581087350845337, 'eval_runtime': 2.4873, 'eval_samples_per_second': 4.825, 'eval_steps_per_second': 1.206, 'epoch': 0.85}
                                                         85%|████████▍ | 2750/3250 [12:53:54<2:14:06, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2750
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2750/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2750


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2750

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3901, 'learning_rate': 5.711649083407256e-06, 'epoch': 0.85}
{'loss': 0.4156, 'learning_rate': 5.689223562447843e-06, 'epoch': 0.85}
{'loss': 0.4345, 'learning_rate': 5.666839496499022e-06, 'epoch': 0.85}
{'loss': 0.3917, 'learning_rate': 5.644496906502233e-06, 'epoch': 0.85}
{'loss': 0.4114, 'learning_rate': 5.622195813360126e-06, 'epoch': 0.85}
 85%|████████▍ | 2751/3250 [12:54:11<2:21:38, 17.03s/it]                                                         85%|████████▍ | 2751/3250 [12:54:11<2:21:38, 17.03s/it] 85%|████████▍ | 2752/3250 [12:54:27<2:18:27, 16.68s/it]                                                         85%|████████▍ | 2752/3250 [12:54:27<2:18:27, 16.68s/it] 85%|████████▍ | 2753/3250 [12:54:43<2:16:13, 16.45s/it]                                                         85%|████████▍ | 2753/3250 [12:54:43<2:16:13, 16.45s/it] 85%|████████▍ | 2754/3250 [12:54:59<2:14:31, 16.27s/it]                                                         85%|████████▍ | 2754/3250 [12:54:59<2:14:31, 16.27s/it] 85%|████████▍ | 2755/3250 [12:55:15<2:14:53, 16.35s/it]                                                         85%|████████▍ | 2755/3250 [12:55:15<2:1{'loss': 0.4013, 'learning_rate': 5.599936237936515e-06, 'epoch': 0.85}
{'loss': 0.3992, 'learning_rate': 5.577718201056392e-06, 'epoch': 0.85}
{'loss': 0.3704, 'learning_rate': 5.5555417235058526e-06, 'epoch': 0.85}
{'loss': 0.4259, 'learning_rate': 5.533406826032133e-06, 'epoch': 0.85}
{'loss': 0.3922, 'learning_rate': 5.5113135293435815e-06, 'epoch': 0.85}
4:53, 16.35s/it] 85%|████████▍ | 2756/3250 [12:55:31<2:13:29, 16.21s/it]                                                         85%|████████▍ | 2756/3250 [12:55:31<2:13:29, 16.21s/it] 85%|████████▍ | 2757/3250 [12:55:47<2:12:26, 16.12s/it]                                                         85%|████████▍ | 2757/3250 [12:55:47<2:12:26, 16.12s/it] 85%|████████▍ | 2758/3250 [12:56:03<2:11:35, 16.05s/it]                                                         85%|████████▍ | 2758/3250 [12:56:03<2:11:35, 16.05s/it] 85%|████████▍ | 2759/3250 [12:56:19<2:10:57, 16.00s/it]                                                         85%|████████▍ | 2759/3250 [12:56:19<2:10:57, 16.00s/it] 85%|████████▍ | 2760/3250 [12:56:35<2:10:31, 15.98s/it]                                                         85%|████████▍ | 2760/3250 [12:56:35<2:10:31, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7583658695220947, 'eval_runtime': 2.4916, 'eval_samples_per_second': 4.816, 'eval_steps_per_second': 1.204, 'epoch': 0.85}
                                                         85%|████████▍ | 2760/3250 [12:56:37<2:10:31, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2760
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2760/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2760


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3758, 'learning_rate': 5.489261854109606e-06, 'epoch': 0.85}
{'loss': 0.3766, 'learning_rate': 5.467251820960701e-06, 'epoch': 0.85}
{'loss': 0.3831, 'learning_rate': 5.4452834504883535e-06, 'epoch': 0.85}
{'loss': 0.4072, 'learning_rate': 5.423356763245119e-06, 'epoch': 0.85}
{'loss': 0.3936, 'learning_rate': 5.401471779744538e-06, 'epoch': 0.85}
 85%|████████▍ | 2761/3250 [12:57:12<3:03:21, 22.50s/it]                                                         85%|████████▍ | 2761/3250 [12:57:12<3:03:21, 22.50s/it] 85%|████████▍ | 2762/3250 [12:57:28<2:46:52, 20.52s/it]                                                         85%|████████▍ | 2762/3250 [12:57:28<2:46:52, 20.52s/it] 85%|████████▌ | 2763/3250 [12:57:44<2:35:16, 19.13s/it]                                                         85%|████████▌ | 2763/3250 [12:57:44<2:35:16, 19.13s/it] 85%|████████▌ | 2764/3250 [12:58:00<2:27:07, 18.16s/it]                                                         85%|████████▌ | 2764/3250 [12:58:00<2:27:07, 18.16s/it] 85%|████████▌ | 2765/3250 [12:58:16<2:21:21, 17.49s/it]                                                         85%|████████▌ | 2765/3250 [12:58:16<2:2{'loss': 0.4055, 'learning_rate': 5.3796285204611495e-06, 'epoch': 0.85}
{'loss': 0.9139, 'learning_rate': 5.357827005830435e-06, 'epoch': 0.85}
{'loss': 0.394, 'learning_rate': 5.336067256248844e-06, 'epoch': 0.85}
{'loss': 0.4045, 'learning_rate': 5.314349292073739e-06, 'epoch': 0.85}
{'loss': 0.4137, 'learning_rate': 5.292673133623371e-06, 'epoch': 0.85}
1:21, 17.49s/it] 85%|████████▌ | 2766/3250 [12:58:32<2:17:13, 17.01s/it]                                                         85%|████████▌ | 2766/3250 [12:58:32<2:17:13, 17.01s/it] 85%|████████▌ | 2767/3250 [12:58:48<2:14:10, 16.67s/it]                                                         85%|████████▌ | 2767/3250 [12:58:48<2:14:10, 16.67s/it] 85%|████████▌ | 2768/3250 [12:59:04<2:12:02, 16.44s/it]                                                         85%|████████▌ | 2768/3250 [12:59:04<2:12:02, 16.44s/it] 85%|████████▌ | 2769/3250 [12:59:20<2:10:30, 16.28s/it]                                                         85%|████████▌ | 2769/3250 [12:59:20<2:10:30, 16.28s/it] 85%|████████▌ | 2770/3250 [12:59:35<2:09:20, 16.17s/it]                                                         85%|████████▌ | 2770/3250 [12:59:35<2:09:20, 16.17s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7606191635131836, 'eval_runtime': 2.4955, 'eval_samples_per_second': 4.809, 'eval_steps_per_second': 1.202, 'epoch': 0.85}
                                                         85%|████████▌ | 2770/3250 [12:59:38<2:09:20, 16.17s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2770
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2770
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2770
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2770

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4059, 'learning_rate': 5.271038801176919e-06, 'epoch': 0.85}
{'loss': 0.3894, 'learning_rate': 5.249446314974416e-06, 'epoch': 0.85}
{'loss': 0.3899, 'learning_rate': 5.227895695216739e-06, 'epoch': 0.85}
{'loss': 0.4571, 'learning_rate': 5.206386962065602e-06, 'epoch': 0.85}
{'loss': 0.4115, 'learning_rate': 5.184920135643539e-06, 'epoch': 0.85}
 85%|████████▌ | 2771/3250 [13:00:09<2:51:35, 21.49s/it]                                                         85%|████████▌ | 2771/3250 [13:00:09<2:51:35, 21.49s/it] 85%|████████▌ | 2772/3250 [13:00:25<2:37:51, 19.81s/it]                                                         85%|████████▌ | 2772/3250 [13:00:25<2:37:51, 19.81s/it] 85%|████████▌ | 2773/3250 [13:00:41<2:28:11, 18.64s/it]                                                         85%|████████▌ | 2773/3250 [13:00:41<2:28:11, 18.64s/it] 85%|████████▌ | 2774/3250 [13:00:57<2:21:19, 17.81s/it]                                                         85%|████████▌ | 2774/3250 [13:00:57<2:21:19, 17.81s/it] 85%|████████▌ | 2775/3250 [13:01:13<2:16:29, 17.24s/it]                                                         85%|████████▌ | 2775/3250 [13:01:13<2:1{'loss': 0.3893, 'learning_rate': 5.163495236033855e-06, 'epoch': 0.85}
{'loss': 0.3729, 'learning_rate': 5.142112283280653e-06, 'epoch': 0.85}
{'loss': 0.4212, 'learning_rate': 5.120771297388788e-06, 'epoch': 0.85}
{'loss': 0.4221, 'learning_rate': 5.099472298323843e-06, 'epoch': 0.86}
{'loss': 0.397, 'learning_rate': 5.078215306012135e-06, 'epoch': 0.86}
6:29, 17.24s/it] 85%|████████▌ | 2776/3250 [13:01:29<2:13:01, 16.84s/it]                                                         85%|████████▌ | 2776/3250 [13:01:29<2:13:01, 16.84s/it] 85%|████████▌ | 2777/3250 [13:01:45<2:10:33, 16.56s/it]                                                         85%|████████▌ | 2777/3250 [13:01:45<2:10:33, 16.56s/it] 85%|████████▌ | 2778/3250 [13:02:01<2:08:44, 16.37s/it]                                                         85%|████████▌ | 2778/3250 [13:02:01<2:08:44, 16.37s/it] 86%|████████▌ | 2779/3250 [13:02:17<2:07:21, 16.22s/it]                                                         86%|████████▌ | 2779/3250 [13:02:17<2:07:21, 16.22s/it] 86%|████████▌ | 2780/3250 [13:02:32<2:06:18, 16.12s/it]                                                         86%|████████▌ | 2780/3250 [13:02:32<2:06:18, 16.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7585961222648621, 'eval_runtime': 2.4824, 'eval_samples_per_second': 4.834, 'eval_steps_per_second': 1.209, 'epoch': 0.86}
                                                         86%|████████▌ | 2780/3250 [13:02:35<2:06:18, 16.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2780
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2780 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2780

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2780

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2780/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3991, 'learning_rate': 5.057000340340678e-06, 'epoch': 0.86}
{'loss': 0.4062, 'learning_rate': 5.035827421157146e-06, 'epoch': 0.86}
{'loss': 0.403, 'learning_rate': 5.014696568269905e-06, 'epoch': 0.86}
{'loss': 0.4217, 'learning_rate': 4.993607801447958e-06, 'epoch': 0.86}
{'loss': 0.4122, 'learning_rate': 4.9725611404209285e-06, 'epoch': 0.86}
 86%|████████▌ | 2781/3250 [13:02:52<2:13:03, 17.02s/it]                                                         86%|████████▌ | 2781/3250 [13:02:52<2:13:03, 17.02s/it] 86%|████████▌ | 2782/3250 [13:03:07<2:10:06, 16.68s/it]                                                         86%|████████▌ | 2782/3250 [13:03:08<2:10:06, 16.68s/it] 86%|████████▌ | 2783/3250 [13:03:23<2:08:01, 16.45s/it]                                                         86%|████████▌ | 2783/3250 [13:03:23<2:08:01, 16.45s/it] 86%|████████▌ | 2784/3250 [13:03:39<2:06:30, 16.29s/it]                                                         86%|████████▌ | 2784/3250 [13:03:39<2:06:30, 16.29s/it] 86%|████████▌ | 2785/3250 [13:03:55<2:05:24, 16.18s/it]                                                         86%|████████▌ | 2785/3250 [13:03:55<2:0{'loss': 0.3945, 'learning_rate': 4.951556604879048e-06, 'epoch': 0.86}
{'loss': 0.4208, 'learning_rate': 4.930594214473144e-06, 'epoch': 0.86}
{'loss': 0.3901, 'learning_rate': 4.909673988814601e-06, 'epoch': 0.86}
{'loss': 0.4101, 'learning_rate': 4.888795947475372e-06, 'epoch': 0.86}
{'loss': 0.4002, 'learning_rate': 4.86796010998794e-06, 'epoch': 0.86}
5:24, 16.18s/it] 86%|████████▌ | 2786/3250 [13:04:11<2:04:29, 16.10s/it]                                                         86%|████████▌ | 2786/3250 [13:04:11<2:04:29, 16.10s/it] 86%|████████▌ | 2787/3250 [13:04:28<2:05:33, 16.27s/it]                                                         86%|████████▌ | 2787/3250 [13:04:28<2:05:33, 16.27s/it] 86%|████████▌ | 2788/3250 [13:04:44<2:04:24, 16.16s/it]                                                         86%|████████▌ | 2788/3250 [13:04:44<2:04:24, 16.16s/it] 86%|████████▌ | 2789/3250 [13:05:00<2:03:32, 16.08s/it]                                                         86%|████████▌ | 2789/3250 [13:05:00<2:03:32, 16.08s/it] 86%|████████▌ | 2790/3250 [13:05:16<2:02:51, 16.02s/it]                                                         86%|████████▌ | 2790/3250 [13:05:16<2:02:51, 16.02s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7560427188873291, 'eval_runtime': 2.4776, 'eval_samples_per_second': 4.843, 'eval_steps_per_second': 1.211, 'epoch': 0.86}
                                                         86%|████████▌ | 2790/3250 [13:05:18<2:02:51, 16.02s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2790
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2790/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2790


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2790

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3948, 'learning_rate': 4.847166495845301e-06, 'epoch': 0.86}
{'loss': 0.3879, 'learning_rate': 4.826415124500955e-06, 'epoch': 0.86}
{'loss': 0.3825, 'learning_rate': 4.805706015368883e-06, 'epoch': 0.86}
{'loss': 0.4024, 'learning_rate': 4.785039187823503e-06, 'epoch': 0.86}
{'loss': 0.4098, 'learning_rate': 4.764414661199707e-06, 'epoch': 0.86}
 86%|████████▌ | 2791/3250 [13:05:35<2:09:39, 16.95s/it]                                                         86%|████████▌ | 2791/3250 [13:05:35<2:09:39, 16.95s/it] 86%|████████▌ | 2792/3250 [13:05:51<2:06:57, 16.63s/it]                                                         86%|████████▌ | 2792/3250 [13:05:51<2:06:57, 16.63s/it] 86%|████████▌ | 2793/3250 [13:06:06<2:04:59, 16.41s/it]                                                         86%|████████▌ | 2793/3250 [13:06:06<2:04:59, 16.41s/it] 86%|████████▌ | 2794/3250 [13:06:22<2:03:30, 16.25s/it]                                                         86%|████████▌ | 2794/3250 [13:06:22<2:03:30, 16.25s/it] 86%|████████▌ | 2795/3250 [13:06:38<2:02:26, 16.15s/it]                                                         86%|████████▌ | 2795/3250 [13:06:38<2:0{'loss': 0.4139, 'learning_rate': 4.743832454792796e-06, 'epoch': 0.86}
{'loss': 0.9257, 'learning_rate': 4.723292587858485e-06, 'epoch': 0.86}
{'loss': 0.3914, 'learning_rate': 4.702795079612876e-06, 'epoch': 0.86}
{'loss': 0.3939, 'learning_rate': 4.682339949232456e-06, 'epoch': 0.86}
{'loss': 0.4132, 'learning_rate': 4.661927215854028e-06, 'epoch': 0.86}
2:26, 16.15s/it] 86%|████████▌ | 2796/3250 [13:06:54<2:01:34, 16.07s/it]                                                         86%|████████▌ | 2796/3250 [13:06:54<2:01:34, 16.07s/it] 86%|████████▌ | 2797/3250 [13:07:10<2:00:53, 16.01s/it]                                                         86%|████████▌ | 2797/3250 [13:07:10<2:00:53, 16.01s/it] 86%|████████▌ | 2798/3250 [13:07:26<2:00:19, 15.97s/it]                                                         86%|████████▌ | 2798/3250 [13:07:26<2:00:19, 15.97s/it] 86%|████████▌ | 2799/3250 [13:07:42<1:59:53, 15.95s/it]                                                         86%|████████▌ | 2799/3250 [13:07:42<1:59:53, 15.95s/it] 86%|████████▌ | 2800/3250 [13:07:58<1:59:30, 15.94s/it]                                                         86%|████████▌ | 2800/3250 [13:07:58<1:59:30, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7598156929016113, 'eval_runtime': 2.4892, 'eval_samples_per_second': 4.821, 'eval_steps_per_second': 1.205, 'epoch': 0.86}
                                                         86%|████████▌ | 2800/3250 [13:08:00<1:59:30, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2800
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2800I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2800 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2800


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4201, 'learning_rate': 4.641556898574762e-06, 'epoch': 0.86}
{'loss': 0.3969, 'learning_rate': 4.621229016452156e-06, 'epoch': 0.86}
{'loss': 0.395, 'learning_rate': 4.600943588503959e-06, 'epoch': 0.86}
{'loss': 0.4415, 'learning_rate': 4.580700633708246e-06, 'epoch': 0.86}
{'loss': 0.4272, 'learning_rate': 4.5605001710033454e-06, 'epoch': 0.86}
 86%|████████▌ | 2801/3250 [13:08:50<3:21:52, 26.98s/it]                                                         86%|████████▌ | 2801/3250 [13:08:50<3:21:52, 26.98s/it] 86%|████████▌ | 2802/3250 [13:09:06<2:56:38, 23.66s/it]                                                         86%|████████▌ | 2802/3250 [13:09:06<2:56:38, 23.66s/it] 86%|████████▌ | 2803/3250 [13:09:22<2:38:53, 21.33s/it]                                                         86%|████████▌ | 2803/3250 [13:09:22<2:38:53, 21.33s/it] 86%|████████▋ | 2804/3250 [13:09:39<2:28:40, 20.00s/it]                                                         86%|████████▋ | 2804/3250 [13:09:39<2:28:40, 20.00s/it] 86%|████████▋ | 2805/3250 [13:09:55<2:19:13, 18.77s/it]                                                         86%|████████▋ | 2805/3250 [13:09:55<2:1{'loss': 0.3917, 'learning_rate': 4.540342219287836e-06, 'epoch': 0.86}
{'loss': 0.3747, 'learning_rate': 4.520226797420501e-06, 'epoch': 0.86}
{'loss': 0.4246, 'learning_rate': 4.500153924220357e-06, 'epoch': 0.86}
{'loss': 0.3977, 'learning_rate': 4.48012361846662e-06, 'epoch': 0.86}
{'loss': 0.4035, 'learning_rate': 4.46013589889866e-06, 'epoch': 0.86}
9:13, 18.77s/it] 86%|████████▋ | 2806/3250 [13:10:11<2:12:30, 17.91s/it]                                                         86%|████████▋ | 2806/3250 [13:10:11<2:12:30, 17.91s/it] 86%|████████▋ | 2807/3250 [13:10:27<2:07:46, 17.31s/it]                                                         86%|████████▋ | 2807/3250 [13:10:27<2:07:46, 17.31s/it] 86%|████████▋ | 2808/3250 [13:10:43<2:04:24, 16.89s/it]                                                         86%|████████▋ | 2808/3250 [13:10:43<2:04:24, 16.89s/it] 86%|████████▋ | 2809/3250 [13:10:59<2:01:52, 16.58s/it]                                                         86%|████████▋ | 2809/3250 [13:10:59<2:01:52, 16.58s/it] 86%|████████▋ | 2810/3250 [13:11:14<2:00:06, 16.38s/it]                                                         86%|████████▋ | 2810/3250 [13:11:14<2:00:06, 16.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7589067220687866, 'eval_runtime': 2.6007, 'eval_samples_per_second': 4.614, 'eval_steps_per_second': 1.154, 'epoch': 0.86}
                                                         86%|████████▋ | 2810/3250 [13:11:17<2:00:06, 16.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2810
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2810
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2810/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2810


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2810
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2810/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3961, 'learning_rate': 4.4401907842160306e-06, 'epoch': 0.86}
{'loss': 0.4009, 'learning_rate': 4.420288293078395e-06, 'epoch': 0.87}
{'loss': 0.4037, 'learning_rate': 4.4004284441055645e-06, 'epoch': 0.87}
{'loss': 0.4254, 'learning_rate': 4.380611255877448e-06, 'epoch': 0.87}
{'loss': 0.4048, 'learning_rate': 4.360836746934055e-06, 'epoch': 0.87}
 86%|████████▋ | 2811/3250 [13:11:34<2:06:22, 17.27s/it]                                                         86%|████████▋ | 2811/3250 [13:11:34<2:06:22, 17.27s/it] 87%|████████▋ | 2812/3250 [13:11:50<2:03:03, 16.86s/it]                                                         87%|████████▋ | 2812/3250 [13:11:50<2:03:03, 16.86s/it] 87%|████████▋ | 2813/3250 [13:12:06<2:00:41, 16.57s/it]                                                         87%|████████▋ | 2813/3250 [13:12:06<2:00:41, 16.57s/it] 87%|████████▋ | 2814/3250 [13:12:21<1:58:54, 16.36s/it]                                                         87%|████████▋ | 2814/3250 [13:12:21<1:58:54, 16.36s/it] 87%|████████▋ | 2815/3250 [13:12:37<1:57:30, 16.21s/it]                                                         87%|████████▋ | 2815/3250 [13:12:37<1:5{'loss': 0.3993, 'learning_rate': 4.341104935775442e-06, 'epoch': 0.87}
{'loss': 0.4058, 'learning_rate': 4.321415840861748e-06, 'epoch': 0.87}
{'loss': 0.4136, 'learning_rate': 4.301769480613116e-06, 'epoch': 0.87}
{'loss': 0.3684, 'learning_rate': 4.282165873409743e-06, 'epoch': 0.87}
{'loss': 0.4279, 'learning_rate': 4.262605037591799e-06, 'epoch': 0.87}
7:30, 16.21s/it] 87%|████████▋ | 2816/3250 [13:12:53<1:56:30, 16.11s/it]                                                         87%|████████▋ | 2816/3250 [13:12:53<1:56:30, 16.11s/it] 87%|████████▋ | 2817/3250 [13:13:09<1:55:43, 16.04s/it]                                                         87%|████████▋ | 2817/3250 [13:13:09<1:55:43, 16.04s/it] 87%|████████▋ | 2818/3250 [13:13:25<1:55:05, 15.98s/it]                                                         87%|████████▋ | 2818/3250 [13:13:25<1:55:05, 15.98s/it] 87%|████████▋ | 2819/3250 [13:13:41<1:54:33, 15.95s/it]                                                         87%|████████▋ | 2819/3250 [13:13:41<1:54:33, 15.95s/it] 87%|████████▋ | 2820/3250 [13:13:57<1:54:56, 16.04s/it]                                                         87%|████████▋ | 2820/3250 [13:13:57<1:54:56, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7568493485450745, 'eval_runtime': 2.5105, 'eval_samples_per_second': 4.78, 'eval_steps_per_second': 1.195, 'epoch': 0.87}
                                                         87%|████████▋ | 2820/3250 [13:14:00<1:54:56, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2820
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2820/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2820

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2820

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3953, 'learning_rate': 4.243086991459455e-06, 'epoch': 0.87}
{'loss': 0.3933, 'learning_rate': 4.223611753272849e-06, 'epoch': 0.87}
{'loss': 0.383, 'learning_rate': 4.2041793412520734e-06, 'epoch': 0.87}
{'loss': 0.3926, 'learning_rate': 4.1847897735771464e-06, 'epoch': 0.87}
{'loss': 0.3952, 'learning_rate': 4.165443068387998e-06, 'epoch': 0.87}
 87%|████████▋ | 2821/3250 [13:14:16<2:01:28, 16.99s/it]                                                         87%|████████▋ | 2821/3250 [13:14:16<2:01:28, 16.99s/it] 87%|████████▋ | 2822/3250 [13:14:32<1:58:45, 16.65s/it]                                                         87%|████████▋ | 2822/3250 [13:14:32<1:58:45, 16.65s/it] 87%|████████▋ | 2823/3250 [13:14:48<1:56:48, 16.41s/it]                                                         87%|████████▋ | 2823/3250 [13:14:48<1:56:48, 16.41s/it] 87%|████████▋ | 2824/3250 [13:15:05<1:58:06, 16.64s/it]                                                         87%|████████▋ | 2824/3250 [13:15:05<1:58:06, 16.64s/it] 87%|████████▋ | 2825/3250 [13:15:21<1:56:13, 16.41s/it]                                                         87%|████████▋ | 2825/3250 [13:15:21<1:5{'loss': 0.4237, 'learning_rate': 4.146139243784475e-06, 'epoch': 0.87}
{'loss': 0.4032, 'learning_rate': 4.126878317826294e-06, 'epoch': 0.87}
{'loss': 0.9005, 'learning_rate': 4.1076603085330405e-06, 'epoch': 0.87}
{'loss': 0.3949, 'learning_rate': 4.088485233884165e-06, 'epoch': 0.87}
{'loss': 0.4234, 'learning_rate': 4.069353111818913e-06, 'epoch': 0.87}
6:13, 16.41s/it] 87%|████████▋ | 2826/3250 [13:15:37<1:54:46, 16.24s/it]                                                         87%|████████▋ | 2826/3250 [13:15:37<1:54:46, 16.24s/it] 87%|████████▋ | 2827/3250 [13:15:53<1:53:41, 16.13s/it]                                                         87%|████████▋ | 2827/3250 [13:15:53<1:53:41, 16.13s/it] 87%|████████▋ | 2828/3250 [13:16:09<1:52:46, 16.03s/it]                                                         87%|████████▋ | 2828/3250 [13:16:09<1:52:46, 16.03s/it] 87%|████████▋ | 2829/3250 [13:16:24<1:52:07, 15.98s/it]                                                         87%|████████▋ | 2829/3250 [13:16:24<1:52:07, 15.98s/it] 87%|████████▋ | 2830/3250 [13:16:40<1:51:36, 15.94s/it]                                                         87%|████████▋ | 2830/3250 [13:16:40<1:51:36, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7589386701583862, 'eval_runtime': 3.1081, 'eval_samples_per_second': 3.861, 'eval_steps_per_second': 0.965, 'epoch': 0.87}
                                                         87%|████████▋ | 2830/3250 [13:16:43<1:51:36, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2830
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2830 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2830

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2830

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2830/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2830/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3966, 'learning_rate': 4.050263960236384e-06, 'epoch': 0.87}
{'loss': 0.4094, 'learning_rate': 4.031217796995457e-06, 'epoch': 0.87}
{'loss': 0.3929, 'learning_rate': 4.012214639914796e-06, 'epoch': 0.87}
{'loss': 0.4443, 'learning_rate': 3.9932545067728366e-06, 'epoch': 0.87}
{'loss': 0.4237, 'learning_rate': 3.97433741530776e-06, 'epoch': 0.87}
 87%|████████▋ | 2831/3250 [13:17:31<3:04:03, 26.36s/it]                                                         87%|████████▋ | 2831/3250 [13:17:31<3:04:03, 26.36s/it] 87%|████████▋ | 2832/3250 [13:17:47<2:41:51, 23.23s/it]                                                         87%|████████▋ | 2832/3250 [13:17:47<2:41:51, 23.23s/it] 87%|████████▋ | 2833/3250 [13:18:03<2:26:09, 21.03s/it]                                                         87%|████████▋ | 2833/3250 [13:18:03<2:26:09, 21.03s/it] 87%|████████▋ | 2834/3250 [13:18:19<2:15:06, 19.49s/it]                                                         87%|████████▋ | 2834/3250 [13:18:19<2:15:06, 19.49s/it] 87%|████████▋ | 2835/3250 [13:18:35<2:07:19, 18.41s/it]                                                         87%|████████▋ | 2835/3250 [13:18:35<2:0{'loss': 0.3932, 'learning_rate': 3.955463383217478e-06, 'epoch': 0.87}
{'loss': 0.3953, 'learning_rate': 3.936632428159609e-06, 'epoch': 0.87}
{'loss': 0.4054, 'learning_rate': 3.917844567751483e-06, 'epoch': 0.87}
{'loss': 0.3963, 'learning_rate': 3.899099819570112e-06, 'epoch': 0.87}
{'loss': 0.4093, 'learning_rate': 3.8803982011521685e-06, 'epoch': 0.87}
7:19, 18.41s/it] 87%|████████▋ | 2836/3250 [13:18:50<2:01:50, 17.66s/it]                                                         87%|████████▋ | 2836/3250 [13:18:50<2:01:50, 17.66s/it] 87%|████████▋ | 2837/3250 [13:19:07<2:00:00, 17.43s/it]                                                         87%|████████▋ | 2837/3250 [13:19:07<2:00:00, 17.43s/it] 87%|████████▋ | 2838/3250 [13:19:23<1:56:31, 16.97s/it]                                                         87%|████████▋ | 2838/3250 [13:19:23<1:56:31, 16.97s/it] 87%|████████▋ | 2839/3250 [13:19:39<1:54:01, 16.65s/it]                                                         87%|████████▋ | 2839/3250 [13:19:39<1:54:01, 16.65s/it] 87%|████████▋ | 2840/3250 [13:19:55<1:52:12, 16.42s/it]                                                         87%|████████▋ | 2840/3250 [13:19:55<1:52:12, 16.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7591063380241394, 'eval_runtime': 2.6885, 'eval_samples_per_second': 4.463, 'eval_steps_per_second': 1.116, 'epoch': 0.87}
                                                         87%|████████▋ | 2840/3250 [13:19:58<1:52:12, 16.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2840
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2840

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4011, 'learning_rate': 3.861739729993991e-06, 'epoch': 0.87}
{'loss': 0.3887, 'learning_rate': 3.843124423551536e-06, 'epoch': 0.87}
{'loss': 0.401, 'learning_rate': 3.824552299240364e-06, 'epoch': 0.87}
{'loss': 0.423, 'learning_rate': 3.8060233744356633e-06, 'epoch': 0.88}
{'loss': 0.395, 'learning_rate': 3.7875376664722016e-06, 'epoch': 0.88}
 87%|████████▋ | 2841/3250 [13:20:14<1:58:01, 17.31s/it]                                                         87%|████████▋ | 2841/3250 [13:20:14<1:58:01, 17.31s/it] 87%|████████▋ | 2842/3250 [13:20:31<1:55:35, 17.00s/it]                                                         87%|████████▋ | 2842/3250 [13:20:31<1:55:35, 17.00s/it] 87%|████████▋ | 2843/3250 [13:20:47<1:53:01, 16.66s/it]                                                         87%|████████▋ | 2843/3250 [13:20:47<1:53:01, 16.66s/it] 88%|████████▊ | 2844/3250 [13:21:02<1:51:08, 16.42s/it]                                                         88%|████████▊ | 2844/3250 [13:21:02<1:51:08, 16.42s/it] 88%|████████▊ | 2845/3250 [13:21:18<1:49:46, 16.26s/it]                                                         88%|████████▊ | 2845/3250 [13:21:18<1:4{'loss': 0.401, 'learning_rate': 3.7690951926443007e-06, 'epoch': 0.88}
{'loss': 0.4088, 'learning_rate': 3.750695970205853e-06, 'epoch': 0.88}
{'loss': 0.3988, 'learning_rate': 3.732340016370267e-06, 'epoch': 0.88}
{'loss': 0.3797, 'learning_rate': 3.7140273483104838e-06, 'epoch': 0.88}
{'loss': 0.4358, 'learning_rate': 3.6957579831589538e-06, 'epoch': 0.88}
9:46, 16.26s/it] 88%|████████▊ | 2846/3250 [13:21:34<1:48:43, 16.15s/it]                                                         88%|████████▊ | 2846/3250 [13:21:34<1:48:43, 16.15s/it] 88%|████████▊ | 2847/3250 [13:21:50<1:47:54, 16.07s/it]                                                         88%|████████▊ | 2847/3250 [13:21:50<1:47:54, 16.07s/it] 88%|████████▊ | 2848/3250 [13:22:06<1:47:15, 16.01s/it]                                                         88%|████████▊ | 2848/3250 [13:22:06<1:47:15, 16.01s/it] 88%|████████▊ | 2849/3250 [13:22:23<1:48:16, 16.20s/it]                                                         88%|████████▊ | 2849/3250 [13:22:23<1:48:16, 16.20s/it] 88%|████████▊ | 2850/3250 [13:22:39<1:47:48, 16.17s/it]                                                         88%|████████▊ | 2850/3250 [13:22:39<1:47:48, 16.17s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7577041387557983, 'eval_runtime': 3.1054, 'eval_samples_per_second': 3.864, 'eval_steps_per_second': 0.966, 'epoch': 0.88}
                                                         88%|████████▊ | 2850/3250 [13:22:42<1:47:48, 16.17s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2850
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2850the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2850
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2850

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4035, 'learning_rate': 3.6775319380076e-06, 'epoch': 0.88}
{'loss': 0.3897, 'learning_rate': 3.659349229907827e-06, 'epoch': 0.88}
{'loss': 0.3909, 'learning_rate': 3.641209875870505e-06, 'epoch': 0.88}
{'loss': 0.3884, 'learning_rate': 3.62311389286592e-06, 'epoch': 0.88}
{'loss': 0.4082, 'learning_rate': 3.6050612978237862e-06, 'epoch': 0.88}
 88%|████████▊ | 2851/3250 [13:22:59<1:54:49, 17.27s/it]                                                         88%|████████▊ | 2851/3250 [13:22:59<1:54:49, 17.27s/it] 88%|████████▊ | 2852/3250 [13:23:14<1:51:47, 16.85s/it]                                                         88%|████████▊ | 2852/3250 [13:23:14<1:51:47, 16.85s/it] 88%|████████▊ | 2853/3250 [13:23:31<1:50:07, 16.64s/it]                                                         88%|████████▊ | 2853/3250 [13:23:31<1:50:07, 16.64s/it] 88%|████████▊ | 2854/3250 [13:23:46<1:48:20, 16.41s/it]                                                         88%|████████▊ | 2854/3250 [13:23:46<1:48:20, 16.41s/it] 88%|████████▊ | 2855/3250 [13:24:03<1:47:29, 16.33s/it]                                                         88%|████████▊ | 2855/3250 [13:24:03<1:4{'loss': 0.395, 'learning_rate': 3.5870521076332486e-06, 'epoch': 0.88}
{'loss': 0.4079, 'learning_rate': 3.5690863391428296e-06, 'epoch': 0.88}
{'loss': 0.9117, 'learning_rate': 3.551164009160429e-06, 'epoch': 0.88}
{'loss': 0.3985, 'learning_rate': 3.533285134453307e-06, 'epoch': 0.88}
{'loss': 0.3975, 'learning_rate': 3.5154497317480774e-06, 'epoch': 0.88}
7:29, 16.33s/it] 88%|████████▊ | 2856/3250 [13:24:18<1:46:25, 16.21s/it]                                                         88%|████████▊ | 2856/3250 [13:24:18<1:46:25, 16.21s/it] 88%|████████▊ | 2857/3250 [13:24:34<1:45:32, 16.11s/it]                                                         88%|████████▊ | 2857/3250 [13:24:34<1:45:32, 16.11s/it] 88%|████████▊ | 2858/3250 [13:24:50<1:44:46, 16.04s/it]                                                         88%|████████▊ | 2858/3250 [13:24:50<1:44:46, 16.04s/it] 88%|████████▊ | 2859/3250 [13:25:06<1:44:14, 16.00s/it]                                                         88%|████████▊ | 2859/3250 [13:25:06<1:44:14, 16.00s/it] 88%|████████▊ | 2860/3250 [13:25:22<1:43:47, 15.97s/it]                                                         88%|████████▊ | 2860/3250 [13:25:22<1:43:47, 15.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7587582468986511, 'eval_runtime': 2.6972, 'eval_samples_per_second': 4.449, 'eval_steps_per_second': 1.112, 'epoch': 0.88}
                                                         88%|████████▊ | 2860/3250 [13:25:25<1:43:47, 15.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2860
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2860/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2860

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4232, 'learning_rate': 3.497657817730665e-06, 'epoch': 0.88}
{'loss': 0.408, 'learning_rate': 3.4799094090463226e-06, 'epoch': 0.88}
{'loss': 0.384, 'learning_rate': 3.462204522299606e-06, 'epoch': 0.88}
{'loss': 0.3916, 'learning_rate': 3.4445431740543434e-06, 'epoch': 0.88}
{'loss': 0.4505, 'learning_rate': 3.4269253808336455e-06, 'epoch': 0.88}
 88%|████████▊ | 2861/3250 [13:25:41<1:50:03, 16.97s/it]                                                         88%|████████▊ | 2861/3250 [13:25:41<1:50:03, 16.97s/it] 88%|████████▊ | 2862/3250 [13:25:57<1:47:38, 16.65s/it]                                                         88%|████████▊ | 2862/3250 [13:25:57<1:47:38, 16.65s/it] 88%|████████▊ | 2863/3250 [13:26:13<1:45:51, 16.41s/it]                                                         88%|████████▊ | 2863/3250 [13:26:13<1:45:51, 16.41s/it] 88%|████████▊ | 2864/3250 [13:26:29<1:44:28, 16.24s/it]                                                         88%|████████▊ | 2864/3250 [13:26:29<1:44:28, 16.24s/it] 88%|████████▊ | 2865/3250 [13:26:45<1:43:26, 16.12s/it]                                                         88%|████████▊ | 2865/3250 [13:26:45<1:4{'loss': 0.4074, 'learning_rate': 3.4093511591198445e-06, 'epoch': 0.88}
{'loss': 0.3958, 'learning_rate': 3.391820525354539e-06, 'epoch': 0.88}
{'loss': 0.3709, 'learning_rate': 3.374333495938542e-06, 'epoch': 0.88}
{'loss': 0.4176, 'learning_rate': 3.3568900872318564e-06, 'epoch': 0.88}
{'loss': 0.4131, 'learning_rate': 3.3394903155537115e-06, 'epoch': 0.88}
3:26, 16.12s/it] 88%|████████▊ | 2866/3250 [13:27:01<1:42:36, 16.03s/it]                                                         88%|████████▊ | 2866/3250 [13:27:01<1:42:36, 16.03s/it] 88%|████████▊ | 2867/3250 [13:27:16<1:41:58, 15.97s/it]                                                         88%|████████▊ | 2867/3250 [13:27:16<1:41:58, 15.97s/it] 88%|████████▊ | 2868/3250 [13:27:32<1:41:27, 15.94s/it]                                                         88%|████████▊ | 2868/3250 [13:27:32<1:41:27, 15.94s/it] 88%|████████▊ | 2869/3250 [13:27:49<1:42:19, 16.11s/it]                                                         88%|████████▊ | 2869/3250 [13:27:49<1:42:19, 16.11s/it] 88%|████████▊ | 2870/3250 [13:28:05<1:41:32, 16.03s/it]                                                         88%|████████▊ | 2870/3250 [13:28:05<1:41:32, 16.03s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7580627202987671, 'eval_runtime': 2.4756, 'eval_samples_per_second': 4.847, 'eval_steps_per_second': 1.212, 'epoch': 0.88}
                                                         88%|████████▊ | 2870/3250 [13:28:07<1:41:32, 16.03s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2870
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2870/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2870

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2870

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3907, 'learning_rate': 3.322134197182464e-06, 'epoch': 0.88}
{'loss': 0.386, 'learning_rate': 3.3048217483556744e-06, 'epoch': 0.88}
{'loss': 0.4159, 'learning_rate': 3.2875529852700147e-06, 'epoch': 0.88}
{'loss': 0.4063, 'learning_rate': 3.270327924081301e-06, 'epoch': 0.88}
{'loss': 0.4239, 'learning_rate': 3.253146580904476e-06, 'epoch': 0.88}
 88%|████████▊ | 2871/3250 [13:28:24<1:47:00, 16.94s/it]                                                         88%|████████▊ | 2871/3250 [13:28:24<1:47:00, 16.94s/it] 88%|████████▊ | 2872/3250 [13:28:40<1:44:38, 16.61s/it]                                                         88%|████████▊ | 2872/3250 [13:28:40<1:44:38, 16.61s/it] 88%|████████▊ | 2873/3250 [13:28:55<1:42:54, 16.38s/it]                                                         88%|████████▊ | 2873/3250 [13:28:55<1:42:54, 16.38s/it] 88%|████████▊ | 2874/3250 [13:29:11<1:41:37, 16.22s/it]                                                         88%|████████▊ | 2874/3250 [13:29:11<1:41:37, 16.22s/it] 88%|████████▊ | 2875/3250 [13:29:27<1:40:39, 16.10s/it]                                                         88%|████████▊ | 2875/3250 [13:29:27<1:4{'loss': 0.417, 'learning_rate': 3.2360089718135587e-06, 'epoch': 0.88}
{'loss': 0.398, 'learning_rate': 3.2189151128416695e-06, 'epoch': 0.89}
{'loss': 0.4205, 'learning_rate': 3.201865019981004e-06, 'epoch': 0.89}
{'loss': 0.3884, 'learning_rate': 3.184858709182775e-06, 'epoch': 0.89}
{'loss': 0.4187, 'learning_rate': 3.167896196357284e-06, 'epoch': 0.89}
0:39, 16.10s/it] 88%|████████▊ | 2876/3250 [13:29:43<1:39:53, 16.03s/it]                                                         88%|████████▊ | 2876/3250 [13:29:43<1:39:53, 16.03s/it] 89%|████████▊ | 2877/3250 [13:29:59<1:39:18, 15.97s/it]                                                         89%|████████▊ | 2877/3250 [13:29:59<1:39:18, 15.97s/it] 89%|████████▊ | 2878/3250 [13:30:15<1:38:47, 15.94s/it]                                                         89%|████████▊ | 2878/3250 [13:30:15<1:38:47, 15.94s/it] 89%|████████▊ | 2879/3250 [13:30:30<1:38:22, 15.91s/it]                                                         89%|████████▊ | 2879/3250 [13:30:30<1:38:22, 15.91s/it] 89%|████████▊ | 2880/3250 [13:30:46<1:37:58, 15.89s/it]                                                         89%|████████▊ | 2880/3250 [13:30:46<1:37:58, 15.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7583779096603394, 'eval_runtime': 2.469, 'eval_samples_per_second': 4.86, 'eval_steps_per_second': 1.215, 'epoch': 0.89}
                                                         89%|████████▊ | 2880/3250 [13:30:49<1:37:58, 15.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2880
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2880/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2880

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2880

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4032, 'learning_rate': 3.1509774973738193e-06, 'epoch': 0.89}
{'loss': 0.3876, 'learning_rate': 3.134102628060698e-06, 'epoch': 0.89}
{'loss': 0.3897, 'learning_rate': 3.117271604205241e-06, 'epoch': 0.89}
{'loss': 0.3821, 'learning_rate': 3.1004844415537194e-06, 'epoch': 0.89}
{'loss': 0.4156, 'learning_rate': 3.0837411558113984e-06, 'epoch': 0.89}
 89%|████████▊ | 2881/3250 [13:31:05<1:43:31, 16.83s/it]                                                         89%|████████▊ | 2881/3250 [13:31:05<1:43:31, 16.83s/it] 89%|████████▊ | 2882/3250 [13:31:21<1:41:25, 16.54s/it]                                                         89%|████████▊ | 2882/3250 [13:31:21<1:41:25, 16.54s/it] 89%|████████▊ | 2883/3250 [13:31:37<1:39:52, 16.33s/it]                                                         89%|████████▊ | 2883/3250 [13:31:37<1:39:52, 16.33s/it] 89%|████████▊ | 2884/3250 [13:31:53<1:38:42, 16.18s/it]                                                         89%|████████▊ | 2884/3250 [13:31:53<1:38:42, 16.18s/it] 89%|████████▉ | 2885/3250 [13:32:09<1:37:47, 16.08s/it]                                                         89%|████████▉ | 2885/3250 [13:32:09<1:3{'loss': 0.4032, 'learning_rate': 3.067041762642475e-06, 'epoch': 0.89}
{'loss': 0.406, 'learning_rate': 3.050386277670103e-06, 'epoch': 0.89}
{'loss': 0.9238, 'learning_rate': 3.033774716476329e-06, 'epoch': 0.89}
{'loss': 0.3895, 'learning_rate': 3.017207094602126e-06, 'epoch': 0.89}
{'loss': 0.373, 'learning_rate': 3.000683427547374e-06, 'epoch': 0.89}
7:47, 16.08s/it] 89%|████████▉ | 2886/3250 [13:32:25<1:37:38, 16.09s/it]                                                         89%|████████▉ | 2886/3250 [13:32:25<1:37:38, 16.09s/it] 89%|████████▉ | 2887/3250 [13:32:41<1:36:55, 16.02s/it]                                                         89%|████████▉ | 2887/3250 [13:32:41<1:36:55, 16.02s/it] 89%|████████▉ | 2888/3250 [13:32:57<1:36:18, 15.96s/it]                                                         89%|████████▉ | 2888/3250 [13:32:57<1:36:18, 15.96s/it] 89%|████████▉ | 2889/3250 [13:33:13<1:36:12, 15.99s/it]                                                         89%|████████▉ | 2889/3250 [13:33:13<1:36:12, 15.99s/it] 89%|████████▉ | 2890/3250 [13:33:28<1:35:39, 15.94s/it]                                                         89%|████████▉ | 2890/3250 [13:33:28<1:35:39, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7593818306922913, 'eval_runtime': 2.4711, 'eval_samples_per_second': 4.856, 'eval_steps_per_second': 1.214, 'epoch': 0.89}
                                                         89%|████████▉ | 2890/3250 [13:33:31<1:35:39, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2890
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in  
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2890/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2890

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4118, 'learning_rate': 2.9842037307707906e-06, 'epoch': 0.89}
{'loss': 0.4149, 'learning_rate': 2.9677680196899925e-06, 'epoch': 0.89}
{'loss': 0.386, 'learning_rate': 2.9513763096814305e-06, 'epoch': 0.89}
{'loss': 0.3959, 'learning_rate': 2.935028616080393e-06, 'epoch': 0.89}
{'loss': 0.4467, 'learning_rate': 2.918724954180985e-06, 'epoch': 0.89}
 89%|████████▉ | 2891/3250 [13:33:47<1:40:59, 16.88s/it]                                                         89%|████████▉ | 2891/3250 [13:33:47<1:40:59, 16.88s/it] 89%|████████▉ | 2892/3250 [13:34:03<1:38:50, 16.57s/it]                                                         89%|████████▉ | 2892/3250 [13:34:03<1:38:50, 16.57s/it] 89%|████████▉ | 2893/3250 [13:34:19<1:37:14, 16.34s/it]                                                         89%|████████▉ | 2893/3250 [13:34:19<1:37:14, 16.34s/it] 89%|████████▉ | 2894/3250 [13:34:35<1:36:02, 16.19s/it]                                                         89%|████████▉ | 2894/3250 [13:34:35<1:36:02, 16.19s/it] 89%|████████▉ | 2895/3250 [13:34:51<1:35:09, 16.08s/it]                                                         89%|████████▉ | 2895/3250 [13:34:51<1:3{'loss': 0.4256, 'learning_rate': 2.9024653392361324e-06, 'epoch': 0.89}
{'loss': 0.3873, 'learning_rate': 2.886249786457523e-06, 'epoch': 0.89}
{'loss': 0.3767, 'learning_rate': 2.8700783110156503e-06, 'epoch': 0.89}
{'loss': 0.4168, 'learning_rate': 2.8539509280397614e-06, 'epoch': 0.89}
{'loss': 0.3959, 'learning_rate': 2.8378676526178482e-06, 'epoch': 0.89}
5:09, 16.08s/it] 89%|████████▉ | 2896/3250 [13:35:07<1:34:26, 16.01s/it]                                                         89%|████████▉ | 2896/3250 [13:35:07<1:34:26, 16.01s/it] 89%|████████▉ | 2897/3250 [13:35:22<1:33:50, 15.95s/it]                                                         89%|████████▉ | 2897/3250 [13:35:22<1:33:50, 15.95s/it] 89%|████████▉ | 2898/3250 [13:35:38<1:33:21, 15.91s/it]                                                         89%|████████▉ | 2898/3250 [13:35:38<1:33:21, 15.91s/it] 89%|████████▉ | 2899/3250 [13:35:54<1:32:56, 15.89s/it]                                                         89%|████████▉ | 2899/3250 [13:35:54<1:32:56, 15.89s/it] 89%|████████▉ | 2900/3250 [13:36:10<1:32:34, 15.87s/it]                                                         89%|████████▉ | 2900/3250 [13:36:10<1:32:34, 15.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.758013129234314, 'eval_runtime': 2.4832, 'eval_samples_per_second': 4.832, 'eval_steps_per_second': 1.208, 'epoch': 0.89}
                                                         89%|████████▉ | 2900/3250 [13:36:12<1:32:34, 15.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2900
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2900 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2900
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2900

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3988, 'learning_rate': 2.8218284997966527e-06, 'epoch': 0.89}
{'loss': 0.3866, 'learning_rate': 2.8058334845816213e-06, 'epoch': 0.89}
{'loss': 0.4062, 'learning_rate': 2.789882621936912e-06, 'epoch': 0.89}
{'loss': 0.4046, 'learning_rate': 2.7739759267853827e-06, 'epoch': 0.89}
{'loss': 0.4258, 'learning_rate': 2.758113414008551e-06, 'epoch': 0.89}
 89%|████████▉ | 2901/3250 [13:36:29<1:38:03, 16.86s/it]                                                         89%|████████▉ | 2901/3250 [13:36:29<1:38:03, 16.86s/it] 89%|████████▉ | 2902/3250 [13:36:45<1:36:38, 16.66s/it]                                                         89%|████████▉ | 2902/3250 [13:36:45<1:36:38, 16.66s/it] 89%|████████▉ | 2903/3250 [13:37:01<1:34:55, 16.41s/it]                                                         89%|████████▉ | 2903/3250 [13:37:01<1:34:55, 16.41s/it] 89%|████████▉ | 2904/3250 [13:37:17<1:33:38, 16.24s/it]                                                         89%|████████▉ | 2904/3250 [13:37:17<1:33:38, 16.24s/it] 89%|████████▉ | 2905/3250 [13:37:33<1:32:40, 16.12s/it]                                                         89%|████████▉ | 2905/3250 [13:37:33<1:3{'loss': 0.404, 'learning_rate': 2.7422950984466233e-06, 'epoch': 0.89}
{'loss': 0.3984, 'learning_rate': 2.7265209948984514e-06, 'epoch': 0.89}
{'loss': 0.4013, 'learning_rate': 2.7107911181215197e-06, 'epoch': 0.89}
{'loss': 0.4115, 'learning_rate': 2.695105482831928e-06, 'epoch': 0.9}
{'loss': 0.3714, 'learning_rate': 2.6794641037043988e-06, 'epoch': 0.9}
2:40, 16.12s/it] 89%|████████▉ | 2906/3250 [13:37:49<1:31:54, 16.03s/it]                                                         89%|████████▉ | 2906/3250 [13:37:49<1:31:54, 16.03s/it] 89%|████████▉ | 2907/3250 [13:38:04<1:31:16, 15.97s/it]                                                         89%|████████▉ | 2907/3250 [13:38:04<1:31:16, 15.97s/it] 89%|████████▉ | 2908/3250 [13:38:20<1:30:47, 15.93s/it]                                                         89%|████████▉ | 2908/3250 [13:38:20<1:30:47, 15.93s/it] 90%|████████▉ | 2909/3250 [13:38:36<1:30:21, 15.90s/it]                                                         90%|████████▉ | 2909/3250 [13:38:36<1:30:21, 15.90s/it] 90%|████████▉ | 2910/3250 [13:38:52<1:29:58, 15.88s/it]                                                         90%|████████▉ | 2910/3250 [13:38:52<1:29:58, 15.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7579250931739807, 'eval_runtime': 2.4696, 'eval_samples_per_second': 4.859, 'eval_steps_per_second': 1.215, 'epoch': 0.9}
                                                         90%|████████▉ | 2910/3250 [13:38:54<1:29:58, 15.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2910
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2910
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2910
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2910

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4156, 'learning_rate': 2.6638669953722496e-06, 'epoch': 0.9}
{'loss': 0.3834, 'learning_rate': 2.648314172427374e-06, 'epoch': 0.9}
{'loss': 0.3936, 'learning_rate': 2.6328056494202445e-06, 'epoch': 0.9}
{'loss': 0.381, 'learning_rate': 2.6173414408598827e-06, 'epoch': 0.9}
{'loss': 0.3861, 'learning_rate': 2.6019215612138383e-06, 'epoch': 0.9}
 90%|████████▉ | 2911/3250 [13:39:24<1:57:35, 20.81s/it]                                                         90%|████████▉ | 2911/3250 [13:39:24<1:57:35, 20.81s/it] 90%|████████▉ | 2912/3250 [13:39:40<1:48:51, 19.33s/it]                                                         90%|████████▉ | 2912/3250 [13:39:40<1:48:51, 19.33s/it] 90%|████████▉ | 2913/3250 [13:39:56<1:42:39, 18.28s/it]                                                         90%|████████▉ | 2913/3250 [13:39:56<1:42:39, 18.28s/it] 90%|████████▉ | 2914/3250 [13:40:12<1:38:16, 17.55s/it]                                                         90%|████████▉ | 2914/3250 [13:40:12<1:38:16, 17.55s/it] 90%|████████▉ | 2915/3250 [13:40:28<1:35:06, 17.04s/it]                                                         90%|████████▉ | 2915/3250 [13:40:28<1:3{'loss': 0.3981, 'learning_rate': 2.5865460249082097e-06, 'epoch': 0.9}
{'loss': 0.4213, 'learning_rate': 2.571214846327602e-06, 'epoch': 0.9}
{'loss': 0.3979, 'learning_rate': 2.5559280398151253e-06, 'epoch': 0.9}
{'loss': 0.9099, 'learning_rate': 2.5406856196723672e-06, 'epoch': 0.9}
{'loss': 0.3926, 'learning_rate': 2.525487600159404e-06, 'epoch': 0.9}
5:06, 17.04s/it] 90%|████████▉ | 2916/3250 [13:40:43<1:32:51, 16.68s/it]                                                         90%|████████▉ | 2916/3250 [13:40:44<1:32:51, 16.68s/it] 90%|████████▉ | 2917/3250 [13:40:59<1:31:10, 16.43s/it]                                                         90%|████████▉ | 2917/3250 [13:40:59<1:31:10, 16.43s/it] 90%|████████▉ | 2918/3250 [13:41:16<1:30:43, 16.40s/it]                                                         90%|████████▉ | 2918/3250 [13:41:16<1:30:43, 16.40s/it] 90%|████████▉ | 2919/3250 [13:41:31<1:29:27, 16.22s/it]                                                         90%|████████▉ | 2919/3250 [13:41:31<1:29:27, 16.22s/it] 90%|████████▉ | 2920/3250 [13:41:47<1:28:33, 16.10s/it]                                                         90%|████████▉ | 2920/3250 [13:41:47<1:28:33, 16.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7578490376472473, 'eval_runtime': 2.4766, 'eval_samples_per_second': 4.845, 'eval_steps_per_second': 1.211, 'epoch': 0.9}
                                                         90%|████████▉ | 2920/3250 [13:41:50<1:28:33, 16.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2920
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2920/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2920

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2920/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4182, 'learning_rate': 2.5103339954947626e-06, 'epoch': 0.9}
{'loss': 0.4021, 'learning_rate': 2.4952248198554073e-06, 'epoch': 0.9}
{'loss': 0.4078, 'learning_rate': 2.480160087376754e-06, 'epoch': 0.9}
{'loss': 0.3827, 'learning_rate': 2.465139812152639e-06, 'epoch': 0.9}
{'loss': 0.4514, 'learning_rate': 2.450164008235306e-06, 'epoch': 0.9}
 90%|████████▉ | 2921/3250 [13:42:06<1:33:12, 17.00s/it]                                                         90%|████████▉ | 2921/3250 [13:42:06<1:33:12, 17.00s/it] 90%|████████▉ | 2922/3250 [13:42:22<1:31:00, 16.65s/it]                                                         90%|████████▉ | 2922/3250 [13:42:22<1:31:00, 16.65s/it] 90%|████████▉ | 2923/3250 [13:42:38<1:29:25, 16.41s/it]                                                         90%|████████▉ | 2923/3250 [13:42:38<1:29:25, 16.41s/it] 90%|████████▉ | 2924/3250 [13:42:54<1:28:12, 16.24s/it]                                                         90%|████████▉ | 2924/3250 [13:42:54<1:28:12, 16.24s/it] 90%|█████████ | 2925/3250 [13:43:10<1:27:17, 16.12s/it]                                                         90%|█████████ | 2925/3250 [13:43:10<1:2{'loss': 0.4248, 'learning_rate': 2.435232689635386e-06, 'epoch': 0.9}
{'loss': 0.3866, 'learning_rate': 2.4203458703218997e-06, 'epoch': 0.9}
{'loss': 0.393, 'learning_rate': 2.4055035642222224e-06, 'epoch': 0.9}
{'loss': 0.4057, 'learning_rate': 2.390705785222097e-06, 'epoch': 0.9}
{'loss': 0.3938, 'learning_rate': 2.3759525471656162e-06, 'epoch': 0.9}
7:17, 16.12s/it] 90%|█████████ | 2926/3250 [13:43:26<1:26:33, 16.03s/it]                                                         90%|█████████ | 2926/3250 [13:43:26<1:26:33, 16.03s/it] 90%|█████████ | 2927/3250 [13:43:41<1:25:59, 15.97s/it]                                                         90%|█████████ | 2927/3250 [13:43:41<1:25:59, 15.97s/it] 90%|█████████ | 2928/3250 [13:43:57<1:25:29, 15.93s/it]                                                         90%|█████████ | 2928/3250 [13:43:57<1:25:29, 15.93s/it] 90%|█████████ | 2929/3250 [13:44:13<1:25:05, 15.90s/it]                                                         90%|█████████ | 2929/3250 [13:44:13<1:25:05, 15.90s/it] 90%|█████████ | 2930/3250 [13:44:29<1:24:44, 15.89s/it]                                                         90%|█████████ | 2930/3250 [13:44:29<1:24:44, 15.89s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7584968209266663, 'eval_runtime': 2.4746, 'eval_samples_per_second': 4.849, 'eval_steps_per_second': 1.212, 'epoch': 0.9}
                                                         90%|█████████ | 2930/3250 [13:44:31<1:24:44, 15.89s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2930
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2930 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2930
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2930

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4081, 'learning_rate': 2.361243863855184e-06, 'epoch': 0.9}
{'loss': 0.3988, 'learning_rate': 2.3465797490515418e-06, 'epoch': 0.9}
{'loss': 0.3903, 'learning_rate': 2.3319602164737053e-06, 'epoch': 0.9}
{'loss': 0.4038, 'learning_rate': 2.3173852797990114e-06, 'epoch': 0.9}
{'loss': 0.4116, 'learning_rate': 2.3028549526630583e-06, 'epoch': 0.9}
 90%|█████████ | 2931/3250 [13:44:48<1:29:50, 16.90s/it]                                                         90%|█████████ | 2931/3250 [13:44:48<1:29:50, 16.90s/it] 90%|█████████ | 2932/3250 [13:45:04<1:28:01, 16.61s/it]                                                         90%|█████████ | 2932/3250 [13:45:04<1:28:01, 16.61s/it] 90%|█████████ | 2933/3250 [13:45:20<1:26:40, 16.41s/it]                                                         90%|█████████ | 2933/3250 [13:45:20<1:26:40, 16.41s/it] 90%|█████████ | 2934/3250 [13:45:36<1:25:48, 16.29s/it]                                                         90%|█████████ | 2934/3250 [13:45:36<1:25:48, 16.29s/it] 90%|█████████ | 2935/3250 [13:45:52<1:25:33, 16.30s/it]                                                         90%|█████████ | 2935/3250 [13:45:52<1:2{'loss': 0.404, 'learning_rate': 2.288369248659722e-06, 'epoch': 0.9}
{'loss': 0.3987, 'learning_rate': 2.2739281813411116e-06, 'epoch': 0.9}
{'loss': 0.4043, 'learning_rate': 2.2595317642176038e-06, 'epoch': 0.9}
{'loss': 0.4076, 'learning_rate': 2.2451800107577805e-06, 'epoch': 0.9}
{'loss': 0.3721, 'learning_rate': 2.2308729343884395e-06, 'epoch': 0.9}
5:33, 16.30s/it] 90%|█████████ | 2936/3250 [13:46:08<1:24:42, 16.19s/it]                                                         90%|█████████ | 2936/3250 [13:46:08<1:24:42, 16.19s/it] 90%|█████████ | 2937/3250 [13:46:24<1:24:03, 16.11s/it]                                                         90%|█████████ | 2937/3250 [13:46:24<1:24:03, 16.11s/it] 90%|█████████ | 2938/3250 [13:46:40<1:23:31, 16.06s/it]                                                         90%|█████████ | 2938/3250 [13:46:40<1:23:31, 16.06s/it] 90%|█████████ | 2939/3250 [13:46:56<1:23:03, 16.02s/it]                                                         90%|█████████ | 2939/3250 [13:46:56<1:23:03, 16.02s/it] 90%|█████████ | 2940/3250 [13:47:12<1:22:39, 16.00s/it]                                                         90%|█████████ | 2940/3250 [13:47:12<1:22:39, 16.00s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7563043236732483, 'eval_runtime': 2.7068, 'eval_samples_per_second': 4.433, 'eval_steps_per_second': 1.108, 'epoch': 0.9}
                                                         90%|█████████ | 2940/3250 [13:47:15<1:22:39, 16.00s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2940
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2940 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2940

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4293, 'learning_rate': 2.2166105484945853e-06, 'epoch': 0.9}
{'loss': 0.3943, 'learning_rate': 2.202392866419423e-06, 'epoch': 0.91}
{'loss': 0.3817, 'learning_rate': 2.188219901464317e-06, 'epoch': 0.91}
{'loss': 0.3854, 'learning_rate': 2.1740916668888113e-06, 'epoch': 0.91}
{'loss': 0.3822, 'learning_rate': 2.160008175910605e-06, 'epoch': 0.91}
 90%|█████████ | 2941/3250 [13:47:46<1:50:19, 21.42s/it]                                                         90%|█████████ | 2941/3250 [13:47:46<1:50:19, 21.42s/it] 91%|█████████ | 2942/3250 [13:48:02<1:41:30, 19.77s/it]                                                         91%|█████████ | 2942/3250 [13:48:02<1:41:30, 19.77s/it] 91%|█████████ | 2943/3250 [13:48:18<1:35:16, 18.62s/it]                                                         91%|█████████ | 2943/3250 [13:48:18<1:35:16, 18.62s/it] 91%|█████████ | 2944/3250 [13:48:34<1:30:50, 17.81s/it]                                                         91%|█████████ | 2944/3250 [13:48:34<1:30:50, 17.81s/it] 91%|█████████ | 2945/3250 [13:48:50<1:27:39, 17.24s/it]                                                         91%|█████████ | 2945/3250 [13:48:50<1:2{'loss': 0.4106, 'learning_rate': 2.1459694417055034e-06, 'epoch': 0.91}
{'loss': 0.3939, 'learning_rate': 2.131975477407483e-06, 'epoch': 0.91}
{'loss': 0.4037, 'learning_rate': 2.1180262961086108e-06, 'epoch': 0.91}
{'loss': 0.9166, 'learning_rate': 2.1041219108590692e-06, 'epoch': 0.91}
{'loss': 0.395, 'learning_rate': 2.0902623346671258e-06, 'epoch': 0.91}
7:39, 17.24s/it] 91%|█████████ | 2946/3250 [13:49:06<1:25:22, 16.85s/it]                                                         91%|█████████ | 2946/3250 [13:49:06<1:25:22, 16.85s/it] 91%|█████████ | 2947/3250 [13:49:22<1:23:41, 16.57s/it]                                                         91%|█████████ | 2947/3250 [13:49:22<1:23:41, 16.57s/it] 91%|█████████ | 2948/3250 [13:49:38<1:22:26, 16.38s/it]                                                         91%|█████████ | 2948/3250 [13:49:38<1:22:26, 16.38s/it] 91%|█████████ | 2949/3250 [13:49:54<1:21:27, 16.24s/it]                                                         91%|█████████ | 2949/3250 [13:49:54<1:21:27, 16.24s/it] 91%|█████████ | 2950/3250 [13:50:10<1:20:45, 16.15s/it]                                                         91%|█████████ | 2950/3250 [13:50:10<1:20:45, 16.15s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7586849927902222, 'eval_runtime': 2.462, 'eval_samples_per_second': 4.874, 'eval_steps_per_second': 1.219, 'epoch': 0.91}
                                                         91%|█████████ | 2950/3250 [13:50:12<1:20:45, 16.15s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2950
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2950
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2950the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2950

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3949, 'learning_rate': 2.076447580499119e-06, 'epoch': 0.91}
{'loss': 0.4223, 'learning_rate': 2.0626776612794607e-06, 'epoch': 0.91}
{'loss': 0.4158, 'learning_rate': 2.0489525898906294e-06, 'epoch': 0.91}
{'loss': 0.3936, 'learning_rate': 2.0352723791731366e-06, 'epoch': 0.91}
{'loss': 0.404, 'learning_rate': 2.021637041925506e-06, 'epoch': 0.91}
 91%|█████████ | 2951/3250 [13:50:29<1:25:22, 17.13s/it]                                                         91%|█████████ | 2951/3250 [13:50:29<1:25:22, 17.13s/it] 91%|█████████ | 2952/3250 [13:50:45<1:23:17, 16.77s/it]                                                         91%|█████████ | 2952/3250 [13:50:45<1:23:17, 16.77s/it] 91%|█████████ | 2953/3250 [13:51:01<1:21:46, 16.52s/it]                                                         91%|█████████ | 2953/3250 [13:51:01<1:21:46, 16.52s/it] 91%|█████████ | 2954/3250 [13:51:17<1:20:36, 16.34s/it]                                                         91%|█████████ | 2954/3250 [13:51:17<1:20:36, 16.34s/it] 91%|█████████ | 2955/3250 [13:51:33<1:19:43, 16.21s/it]                                                         91%|█████████ | 2955/3250 [13:51:33<1:1{'loss': 0.4586, 'learning_rate': 2.0080465909043113e-06, 'epoch': 0.91}
{'loss': 0.4068, 'learning_rate': 1.99450103882412e-06, 'epoch': 0.91}
{'loss': 0.3985, 'learning_rate': 1.98100039835748e-06, 'epoch': 0.91}
{'loss': 0.3741, 'learning_rate': 1.967544682134942e-06, 'epoch': 0.91}
{'loss': 0.424, 'learning_rate': 1.9541339027450256e-06, 'epoch': 0.91}
9:43, 16.21s/it] 91%|█████████ | 2956/3250 [13:51:49<1:19:02, 16.13s/it]                                                         91%|█████████ | 2956/3250 [13:51:49<1:19:02, 16.13s/it] 91%|█████████ | 2957/3250 [13:52:05<1:18:30, 16.08s/it]                                                         91%|█████████ | 2957/3250 [13:52:05<1:18:30, 16.08s/it] 91%|█████████ | 2958/3250 [13:52:20<1:18:01, 16.03s/it]                                                         91%|█████████ | 2958/3250 [13:52:20<1:18:01, 16.03s/it] 91%|█████████ | 2959/3250 [13:52:36<1:17:35, 16.00s/it]                                                         91%|█████████ | 2959/3250 [13:52:36<1:17:35, 16.00s/it] 91%|█████████ | 2960/3250 [13:52:52<1:17:13, 15.98s/it]                                                         91%|█████████ | 2960/3250 [13:52:52<1:17:13, 15.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7582637071609497, 'eval_runtime': 2.4779, 'eval_samples_per_second': 4.843, 'eval_steps_per_second': 1.211, 'epoch': 0.91}
                                                         91%|█████████ | 2960/3250 [13:52:55<1:17:13, 15.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2960
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2960
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2960
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2960

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4211, 'learning_rate': 1.940768072734195e-06, 'epoch': 0.91}
{'loss': 0.3993, 'learning_rate': 1.92744720460688e-06, 'epoch': 0.91}
{'loss': 0.3901, 'learning_rate': 1.914171310825441e-06, 'epoch': 0.91}
{'loss': 0.4043, 'learning_rate': 1.9009404038101474e-06, 'epoch': 0.91}
{'loss': 0.4109, 'learning_rate': 1.887754495939198e-06, 'epoch': 0.91}
 91%|█████████ | 2961/3250 [13:53:40<2:03:27, 25.63s/it]                                                         91%|█████████ | 2961/3250 [13:53:40<2:03:27, 25.63s/it] 91%|█████████ | 2962/3250 [13:53:56<1:49:02, 22.72s/it]                                                         91%|█████████ | 2962/3250 [13:53:56<1:49:02, 22.72s/it] 91%|█████████ | 2963/3250 [13:54:12<1:38:54, 20.68s/it]                                                         91%|█████████ | 2963/3250 [13:54:12<1:38:54, 20.68s/it] 91%|█████████ | 2964/3250 [13:54:28<1:31:46, 19.25s/it]                                                         91%|█████████ | 2964/3250 [13:54:28<1:31:46, 19.25s/it] 91%|█████████ | 2965/3250 [13:54:44<1:26:42, 18.25s/it]                                                         91%|█████████ | 2965/3250 [13:54:44<1:2{'loss': 0.4235, 'learning_rate': 1.8746135995486858e-06, 'epoch': 0.91}
{'loss': 0.4145, 'learning_rate': 1.8615177269326045e-06, 'epoch': 0.91}
{'loss': 0.3965, 'learning_rate': 1.848466890342815e-06, 'epoch': 0.91}
{'loss': 0.4079, 'learning_rate': 1.8354611019890332e-06, 'epoch': 0.91}
{'loss': 0.3845, 'learning_rate': 1.8225003740388547e-06, 'epoch': 0.91}
6:42, 18.25s/it] 91%|█████████▏| 2966/3250 [13:55:00<1:23:03, 17.55s/it]                                                         91%|█████████▏| 2966/3250 [13:55:00<1:23:03, 17.55s/it] 91%|█████████▏| 2967/3250 [13:55:16<1:20:28, 17.06s/it]                                                         91%|█████████▏| 2967/3250 [13:55:16<1:20:28, 17.06s/it] 91%|█████████▏| 2968/3250 [13:55:33<1:19:45, 16.97s/it]                                                         91%|█████████▏| 2968/3250 [13:55:33<1:19:45, 16.97s/it] 91%|█████████▏| 2969/3250 [13:55:49<1:18:14, 16.70s/it]                                                         91%|█████████▏| 2969/3250 [13:55:49<1:18:14, 16.70s/it] 91%|█████████▏| 2970/3250 [13:56:05<1:16:53, 16.48s/it]                                                         91%|█████████▏| 2970/3250 [13:56:05<1:16:53, 16.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7574707269668579, 'eval_runtime': 2.4835, 'eval_samples_per_second': 4.832, 'eval_steps_per_second': 1.208, 'epoch': 0.91}
                                                         91%|█████████▏| 2970/3250 [13:56:07<1:16:53, 16.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2970
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in  the checkpoint model will be saved in 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2970
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2970
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2970

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4047, 'learning_rate': 1.8095847186177073e-06, 'epoch': 0.91}
{'loss': 0.3895, 'learning_rate': 1.7967141478088422e-06, 'epoch': 0.91}
{'loss': 0.3769, 'learning_rate': 1.78388867365335e-06, 'epoch': 0.91}
{'loss': 0.383, 'learning_rate': 1.7711083081501156e-06, 'epoch': 0.92}
{'loss': 0.3818, 'learning_rate': 1.7583730632558304e-06, 'epoch': 0.92}
 91%|█████████▏| 2971/3250 [13:56:38<1:39:24, 21.38s/it]                                                         91%|█████████▏| 2971/3250 [13:56:38<1:39:24, 21.38s/it] 91%|█████████▏| 2972/3250 [13:56:53<1:31:29, 19.75s/it]                                                         91%|█████████▏| 2972/3250 [13:56:53<1:31:29, 19.75s/it] 91%|█████████▏| 2973/3250 [13:57:10<1:26:05, 18.65s/it]                                                         91%|█████████▏| 2973/3250 [13:57:10<1:26:05, 18.65s/it] 92%|█████████▏| 2974/3250 [13:57:26<1:22:03, 17.84s/it]                                                         92%|█████████▏| 2974/3250 [13:57:26<1:22:03, 17.84s/it] 92%|█████████▏| 2975/3250 [13:57:41<1:19:09, 17.27s/it]                                                         92%|█████████▏| 297{'loss': 0.404, 'learning_rate': 1.7456829508849748e-06, 'epoch': 0.92}
{'loss': 0.3991, 'learning_rate': 1.733037982909791e-06, 'epoch': 0.92}
{'loss': 0.4038, 'learning_rate': 1.7204381711603045e-06, 'epoch': 0.92}
{'loss': 0.9172, 'learning_rate': 1.7078835274242922e-06, 'epoch': 0.92}
{'loss': 0.3799, 'learning_rate': 1.6953740634472581e-06, 'epoch': 0.92}
5/3250 [13:57:41<1:19:09, 17.27s/it] 92%|█████████▏| 2976/3250 [13:57:57<1:17:03, 16.88s/it]                                                         92%|█████████▏| 2976/3250 [13:57:57<1:17:03, 16.88s/it] 92%|█████████▏| 2977/3250 [13:58:13<1:15:30, 16.60s/it]                                                         92%|█████████▏| 2977/3250 [13:58:13<1:15:30, 16.60s/it] 92%|█████████▏| 2978/3250 [13:58:29<1:14:21, 16.40s/it]                                                         92%|█████████▏| 2978/3250 [13:58:29<1:14:21, 16.40s/it] 92%|█████████▏| 2979/3250 [13:58:45<1:13:26, 16.26s/it]                                                         92%|█████████▏| 2979/3250 [13:58:45<1:13:26, 16.26s/it] 92%|█████████▏| 2980/3250 [13:59:01<1:12:43, 16.16s/it]                                                         92%|█████████▏| 2980/3250 [13:59:01<1:12:43, 16.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7580025792121887, 'eval_runtime': 2.4718, 'eval_samples_per_second': 4.855, 'eval_steps_per_second': 1.214, 'epoch': 0.92}
                                                         92%|█████████▏| 2980/3250 [13:59:04<1:12:43, 16.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2980
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2980 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2980

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2980

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3804, 'learning_rate': 1.6829097909324632e-06, 'epoch': 0.92}
{'loss': 0.4141, 'learning_rate': 1.6704907215408573e-06, 'epoch': 0.92}
{'loss': 0.4119, 'learning_rate': 1.658116866891135e-06, 'epoch': 0.92}
{'loss': 0.3903, 'learning_rate': 1.6457882385596646e-06, 'epoch': 0.92}
{'loss': 0.3998, 'learning_rate': 1.6335048480805083e-06, 'epoch': 0.92}
 92%|█████████▏| 2981/3250 [13:59:34<1:34:28, 21.07s/it]                                                         92%|█████████▏| 2981/3250 [13:59:34<1:34:28, 21.07s/it] 92%|█████████▏| 2982/3250 [13:59:50<1:27:13, 19.53s/it]                                                         92%|█████████▏| 2982/3250 [13:59:50<1:27:13, 19.53s/it] 92%|█████████▏| 2983/3250 [14:00:06<1:22:05, 18.45s/it]                                                         92%|█████████▏| 2983/3250 [14:00:06<1:22:05, 18.45s/it] 92%|█████████▏| 2984/3250 [14:00:22<1:18:45, 17.77s/it]                                                         92%|█████████▏| 2984/3250 [14:00:22<1:18:45, 17.77s/it] 92%|█████████▏| 2985/3250 [14:00:38<1:16:00, 17.21s/it]                                                         92%|█████████▏| 298{'loss': 0.4371, 'learning_rate': 1.6212667069454291e-06, 'epoch': 0.92}
{'loss': 0.4261, 'learning_rate': 1.6090738266038186e-06, 'epoch': 0.92}
{'loss': 0.386, 'learning_rate': 1.596926218462752e-06, 'epoch': 0.92}
{'loss': 0.3678, 'learning_rate': 1.584823893886933e-06, 'epoch': 0.92}
{'loss': 0.4145, 'learning_rate': 1.572766864198716e-06, 'epoch': 0.92}
5/3250 [14:00:38<1:16:00, 17.21s/it] 92%|█████████▏| 2986/3250 [14:00:54<1:13:59, 16.82s/it]                                                         92%|█████████▏| 2986/3250 [14:00:54<1:13:59, 16.82s/it] 92%|█████████▏| 2987/3250 [14:01:09<1:12:30, 16.54s/it]                                                         92%|█████████▏| 2987/3250 [14:01:09<1:12:30, 16.54s/it] 92%|█████████▏| 2988/3250 [14:01:25<1:11:23, 16.35s/it]                                                         92%|█████████▏| 2988/3250 [14:01:25<1:11:23, 16.35s/it] 92%|█████████▏| 2989/3250 [14:01:41<1:10:31, 16.21s/it]                                                         92%|█████████▏| 2989/3250 [14:01:41<1:10:31, 16.21s/it] 92%|█████████▏| 2990/3250 [14:01:57<1:09:50, 16.12s/it]                                                         92%|█████████▏| 2990/3250 [14:01:57<1:09:50, 16.12s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7588180303573608, 'eval_runtime': 2.4711, 'eval_samples_per_second': 4.856, 'eval_steps_per_second': 1.214, 'epoch': 0.92}
                                                         92%|█████████▏| 2990/3250 [14:02:00<1:09:50, 16.12s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-2990
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2990
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2990/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2990


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-2990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4008, 'learning_rate': 1.5607551406780717e-06, 'epoch': 0.92}
{'loss': 0.4037, 'learning_rate': 1.548788734562584e-06, 'epoch': 0.92}
{'loss': 0.3933, 'learning_rate': 1.5368676570474471e-06, 'epoch': 0.92}
{'loss': 0.4038, 'learning_rate': 1.524991919285429e-06, 'epoch': 0.92}
{'loss': 0.4068, 'learning_rate': 1.5131615323869085e-06, 'epoch': 0.92}
 92%|█████████▏| 2991/3250 [14:02:35<1:37:33, 22.60s/it]                                                         92%|█████████▏| 2991/3250 [14:02:35<1:37:33, 22.60s/it] 92%|█████████▏| 2992/3250 [14:02:51<1:28:33, 20.59s/it]                                                         92%|█████████▏| 2992/3250 [14:02:51<1:28:33, 20.59s/it] 92%|█████████▏| 2993/3250 [14:03:07<1:22:13, 19.19s/it]                                                         92%|█████████▏| 2993/3250 [14:03:07<1:22:13, 19.19s/it] 92%|█████████▏| 2994/3250 [14:03:23<1:17:43, 18.22s/it]                                                         92%|█████████▏| 2994/3250 [14:03:23<1:17:43, 18.22s/it] 92%|█████████▏| 2995/3250 [14:03:39<1:14:30, 17.53s/it]                                                         92%|█████████▏| 299{'loss': 0.4315, 'learning_rate': 1.501376507419805e-06, 'epoch': 0.92}
{'loss': 0.3986, 'learning_rate': 1.4896368554096318e-06, 'epoch': 0.92}
{'loss': 0.4002, 'learning_rate': 1.4779425873394259e-06, 'epoch': 0.92}
{'loss': 0.4039, 'learning_rate': 1.4662937141497911e-06, 'epoch': 0.92}
{'loss': 0.4117, 'learning_rate': 1.4546902467388268e-06, 'epoch': 0.92}
5/3250 [14:03:39<1:14:30, 17.53s/it] 92%|█████████▏| 2996/3250 [14:03:54<1:12:09, 17.05s/it]                                                         92%|█████████▏| 2996/3250 [14:03:54<1:12:09, 17.05s/it] 92%|█████████▏| 2997/3250 [14:04:10<1:10:27, 16.71s/it]                                                         92%|█████████▏| 2997/3250 [14:04:10<1:10:27, 16.71s/it] 92%|█████████▏| 2998/3250 [14:04:26<1:09:11, 16.48s/it]                                                         92%|█████████▏| 2998/3250 [14:04:26<1:09:11, 16.48s/it] 92%|█████████▏| 2999/3250 [14:04:42<1:08:14, 16.31s/it]                                                         92%|█████████▏| 2999/3250 [14:04:42<1:08:14, 16.31s/it] 92%|█████████▏| 3000/3250 [14:04:59<1:08:18, 16.39s/it]                                                         92%|█████████▏| 3000/3250 [14:04:59<1:08:18, 16.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7578870058059692, 'eval_runtime': 2.4746, 'eval_samples_per_second': 4.849, 'eval_steps_per_second': 1.212, 'epoch': 0.92}
                                                         92%|█████████▏| 3000/3250 [14:05:01<1:08:18, 16.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3000
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3000
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3000
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3000

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3745, 'learning_rate': 1.443132195962188e-06, 'epoch': 0.92}
{'loss': 0.4377, 'learning_rate': 1.431619572633014e-06, 'epoch': 0.92}
{'loss': 0.3884, 'learning_rate': 1.4201523875219724e-06, 'epoch': 0.92}
{'loss': 0.3922, 'learning_rate': 1.4087306513571985e-06, 'epoch': 0.92}
{'loss': 0.3778, 'learning_rate': 1.3973543748243e-06, 'epoch': 0.92}
 92%|█████████▏| 3001/3250 [14:05:32<1:28:20, 21.29s/it]                                                         92%|█████████▏| 3001/3250 [14:05:32<1:28:20, 21.29s/it] 92%|█████████▏| 3002/3250 [14:05:48<1:21:21, 19.68s/it]                                                         92%|█████████▏| 3002/3250 [14:05:48<1:21:21, 19.68s/it] 92%|█████████▏| 3003/3250 [14:06:03<1:16:25, 18.56s/it]                                                         92%|█████████▏| 3003/3250 [14:06:03<1:16:25, 18.56s/it] 92%|█████████▏| 3004/3250 [14:06:19<1:12:53, 17.78s/it]                                                         92%|█████████▏| 3004/3250 [14:06:19<1:12:53, 17.78s/it] 92%|█████████▏| 3005/3250 [14:06:35<1:10:20, 17.23s/it]                                                         92%|█████████▏| 300{'loss': 0.3924, 'learning_rate': 1.3860235685663913e-06, 'epoch': 0.92}
{'loss': 0.4077, 'learning_rate': 1.3747382431840095e-06, 'epoch': 0.93}
{'loss': 0.4171, 'learning_rate': 1.3634984092351588e-06, 'epoch': 0.93}
{'loss': 0.4081, 'learning_rate': 1.3523040772352835e-06, 'epoch': 0.93}
{'loss': 0.9106, 'learning_rate': 1.341155257657256e-06, 'epoch': 0.93}
5/3250 [14:06:35<1:10:20, 17.23s/it] 92%|█████████▏| 3006/3250 [14:06:51<1:08:28, 16.84s/it]                                                         92%|█████████▏| 3006/3250 [14:06:51<1:08:28, 16.84s/it] 93%|█████████▎| 3007/3250 [14:07:07<1:07:05, 16.57s/it]                                                         93%|█████████▎| 3007/3250 [14:07:07<1:07:05, 16.57s/it] 93%|█████████▎| 3008/3250 [14:07:23<1:05:57, 16.35s/it]                                                         93%|█████████▎| 3008/3250 [14:07:23<1:05:57, 16.35s/it] 93%|█████████▎| 3009/3250 [14:07:39<1:05:04, 16.20s/it]                                                         93%|█████████▎| 3009/3250 [14:07:39<1:05:04, 16.20s/it] 93%|█████████▎| 3010/3250 [14:07:55<1:04:19, 16.08s/it]                                                         93%|█████████▎| 3010/3250 [14:07:55<1:04:19, 16.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7579917311668396, 'eval_runtime': 2.4704, 'eval_samples_per_second': 4.858, 'eval_steps_per_second': 1.214, 'epoch': 0.93}
                                                         93%|█████████▎| 3010/3250 [14:07:57<1:04:19, 16.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3010
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3010/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3010

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3010

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3846, 'learning_rate': 1.3300519609313832e-06, 'epoch': 0.93}
{'loss': 0.4075, 'learning_rate': 1.31899419744535e-06, 'epoch': 0.93}
{'loss': 0.3962, 'learning_rate': 1.3079819775442703e-06, 'epoch': 0.93}
{'loss': 0.4023, 'learning_rate': 1.297015311530647e-06, 'epoch': 0.93}
{'loss': 0.3723, 'learning_rate': 1.2860942096643569e-06, 'epoch': 0.93}
 93%|█████████▎| 3011/3250 [14:08:14<1:07:49, 17.03s/it]                                                         93%|█████████▎| 3011/3250 [14:08:14<1:07:49, 17.03s/it] 93%|█████████▎| 3012/3250 [14:08:30<1:06:07, 16.67s/it]                                                         93%|█████████▎| 3012/3250 [14:08:30<1:06:07, 16.67s/it] 93%|█████████▎| 3013/3250 [14:08:46<1:04:53, 16.43s/it]                                                         93%|█████████▎| 3013/3250 [14:08:46<1:04:53, 16.43s/it] 93%|█████████▎| 3014/3250 [14:09:01<1:03:56, 16.26s/it]                                                         93%|█████████▎| 3014/3250 [14:09:02<1:03:56, 16.26s/it] 93%|█████████▎| 3015/3250 [14:09:17<1:03:13, 16.14s/it]                                                         93%|█████████▎| 301{'loss': 0.4407, 'learning_rate': 1.2752186821626488e-06, 'epoch': 0.93}
{'loss': 0.4168, 'learning_rate': 1.2643887392001563e-06, 'epoch': 0.93}
{'loss': 0.3814, 'learning_rate': 1.2536043909088191e-06, 'epoch': 0.93}
{'loss': 0.3892, 'learning_rate': 1.2428656473779721e-06, 'epoch': 0.93}
{'loss': 0.4078, 'learning_rate': 1.232172518654251e-06, 'epoch': 0.93}
5/3250 [14:09:17<1:03:13, 16.14s/it] 93%|█████████▎| 3016/3250 [14:09:35<1:04:31, 16.54s/it]                                                         93%|█████████▎| 3016/3250 [14:09:35<1:04:31, 16.54s/it] 93%|█████████▎| 3017/3250 [14:09:51<1:03:48, 16.43s/it]                                                         93%|█████████▎| 3017/3250 [14:09:51<1:03:48, 16.43s/it] 93%|█████████▎| 3018/3250 [14:10:07<1:03:18, 16.37s/it]                                                         93%|█████████▎| 3018/3250 [14:10:07<1:03:18, 16.37s/it] 93%|█████████▎| 3019/3250 [14:10:23<1:02:22, 16.20s/it]                                                         93%|█████████▎| 3019/3250 [14:10:23<1:02:22, 16.20s/it] 93%|█████████▎| 3020/3250 [14:10:39<1:01:40, 16.09s/it]                                                         93%|█████████▎| 3020/3250 [14:10:39<1:01:40, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7582154870033264, 'eval_runtime': 2.4868, 'eval_samples_per_second': 4.825, 'eval_steps_per_second': 1.206, 'epoch': 0.93}
                                                         93%|█████████▎| 3020/3250 [14:10:41<1:01:40, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3020
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3020
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3020
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3020

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3968, 'learning_rate': 1.2215250147416313e-06, 'epoch': 0.93}
{'loss': 0.4013, 'learning_rate': 1.2109231456014058e-06, 'epoch': 0.93}
{'loss': 0.4008, 'learning_rate': 1.2003669211521628e-06, 'epoch': 0.93}
{'loss': 0.3917, 'learning_rate': 1.189856351269797e-06, 'epoch': 0.93}
{'loss': 0.4056, 'learning_rate': 1.1793914457874756e-06, 'epoch': 0.93}
 93%|█████████▎| 3021/3250 [14:11:16<1:25:55, 22.51s/it]                                                         93%|█████████▎| 3021/3250 [14:11:16<1:25:55, 22.51s/it] 93%|█████████▎| 3022/3250 [14:11:32<1:17:53, 20.50s/it]                                                         93%|█████████▎| 3022/3250 [14:11:32<1:17:53, 20.50s/it] 93%|█████████▎| 3023/3250 [14:11:48<1:12:14, 19.09s/it]                                                         93%|█████████▎| 3023/3250 [14:11:48<1:12:14, 19.09s/it] 93%|█████████▎| 3024/3250 [14:12:04<1:08:12, 18.11s/it]                                                         93%|█████████▎| 3024/3250 [14:12:04<1:08:12, 18.11s/it] 93%|█████████▎| 3025/3250 [14:12:20<1:05:18, 17.42s/it]                                                         93%|█████████▎| 302{'loss': 0.4155, 'learning_rate': 1.1689722144956671e-06, 'epoch': 0.93}
{'loss': 0.3995, 'learning_rate': 1.1585986671420967e-06, 'epoch': 0.93}
{'loss': 0.4023, 'learning_rate': 1.148270813431751e-06, 'epoch': 0.93}
{'loss': 0.4003, 'learning_rate': 1.1379886630268677e-06, 'epoch': 0.93}
{'loss': 0.4101, 'learning_rate': 1.1277522255469296e-06, 'epoch': 0.93}
5/3250 [14:12:20<1:05:18, 17.42s/it] 93%|█████████▎| 3026/3250 [14:12:35<1:03:13, 16.94s/it]                                                         93%|█████████▎| 3026/3250 [14:12:35<1:03:13, 16.94s/it] 93%|█████████▎| 3027/3250 [14:12:51<1:01:39, 16.59s/it]                                                         93%|█████████▎| 3027/3250 [14:12:51<1:01:39, 16.59s/it] 93%|█████████▎| 3028/3250 [14:13:07<1:00:29, 16.35s/it]                                                         93%|█████████▎| 3028/3250 [14:13:07<1:00:29, 16.35s/it] 93%|█████████▎| 3029/3250 [14:13:23<59:36, 16.18s/it]                                                         93%|█████████▎| 3029/3250 [14:13:23<59:36, 16.18s/it] 93%|█████████▎| 3030/3250 [14:13:39<58:54, 16.07s/it]                                                       93%|█████████▎| 3030/3250 [14:13:39<58:54, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7578767538070679, 'eval_runtime': 2.4524, 'eval_samples_per_second': 4.893, 'eval_steps_per_second': 1.223, 'epoch': 0.93}
                                                       93%|█████████▎| 3030/3250 [14:13:41<58:54, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3030
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3030/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3030

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3030

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3681, 'learning_rate': 1.1175615105686433e-06, 'epoch': 0.93}
{'loss': 0.4309, 'learning_rate': 1.107416527625954e-06, 'epoch': 0.93}
{'loss': 0.3904, 'learning_rate': 1.0973172862100145e-06, 'epoch': 0.93}
{'loss': 0.3866, 'learning_rate': 1.0872637957691834e-06, 'epoch': 0.93}
{'loss': 0.3768, 'learning_rate': 1.0772560657090202e-06, 'epoch': 0.93}
 93%|█████████▎| 3031/3250 [14:13:58<1:01:47, 16.93s/it]                                                         93%|█████████▎| 3031/3250 [14:13:58<1:01:47, 16.93s/it] 93%|█████████▎| 3032/3250 [14:14:13<1:00:16, 16.59s/it]                                                         93%|█████████▎| 3032/3250 [14:14:13<1:00:16, 16.59s/it] 93%|█████████▎| 3033/3250 [14:14:30<59:32, 16.46s/it]                                                         93%|█████████▎| 3033/3250 [14:14:30<59:32, 16.46s/it] 93%|█████████▎| 3034/3250 [14:14:46<58:47, 16.33s/it]                                                       93%|█████████▎| 3034/3250 [14:14:46<58:47, 16.33s/it] 93%|█████████▎| 3035/3250 [14:15:01<57:57, 16.17s/it]                                                       93%|█████████▎| 3035/3250 [14:15:{'loss': 0.39, 'learning_rate': 1.0672941053922635e-06, 'epoch': 0.93}
{'loss': 0.4042, 'learning_rate': 1.0573779241388471e-06, 'epoch': 0.93}
{'loss': 0.387, 'learning_rate': 1.0475075312258664e-06, 'epoch': 0.93}
{'loss': 0.4038, 'learning_rate': 1.037682935887585e-06, 'epoch': 0.94}
{'loss': 0.9147, 'learning_rate': 1.0279041473154116e-06, 'epoch': 0.94}
01<57:57, 16.17s/it] 93%|█████████▎| 3036/3250 [14:15:17<57:17, 16.06s/it]                                                       93%|█████████▎| 3036/3250 [14:15:17<57:17, 16.06s/it] 93%|█████████▎| 3037/3250 [14:15:33<56:44, 15.99s/it]                                                       93%|█████████▎| 3037/3250 [14:15:33<56:44, 15.99s/it] 93%|█████████▎| 3038/3250 [14:15:49<56:16, 15.93s/it]                                                       93%|█████████▎| 3038/3250 [14:15:49<56:16, 15.93s/it] 94%|█████████▎| 3039/3250 [14:16:05<55:51, 15.89s/it]                                                       94%|█████████▎| 3039/3250 [14:16:05<55:51, 15.89s/it] 94%|█████████▎| 3040/3250 [14:16:20<55:28, 15.85s/it]                                                       94%|█████████▎| 3040/3250 [14:16:20<55:28, 15.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7592355608940125, 'eval_runtime': 2.5075, 'eval_samples_per_second': 4.786, 'eval_steps_per_second': 1.196, 'epoch': 0.94}
                                                       94%|█████████▎| 3040/3250 [14:16:23<55:28, 15.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3040
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3040/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3040


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3040

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4023, 'learning_rate': 1.0181711746579058e-06, 'epoch': 0.94}
{'loss': 0.3975, 'learning_rate': 1.008484027020773e-06, 'epoch': 0.94}
{'loss': 0.4193, 'learning_rate': 9.988427134668298e-07, 'epoch': 0.94}
{'loss': 0.4053, 'learning_rate': 9.892472430160171e-07, 'epoch': 0.94}
{'loss': 0.3917, 'learning_rate': 9.796976246454037e-07, 'epoch': 0.94}
 94%|█████████▎| 3041/3250 [14:16:39<58:42, 16.85s/it]                                                       94%|█████████▎| 3041/3250 [14:16:39<58:42, 16.85s/it] 94%|█████████▎| 3042/3250 [14:16:55<57:20, 16.54s/it]                                                       94%|█████████▎| 3042/3250 [14:16:55<57:20, 16.54s/it] 94%|█████████▎| 3043/3250 [14:17:11<56:17, 16.32s/it]                                                       94%|█████████▎| 3043/3250 [14:17:11<56:17, 16.32s/it] 94%|█████████▎| 3044/3250 [14:17:27<55:29, 16.16s/it]                                                       94%|█████████▎| 3044/3250 [14:17:27<55:29, 16.16s/it] 94%|█████████▎| 3045/3250 [14:17:43<54:51, 16.06s/it]                                                       94%|█████████▎| 3045/3250 [14:17:43<54:51, 16.0{'loss': 0.3915, 'learning_rate': 9.701938672891376e-07, 'epoch': 0.94}
{'loss': 0.4594, 'learning_rate': 9.607359798384785e-07, 'epoch': 0.94}
{'loss': 0.4065, 'learning_rate': 9.513239711417654e-07, 'epoch': 0.94}
{'loss': 0.4014, 'learning_rate': 9.419578500044157e-07, 'epoch': 0.94}
{'loss': 0.3677, 'learning_rate': 9.326376251889202e-07, 'epoch': 0.94}
6s/it] 94%|█████████▎| 3046/3250 [14:17:58<54:19, 15.98s/it]                                                       94%|█████████▎| 3046/3250 [14:17:59<54:19, 15.98s/it] 94%|█████████▍| 3047/3250 [14:18:14<53:52, 15.93s/it]                                                       94%|█████████▍| 3047/3250 [14:18:14<53:52, 15.93s/it] 94%|█████████▍| 3048/3250 [14:18:30<53:29, 15.89s/it]                                                       94%|█████████▍| 3048/3250 [14:18:30<53:29, 15.89s/it] 94%|█████████▍| 3049/3250 [14:18:46<53:09, 15.87s/it]                                                       94%|█████████▍| 3049/3250 [14:18:46<53:09, 15.87s/it] 94%|█████████▍| 3050/3250 [14:19:03<53:38, 16.09s/it]                                                       94%|█████████▍| 3050/3250 [14:19:03<53:38, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7587736248970032, 'eval_runtime': 2.454, 'eval_samples_per_second': 4.89, 'eval_steps_per_second': 1.222, 'epoch': 0.94}
                                                       94%|█████████▍| 3050/3250 [14:19:05<53:38, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3050
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3050
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3050

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.423, 'learning_rate': 9.233633054148205e-07, 'epoch': 0.94}
{'loss': 0.4202, 'learning_rate': 9.141348993587317e-07, 'epoch': 0.94}
{'loss': 0.3935, 'learning_rate': 9.049524156542977e-07, 'epoch': 0.94}
{'loss': 0.3906, 'learning_rate': 8.958158628922019e-07, 'epoch': 0.94}
{'loss': 0.4018, 'learning_rate': 8.867252496201572e-07, 'epoch': 0.94}
 94%|█████████▍| 3051/3250 [14:19:53<1:28:02, 26.55s/it]                                                         94%|█████████▍| 3051/3250 [14:19:53<1:28:02, 26.55s/it] 94%|█████████▍| 3052/3250 [14:20:09<1:17:04, 23.36s/it]                                                         94%|█████████▍| 3052/3250 [14:20:09<1:17:04, 23.36s/it] 94%|█████████▍| 3053/3250 [14:20:25<1:09:13, 21.09s/it]                                                         94%|█████████▍| 3053/3250 [14:20:25<1:09:13, 21.09s/it] 94%|█████████▍| 3054/3250 [14:20:41<1:03:42, 19.50s/it]                                                         94%|█████████▍| 3054/3250 [14:20:41<1:03:42, 19.50s/it] 94%|█████████▍| 3055/3250 [14:20:57<59:45, 18.39s/it]                                                         94%|█████████▍| 3055/{'loss': 0.3966, 'learning_rate': 8.776805843429103e-07, 'epoch': 0.94}
{'loss': 0.4302, 'learning_rate': 8.686818755221982e-07, 'epoch': 0.94}
{'loss': 0.412, 'learning_rate': 8.597291315767808e-07, 'epoch': 0.94}
{'loss': 0.3966, 'learning_rate': 8.508223608824084e-07, 'epoch': 0.94}
{'loss': 0.4065, 'learning_rate': 8.419615717718377e-07, 'epoch': 0.94}
3250 [14:20:57<59:45, 18.39s/it] 94%|█████████▍| 3056/3250 [14:21:13<56:56, 17.61s/it]                                                       94%|█████████▍| 3056/3250 [14:21:13<56:56, 17.61s/it] 94%|█████████▍| 3057/3250 [14:21:28<54:53, 17.07s/it]                                                       94%|█████████▍| 3057/3250 [14:21:28<54:53, 17.07s/it] 94%|█████████▍| 3058/3250 [14:21:44<53:23, 16.68s/it]                                                       94%|█████████▍| 3058/3250 [14:21:44<53:23, 16.68s/it] 94%|█████████▍| 3059/3250 [14:22:00<52:15, 16.41s/it]                                                       94%|█████████▍| 3059/3250 [14:22:00<52:15, 16.41s/it] 94%|█████████▍| 3060/3250 [14:22:16<51:23, 16.23s/it]                                                       94%|█████████▍| 3060/3250 [14:22:16<51:23, 16.23s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7581519484519958, 'eval_runtime': 2.4548, 'eval_samples_per_second': 4.888, 'eval_steps_per_second': 1.222, 'epoch': 0.94}
                                                       94%|█████████▍| 3060/3250 [14:22:18<51:23, 16.23s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3060
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3060 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3060
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3060

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3843, 'learning_rate': 8.331467725347708e-07, 'epoch': 0.94}
{'loss': 0.3992, 'learning_rate': 8.243779714179168e-07, 'epoch': 0.94}
{'loss': 0.3903, 'learning_rate': 8.156551766249409e-07, 'epoch': 0.94}
{'loss': 0.3755, 'learning_rate': 8.069783963164656e-07, 'epoch': 0.94}
{'loss': 0.3833, 'learning_rate': 7.983476386100641e-07, 'epoch': 0.94}
 94%|█████████▍| 3061/3250 [14:22:35<53:42, 17.05s/it]                                                       94%|█████████▍| 3061/3250 [14:22:35<53:42, 17.05s/it] 94%|█████████▍| 3062/3250 [14:22:50<52:14, 16.67s/it]                                                       94%|█████████▍| 3062/3250 [14:22:50<52:14, 16.67s/it] 94%|█████████▍| 3063/3250 [14:23:06<51:09, 16.41s/it]                                                       94%|█████████▍| 3063/3250 [14:23:06<51:09, 16.41s/it] 94%|█████████▍| 3064/3250 [14:23:22<50:18, 16.23s/it]                                                       94%|█████████▍| 3064/3250 [14:23:22<50:18, 16.23s/it] 94%|█████████▍| 3065/3250 [14:23:38<49:39, 16.10s/it]                                                       94%|█████████▍| 3065/3250 [14:23:38<49:39, 16.1{'loss': 0.376, 'learning_rate': 7.897629115802551e-07, 'epoch': 0.94}
{'loss': 0.4042, 'learning_rate': 7.812242232584865e-07, 'epoch': 0.94}
{'loss': 0.4002, 'learning_rate': 7.727315816331515e-07, 'epoch': 0.94}
{'loss': 0.4152, 'learning_rate': 7.642849946495445e-07, 'epoch': 0.94}
{'loss': 0.9235, 'learning_rate': 7.558844702098833e-07, 'epoch': 0.94}
0s/it] 94%|█████████▍| 3066/3250 [14:23:54<49:22, 16.10s/it]                                                       94%|█████████▍| 3066/3250 [14:23:54<49:22, 16.10s/it] 94%|█████████▍| 3067/3250 [14:24:10<48:50, 16.02s/it]                                                       94%|█████████▍| 3067/3250 [14:24:10<48:50, 16.02s/it] 94%|█████████▍| 3068/3250 [14:24:26<48:23, 15.95s/it]                                                       94%|█████████▍| 3068/3250 [14:24:26<48:23, 15.95s/it] 94%|█████████▍| 3069/3250 [14:24:41<47:59, 15.91s/it]                                                       94%|█████████▍| 3069/3250 [14:24:41<47:59, 15.91s/it] 94%|█████████▍| 3070/3250 [14:24:57<47:35, 15.87s/it]                                                       94%|█████████▍| 3070/3250 [14:24:57<47:35, 15.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7589025497436523, 'eval_runtime': 2.974, 'eval_samples_per_second': 4.035, 'eval_steps_per_second': 1.009, 'epoch': 0.94}
                                                       94%|█████████▍| 3070/3250 [14:25:00<47:35, 15.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3070
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3070
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3070
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3070

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3912, 'learning_rate': 7.475300161732978e-07, 'epoch': 0.94}
{'loss': 0.3864, 'learning_rate': 7.392216403558027e-07, 'epoch': 0.95}
{'loss': 0.4125, 'learning_rate': 7.309593505303136e-07, 'epoch': 0.95}
{'loss': 0.4211, 'learning_rate': 7.227431544266194e-07, 'epoch': 0.95}
{'loss': 0.3869, 'learning_rate': 7.145730597314049e-07, 'epoch': 0.95}
 94%|█████████▍| 3071/3250 [14:25:17<50:41, 16.99s/it]                                                       94%|█████████▍| 3071/3250 [14:25:17<50:41, 16.99s/it] 95%|█████████▍| 3072/3250 [14:25:33<49:25, 16.66s/it]                                                       95%|█████████▍| 3072/3250 [14:25:33<49:25, 16.66s/it] 95%|█████████▍| 3073/3250 [14:25:49<48:23, 16.41s/it]                                                       95%|█████████▍| 3073/3250 [14:25:49<48:23, 16.41s/it] 95%|█████████▍| 3074/3250 [14:26:04<47:35, 16.22s/it]                                                       95%|█████████▍| 3074/3250 [14:26:04<47:35, 16.22s/it] 95%|█████████▍| 3075/3250 [14:26:20<46:56, 16.10s/it]                                                       95%|█████████▍| 3075/3250 [14:26:20<46:56, 16.1{'loss': 0.4015, 'learning_rate': 7.064490740882057e-07, 'epoch': 0.95}
{'loss': 0.4506, 'learning_rate': 6.983712050974367e-07, 'epoch': 0.95}
{'loss': 0.4258, 'learning_rate': 6.903394603163582e-07, 'epoch': 0.95}
{'loss': 0.3805, 'learning_rate': 6.823538472590707e-07, 'epoch': 0.95}
{'loss': 0.3756, 'learning_rate': 6.744143733965369e-07, 'epoch': 0.95}
0s/it] 95%|█████████▍| 3076/3250 [14:26:36<46:25, 16.01s/it]                                                       95%|█████████▍| 3076/3250 [14:26:36<46:25, 16.01s/it] 95%|█████████▍| 3077/3250 [14:26:52<45:58, 15.95s/it]                                                       95%|█████████▍| 3077/3250 [14:26:52<45:58, 15.95s/it] 95%|█████████▍| 3078/3250 [14:27:08<45:35, 15.90s/it]                                                       95%|█████████▍| 3078/3250 [14:27:08<45:35, 15.90s/it] 95%|█████████▍| 3079/3250 [14:27:24<46:09, 16.20s/it]                                                       95%|█████████▍| 3079/3250 [14:27:24<46:09, 16.20s/it] 95%|█████████▍| 3080/3250 [14:27:40<45:36, 16.10s/it]                                                       95%|█████████▍| 3080/3250 [14:27:40<45:36, 16.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7592885494232178, 'eval_runtime': 3.853, 'eval_samples_per_second': 3.114, 'eval_steps_per_second': 0.779, 'epoch': 0.95}
                                                       95%|█████████▍| 3080/3250 [14:27:44<45:36, 16.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3080
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3080
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3080
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3080

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3080/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3080/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4149, 'learning_rate': 6.66521046156543e-07, 'epoch': 0.95}
{'loss': 0.4004, 'learning_rate': 6.58673872923693e-07, 'epoch': 0.95}
{'loss': 0.408, 'learning_rate': 6.508728610394255e-07, 'epoch': 0.95}
{'loss': 0.4058, 'learning_rate': 6.431180178019969e-07, 'epoch': 0.95}
{'loss': 0.4061, 'learning_rate': 6.354093504664538e-07, 'epoch': 0.95}
 95%|█████████▍| 3081/3250 [14:28:01<49:00, 17.40s/it]                                                       95%|█████████▍| 3081/3250 [14:28:01<49:00, 17.40s/it] 95%|█████████▍| 3082/3250 [14:28:17<47:45, 17.05s/it]                                                       95%|█████████▍| 3082/3250 [14:28:17<47:45, 17.05s/it] 95%|█████████▍| 3083/3250 [14:28:33<46:25, 16.68s/it]                                                       95%|█████████▍| 3083/3250 [14:28:33<46:25, 16.68s/it] 95%|█████████▍| 3084/3250 [14:28:49<45:26, 16.43s/it]                                                       95%|█████████▍| 3084/3250 [14:28:49<45:26, 16.43s/it] 95%|█████████▍| 3085/3250 [14:29:05<44:49, 16.30s/it]                                                       95%|█████████▍| 3085/3250 [14:29:05<44:49, 16.3{'loss': 0.4041, 'learning_rate': 6.277468662446495e-07, 'epoch': 0.95}
{'loss': 0.4351, 'learning_rate': 6.201305723052331e-07, 'epoch': 0.95}
{'loss': 0.4024, 'learning_rate': 6.125604757736436e-07, 'epoch': 0.95}
{'loss': 0.4117, 'learning_rate': 6.050365837320992e-07, 'epoch': 0.95}
{'loss': 0.4049, 'learning_rate': 5.97558903219575e-07, 'epoch': 0.95}
0s/it] 95%|█████████▍| 3086/3250 [14:29:20<44:12, 16.17s/it]                                                       95%|█████████▍| 3086/3250 [14:29:22<44:12, 16.17s/it] 95%|█████████▍| 3087/3250 [14:29:37<44:34, 16.41s/it]                                                       95%|█████████▍| 3087/3250 [14:29:37<44:34, 16.41s/it] 95%|█████████▌| 3088/3250 [14:29:53<43:59, 16.29s/it]                                                       95%|█████████▌| 3088/3250 [14:29:53<43:59, 16.29s/it] 95%|█████████▌| 3089/3250 [14:30:09<43:18, 16.14s/it]                                                       95%|█████████▌| 3089/3250 [14:30:09<43:18, 16.14s/it] 95%|█████████▌| 3090/3250 [14:30:25<42:47, 16.04s/it]                                                       95%|█████████▌| 3090/3250 [14:30:25<42:47, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7593992352485657, 'eval_runtime': 2.613, 'eval_samples_per_second': 4.592, 'eval_steps_per_second': 1.148, 'epoch': 0.95}
                                                       95%|█████████▌| 3090/3250 [14:30:28<42:47, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3090
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3090
the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3090/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3090


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4149, 'learning_rate': 5.901274412318359e-07, 'epoch': 0.95}
{'loss': 0.379, 'learning_rate': 5.827422047213926e-07, 'epoch': 0.95}
{'loss': 0.4353, 'learning_rate': 5.754032005975129e-07, 'epoch': 0.95}
{'loss': 0.3958, 'learning_rate': 5.681104357262101e-07, 'epoch': 0.95}
{'loss': 0.3923, 'learning_rate': 5.608639169302543e-07, 'epoch': 0.95}
 95%|█████████▌| 3091/3250 [14:30:44<45:04, 17.01s/it]                                                       95%|█████████▌| 3091/3250 [14:30:44<45:04, 17.01s/it] 95%|█████████▌| 3092/3250 [14:31:00<43:53, 16.67s/it]                                                       95%|█████████▌| 3092/3250 [14:31:00<43:53, 16.67s/it] 95%|█████████▌| 3093/3250 [14:31:16<43:00, 16.44s/it]                                                       95%|█████████▌| 3093/3250 [14:31:16<43:00, 16.44s/it] 95%|█████████▌| 3094/3250 [14:31:32<42:26, 16.32s/it]                                                       95%|█████████▌| 3094/3250 [14:31:32<42:26, 16.32s/it] 95%|█████████▌| 3095/3250 [14:31:48<41:49, 16.19s/it]                                                       95%|█████████▌| 3095/3250 [14:31:48<41:49, 16.1{'loss': 0.3856, 'learning_rate': 5.536636509891225e-07, 'epoch': 0.95}
{'loss': 0.3927, 'learning_rate': 5.465096446390428e-07, 'epoch': 0.95}
{'loss': 0.3958, 'learning_rate': 5.394019045729448e-07, 'epoch': 0.95}
{'loss': 0.4166, 'learning_rate': 5.323404374404983e-07, 'epoch': 0.95}
{'loss': 0.4014, 'learning_rate': 5.253252498480576e-07, 'epoch': 0.95}
9s/it] 95%|█████████▌| 3096/3250 [14:32:04<41:18, 16.10s/it]                                                       95%|█████████▌| 3096/3250 [14:32:04<41:18, 16.10s/it] 95%|█████████▌| 3097/3250 [14:32:20<40:52, 16.03s/it]                                                       95%|█████████▌| 3097/3250 [14:32:20<40:52, 16.03s/it] 95%|█████████▌| 3098/3250 [14:32:36<40:29, 15.98s/it]                                                       95%|█████████▌| 3098/3250 [14:32:36<40:29, 15.98s/it] 95%|█████████▌| 3099/3250 [14:32:52<40:34, 16.12s/it]                                                       95%|█████████▌| 3099/3250 [14:32:52<40:34, 16.12s/it] 95%|█████████▌| 3100/3250 [14:33:08<40:07, 16.05s/it]                                                       95%|█████████▌| 3100/3250 [14:33:08<40:07, 16.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7591207027435303, 'eval_runtime': 2.458, 'eval_samples_per_second': 4.882, 'eval_steps_per_second': 1.221, 'epoch': 0.95}
                                                       95%|█████████▌| 3100/3250 [14:33:10<40:07, 16.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3100
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3100/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3100

 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3100

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3100/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3100/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9056, 'learning_rate': 5.183563483587006e-07, 'epoch': 0.95}
{'loss': 0.3882, 'learning_rate': 5.114337394921953e-07, 'epoch': 0.95}
{'loss': 0.4188, 'learning_rate': 5.045574297249833e-07, 'epoch': 0.95}
{'loss': 0.4097, 'learning_rate': 4.977274254902187e-07, 'epoch': 0.96}
{'loss': 0.4128, 'learning_rate': 4.909437331777179e-07, 'epoch': 0.96}
 95%|█████████▌| 3101/3250 [14:33:27<42:06, 16.96s/it]                                                       95%|█████████▌| 3101/3250 [14:33:27<42:06, 16.96s/it] 95%|█████████▌| 3102/3250 [14:33:43<41:01, 16.63s/it]                                                       95%|█████████▌| 3102/3250 [14:33:43<41:01, 16.63s/it] 95%|█████████▌| 3103/3250 [14:33:59<40:12, 16.41s/it]                                                       95%|█████████▌| 3103/3250 [14:33:59<40:12, 16.41s/it] 96%|█████████▌| 3104/3250 [14:34:15<39:33, 16.26s/it]                                                       96%|█████████▌| 3104/3250 [14:34:15<39:33, 16.26s/it] 96%|█████████▌| 3105/3250 [14:34:31<39:00, 16.14s/it]                                                       96%|█████████▌| 3105/3250 [14:34:31<39:00, 16.1{'loss': 0.3925, 'learning_rate': 4.842063591339763e-07, 'epoch': 0.96}
{'loss': 0.434, 'learning_rate': 4.77515309662152e-07, 'epoch': 0.96}
{'loss': 0.4292, 'learning_rate': 4.7087059102206564e-07, 'epoch': 0.96}
{'loss': 0.3828, 'learning_rate': 4.6427220943019436e-07, 'epoch': 0.96}
{'loss': 0.3925, 'learning_rate': 4.577201710596612e-07, 'epoch': 0.96}
4s/it] 96%|█████████▌| 3106/3250 [14:34:46<38:33, 16.07s/it]                                                       96%|█████████▌| 3106/3250 [14:34:46<38:33, 16.07s/it] 96%|█████████▌| 3107/3250 [14:35:02<38:08, 16.00s/it]                                                       96%|█████████▌| 3107/3250 [14:35:02<38:08, 16.00s/it] 96%|█████████▌| 3108/3250 [14:35:18<37:46, 15.96s/it]                                                       96%|█████████▌| 3108/3250 [14:35:18<37:46, 15.96s/it] 96%|█████████▌| 3109/3250 [14:35:34<37:26, 15.93s/it]                                                       96%|█████████▌| 3109/3250 [14:35:34<37:26, 15.93s/it] 96%|█████████▌| 3110/3250 [14:35:50<37:08, 15.92s/it]                                                       96%|█████████▌| 3110/3250 [14:35:50<37:08, 15.92s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7589017748832703, 'eval_runtime': 2.4615, 'eval_samples_per_second': 4.875, 'eval_steps_per_second': 1.219, 'epoch': 0.96}
                                                       96%|█████████▌| 3110/3250 [14:35:52<37:08, 15.92s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3110
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3110 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3110

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4081, 'learning_rate': 4.512144820402409e-07, 'epoch': 0.96}
{'loss': 0.4051, 'learning_rate': 4.447551484583312e-07, 'epoch': 0.96}
{'loss': 0.3945, 'learning_rate': 4.3834217635697616e-07, 'epoch': 0.96}
{'loss': 0.4023, 'learning_rate': 4.3197557173584315e-07, 'epoch': 0.96}
{'loss': 0.3989, 'learning_rate': 4.2565534055121204e-07, 'epoch': 0.96}
 96%|█████████▌| 3111/3250 [14:36:09<39:11, 16.92s/it]                                                       96%|█████████▌| 3111/3250 [14:36:09<39:11, 16.92s/it] 96%|█████████▌| 3112/3250 [14:36:25<38:11, 16.61s/it]                                                       96%|█████████▌| 3112/3250 [14:36:25<38:11, 16.61s/it] 96%|█████████▌| 3113/3250 [14:36:41<37:24, 16.38s/it]                                                       96%|█████████▌| 3113/3250 [14:36:41<37:24, 16.38s/it] 96%|█████████▌| 3114/3250 [14:36:57<36:46, 16.23s/it]                                                       96%|█████████▌| 3114/3250 [14:36:57<36:46, 16.23s/it] 96%|█████████▌| 3115/3250 [14:37:13<36:31, 16.23s/it]                                                       96%|█████████▌| 3115/3250 [14:37:13<36:31, 16.2{'loss': 0.4038, 'learning_rate': 4.193814887159919e-07, 'epoch': 0.96}
{'loss': 0.415, 'learning_rate': 4.131540220996877e-07, 'epoch': 0.96}
{'loss': 0.3978, 'learning_rate': 4.069729465284167e-07, 'epoch': 0.96}
{'loss': 0.3992, 'learning_rate': 4.0083826778489206e-07, 'epoch': 0.96}
{'loss': 0.4032, 'learning_rate': 3.947499916084285e-07, 'epoch': 0.96}
3s/it] 96%|█████████▌| 3116/3250 [14:37:29<36:00, 16.13s/it]                                                       96%|█████████▌| 3116/3250 [14:37:29<36:00, 16.13s/it] 96%|█████████▌| 3117/3250 [14:37:45<35:35, 16.05s/it]                                                       96%|█████████▌| 3117/3250 [14:37:45<35:35, 16.05s/it] 96%|█████████▌| 3118/3250 [14:38:01<35:11, 16.00s/it]                                                       96%|█████████▌| 3118/3250 [14:38:01<35:11, 16.00s/it] 96%|█████████▌| 3119/3250 [14:38:17<34:51, 15.97s/it]                                                       96%|█████████▌| 3119/3250 [14:38:17<34:51, 15.97s/it] 96%|█████████▌| 3120/3250 [14:38:32<34:32, 15.94s/it]                                                       96%|█████████▌| 3120/3250 [14:38:32<34:32, 15.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.75882488489151, 'eval_runtime': 2.4644, 'eval_samples_per_second': 4.869, 'eval_steps_per_second': 1.217, 'epoch': 0.96}
                                                       96%|█████████▌| 3120/3250 [14:38:35<34:32, 15.94s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3120
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3120
the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3120
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3120

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4197, 'learning_rate': 3.887081236949086e-07, 'epoch': 0.96}
{'loss': 0.3661, 'learning_rate': 3.8271266969682194e-07, 'epoch': 0.96}
{'loss': 0.4351, 'learning_rate': 3.767636352232151e-07, 'epoch': 0.96}
{'loss': 0.3887, 'learning_rate': 3.7086102583972494e-07, 'epoch': 0.96}
{'loss': 0.3831, 'learning_rate': 3.6500484706853964e-07, 'epoch': 0.96}
 96%|█████████▌| 3121/3250 [14:39:10<48:05, 22.37s/it]                                                       96%|█████████▌| 3121/3250 [14:39:10<48:05, 22.37s/it] 96%|█████████▌| 3122/3250 [14:39:26<43:33, 20.41s/it]                                                       96%|█████████▌| 3122/3250 [14:39:26<43:33, 20.41s/it] 96%|█████████▌| 3123/3250 [14:39:42<40:19, 19.05s/it]                                                       96%|█████████▌| 3123/3250 [14:39:42<40:19, 19.05s/it] 96%|█████████▌| 3124/3250 [14:39:57<38:00, 18.10s/it]                                                       96%|█████████▌| 3124/3250 [14:39:57<38:00, 18.10s/it] 96%|█████████▌| 3125/3250 [14:40:13<36:18, 17.43s/it]                                                       96%|█████████▌| 3125/3250 [14:40:13<36:18, 17.4{'loss': 0.3778, 'learning_rate': 3.591951043884212e-07, 'epoch': 0.96}
{'loss': 0.3873, 'learning_rate': 3.534318032346773e-07, 'epoch': 0.96}
{'loss': 0.4005, 'learning_rate': 3.4771494899917265e-07, 'epoch': 0.96}
{'loss': 0.3897, 'learning_rate': 3.420445470303235e-07, 'epoch': 0.96}
{'loss': 0.4032, 'learning_rate': 3.364206026330752e-07, 'epoch': 0.96}
3s/it] 96%|█████████▌| 3126/3250 [14:40:29<35:03, 16.96s/it]                                                       96%|█████████▌| 3126/3250 [14:40:29<35:03, 16.96s/it] 96%|█████████▌| 3127/3250 [14:40:45<34:05, 16.63s/it]                                                       96%|█████████▌| 3127/3250 [14:40:45<34:05, 16.63s/it] 96%|█████████▌| 3128/3250 [14:41:01<33:21, 16.40s/it]                                                       96%|█████████▌| 3128/3250 [14:41:01<33:21, 16.40s/it] 96%|█████████▋| 3129/3250 [14:41:17<32:44, 16.24s/it]                                                       96%|█████████▋| 3129/3250 [14:41:17<32:44, 16.24s/it] 96%|█████████▋| 3130/3250 [14:41:33<32:15, 16.13s/it]                                                       96%|█████████▋| 3130/3250 [14:41:33<32:15, 16.13s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7578109502792358, 'eval_runtime': 2.4533, 'eval_samples_per_second': 4.891, 'eval_steps_per_second': 1.223, 'epoch': 0.96}
                                                       96%|█████████▋| 3130/3250 [14:41:35<32:15, 16.13s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3130
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3130the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3130
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3130

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9139, 'learning_rate': 3.3084312106892446e-07, 'epoch': 0.96}
{'loss': 0.4025, 'learning_rate': 3.2531210755588624e-07, 'epoch': 0.96}
{'loss': 0.3903, 'learning_rate': 3.1982756726851584e-07, 'epoch': 0.96}
{'loss': 0.4126, 'learning_rate': 3.143895053378698e-07, 'epoch': 0.96}
{'loss': 0.4102, 'learning_rate': 3.0899792685155083e-07, 'epoch': 0.96}
 96%|█████████▋| 3131/3250 [14:41:52<33:47, 17.03s/it]                                                       96%|█████████▋| 3131/3250 [14:41:52<33:47, 17.03s/it] 96%|█████████▋| 3132/3250 [14:42:08<32:57, 16.76s/it]                                                       96%|█████████▋| 3132/3250 [14:42:08<32:57, 16.76s/it] 96%|█████████▋| 3133/3250 [14:42:24<32:09, 16.49s/it]                                                       96%|█████████▋| 3133/3250 [14:42:24<32:09, 16.49s/it] 96%|█████████▋| 3134/3250 [14:42:40<31:30, 16.30s/it]                                                       96%|█████████▋| 3134/3250 [14:42:40<31:30, 16.30s/it] 96%|█████████▋| 3135/3250 [14:42:55<30:59, 16.17s/it]                                                       96%|█████████▋| 3135/3250 [14:42:55<30:59, 16.1{'loss': 0.3839, 'learning_rate': 3.036528368536462e-07, 'epoch': 0.96}
{'loss': 0.3912, 'learning_rate': 2.9835424034476146e-07, 'epoch': 0.97}
{'loss': 0.4528, 'learning_rate': 2.9310214228202013e-07, 'epoch': 0.97}
{'loss': 0.3923, 'learning_rate': 2.8789654757901965e-07, 'epoch': 0.97}
{'loss': 0.401, 'learning_rate': 2.8273746110585863e-07, 'epoch': 0.97}
7s/it] 96%|█████████▋| 3136/3250 [14:43:11<30:32, 16.07s/it]                                                       96%|█████████▋| 3136/3250 [14:43:11<30:32, 16.07s/it] 97%|█████████▋| 3137/3250 [14:43:27<30:08, 16.01s/it]                                                       97%|█████████▋| 3137/3250 [14:43:27<30:08, 16.01s/it] 97%|█████████▋| 3138/3250 [14:43:43<29:47, 15.96s/it]                                                       97%|█████████▋| 3138/3250 [14:43:43<29:47, 15.96s/it] 97%|█████████▋| 3139/3250 [14:43:59<29:28, 15.93s/it]                                                       97%|█████████▋| 3139/3250 [14:43:59<29:28, 15.93s/it] 97%|█████████▋| 3140/3250 [14:44:15<29:09, 15.91s/it]                                                       97%|█████████▋| 3140/3250 [14:44:15<29:09, 15.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7589654326438904, 'eval_runtime': 2.4567, 'eval_samples_per_second': 4.885, 'eval_steps_per_second': 1.221, 'epoch': 0.97}
                                                       97%|█████████▋| 3140/3250 [14:44:17<29:09, 15.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3140
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3140
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3140

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3673, 'learning_rate': 2.776248876891319e-07, 'epoch': 0.97}
{'loss': 0.4071, 'learning_rate': 2.725588321119188e-07, 'epoch': 0.97}
{'loss': 0.4261, 'learning_rate': 2.675392991137671e-07, 'epoch': 0.97}
{'loss': 0.3917, 'learning_rate': 2.625662933907147e-07, 'epoch': 0.97}
{'loss': 0.3907, 'learning_rate': 2.5763981959526786e-07, 'epoch': 0.97}
 97%|█████████▋| 3141/3250 [14:44:47<37:49, 20.83s/it]                                                       97%|█████████▋| 3141/3250 [14:44:47<37:49, 20.83s/it] 97%|█████████▋| 3142/3250 [14:45:03<34:48, 19.34s/it]                                                       97%|█████████▋| 3142/3250 [14:45:03<34:48, 19.34s/it] 97%|█████████▋| 3143/3250 [14:45:19<32:38, 18.30s/it]                                                       97%|█████████▋| 3143/3250 [14:45:19<32:38, 18.30s/it] 97%|█████████▋| 3144/3250 [14:45:35<31:02, 17.57s/it]                                                       97%|█████████▋| 3144/3250 [14:45:35<31:02, 17.57s/it] 97%|█████████▋| 3145/3250 [14:45:51<29:51, 17.07s/it]                                                       97%|█████████▋| 3145/3250 [14:45:51<29:51, 17.0{'loss': 0.4106, 'learning_rate': 2.527598823363786e-07, 'epoch': 0.97}
{'loss': 0.4082, 'learning_rate': 2.4792648617950056e-07, 'epoch': 0.97}
{'loss': 0.4327, 'learning_rate': 2.4313963564650546e-07, 'epoch': 0.97}
{'loss': 0.4069, 'learning_rate': 2.3839933521575543e-07, 'epoch': 0.97}
{'loss': 0.3969, 'learning_rate': 2.3370558932203635e-07, 'epoch': 0.97}
7s/it] 97%|█████████▋| 3146/3250 [14:46:06<28:57, 16.71s/it]                                                       97%|█████████▋| 3146/3250 [14:46:06<28:57, 16.71s/it] 97%|█████████▋| 3147/3250 [14:46:22<28:15, 16.46s/it]                                                       97%|█████████▋| 3147/3250 [14:46:22<28:15, 16.46s/it] 97%|█████████▋| 3148/3250 [14:46:39<27:53, 16.41s/it]                                                       97%|█████████▋| 3148/3250 [14:46:39<27:53, 16.41s/it] 97%|█████████▋| 3149/3250 [14:46:54<27:21, 16.25s/it]                                                       97%|█████████▋| 3149/3250 [14:46:54<27:21, 16.25s/it] 97%|█████████▋| 3150/3250 [14:47:10<26:53, 16.14s/it]                                                       97%|█████████▋| 3150/3250 [14:47:10<26:53, 16.14s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7592198252677917, 'eval_runtime': 2.6926, 'eval_samples_per_second': 4.457, 'eval_steps_per_second': 1.114, 'epoch': 0.97}
                                                       97%|█████████▋| 3150/3250 [14:47:13<26:53, 16.14s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3150
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3150 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3150
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3150


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4228, 'learning_rate': 2.290584023565856e-07, 'epoch': 0.97}
{'loss': 0.3918, 'learning_rate': 2.2445777866709205e-07, 'epoch': 0.97}
{'loss': 0.413, 'learning_rate': 2.199037225576739e-07, 'epoch': 0.97}
{'loss': 0.399, 'learning_rate': 2.153962382888841e-07, 'epoch': 0.97}
{'loss': 0.3893, 'learning_rate': 2.1093533007770506e-07, 'epoch': 0.97}
 97%|█████████▋| 3151/3250 [14:47:43<34:56, 21.18s/it]                                                       97%|█████████▋| 3151/3250 [14:47:43<34:56, 21.18s/it] 97%|█████████▋| 3152/3250 [14:47:59<32:00, 19.59s/it]                                                       97%|█████████▋| 3152/3250 [14:47:59<32:00, 19.59s/it] 97%|█████████▋| 3153/3250 [14:48:15<29:52, 18.48s/it]                                                       97%|█████████▋| 3153/3250 [14:48:15<29:52, 18.48s/it] 97%|█████████▋| 3154/3250 [14:48:31<28:19, 17.70s/it]                                                       97%|█████████▋| 3154/3250 [14:48:31<28:19, 17.70s/it] 97%|█████████▋| 3155/3250 [14:48:47<27:10, 17.16s/it]                                                       97%|█████████▋| 3155/3250 [14:48:47<27:10, 17.1{'loss': 0.3825, 'learning_rate': 2.0652100209755388e-07, 'epoch': 0.97}
{'loss': 0.3829, 'learning_rate': 2.0215325847825485e-07, 'epoch': 0.97}
{'loss': 0.407, 'learning_rate': 1.978321033060504e-07, 'epoch': 0.97}
{'loss': 0.3975, 'learning_rate': 1.935575406236123e-07, 'epoch': 0.97}
{'loss': 0.4101, 'learning_rate': 1.8932957443000832e-07, 'epoch': 0.97}
6s/it] 97%|█████████▋| 3156/3250 [14:49:03<26:16, 16.77s/it]                                                       97%|█████████▋| 3156/3250 [14:49:03<26:16, 16.77s/it] 97%|█████████▋| 3157/3250 [14:49:19<25:34, 16.50s/it]                                                       97%|█████████▋| 3157/3250 [14:49:19<25:34, 16.50s/it] 97%|█████████▋| 3158/3250 [14:49:34<25:00, 16.31s/it]                                                       97%|█████████▋| 3158/3250 [14:49:34<25:00, 16.31s/it] 97%|█████████▋| 3159/3250 [14:49:50<24:32, 16.19s/it]                                                       97%|█████████▋| 3159/3250 [14:49:50<24:32, 16.19s/it] 97%|█████████▋| 3160/3250 [14:50:06<24:09, 16.10s/it]                                                       97%|█████████▋| 3160/3250 [14:50:06<24:09, 16.10s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7591269612312317, 'eval_runtime': 2.4633, 'eval_samples_per_second': 4.872, 'eval_steps_per_second': 1.218, 'epoch': 0.97}
                                                       97%|█████████▋| 3160/3250 [14:50:09<24:09, 16.10s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3160
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3160
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3160
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3160

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9279, 'learning_rate': 1.851482086807299e-07, 'epoch': 0.97}
{'loss': 0.3812, 'learning_rate': 1.8101344728764234e-07, 'epoch': 0.97}
{'loss': 0.3884, 'learning_rate': 1.7692529411904578e-07, 'epoch': 0.97}
{'loss': 0.413, 'learning_rate': 1.72883752999603e-07, 'epoch': 0.97}
{'loss': 0.4171, 'learning_rate': 1.688888277103895e-07, 'epoch': 0.97}
 97%|█████████▋| 3161/3250 [14:50:40<31:42, 21.38s/it]                                                       97%|█████████▋| 3161/3250 [14:50:40<31:42, 21.38s/it] 97%|█████████▋| 3162/3250 [14:50:56<28:55, 19.72s/it]                                                       97%|█████████▋| 3162/3250 [14:50:56<28:55, 19.72s/it] 97%|█████████▋| 3163/3250 [14:51:12<26:56, 18.58s/it]                                                       97%|█████████▋| 3163/3250 [14:51:12<26:56, 18.58s/it] 97%|█████████▋| 3164/3250 [14:51:28<25:34, 17.85s/it]                                                       97%|█████████▋| 3164/3250 [14:51:28<25:34, 17.85s/it] 97%|█████████▋| 3165/3250 [14:51:44<24:26, 17.26s/it]                                                       97%|█████████▋| 3165/3250 [14:51:44<24:26, 17.2{'loss': 0.3979, 'learning_rate': 1.6494052198886555e-07, 'epoch': 0.97}
{'loss': 0.3959, 'learning_rate': 1.6103883952886534e-07, 'epoch': 0.97}
{'loss': 0.4436, 'learning_rate': 1.5718378398063005e-07, 'epoch': 0.97}
{'loss': 0.4156, 'learning_rate': 1.5337535895074695e-07, 'epoch': 0.98}
{'loss': 0.3896, 'learning_rate': 1.4961356800219927e-07, 'epoch': 0.98}
6s/it] 97%|█████████▋| 3166/3250 [14:52:00<23:34, 16.84s/it]                                                       97%|█████████▋| 3166/3250 [14:52:00<23:34, 16.84s/it] 97%|█████████▋| 3167/3250 [14:52:15<22:53, 16.55s/it]                                                       97%|█████████▋| 3167/3250 [14:52:15<22:53, 16.55s/it] 97%|█████████▋| 3168/3250 [14:52:31<22:20, 16.35s/it]                                                       97%|█████████▋| 3168/3250 [14:52:31<22:20, 16.35s/it] 98%|█████████▊| 3169/3250 [14:52:47<21:53, 16.21s/it]                                                       98%|█████████▊| 3169/3250 [14:52:47<21:53, 16.21s/it] 98%|█████████▊| 3170/3250 [14:53:03<21:28, 16.11s/it]                                                       98%|█████████▊| 3170/3250 [14:53:03<21:28, 16.11s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7587403655052185, 'eval_runtime': 2.4622, 'eval_samples_per_second': 4.874, 'eval_steps_per_second': 1.218, 'epoch': 0.98}
                                                       98%|█████████▊| 3170/3250 [14:53:06<21:28, 16.11s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3170
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3170
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3170the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3170

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3170/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3794, 'learning_rate': 1.4589841465433297e-07, 'epoch': 0.98}
{'loss': 0.4184, 'learning_rate': 1.4222990238287325e-07, 'epoch': 0.98}
{'loss': 0.3986, 'learning_rate': 1.3860803461989146e-07, 'epoch': 0.98}
{'loss': 0.4044, 'learning_rate': 1.3503281475383823e-07, 'epoch': 0.98}
{'loss': 0.3978, 'learning_rate': 1.3150424612951573e-07, 'epoch': 0.98}
 98%|█████████▊| 3171/3250 [14:53:50<33:21, 25.34s/it]                                                       98%|█████████▊| 3171/3250 [14:53:50<33:21, 25.34s/it] 98%|█████████▊| 3172/3250 [14:54:06<29:15, 22.51s/it]                                                       98%|█████████▊| 3172/3250 [14:54:06<29:15, 22.51s/it] 98%|█████████▊| 3173/3250 [14:54:22<26:20, 20.52s/it]                                                       98%|█████████▊| 3173/3250 [14:54:22<26:20, 20.52s/it] 98%|█████████▊| 3174/3250 [14:54:38<24:14, 19.14s/it]                                                       98%|█████████▊| 3174/3250 [14:54:38<24:14, 19.14s/it] 98%|█████████▊| 3175/3250 [14:54:54<22:43, 18.18s/it]                                                       98%|█████████▊| 3175/3250 [14:54:54<22:43, 18.1{'loss': 0.4045, 'learning_rate': 1.280223320480778e-07, 'epoch': 0.98}
{'loss': 0.4031, 'learning_rate': 1.2458707576703533e-07, 'epoch': 0.98}
{'loss': 0.428, 'learning_rate': 1.2119848050025085e-07, 'epoch': 0.98}
{'loss': 0.4021, 'learning_rate': 1.178565494179218e-07, 'epoch': 0.98}
{'loss': 0.4081, 'learning_rate': 1.1456128564660273e-07, 'epoch': 0.98}
8s/it] 98%|█████████▊| 3176/3250 [14:55:10<21:34, 17.50s/it]                                                       98%|█████████▊| 3176/3250 [14:55:10<21:34, 17.50s/it] 98%|█████████▊| 3177/3250 [14:55:25<20:42, 17.02s/it]                                                       98%|█████████▊| 3177/3250 [14:55:25<20:42, 17.02s/it] 98%|█████████▊| 3178/3250 [14:55:41<20:01, 16.69s/it]                                                       98%|█████████▊| 3178/3250 [14:55:41<20:01, 16.69s/it] 98%|█████████▊| 3179/3250 [14:55:57<19:28, 16.45s/it]                                                       98%|█████████▊| 3179/3250 [14:55:57<19:28, 16.45s/it] 98%|█████████▊| 3180/3250 [14:56:13<19:00, 16.29s/it]                                                       98%|█████████▊| 3180/3250 [14:56:13<19:00, 16.29s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7588145732879639, 'eval_runtime': 2.4641, 'eval_samples_per_second': 4.87, 'eval_steps_per_second': 1.217, 'epoch': 0.98}
                                                       98%|█████████▊| 3180/3250 [14:56:16<19:00, 16.29s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3180
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3180
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3180/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3180


the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3995, 'learning_rate': 1.1131269226918317e-07, 'epoch': 0.98}
{'loss': 0.4033, 'learning_rate': 1.0811077232488753e-07, 'epoch': 0.98}
{'loss': 0.3682, 'learning_rate': 1.0495552880927517e-07, 'epoch': 0.98}
{'loss': 0.4248, 'learning_rate': 1.0184696467424037e-07, 'epoch': 0.98}
{'loss': 0.3774, 'learning_rate': 9.878508282800126e-08, 'epoch': 0.98}
 98%|█████████▊| 3181/3250 [14:56:33<19:55, 17.33s/it]                                                       98%|█████████▊| 3181/3250 [14:56:33<19:55, 17.33s/it] 98%|█████████▊| 3182/3250 [14:56:49<19:09, 16.90s/it]                                                       98%|█████████▊| 3182/3250 [14:56:49<19:09, 16.90s/it] 98%|█████████▊| 3183/3250 [14:57:05<18:32, 16.60s/it]                                                       98%|█████████▊| 3183/3250 [14:57:05<18:32, 16.60s/it] 98%|█████████▊| 3184/3250 [14:57:21<18:02, 16.40s/it]                                                       98%|█████████▊| 3184/3250 [14:57:21<18:02, 16.40s/it] 98%|█████████▊| 3185/3250 [14:57:37<17:36, 16.25s/it]                                                       98%|█████████▊| 3185/3250 [14:57:37<17:36, 16.2{'loss': 0.382, 'learning_rate': 9.576988613511085e-08, 'epoch': 0.98}
{'loss': 0.3807, 'learning_rate': 9.280137741643491e-08, 'epoch': 0.98}
{'loss': 0.3916, 'learning_rate': 8.987955944917414e-08, 'epoch': 0.98}
{'loss': 0.3928, 'learning_rate': 8.700443496683086e-08, 'epoch': 0.98}
{'loss': 0.4114, 'learning_rate': 8.417600665923675e-08, 'epoch': 0.98}
5s/it] 98%|█████████▊| 3186/3250 [14:57:52<17:13, 16.15s/it]                                                       98%|█████████▊| 3186/3250 [14:57:52<17:13, 16.15s/it] 98%|█████████▊| 3187/3250 [14:58:08<16:52, 16.07s/it]                                                       98%|█████████▊| 3187/3250 [14:58:08<16:52, 16.07s/it] 98%|█████████▊| 3188/3250 [14:58:24<16:32, 16.02s/it]                                                       98%|█████████▊| 3188/3250 [14:58:24<16:32, 16.02s/it] 98%|█████████▊| 3189/3250 [14:58:40<16:15, 15.99s/it]                                                       98%|█████████▊| 3189/3250 [14:58:40<16:15, 15.99s/it] 98%|█████████▊| 3190/3250 [14:58:56<15:57, 15.96s/it]                                                       98%|█████████▊| 3190/3250 [14:58:56<15:57, 15.96s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7579134702682495, 'eval_runtime': 2.4663, 'eval_samples_per_second': 4.866, 'eval_steps_per_second': 1.216, 'epoch': 0.98}
                                                       98%|█████████▊| 3190/3250 [14:58:59<15:57, 15.96s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3190
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3190 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3190
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3190

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3944, 'learning_rate': 8.139427717253622e-08, 'epoch': 0.98}
{'loss': 0.9041, 'learning_rate': 7.865924910916977e-08, 'epoch': 0.98}
{'loss': 0.3872, 'learning_rate': 7.597092502790725e-08, 'epoch': 0.98}
{'loss': 0.4149, 'learning_rate': 7.332930744380906e-08, 'epoch': 0.98}
{'loss': 0.3977, 'learning_rate': 7.073439882824273e-08, 'epoch': 0.98}
 98%|█████████▊| 3191/3250 [14:59:29<20:50, 21.19s/it]                                                       98%|█████████▊| 3191/3250 [14:59:29<20:50, 21.19s/it] 98%|█████████▊| 3192/3250 [14:59:45<18:56, 19.60s/it]                                                       98%|█████████▊| 3192/3250 [14:59:45<18:56, 19.60s/it] 98%|█████████▊| 3193/3250 [15:00:01<17:34, 18.49s/it]                                                       98%|█████████▊| 3193/3250 [15:00:01<17:34, 18.49s/it] 98%|█████████▊| 3194/3250 [15:00:17<16:32, 17.72s/it]                                                       98%|█████████▊| 3194/3250 [15:00:17<16:32, 17.72s/it] 98%|█████████▊| 3195/3250 [15:00:33<15:44, 17.18s/it]                                                       98%|█████████▊| 3195/3250 [15:00:33<15:44, 17.1{'loss': 0.4087, 'learning_rate': 6.818620160887746e-08, 'epoch': 0.98}
{'loss': 0.388, 'learning_rate': 6.568471816968957e-08, 'epoch': 0.98}
{'loss': 0.4448, 'learning_rate': 6.322995085094041e-08, 'epoch': 0.98}
{'loss': 0.4239, 'learning_rate': 6.08219019491929e-08, 'epoch': 0.98}
{'loss': 0.3884, 'learning_rate': 5.846057371730052e-08, 'epoch': 0.98}
8s/it] 98%|█████████▊| 3196/3250 [15:00:49<15:07, 16.80s/it]                                                       98%|█████████▊| 3196/3250 [15:00:49<15:07, 16.80s/it] 98%|█████████▊| 3197/3250 [15:01:05<14:39, 16.60s/it]                                                       98%|█████████▊| 3197/3250 [15:01:05<14:39, 16.60s/it] 98%|█████████▊| 3198/3250 [15:01:21<14:12, 16.39s/it]                                                       98%|█████████▊| 3198/3250 [15:01:21<14:12, 16.39s/it] 98%|█████████▊| 3199/3250 [15:01:37<13:48, 16.24s/it]                                                       98%|█████████▊| 3199/3250 [15:01:37<13:48, 16.24s/it] 98%|█████████▊| 3200/3250 [15:01:53<13:27, 16.14s/it]                                                       98%|█████████▊| 3200/3250 [15:01:53<13:27, 16.14s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.75847989320755, 'eval_runtime': 2.4793, 'eval_samples_per_second': 4.84, 'eval_steps_per_second': 1.21, 'epoch': 0.98}
                                                       98%|█████████▊| 3200/3250 [15:01:55<13:27, 16.14s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3200
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3200
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3200

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3200
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3200/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3882, 'learning_rate': 5.614596836440722e-08, 'epoch': 0.98}
{'loss': 0.4076, 'learning_rate': 5.3878088055947515e-08, 'epoch': 0.99}
{'loss': 0.4022, 'learning_rate': 5.165693491363532e-08, 'epoch': 0.99}
{'loss': 0.4037, 'learning_rate': 4.9482511015475074e-08, 'epoch': 0.99}
{'loss': 0.3933, 'learning_rate': 4.735481839575617e-08, 'epoch': 0.99}
 98%|█████████▊| 3201/3250 [15:02:27<17:29, 21.42s/it]                                                       98%|█████████▊| 3201/3250 [15:02:27<17:29, 21.42s/it] 99%|█████████▊| 3202/3250 [15:02:43<15:48, 19.76s/it]                                                       99%|█████████▊| 3202/3250 [15:02:43<15:48, 19.76s/it] 99%|█████████▊| 3203/3250 [15:02:58<14:34, 18.60s/it]                                                       99%|█████████▊| 3203/3250 [15:02:58<14:34, 18.60s/it] 99%|█████████▊| 3204/3250 [15:03:14<13:38, 17.79s/it]                                                       99%|█████████▊| 3204/3250 [15:03:14<13:38, 17.79s/it] 99%|█████████▊| 3205/3250 [15:03:30<12:54, 17.22s/it]                                                       99%|█████████▊| 3205/3250 [15:03:30<12:54, 17.2{'loss': 0.3983, 'learning_rate': 4.527385904504189e-08, 'epoch': 0.99}
{'loss': 0.4086, 'learning_rate': 4.323963491016936e-08, 'epoch': 0.99}
{'loss': 0.4219, 'learning_rate': 4.1252147894277336e-08, 'epoch': 0.99}
{'loss': 0.3961, 'learning_rate': 3.9311399856745145e-08, 'epoch': 0.99}
{'loss': 0.4177, 'learning_rate': 3.741739261324817e-08, 'epoch': 0.99}
2s/it] 99%|█████████▊| 3206/3250 [15:03:46<12:22, 16.87s/it]                                                       99%|█████████▊| 3206/3250 [15:03:46<12:22, 16.87s/it] 99%|█████████▊| 3207/3250 [15:04:02<11:54, 16.62s/it]                                                       99%|█████████▊| 3207/3250 [15:04:02<11:54, 16.62s/it] 99%|█████████▊| 3208/3250 [15:04:18<11:29, 16.41s/it]                                                       99%|█████████▊| 3208/3250 [15:04:18<11:29, 16.41s/it] 99%|█████████▊| 3209/3250 [15:04:34<11:06, 16.26s/it]                                                       99%|█████████▊| 3209/3250 [15:04:34<11:06, 16.26s/it] 99%|█████████▉| 3210/3250 [15:04:50<10:45, 16.14s/it]                                                       99%|█████████▉| 3210/3250 [15:04:50<10:45, 16.14s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7585523724555969, 'eval_runtime': 2.4662, 'eval_samples_per_second': 4.866, 'eval_steps_per_second': 1.216, 'epoch': 0.99}
                                                       99%|█████████▉| 3210/3250 [15:04:52<10:45, 16.14s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3210
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3210 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3210

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4101, 'learning_rate': 3.557012793573011e-08, 'epoch': 0.99}
{'loss': 0.4172, 'learning_rate': 3.376960755240299e-08, 'epoch': 0.99}
{'loss': 0.3662, 'learning_rate': 3.2015833147741594e-08, 'epoch': 0.99}
{'loss': 0.4207, 'learning_rate': 3.030880636249456e-08, 'epoch': 0.99}
{'loss': 0.3908, 'learning_rate': 2.8648528793673302e-08, 'epoch': 0.99}
 99%|█████████▉| 3211/3250 [15:05:23<13:45, 21.16s/it]                                                       99%|█████████▉| 3211/3250 [15:05:23<13:45, 21.16s/it] 99%|█████████▉| 3212/3250 [15:05:39<12:23, 19.56s/it]                                                       99%|█████████▉| 3212/3250 [15:05:39<12:23, 19.56s/it] 99%|█████████▉| 3213/3250 [15:05:55<11:24, 18.49s/it]                                                       99%|█████████▉| 3213/3250 [15:05:55<11:24, 18.49s/it] 99%|█████████▉| 3214/3250 [15:06:11<10:41, 17.81s/it]                                                       99%|█████████▉| 3214/3250 [15:06:11<10:41, 17.81s/it] 99%|█████████▉| 3215/3250 [15:06:27<10:02, 17.21s/it]                                                       99%|█████████▉| 3215/3250 [15:06:27<10:02, 17.2{'loss': 0.3815, 'learning_rate': 2.7035001994552e-08, 'epoch': 0.99}
{'loss': 0.3826, 'learning_rate': 2.5468227474667595e-08, 'epoch': 0.99}
{'loss': 0.3902, 'learning_rate': 2.3948206699819786e-08, 'epoch': 0.99}
{'loss': 0.4105, 'learning_rate': 2.2474941092065492e-08, 'epoch': 0.99}
{'loss': 0.3874, 'learning_rate': 2.1048432029718845e-08, 'epoch': 0.99}
1s/it] 99%|█████████▉| 3216/3250 [15:06:42<09:30, 16.78s/it]                                                       99%|█████████▉| 3216/3250 [15:06:42<09:30, 16.78s/it] 99%|█████████▉| 3217/3250 [15:06:58<09:04, 16.49s/it]                                                       99%|█████████▉| 3217/3250 [15:06:58<09:04, 16.49s/it] 99%|█████████▉| 3218/3250 [15:07:14<08:41, 16.29s/it]                                                       99%|█████████▉| 3218/3250 [15:07:14<08:41, 16.29s/it] 99%|█████████▉| 3219/3250 [15:07:30<08:20, 16.14s/it]                                                       99%|█████████▉| 3219/3250 [15:07:30<08:20, 16.14s/it] 99%|█████████▉| 3220/3250 [15:07:46<08:01, 16.04s/it]                                                       99%|█████████▉| 3220/3250 [15:07:46<08:01, 16.04s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7579262256622314, 'eval_runtime': 2.4675, 'eval_samples_per_second': 4.863, 'eval_steps_per_second': 1.216, 'epoch': 0.99}
                                                       99%|█████████▉| 3220/3250 [15:07:48<08:01, 16.04s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3220
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in the checkpoint model will be saved in 
  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3220/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3220

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3220/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4096, 'learning_rate': 1.9668680847356735e-08, 'epoch': 0.99}
{'loss': 0.91, 'learning_rate': 1.8335688835802167e-08, 'epoch': 0.99}
{'loss': 0.3973, 'learning_rate': 1.7049457242140908e-08, 'epoch': 0.99}
{'loss': 0.3914, 'learning_rate': 1.580998726971039e-08, 'epoch': 0.99}
{'loss': 0.4159, 'learning_rate': 1.4617280078094152e-08, 'epoch': 0.99}
 99%|█████████▉| 3221/3250 [15:08:05<08:11, 16.93s/it]                                                       99%|█████████▉| 3221/3250 [15:08:05<08:11, 16.93s/it] 99%|█████████▉| 3222/3250 [15:08:20<07:44, 16.59s/it]                                                       99%|█████████▉| 3222/3250 [15:08:20<07:44, 16.59s/it] 99%|█████████▉| 3223/3250 [15:08:36<07:21, 16.35s/it]                                                       99%|█████████▉| 3223/3250 [15:08:36<07:21, 16.35s/it] 99%|█████████▉| 3224/3250 [15:08:52<07:00, 16.19s/it]                                                       99%|█████████▉| 3224/3250 [15:08:52<07:00, 16.19s/it] 99%|█████████▉| 3225/3250 [15:09:08<06:41, 16.08s/it]                                                       99%|█████████▉| 3225/3250 [15:09:08<06:41, 16.0{'loss': 0.4034, 'learning_rate': 1.3471336783132949e-08, 'epoch': 0.99}
{'loss': 0.3877, 'learning_rate': 1.2372158456919192e-08, 'epoch': 0.99}
{'loss': 0.388, 'learning_rate': 1.1319746127785857e-08, 'epoch': 0.99}
{'loss': 0.4549, 'learning_rate': 1.0314100780317581e-08, 'epoch': 0.99}
{'loss': 0.3985, 'learning_rate': 9.355223355350662e-09, 'epoch': 0.99}
8s/it] 99%|█████████▉| 3226/3250 [15:09:24<06:25, 16.04s/it]                                                       99%|█████████▉| 3226/3250 [15:09:24<06:25, 16.04s/it] 99%|█████████▉| 3227/3250 [15:09:40<06:07, 15.98s/it]                                                       99%|█████████▉| 3227/3250 [15:09:40<06:07, 15.98s/it] 99%|█████████▉| 3228/3250 [15:09:55<05:50, 15.92s/it]                                                       99%|█████████▉| 3228/3250 [15:09:55<05:50, 15.92s/it] 99%|█████████▉| 3229/3250 [15:10:11<05:33, 15.88s/it]                                                       99%|█████████▉| 3229/3250 [15:10:11<05:33, 15.88s/it] 99%|█████████▉| 3230/3250 [15:10:28<05:21, 16.09s/it]                                                       99%|█████████▉| 3230/3250 [15:10:28<05:21, 16.09s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7577990889549255, 'eval_runtime': 2.4511, 'eval_samples_per_second': 4.896, 'eval_steps_per_second': 1.224, 'epoch': 0.99}
                                                       99%|█████████▉| 3230/3250 [15:10:30<05:21, 16.09s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3230
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3230
I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3230
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3230

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4057, 'learning_rate': 8.443114749967506e-09, 'epoch': 0.99}
{'loss': 0.3759, 'learning_rate': 7.577775817485533e-09, 'epoch': 0.99}
{'loss': 0.4186, 'learning_rate': 6.75920736747937e-09, 'epoch': 0.99}
{'loss': 0.4035, 'learning_rate': 5.987410165758656e-09, 'epoch': 1.0}
{'loss': 0.3884, 'learning_rate': 5.26238493438469e-09, 'epoch': 1.0}
 99%|█████████▉| 3231/3250 [15:11:01<06:45, 21.32s/it]                                                       99%|█████████▉| 3231/3250 [15:11:01<06:45, 21.32s/it] 99%|█████████▉| 3232/3250 [15:11:17<05:53, 19.66s/it]                                                       99%|█████████▉| 3232/3250 [15:11:17<05:53, 19.66s/it] 99%|█████████▉| 3233/3250 [15:11:33<05:14, 18.51s/it]                                                       99%|█████████▉| 3233/3250 [15:11:33<05:14, 18.51s/it]100%|█████████▉| 3234/3250 [15:11:49<04:43, 17.70s/it]                                                      100%|█████████▉| 3234/3250 [15:11:49<04:43, 17.70s/it]100%|█████████▉| 3235/3250 [15:12:05<04:17, 17.14s/it]                                                      100%|█████████▉| 3235/3250 [15:12:05<04:17, 17.1{'loss': 0.3805, 'learning_rate': 4.584132351642678e-09, 'epoch': 1.0}
{'loss': 0.4087, 'learning_rate': 3.9526530520916926e-09, 'epoch': 1.0}
{'loss': 0.412, 'learning_rate': 3.3679476264980582e-09, 'epoch': 1.0}
{'loss': 0.4212, 'learning_rate': 2.830016621885312e-09, 'epoch': 1.0}
{'loss': 0.414, 'learning_rate': 2.3388605415231024e-09, 'epoch': 1.0}
4s/it]100%|█████████▉| 3236/3250 [15:12:21<03:54, 16.76s/it]                                                      100%|█████████▉| 3236/3250 [15:12:21<03:54, 16.76s/it]100%|█████████▉| 3237/3250 [15:12:36<03:34, 16.50s/it]                                                      100%|█████████▉| 3237/3250 [15:12:36<03:34, 16.50s/it]100%|█████████▉| 3238/3250 [15:12:52<03:15, 16.30s/it]                                                      100%|█████████▉| 3238/3250 [15:12:52<03:15, 16.30s/it]100%|█████████▉| 3239/3250 [15:13:08<02:57, 16.16s/it]                                                      100%|█████████▉| 3239/3250 [15:13:08<02:57, 16.16s/it]100%|█████████▉| 3240/3250 [15:13:24<02:40, 16.07s/it]                                                      100%|█████████▉| 3240/3250 [15:13:24<02:40, 16.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7583887577056885, 'eval_runtime': 2.4546, 'eval_samples_per_second': 4.889, 'eval_steps_per_second': 1.222, 'epoch': 1.0}
                                                      100%|█████████▉| 3240/3250 [15:13:26<02:40, 16.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3240
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3240the checkpoint model will be saved in 
 the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3240
 /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3240

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3240/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4007, 'learning_rate': 1.894479844910535e-09, 'epoch': 1.0}
{'loss': 0.4135, 'learning_rate': 1.4968749477872746e-09, 'epoch': 1.0}
{'loss': 0.386, 'learning_rate': 1.1460462221279944e-09, 'epoch': 1.0}
{'loss': 0.4217, 'learning_rate': 8.419939961645807e-10, 'epoch': 1.0}
{'loss': 0.3939, 'learning_rate': 5.847185543417233e-10, 'epoch': 1.0}
100%|█████████▉| 3241/3250 [15:15:40<07:48, 52.04s/it]                                                      100%|█████████▉| 3241/3250 [15:15:40<07:48, 52.04s/it]100%|█████████▉| 3242/3250 [15:15:56<05:29, 41.18s/it]                                                      100%|█████████▉| 3242/3250 [15:15:56<05:29, 41.18s/it]100%|█████████▉| 3243/3250 [15:16:12<03:55, 33.57s/it]                                                      100%|█████████▉| 3243/3250 [15:16:12<03:55, 33.57s/it]100%|█████████▉| 3244/3250 [15:16:27<02:49, 28.24s/it]                                                      100%|█████████▉| 3244/3250 [15:16:27<02:49, 28.24s/it]100%|█████████▉| 3245/3250 [15:16:43<02:02, 24.51s/it]                                                      100%|█████████▉| 3245/3250 [15:16:43<02:02, 24.5{'loss': 0.3925, 'learning_rate': 3.742201373557741e-10, 'epoch': 1.0}
{'loss': 0.3858, 'learning_rate': 2.1049894213809317e-10, 'epoch': 1.0}
{'loss': 0.3806, 'learning_rate': 9.355512186615123e-11, 'epoch': 1.0}
{'loss': 0.41, 'learning_rate': 2.338878593577398e-11, 'epoch': 1.0}
{'loss': 0.391, 'learning_rate': 0.0, 'epoch': 1.0}
1s/it]100%|█████████▉| 3246/3250 [15:16:59<01:27, 21.99s/it]                                                      100%|█████████▉| 3246/3250 [15:16:59<01:27, 21.99s/it]100%|█████████▉| 3247/3250 [15:17:15<01:00, 20.14s/it]                                                      100%|█████████▉| 3247/3250 [15:17:15<01:00, 20.14s/it]100%|█████████▉| 3248/3250 [15:17:31<00:37, 18.84s/it]                                                      100%|█████████▉| 3248/3250 [15:17:31<00:37, 18.84s/it]100%|█████████▉| 3249/3250 [15:17:47<00:17, 17.93s/it]                                                      100%|█████████▉| 3249/3250 [15:17:47<00:17, 17.93s/it]100%|██████████| 3250/3250 [15:18:03<00:00, 17.30s/it]                                                      100%|██████████| 3250/3250 [15:18:03<00:00, 17.30s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7587396502494812, 'eval_runtime': 2.492, 'eval_samples_per_second': 4.815, 'eval_steps_per_second': 1.204, 'epoch': 1.0}
                                                      100%|██████████| 3250/3250 [15:18:05<00:00, 17.30s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/tmp-checkpoint-3250
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL


the checkpoint model will be saved in the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3250/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3250

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent/checkpoint-3250/pytorch_model.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 55128.8223, 'train_samples_per_second': 3.773, 'train_steps_per_second': 0.059, 'train_loss': 0.570346280015432, 'epoch': 1.0}
Saving last checkpoint of the modelSaving last checkpoint of the model

Saving last checkpoint of the model
                                                      100%|██████████| 3250/3250 [15:18:06<00:00, 17.30s/it]100%|██████████| 3250/3250 [15:18:06<00:00, 16.95s/it]
Saving last checkpoint of the model
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: | 0.008 MB of 0.008 MB uploadedwandb: / 0.008 MB of 0.008 MB uploadedwandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: 
wandb: Run history:
wandb:                      eval/loss █▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                   eval/runtime ▁▁▁▁▁▁▁▁▁▁▁▃▁▁▂▁▁▅▁▁▂▁█▁▁▁▁▁▂▃▁▁▃▁▁▁▁▁▁▁
wandb:        eval/samples_per_second ███████████▆██▇██▃██▆█▁▇████▇▅██▅████▇██
wandb:          eval/steps_per_second ███████████▆██▇██▃██▆█▁▇████▇▅██▅████▇██
wandb:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/learning_rate ███████▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:                     train/loss █▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▃▁▂▁▁▁▁▆▁▁▁▁▁▁▆▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.75874
wandb:                   eval/runtime 2.492
wandb:        eval/samples_per_second 4.815
wandb:          eval/steps_per_second 1.204
wandb:                    train/epoch 1.0
wandb:              train/global_step 3250
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.391
wandb:               train/total_flos 4.547801149079552e+18
wandb:               train/train_loss 0.57035
wandb:            train/train_runtime 55128.8223
wandb: train/train_samples_per_second 3.773
wandb:   train/train_steps_per_second 0.059
wandb: 
wandb: 🚀 View run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_7b_2_5_percent at: https://wandb.ai/complex_dnn/huggingface/runs/6cclgrt4
wandb: ️⚡ View job at https://wandb.ai/complex_dnn/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTYzMDY1Mg==/version_details/v9
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /projects/bbvz/bzd2/wandb/run-20240124_175858-6cclgrt4/logs
/var/spool/slurmd/job2891635/slurm_script: line 347: --save_freq=50: command not found
