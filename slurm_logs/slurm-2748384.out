Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Currently Loaded Modules:
  1) cue-login-env/1.0   8) libmd/1.0.4       15) openssl/3.1.3
  2) gcc/11.2.0          9) libbsd/0.11.7     16) util-linux-uuid/2.38.1
  3) ucx/1.11.2         10) expat/2.5.0       17) xz/5.2.4
  4) openmpi/4.1.2      11) gettext/0.19.8.1  18) python/3.11.6
  5) cuda/11.6.1        12) libffi/3.4.4      19) cudnn/8.9.0.131
  6) modtree/gpu        13) libxcrypt/4.4.35  20) anaconda3_gpu/23.9.0
  7) default            14) zlib-ng/2.1.3

 

job is starting on gpub024.delta.ncsa.illinois.edu
WARNING: A conda environment already exists at '/u/bzd2/.conda/envs/hetarth_py10'
Remove existing environment (y/[n])? 

CondaSystemExit: Exiting.

usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...
conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'build', 'content-trust', 'convert', 'debug', 'develop', 'doctor', 'index', 'inspect', 'metapackage', 'render', 'repoquery', 'skeleton', 'verify', 'server', 'repo', 'env', 'token')
Starting script...
Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: torch in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231003+cu121)
Requirement already satisfied: torchvision in /sw/external/python/anaconda3/lib/python3.9/site-packages (0.17.0.dev20231003+cu121)
Requirement already satisfied: torchaudio in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231002)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (2.1.0+6e4932cda8)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (1.24.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (2.31.0)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (9.4.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (4.37.0.dev0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.2)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/peft.git
  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-2c9n_621
  Resolved https://github.com/huggingface/peft.git to commit 0f1e9091cc975eb5458cc163bf1843a34fb42b76
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.4.1)
Requirement already satisfied: torch>=1.13.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (2.2.0.dev20231003+cu121)
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.37.0.dev0)
Requirement already satisfied: tqdm in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.65.0)
Requirement already satisfied: accelerate>=0.21.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.25.0)
Requirement already satisfied: safetensors in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.3.2)
Requirement already satisfied: huggingface-hub>=0.17.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.2)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0+6e4932cda8)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (0.15.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-c6kgiq3b
  Resolved https://github.com/huggingface/transformers to commit 17506d1256c1780efc9e2a5898a828c10ad4ea69
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.3.2)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: datasets in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.14.5)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (1.24.3)
Requirement already satisfied: pyarrow>=8.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.3.7)
Requirement already satisfied: pandas in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.3)
Requirement already satisfied: requests>=2.19.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (4.65.0)
Requirement already satisfied: xxhash in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.2)
Requirement already satisfied: multiprocess in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (2023.6.0)
Requirement already satisfied: aiohttp in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.5)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.19.4)
Requirement already satisfied: packaging in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (5.4.1)
Requirement already satisfied: attrs>=17.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)
Requirement already satisfied: multidict<7.0,>=4.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)
Requirement already satisfied: frozenlist>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: accelerate in /u/bzd2/.local/lib/python3.9/site-packages (0.25.0)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.4.1)
Requirement already satisfied: torch>=1.10.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (2.2.0.dev20231003+cu121)
Requirement already satisfied: huggingface-hub in /u/bzd2/.local/lib/python3.9/site-packages (from accelerate) (0.19.4)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (0.3.2)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0+6e4932cda8)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.65.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: huggingface_hub in /u/bzd2/.local/lib/python3.9/site-packages (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface_hub) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.65.0)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (5.4.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.7.1)
Requirement already satisfied: packaging>=20.9 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (23.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: bitsandbytes in /u/bzd2/.local/lib/python3.9/site-packages (0.41.3.post2)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: wandb in /u/bzd2/.local/lib/python3.9/site-packages (0.16.1)
Requirement already satisfied: Click!=8.0.0,>=7.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (8.0.4)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (3.1.40)
Requirement already satisfied: requests<3,>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (2.31.0)
Requirement already satisfied: psutil>=5.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.9.0)
Requirement already satisfied: sentry-sdk>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (1.38.0)
Requirement already satisfied: docker-pycreds>=0.4.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: PyYAML in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.4.1)
Requirement already satisfied: setproctitle in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.3.2)
Requirement already satisfied: setuptools in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (68.0.0)
Requirement already satisfied: appdirs>=1.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.4.4)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (4.7.1)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (3.20.3)
Requirement already satisfied: six>=1.4.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)
Requirement already satisfied: smmap<6,>=3.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /sw/external/python/anaconda3/lib/python3.9/site-packages (1.3.0)
Requirement already satisfied: numpy>=1.17.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.3)
Requirement already satisfied: scipy>=1.5.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.8.1)
Requirement already satisfied: joblib>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: code_bert_score in /u/bzd2/.local/lib/python3.9/site-packages (0.4.1)
Requirement already satisfied: torch>=1.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.2.0.dev20231003+cu121)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (1.24.3)
Requirement already satisfied: pandas>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.0.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.31.0)
Requirement already satisfied: tqdm>=4.31.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (4.65.0)
Requirement already satisfied: matplotlib in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (3.7.2)
Requirement already satisfied: transformers>=3.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from code_bert_score) (4.37.0.dev0)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2.1.0+6e4932cda8)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.19.4)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.3.2)
Requirement already satisfied: contourpy>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.0.5)
Requirement already satisfied: cycler>=0.10 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (4.25.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.4.4)
Requirement already satisfied: pillow>=6.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (9.4.0)
Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (3.0.9)
Requirement already satisfied: importlib-resources>=3.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (5.2.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2023.7.22)
Requirement already satisfied: zipp>=3.1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->code_bert_score) (3.11.0)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->code_bert_score) (1.16.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->code_bert_score) (2.1.1)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.0.0->code_bert_score) (1.3.0)

WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-12-13 17:47:54.644802: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:47:54.644811: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:47:54.644807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:47:54.644815: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:47:54.644864: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:47:54.644873: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:47:54.644875: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:47:54.644874: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:47:54.648327: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:47:54.648326: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:47:54.648335: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:47:54.648339: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:47:55.808396: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 17:47:55.808403: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 17:47:55.808410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 17:47:55.808481: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/vocab.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/special_tokens_map.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/merges.txt
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer_config.json
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/vocab.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/special_tokens_map.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/merges.txt
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer_config.json
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer_config.json
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'repo_name': 'splunk_forwarder', 'input': 'name: Set logfile permissions', 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'download_count': '8962', 'org_name': 'austincloudguru', 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n"}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'keepalived', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'download_count': '1012', 'org_name': 'buluma', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'zookeeper', 'input': 'name: Pin Cloudera APT repositories', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'download_count': '3388', 'org_name': 'azavea', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'repo_name': 'percona_xtradb_cluster_role', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'download_count': '647', 'org_name': 'panchal_yash', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'logwatch', 'input': 'name: assert | Test logwatch_service', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'download_count': '664', 'org_name': 'robertdebock', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'repo_name': 'network', 'input': 'name: Test BGP - graceful-restart-helper', 'download_link': 'https://old-galaxy.ansible.com/community/network', 'download_count': '723055', 'org_name': 'community', 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n"}
{'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml', 'org_name': 'austincloudguru', 'input': 'name: Set logfile permissions', 'repo_name': 'splunk_forwarder', 'download_count': '8962', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'license': '', 'repo_name': 'tftp', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'download_count': '9882', 'org_name': 'bertvv', 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n"}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/buluma/keepalived/tasks/assert.yml', 'org_name': 'buluma', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'repo_name': 'keepalived', 'download_count': '1012', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'ec2_monitor', 'input': 'name: Install apt packages', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'download_count': '8094', 'org_name': 'amritsingh', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/azavea/zookeeper/tasks/main.yml', 'org_name': 'azavea', 'input': 'name: Pin Cloudera APT repositories', 'repo_name': 'zookeeper', 'download_count': '3388', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper'}
{'license': 'MIT', 'repo_name': 'ansible_role_efs_utils', 'input': 'name: run Debian build script', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'download_count': '1799', 'org_name': 'rhythmictech', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml', 'org_name': 'panchal_yash', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'repo_name': 'percona_xtradb_cluster_role', 'download_count': '647', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role'}{'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'centos_base', 'input': 'name: Vim alias in .bashrc', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'download_count': '12437', 'org_name': 'buluma', 'path': 'data/repos/buluma/centos_base/tasks/main.yml', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml', 'org_name': 'robertdebock', 'input': 'name: assert | Test logwatch_service', 'repo_name': 'logwatch', 'download_count': '664', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml', 'org_name': 'community', 'input': 'name: Test BGP - graceful-restart-helper', 'repo_name': 'network', 'download_count': '723055', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'license': '', 'download_link': 'https://old-galaxy.ansible.com/community/network'}
{'repo_name': 'splunk_forwarder', 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'input': 'name: Set logfile permissions', 'org_name': 'austincloudguru', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'license': 'MIT', 'download_count': '8962', 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'download_count': '8962', 'repo_name': 'splunk_forwarder', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml', 'license': 'MIT', 'input': 'name: Set logfile permissions', 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'org_name': 'austincloudguru'}


{'path': 'data/repos/bertvv/tftp/tasks/selinux.yml', 'org_name': 'bertvv', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'repo_name': 'tftp', 'download_count': '9882', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'license': '', 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'repo_name': 'keepalived', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'org_name': 'buluma', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '1012', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml'}{'download_count': '1012', 'repo_name': 'keepalived', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'org_name': 'buluma'}

{'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml', 'org_name': 'amritsingh', 'input': 'name: Install apt packages', 'repo_name': 'ec2_monitor', 'download_count': '8094', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'repo_name': 'zookeeper', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'input': 'name: Pin Cloudera APT repositories', 'org_name': 'azavea', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '3388', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml'}{'download_count': '3388', 'repo_name': 'zookeeper', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: Pin Cloudera APT repositories', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'org_name': 'azavea'}
{'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml', 'org_name': 'rhythmictech', 'input': 'name: run Debian build script', 'repo_name': 'ansible_role_efs_utils', 'download_count': '1799', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'repo_name': 'percona_xtradb_cluster_role', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'org_name': 'panchal_yash', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'license': '', 'download_count': '647', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml'}{'download_count': '647', 'repo_name': 'percona_xtradb_cluster_role', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml', 'license': '', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'org_name': 'panchal_yash'}{'path': 'data/repos/buluma/centos_base/tasks/main.yml', 'org_name': 'buluma', 'input': 'name: Vim alias in .bashrc', 'repo_name': 'centos_base', 'download_count': '12437', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'download_count': '664', 'repo_name': 'logwatch', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: assert | Test logwatch_service', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'org_name': 'robertdebock'}{'repo_name': 'logwatch', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'input': 'name: assert | Test logwatch_service', 'org_name': 'robertdebock', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '664', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'download_count': '723055', 'repo_name': 'network', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml', 'license': '', 'input': 'name: Test BGP - graceful-restart-helper', 'download_link': 'https://old-galaxy.ansible.com/community/network', 'org_name': 'community'}{'repo_name': 'network', 'download_link': 'https://old-galaxy.ansible.com/community/network', 'input': 'name: Test BGP - graceful-restart-helper', 'org_name': 'community', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'license': '', 'download_count': '723055', 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'download_count': '9882', 'repo_name': 'tftp', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml', 'license': '', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'org_name': 'bertvv'}{'repo_name': 'tftp', 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'org_name': 'bertvv', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'license': '', 'download_count': '9882', 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': 'MIT', 'repo_name': 'etcd', 'input': 'name: Create etcd systemd service', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'download_count': '1748631', 'org_name': 'igor_nikiforov', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n'}


{'repo_name': 'ec2_monitor', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'input': 'name: Install apt packages', 'org_name': 'amritsingh', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '8094', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml'}{'download_count': '8094', 'repo_name': 'ec2_monitor', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: Install apt packages', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'org_name': 'amritsingh'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'repo_name': 'ansible_role_efs_utils', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'input': 'name: run Debian build script', 'org_name': 'rhythmictech', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'license': 'MIT', 'download_count': '1799', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml'}{'download_count': '1799', 'repo_name': 'ansible_role_efs_utils', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml', 'license': 'MIT', 'input': 'name: run Debian build script', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'org_name': 'rhythmictech'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'repo_name': 'centos_base', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'input': 'name: Vim alias in .bashrc', 'org_name': 'buluma', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '12437', 'path': 'data/repos/buluma/centos_base/tasks/main.yml'}{'download_count': '12437', 'repo_name': 'centos_base', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'path': 'data/repos/buluma/centos_base/tasks/main.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'input': 'name: Vim alias in .bashrc', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'org_name': 'buluma'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml', 'org_name': 'igor_nikiforov', 'input': 'name: Create etcd systemd service', 'repo_name': 'etcd', 'download_count': '1748631', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '1748631', 'repo_name': 'etcd', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml', 'license': 'MIT', 'input': 'name: Create etcd systemd service', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'org_name': 'igor_nikiforov'}
{'repo_name': 'etcd', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'input': 'name: Create etcd systemd service', 'org_name': 'igor_nikiforov', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'license': 'MIT', 'download_count': '1748631', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml'}
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<02:40,  2.49it/s]  0%|          | 1/400 [00:00<02:40,  2.49it/s]  0%|          | 1/400 [00:00<02:40,  2.49it/s]  0%|          | 1/400 [00:00<02:40,  2.49it/s]100%|| 400/400 [00:00<00:00, 854.66it/s]100%|| 400/400 [00:00<00:00, 854.38it/s]

100%|| 400/400 [00:00<00:00, 853.30it/s]
100%|| 400/400 [00:00<00:00, 851.46it/s]
The character to token ratio of the dataset is: 3.14
Loading the model
The character to token ratio of the dataset is: 3.14
Loading the model
The character to token ratio of the dataset is: 3.14
Loading the model
The character to token ratio of the dataset is: 3.14
Loading the model
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-3b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2816,
  "n_head": 22,
  "n_inner": 11264,
  "n_layer": 36,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/config.json
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-3b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2816,
  "n_head": 22,
  "n_inner": 11264,
  "n_layer": 36,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-3b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2816,
  "n_head": 22,
  "n_inner": 11264,
  "n_layer": 36,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-3b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2816,
  "n_head": 22,
  "n_inner": 11264,
  "n_layer": 36,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/pytorch_model.bin.index.json
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 1/2 [00:21<00:21, 21.95s/it]Loading checkpoint shards:  50%|     | 1/2 [00:21<00:21, 21.94s/it]Loading checkpoint shards:  50%|     | 1/2 [00:21<00:21, 21.95s/it]Loading checkpoint shards:  50%|     | 1/2 [00:21<00:21, 21.94s/it]Loading checkpoint shards: 100%|| 2/2 [00:25<00:00, 11.34s/it]Loading checkpoint shards: 100%|| 2/2 [00:25<00:00, 12.93s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-3b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|| 2/2 [00:25<00:00, 11.34s/it]Loading checkpoint shards: 100%|| 2/2 [00:25<00:00, 12.93s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-3b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|| 2/2 [00:25<00:00, 11.34s/it]Loading checkpoint shards: 100%|| 2/2 [00:25<00:00, 12.93s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-3b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|| 2/2 [00:25<00:00, 11.35s/it]Loading checkpoint shards: 100%|| 2/2 [00:25<00:00, 12.94s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-3b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/generation_config.json
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/generation_config.json
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/generation_config.json
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 14745600 || all params: 3058056704 || trainable%: 0.482188573570675
Starting main loop
trainable params: 14745600 || all params: 3058056704 || trainable%: 0.482188573570675
Starting main loop
trainable params: 14745600 || all params: 3058056704 || trainable%: 0.482188573570675
Starting main loop
trainable params: 14745600 || all params: 3058056704 || trainable%: 0.482188573570675
Starting main loop
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
Training...
Training...
Training...
Training...
Currently training with a batch size of: 1
***** Running training *****
  Num examples = 416,000
  Num Epochs = 9,223,372,036,854,775,807
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 6,500
  Number of trainable parameters = 14,745,600
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: hetarthvader (complex_dnn). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /projects/bbvz/bzd2/wandb/run-20231213_174945-jzqnomsa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_7_A40
wandb:  View project at https://wandb.ai/complex_dnn/huggingface
wandb:  View run at https://wandb.ai/complex_dnn/huggingface/runs/jzqnomsa
  0%|          | 0/6500 [00:00<?, ?it/s]/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9865, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 1.9769, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 2.0244, 'learning_rate': 9.999999415640199e-05, 'epoch': 0.0}
{'loss': 1.9353, 'learning_rate': 9.999997662560938e-05, 'epoch': 0.0}
{'loss': 1.8194, 'learning_rate': 9.999994740762622e-05, 'epoch': 0.0}
{'loss': 1.8446, 'learning_rate': 9.999990650245936e-05, 'epoch': 0.0}
  0%|          | 1/6500 [00:21<39:14:24, 21.74s/it]                                                     0%|          | 1/6500 [00:21<39:14:24, 21.74s/it]  0%|          | 2/6500 [00:32<27:18:14, 15.13s/it]                                                     0%|          | 2/6500 [00:32<27:18:14, 15.13s/it]  0%|          | 3/6500 [00:42<23:20:31, 12.93s/it]                                                     0%|          | 3/6500 [00:42<23:20:31, 12.93s/it]  0%|          | 4/6500 [00:52<21:28:28, 11.90s/it]                                                     0%|          | 4/6500 [00:52<21:28:28, 11.90s/it]  0%|          | 5/6500 [01:03<20:26:07, 11.33s/it]                                                     0%|          | 5/6500 [01:03<20:26:07, 11.33s/it]  0%|          | 6/6500 [01:13<19:48:23, 10.98s/it]                                                     0%|          | 6/6500 [01:13<19:48:23, 10.98s/it]  0%|          | 7/6500 [01:23<19:25:23, 10.77s/it]                             {'loss': 1.7494, 'learning_rate': 9.999985391011837e-05, 'epoch': 0.0}
{'loss': 1.7936, 'learning_rate': 9.999978963061551e-05, 'epoch': 0.0}
{'loss': 1.7191, 'learning_rate': 9.999971366396584e-05, 'epoch': 0.0}
{'loss': 1.6118, 'learning_rate': 9.99996260101871e-05, 'epoch': 0.0}
                        0%|          | 7/6500 [01:23<19:25:23, 10.77s/it]  0%|          | 8/6500 [01:34<19:10:09, 10.63s/it]                                                     0%|          | 8/6500 [01:34<19:10:09, 10.63s/it]  0%|          | 9/6500 [01:44<18:59:50, 10.54s/it]                                                     0%|          | 9/6500 [01:44<18:59:50, 10.54s/it]  0%|          | 10/6500 [01:54<18:53:10, 10.48s/it]                                                      0%|          | 10/6500 [01:54<18:53:10, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.5904150009155273, 'eval_runtime': 4.071, 'eval_samples_per_second': 5.65, 'eval_steps_per_second': 1.474, 'epoch': 0.0}
                                                      0%|          | 10/6500 [01:58<18:53:10, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-10
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-10
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-10/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.708, 'learning_rate': 9.99995266692998e-05, 'epoch': 0.0}
{'loss': 2.0678, 'learning_rate': 9.999941564132713e-05, 'epoch': 0.0}
{'loss': 1.5885, 'learning_rate': 9.999929292629505e-05, 'epoch': 0.0}
{'loss': 1.5626, 'learning_rate': 9.999915852423225e-05, 'epoch': 0.0}
{'loss': 1.5185, 'learning_rate': 9.999901243517017e-05, 'epoch': 0.0}
{'loss': 1.4421, 'learning_rate': 9.99988546591429e-05, 'epoch': 0.0}
  0%|          | 11/6500 [02:10<21:34:49, 11.97s/it]                                                      0%|          | 11/6500 [02:10<21:34:49, 11.97s/it]  0%|          | 12/6500 [02:20<20:40:22, 11.47s/it]                                                      0%|          | 12/6500 [02:20<20:40:22, 11.47s/it]  0%|          | 13/6500 [02:30<20:03:17, 11.13s/it]                                                      0%|          | 13/6500 [02:30<20:03:17, 11.13s/it]  0%|          | 14/6500 [02:41<19:37:44, 10.89s/it]                                                      0%|          | 14/6500 [02:41<19:37:44, 10.89s/it]  0%|          | 15/6500 [02:51<19:21:14, 10.74s/it]                                                      0%|          | 15/6500 [02:51<19:21:14, 10.74s/it]  0%|          | 16/6500 [03:01<19:08:32, 10.63s/it]                                                      0%|          | 16/6500 [03:01<19:08:32, 10.63s/it]  0%|          | 17/6500 [03:12<19:20:37, 10.74s/it]          {'loss': 1.4483, 'learning_rate': 9.999868519618736e-05, 'epoch': 0.0}
{'loss': 1.4197, 'learning_rate': 9.999850404634316e-05, 'epoch': 0.0}
{'loss': 1.3645, 'learning_rate': 9.999831120965261e-05, 'epoch': 0.0}
{'loss': 1.3929, 'learning_rate': 9.999810668616086e-05, 'epoch': 0.0}
                                            0%|          | 17/6500 [03:12<19:20:37, 10.74s/it]  0%|          | 18/6500 [03:23<19:08:17, 10.63s/it]                                                      0%|          | 18/6500 [03:23<19:08:17, 10.63s/it]  0%|          | 19/6500 [03:33<18:59:00, 10.54s/it]                                                      0%|          | 19/6500 [03:33<18:59:00, 10.54s/it]  0%|          | 20/6500 [03:44<18:53:07, 10.49s/it]                                                      0%|          | 20/6500 [03:44<18:53:07, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.3244149684906006, 'eval_runtime': 4.094, 'eval_samples_per_second': 5.618, 'eval_steps_per_second': 1.466, 'epoch': 0.0}
                                                      0%|          | 20/6500 [03:48<18:53:07, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-20
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-20
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-20
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-20
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-20
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-20/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-20/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3524, 'learning_rate': 9.999789047591562e-05, 'epoch': 0.0}
{'loss': 1.3312, 'learning_rate': 9.999766257896749e-05, 'epoch': 0.0}
{'loss': 1.3445, 'learning_rate': 9.999742299536971e-05, 'epoch': 0.0}
{'loss': 1.3487, 'learning_rate': 9.999717172517832e-05, 'epoch': 0.0}
{'loss': 1.3835, 'learning_rate': 9.999690876845202e-05, 'epoch': 0.0}
{'loss': 1.3402, 'learning_rate': 9.999663412525226e-05, 'epoch': 0.0}
  0%|          | 21/6500 [03:59<21:19:46, 11.85s/it]                                                      0%|          | 21/6500 [03:59<21:19:46, 11.85s/it]  0%|          | 22/6500 [04:09<20:32:31, 11.42s/it]                                                      0%|          | 22/6500 [04:09<20:32:31, 11.42s/it]  0%|          | 23/6500 [04:19<19:59:25, 11.11s/it]                                                      0%|          | 23/6500 [04:19<19:59:25, 11.11s/it]  0%|          | 24/6500 [04:30<19:36:06, 10.90s/it]                                                      0%|          | 24/6500 [04:30<19:36:06, 10.90s/it]  0%|          | 25/6500 [04:40<19:19:30, 10.74s/it]                                                      0%|          | 25/6500 [04:40<19:19:30, 10.74s/it]  0%|          | 26/6500 [04:51<19:08:05, 10.64s/it]                                                      0%|          | 26/6500 [04:51<19:08:05, 10.64s/it]  0%|          | 27/6500 [05:01<19:00:12, 10.57s/it]          {'loss': 1.3061, 'learning_rate': 9.999634779564329e-05, 'epoch': 0.0}
{'loss': 1.3385, 'learning_rate': 9.999604977969197e-05, 'epoch': 0.0}
{'loss': 1.2977, 'learning_rate': 9.999574007746801e-05, 'epoch': 0.0}
{'loss': 1.3019, 'learning_rate': 9.99954186890438e-05, 'epoch': 0.0}
                                            0%|          | 27/6500 [05:01<19:00:12, 10.57s/it]  0%|          | 28/6500 [05:11<18:54:25, 10.52s/it]                                                      0%|          | 28/6500 [05:11<18:54:25, 10.52s/it]  0%|          | 29/6500 [05:22<18:50:20, 10.48s/it]                                                      0%|          | 29/6500 [05:22<18:50:20, 10.48s/it]  0%|          | 30/6500 [05:32<18:47:59, 10.46s/it]                                                      0%|          | 30/6500 [05:32<18:47:59, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.2419424057006836, 'eval_runtime': 3.9906, 'eval_samples_per_second': 5.764, 'eval_steps_per_second': 1.504, 'epoch': 0.0}
                                                      0%|          | 30/6500 [05:36<18:47:59, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-30
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-30
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-30
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-30/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-30/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2487, 'learning_rate': 9.999508561449444e-05, 'epoch': 0.0}
{'loss': 1.2338, 'learning_rate': 9.999474085389778e-05, 'epoch': 0.0}
{'loss': 1.3009, 'learning_rate': 9.999438440733445e-05, 'epoch': 0.01}
{'loss': 1.2468, 'learning_rate': 9.999401627488769e-05, 'epoch': 0.01}
{'loss': 1.253, 'learning_rate': 9.999363645664363e-05, 'epoch': 0.01}
{'loss': 1.2306, 'learning_rate': 9.9993244952691e-05, 'epoch': 0.01}
  0%|          | 31/6500 [05:47<21:12:01, 11.80s/it]                                                      0%|          | 31/6500 [05:47<21:12:01, 11.80s/it]  0%|          | 32/6500 [05:58<20:27:07, 11.38s/it]                                                      0%|          | 32/6500 [05:58<20:27:07, 11.38s/it]  1%|          | 33/6500 [06:08<20:04:20, 11.17s/it]                                                      1%|          | 33/6500 [06:08<20:04:20, 11.17s/it]  1%|          | 34/6500 [06:19<19:39:39, 10.95s/it]                                                      1%|          | 34/6500 [06:19<19:39:39, 10.95s/it]  1%|          | 35/6500 [06:29<19:21:40, 10.78s/it]                                                      1%|          | 35/6500 [06:29<19:21:40, 10.78s/it]  1%|          | 36/6500 [06:39<19:09:43, 10.67s/it]                                                      1%|          | 36/6500 [06:39<19:09:43, 10.67s/it]  1%|          | 37/6500 [06:50<19:01:22, 10.60s/it]          {'loss': 1.2222, 'learning_rate': 9.999284176312134e-05, 'epoch': 0.01}
{'loss': 1.2542, 'learning_rate': 9.999242688802886e-05, 'epoch': 0.01}
{'loss': 1.2729, 'learning_rate': 9.999200032751056e-05, 'epoch': 0.01}
{'loss': 1.1946, 'learning_rate': 9.999156208166614e-05, 'epoch': 0.01}
                                            1%|          | 37/6500 [06:50<19:01:22, 10.60s/it]  1%|          | 38/6500 [07:00<18:55:42, 10.55s/it]                                                      1%|          | 38/6500 [07:00<18:55:42, 10.55s/it]  1%|          | 39/6500 [07:11<18:51:24, 10.51s/it]                                                      1%|          | 39/6500 [07:11<18:51:24, 10.51s/it]  1%|          | 40/6500 [07:21<18:48:37, 10.48s/it]                                                      1%|          | 40/6500 [07:21<18:48:37, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1941254138946533, 'eval_runtime': 4.0158, 'eval_samples_per_second': 5.727, 'eval_steps_per_second': 1.494, 'epoch': 0.01}
                                                      1%|          | 40/6500 [07:25<18:48:37, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-40
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-40
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-40I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-40
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-40
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-40/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2778, 'learning_rate': 9.999111215059804e-05, 'epoch': 0.01}
{'loss': 1.7085, 'learning_rate': 9.999065053441144e-05, 'epoch': 0.01}
{'loss': 1.2428, 'learning_rate': 9.99901772332142e-05, 'epoch': 0.01}
{'loss': 1.1983, 'learning_rate': 9.998969224711698e-05, 'epoch': 0.01}
{'loss': 1.172, 'learning_rate': 9.998919557623315e-05, 'epoch': 0.01}
{'loss': 1.2173, 'learning_rate': 9.99886872206788e-05, 'epoch': 0.01}
  1%|          | 41/6500 [07:36<21:19:34, 11.89s/it]                                                      1%|          | 41/6500 [07:36<21:19:34, 11.89s/it]  1%|          | 42/6500 [07:47<20:31:18, 11.44s/it]                                                      1%|          | 42/6500 [07:47<20:31:18, 11.44s/it]  1%|          | 43/6500 [07:57<19:58:17, 11.13s/it]                                                      1%|          | 43/6500 [07:57<19:58:17, 11.13s/it]  1%|          | 44/6500 [08:08<19:34:45, 10.92s/it]                                                      1%|          | 44/6500 [08:08<19:34:45, 10.92s/it]  1%|          | 45/6500 [08:18<19:17:49, 10.76s/it]                                                      1%|          | 45/6500 [08:18<19:17:49, 10.76s/it]  1%|          | 46/6500 [08:28<19:06:16, 10.66s/it]                                                      1%|          | 46/6500 [08:28<19:06:16, 10.66s/it]  1%|          | 47/6500 [08:39<18:58:34, 10.59s/it]          {'loss': 1.2362, 'learning_rate': 9.998816718057274e-05, 'epoch': 0.01}
{'loss': 1.1792, 'learning_rate': 9.998763545603654e-05, 'epoch': 0.01}
{'loss': 1.1913, 'learning_rate': 9.998709204719447e-05, 'epoch': 0.01}
{'loss': 1.1793, 'learning_rate': 9.998653695417356e-05, 'epoch': 0.01}
                                            1%|          | 47/6500 [08:39<18:58:34, 10.59s/it]  1%|          | 48/6500 [08:49<18:52:57, 10.54s/it]                                                      1%|          | 48/6500 [08:49<18:52:57, 10.54s/it]  1%|          | 49/6500 [09:00<19:09:56, 10.70s/it]                                                      1%|          | 49/6500 [09:00<19:09:56, 10.70s/it]  1%|          | 50/6500 [09:11<19:00:33, 10.61s/it]                                                      1%|          | 50/6500 [09:11<19:00:33, 10.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1612815856933594, 'eval_runtime': 4.0249, 'eval_samples_per_second': 5.714, 'eval_steps_per_second': 1.491, 'epoch': 0.01}
                                                      1%|          | 50/6500 [09:15<19:00:33, 10.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-50
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-50
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-50
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-50
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-50
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-50/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-50/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-50/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-50/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1827, 'learning_rate': 9.998597017710356e-05, 'epoch': 0.01}
{'loss': 1.1443, 'learning_rate': 9.998539171611697e-05, 'epoch': 0.01}
{'loss': 1.195, 'learning_rate': 9.998480157134897e-05, 'epoch': 0.01}
{'loss': 1.2249, 'learning_rate': 9.998419974293752e-05, 'epoch': 0.01}
{'loss': 1.2013, 'learning_rate': 9.998358623102327e-05, 'epoch': 0.01}
{'loss': 1.1924, 'learning_rate': 9.998296103574967e-05, 'epoch': 0.01}
  1%|          | 51/6500 [09:26<21:19:50, 11.91s/it]                                                      1%|          | 51/6500 [09:26<21:19:50, 11.91s/it]  1%|          | 52/6500 [09:36<20:31:58, 11.46s/it]                                                      1%|          | 52/6500 [09:36<20:31:58, 11.46s/it]  1%|          | 53/6500 [09:46<19:58:04, 11.15s/it]                                                      1%|          | 53/6500 [09:46<19:58:04, 11.15s/it]  1%|          | 54/6500 [09:57<19:34:37, 10.93s/it]                                                      1%|          | 54/6500 [09:57<19:34:37, 10.93s/it]  1%|          | 55/6500 [10:07<19:17:35, 10.78s/it]                                                      1%|          | 55/6500 [10:07<19:17:35, 10.78s/it]  1%|          | 56/6500 [10:18<19:05:36, 10.67s/it]                                                      1%|          | 56/6500 [10:18<19:05:36, 10.67s/it]  1%|          | 57/6500 [10:28<18:57:52, 10.60s/it]          {'loss': 1.1842, 'learning_rate': 9.99823241572628e-05, 'epoch': 0.01}
{'loss': 1.2005, 'learning_rate': 9.998167559571158e-05, 'epoch': 0.01}
{'loss': 1.1901, 'learning_rate': 9.998101535124758e-05, 'epoch': 0.01}
{'loss': 1.1753, 'learning_rate': 9.998034342402513e-05, 'epoch': 0.01}
                                            1%|          | 57/6500 [10:28<18:57:52, 10.60s/it]  1%|          | 58/6500 [10:39<18:52:04, 10.54s/it]                                                      1%|          | 58/6500 [10:39<18:52:04, 10.54s/it]  1%|          | 59/6500 [10:49<18:48:07, 10.51s/it]                                                      1%|          | 59/6500 [10:49<18:48:07, 10.51s/it]  1%|          | 60/6500 [10:59<18:45:21, 10.48s/it]                                                      1%|          | 60/6500 [10:59<18:45:21, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1352144479751587, 'eval_runtime': 4.003, 'eval_samples_per_second': 5.746, 'eval_steps_per_second': 1.499, 'epoch': 0.01}
                                                      1%|          | 60/6500 [11:03<18:45:21, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-60
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-60
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-60
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-60/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1025, 'learning_rate': 9.997965981420128e-05, 'epoch': 0.01}
{'loss': 1.1854, 'learning_rate': 9.997896452193584e-05, 'epoch': 0.01}
{'loss': 1.1635, 'learning_rate': 9.997825754739132e-05, 'epoch': 0.01}
{'loss': 1.1308, 'learning_rate': 9.997753889073296e-05, 'epoch': 0.01}
{'loss': 1.1617, 'learning_rate': 9.997680855212877e-05, 'epoch': 0.01}
{'loss': 1.1085, 'learning_rate': 9.997606653174942e-05, 'epoch': 0.01}
  1%|          | 61/6500 [11:14<21:08:38, 11.82s/it]                                                      1%|          | 61/6500 [11:14<21:08:38, 11.82s/it]  1%|          | 62/6500 [11:25<20:23:00, 11.40s/it]                                                      1%|          | 62/6500 [11:25<20:23:00, 11.40s/it]  1%|          | 63/6500 [11:35<19:50:59, 11.10s/it]                                                      1%|          | 63/6500 [11:35<19:50:59, 11.10s/it]  1%|          | 64/6500 [11:46<19:28:20, 10.89s/it]                                                      1%|          | 64/6500 [11:46<19:28:20, 10.89s/it]  1%|          | 65/6500 [11:56<19:21:01, 10.83s/it]                                                      1%|          | 65/6500 [11:56<19:21:01, 10.83s/it]  1%|          | 66/6500 [12:07<19:07:12, 10.70s/it]                                                      1%|          | 66/6500 [12:07<19:07:12, 10.70s/it]  1%|          | 67/6500 [12:17<18:58:16, 10.62s/it]          {'loss': 1.1681, 'learning_rate': 9.99753128297684e-05, 'epoch': 0.01}
{'loss': 1.1705, 'learning_rate': 9.997454744636186e-05, 'epoch': 0.01}
{'loss': 1.1289, 'learning_rate': 9.99737703817087e-05, 'epoch': 0.01}
{'loss': 1.172, 'learning_rate': 9.997298163599056e-05, 'epoch': 0.01}
                                            1%|          | 67/6500 [12:17<18:58:16, 10.62s/it]  1%|          | 68/6500 [12:27<18:51:36, 10.56s/it]                                                      1%|          | 68/6500 [12:27<18:51:36, 10.56s/it]  1%|          | 69/6500 [12:38<18:46:15, 10.51s/it]                                                      1%|          | 69/6500 [12:38<18:46:15, 10.51s/it]  1%|          | 70/6500 [12:48<18:42:51, 10.48s/it]                                                      1%|          | 70/6500 [12:48<18:42:51, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1152887344360352, 'eval_runtime': 4.0023, 'eval_samples_per_second': 5.747, 'eval_steps_per_second': 1.499, 'epoch': 0.01}
                                                      1%|          | 70/6500 [12:52<18:42:51, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-70
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-70
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-70

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-70
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-70/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-70/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1528, 'learning_rate': 9.99721812093918e-05, 'epoch': 0.01}
{'loss': 1.6229, 'learning_rate': 9.99713691020995e-05, 'epoch': 0.01}
{'loss': 1.1281, 'learning_rate': 9.997054531430353e-05, 'epoch': 0.01}
{'loss': 1.1363, 'learning_rate': 9.996970984619641e-05, 'epoch': 0.01}
{'loss': 1.102, 'learning_rate': 9.996886269797344e-05, 'epoch': 0.01}
{'loss': 1.1361, 'learning_rate': 9.996800386983263e-05, 'epoch': 0.01}
  1%|          | 71/6500 [13:03<21:03:06, 11.79s/it]                                                      1%|          | 71/6500 [13:03<21:03:06, 11.79s/it]  1%|          | 72/6500 [13:13<20:18:16, 11.37s/it]                                                      1%|          | 72/6500 [13:13<20:18:16, 11.37s/it]  1%|          | 73/6500 [13:24<19:47:05, 11.08s/it]                                                      1%|          | 73/6500 [13:24<19:47:05, 11.08s/it]  1%|          | 74/6500 [13:34<19:24:54, 10.88s/it]                                                      1%|          | 74/6500 [13:34<19:24:54, 10.88s/it]  1%|          | 75/6500 [13:45<19:09:01, 10.73s/it]                                                      1%|          | 75/6500 [13:45<19:09:01, 10.73s/it]  1%|          | 76/6500 [13:55<18:58:24, 10.63s/it]                                                      1%|          | 76/6500 [13:55<18:58:24, 10.63s/it]  1%|          | 77/6500 [14:06<18:51:05, 10.57s/it]          {'loss': 1.1361, 'learning_rate': 9.996713336197472e-05, 'epoch': 0.01}
{'loss': 1.1028, 'learning_rate': 9.996625117460318e-05, 'epoch': 0.01}
{'loss': 1.1136, 'learning_rate': 9.996535730792425e-05, 'epoch': 0.01}
{'loss': 1.1115, 'learning_rate': 9.996445176214684e-05, 'epoch': 0.01}
                                            1%|          | 77/6500 [14:06<18:51:05, 10.57s/it]  1%|          | 78/6500 [14:16<18:45:11, 10.51s/it]                                                      1%|          | 78/6500 [14:16<18:45:11, 10.51s/it]  1%|          | 79/6500 [14:26<18:42:24, 10.49s/it]                                                      1%|          | 79/6500 [14:26<18:42:24, 10.49s/it]  1%|          | 80/6500 [14:37<18:39:37, 10.46s/it]                                                      1%|          | 80/6500 [14:37<18:39:37, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0994763374328613, 'eval_runtime': 3.9975, 'eval_samples_per_second': 5.754, 'eval_steps_per_second': 1.501, 'epoch': 0.01}
                                                      1%|          | 80/6500 [14:41<18:39:37, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-80
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-80
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-80
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-80/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0903, 'learning_rate': 9.996353453748261e-05, 'epoch': 0.01}
{'loss': 1.0988, 'learning_rate': 9.996260563414597e-05, 'epoch': 0.01}
{'loss': 1.1396, 'learning_rate': 9.996166505235404e-05, 'epoch': 0.01}
{'loss': 1.1469, 'learning_rate': 9.996071279232667e-05, 'epoch': 0.01}
{'loss': 1.1005, 'learning_rate': 9.995974885428647e-05, 'epoch': 0.01}
{'loss': 1.1624, 'learning_rate': 9.995877323845872e-05, 'epoch': 0.01}
  1%|          | 81/6500 [14:52<21:21:01, 11.97s/it]                                                      1%|          | 81/6500 [14:52<21:21:01, 11.97s/it]  1%|         | 82/6500 [15:03<20:31:24, 11.51s/it]                                                      1%|         | 82/6500 [15:03<20:31:24, 11.51s/it]  1%|         | 83/6500 [15:13<19:56:06, 11.18s/it]                                                      1%|         | 83/6500 [15:13<19:56:06, 11.18s/it]  1%|         | 84/6500 [15:24<19:31:38, 10.96s/it]                                                      1%|         | 84/6500 [15:24<19:31:38, 10.96s/it]  1%|         | 85/6500 [15:34<19:13:05, 10.78s/it]                                                      1%|         | 85/6500 [15:34<19:13:05, 10.78s/it]  1%|         | 86/6500 [15:44<19:00:36, 10.67s/it]                                                      1%|         | 86/6500 [15:44<19:00:36, 10.67s/it]  1%|         | 87/6500 [15:55<18:52:36,{'loss': 1.1298, 'learning_rate': 9.995778594507148e-05, 'epoch': 0.01}
{'loss': 1.1332, 'learning_rate': 9.995678697435552e-05, 'epoch': 0.01}
{'loss': 1.1205, 'learning_rate': 9.995577632654436e-05, 'epoch': 0.01}
{'loss': 1.0604, 'learning_rate': 9.99547540018742e-05, 'epoch': 0.01}
 10.60s/it]                                                      1%|         | 87/6500 [15:55<18:52:36, 10.60s/it]  1%|         | 88/6500 [16:05<18:46:29, 10.54s/it]                                                      1%|         | 88/6500 [16:05<18:46:29, 10.54s/it]  1%|         | 89/6500 [16:16<18:41:48, 10.50s/it]                                                      1%|         | 89/6500 [16:16<18:41:48, 10.50s/it]  1%|         | 90/6500 [16:26<18:38:52, 10.47s/it]                                                      1%|         | 90/6500 [16:26<18:38:52, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0836368799209595, 'eval_runtime': 3.9974, 'eval_samples_per_second': 5.754, 'eval_steps_per_second': 1.501, 'epoch': 0.01}
                                                      1%|         | 90/6500 [16:30<18:38:52, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-90
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-90I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-90

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-90
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-90
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-90/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-90/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-90/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-90/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0645, 'learning_rate': 9.995372000058404e-05, 'epoch': 0.01}
{'loss': 1.1356, 'learning_rate': 9.995267432291555e-05, 'epoch': 0.01}
{'loss': 1.1079, 'learning_rate': 9.995161696911315e-05, 'epoch': 0.01}
{'loss': 1.0732, 'learning_rate': 9.9950547939424e-05, 'epoch': 0.01}
{'loss': 1.0983, 'learning_rate': 9.994946723409797e-05, 'epoch': 0.01}
{'loss': 1.0666, 'learning_rate': 9.994837485338766e-05, 'epoch': 0.01}
  1%|         | 91/6500 [16:41<21:01:19, 11.81s/it]                                                      1%|         | 91/6500 [16:41<21:01:19, 11.81s/it]  1%|         | 92/6500 [16:51<20:15:48, 11.38s/it]                                                      1%|         | 92/6500 [16:51<20:15:48, 11.38s/it]  1%|         | 93/6500 [17:02<19:44:27, 11.09s/it]                                                      1%|         | 93/6500 [17:02<19:44:27, 11.09s/it]  1%|         | 94/6500 [17:12<19:22:40, 10.89s/it]                                                      1%|         | 94/6500 [17:12<19:22:40, 10.89s/it]  1%|         | 95/6500 [17:22<19:06:51, 10.74s/it]                                                      1%|         | 95/6500 [17:22<19:06:51, 10.74s/it]  1%|         | 96/6500 [17:33<18:56:18, 10.65s/it]                                                      1%|         | 96/6500 [17:33<18:56:18, 10.65s/it]  1%|         | 97/6500 [17:44<18:57{'loss': 1.1449, 'learning_rate': 9.994727079754844e-05, 'epoch': 0.01}
{'loss': 1.0859, 'learning_rate': 9.994615506683834e-05, 'epoch': 0.02}
{'loss': 1.062, 'learning_rate': 9.994502766151818e-05, 'epoch': 0.02}
{'loss': 1.1191, 'learning_rate': 9.994388858185147e-05, 'epoch': 0.02}
:17, 10.66s/it]                                                      1%|         | 97/6500 [17:44<18:57:17, 10.66s/it]  2%|         | 98/6500 [17:54<18:49:12, 10.58s/it]                                                      2%|         | 98/6500 [17:54<18:49:12, 10.58s/it]  2%|         | 99/6500 [18:04<18:42:54, 10.53s/it]                                                      2%|         | 99/6500 [18:04<18:42:54, 10.53s/it]  2%|         | 100/6500 [18:15<18:38:57, 10.49s/it]                                                       2%|         | 100/6500 [18:15<18:38:57, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0710686445236206, 'eval_runtime': 3.9955, 'eval_samples_per_second': 5.756, 'eval_steps_per_second': 1.502, 'epoch': 0.02}
                                                       2%|         | 100/6500 [18:19<18:38:57, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-100
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-100
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-100/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-100/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.5799, 'learning_rate': 9.994273782810447e-05, 'epoch': 0.02}
{'loss': 1.0899, 'learning_rate': 9.994157540054616e-05, 'epoch': 0.02}
{'loss': 1.0712, 'learning_rate': 9.994040129944824e-05, 'epoch': 0.02}
{'loss': 1.0732, 'learning_rate': 9.993921552508518e-05, 'epoch': 0.02}
{'loss': 1.0538, 'learning_rate': 9.993801807773411e-05, 'epoch': 0.02}
{'loss': 1.0962, 'learning_rate': 9.993680895767495e-05, 'epoch': 0.02}
  2%|         | 101/6500 [18:30<20:56:59, 11.79s/it]                                                       2%|         | 101/6500 [18:30<20:56:59, 11.79s/it]  2%|         | 102/6500 [18:40<20:12:54, 11.37s/it]                                                       2%|         | 102/6500 [18:40<20:12:54, 11.37s/it]  2%|         | 103/6500 [18:50<19:41:42, 11.08s/it]                                                       2%|         | 103/6500 [18:50<19:41:42, 11.08s/it]  2%|         | 104/6500 [19:01<19:19:20, 10.88s/it]                                                       2%|         | 104/6500 [19:01<19:19:20, 10.88s/it]  2%|         | 105/6500 [19:11<19:04:10, 10.74s/it]                                                       2%|         | 105/6500 [19:11<19:04:10, 10.74s/it]  2%|         | 106/6500 [19:22<18:53:35, 10.64s/it]                                                       2%|         | 106/6500 [19:22<18:53:35, 10.64s/it]  2%|         | 10{'loss': 1.076, 'learning_rate': 9.993558816519031e-05, 'epoch': 0.02}
{'loss': 1.0593, 'learning_rate': 9.993435570056556e-05, 'epoch': 0.02}
{'loss': 1.069, 'learning_rate': 9.993311156408876e-05, 'epoch': 0.02}
{'loss': 1.0554, 'learning_rate': 9.993185575605073e-05, 'epoch': 0.02}
7/6500 [19:32<18:45:54, 10.57s/it]                                                       2%|         | 107/6500 [19:32<18:45:54, 10.57s/it]  2%|         | 108/6500 [19:42<18:40:13, 10.52s/it]                                                       2%|         | 108/6500 [19:42<18:40:13, 10.52s/it]  2%|         | 109/6500 [19:53<18:37:05, 10.49s/it]                                                       2%|         | 109/6500 [19:53<18:37:05, 10.49s/it]  2%|         | 110/6500 [20:03<18:34:19, 10.46s/it]                                                       2%|         | 110/6500 [20:03<18:34:19, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0604463815689087, 'eval_runtime': 3.9943, 'eval_samples_per_second': 5.758, 'eval_steps_per_second': 1.502, 'epoch': 0.02}
                                                       2%|         | 110/6500 [20:07<18:34:19, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-110 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-110

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-110/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0517, 'learning_rate': 9.993058827674501e-05, 'epoch': 0.02}
{'loss': 1.0615, 'learning_rate': 9.992930912646787e-05, 'epoch': 0.02}
{'loss': 1.1053, 'learning_rate': 9.99280183055183e-05, 'epoch': 0.02}
{'loss': 1.1064, 'learning_rate': 9.9926715814198e-05, 'epoch': 0.02}
{'loss': 1.0908, 'learning_rate': 9.992540165281145e-05, 'epoch': 0.02}
{'loss': 1.0744, 'learning_rate': 9.992407582166581e-05, 'epoch': 0.02}
  2%|         | 111/6500 [20:18<20:54:08, 11.78s/it]                                                       2%|         | 111/6500 [20:18<20:54:08, 11.78s/it]  2%|         | 112/6500 [20:29<20:10:03, 11.37s/it]                                                       2%|         | 112/6500 [20:29<20:10:03, 11.37s/it]  2%|         | 113/6500 [20:39<19:39:08, 11.08s/it]                                                       2%|         | 113/6500 [20:39<19:39:08, 11.08s/it]  2%|         | 114/6500 [20:50<19:45:05, 11.13s/it]                                                       2%|         | 114/6500 [20:50<19:45:05, 11.13s/it]  2%|         | 115/6500 [21:01<19:21:39, 10.92s/it]                                                       2%|         | 115/6500 [21:01<19:21:39, 10.92s/it]  2%|         | 116/6500 [21:11<19:04:47, 10.76s/it]                                                       2%|         | 116/6500 [21:11<19:04:47, 10.76s/it]  2%|         | 11{'loss': 1.0986, 'learning_rate': 9.9922738321071e-05, 'epoch': 0.02}
{'loss': 1.0759, 'learning_rate': 9.992138915133965e-05, 'epoch': 0.02}
{'loss': 1.076, 'learning_rate': 9.992002831278708e-05, 'epoch': 0.02}
{'loss': 1.0225, 'learning_rate': 9.991865580573143e-05, 'epoch': 0.02}
7/6500 [21:21<18:53:12, 10.65s/it]                                                       2%|         | 117/6500 [21:21<18:53:12, 10.65s/it]  2%|         | 118/6500 [21:32<18:44:18, 10.57s/it]                                                       2%|         | 118/6500 [21:32<18:44:18, 10.57s/it]  2%|         | 119/6500 [21:42<18:38:24, 10.52s/it]                                                       2%|         | 119/6500 [21:42<18:38:24, 10.52s/it]  2%|         | 120/6500 [21:53<18:34:24, 10.48s/it]                                                       2%|         | 120/6500 [21:53<18:34:24, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0509814023971558, 'eval_runtime': 3.9685, 'eval_samples_per_second': 5.796, 'eval_steps_per_second': 1.512, 'epoch': 0.02}
                                                       2%|         | 120/6500 [21:57<18:34:24, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-120
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0354, 'learning_rate': 9.99172716304935e-05, 'epoch': 0.02}
{'loss': 1.0944, 'learning_rate': 9.991587578739684e-05, 'epoch': 0.02}
{'loss': 1.0542, 'learning_rate': 9.991446827676767e-05, 'epoch': 0.02}
{'loss': 1.0679, 'learning_rate': 9.991304909893506e-05, 'epoch': 0.02}
{'loss': 1.0109, 'learning_rate': 9.991161825423067e-05, 'epoch': 0.02}
{'loss': 1.0716, 'learning_rate': 9.9910175742989e-05, 'epoch': 0.02}
  2%|         | 121/6500 [22:08<21:03:13, 11.88s/it]                                                       2%|         | 121/6500 [22:08<21:03:13, 11.88s/it]  2%|         | 122/6500 [22:18<20:16:13, 11.44s/it]                                                       2%|         | 122/6500 [22:18<20:16:13, 11.44s/it]  2%|         | 123/6500 [22:29<19:43:26, 11.13s/it]                                                       2%|         | 123/6500 [22:29<19:43:26, 11.13s/it]  2%|         | 124/6500 [22:39<19:19:54, 10.92s/it]                                                       2%|         | 124/6500 [22:39<19:19:54, 10.92s/it]  2%|         | 125/6500 [22:49<19:04:48, 10.77s/it]                                                       2%|         | 125/6500 [22:49<19:04:48, 10.77s/it]  2%|         | 126/6500 [23:00<18:53:35, 10.67s/it]                                                       2%|         | 126/6500 [23:00<18:53:35, 10.67s/it]  2%|         | 12{'loss': 1.0928, 'learning_rate': 9.990872156554721e-05, 'epoch': 0.02}
{'loss': 1.0618, 'learning_rate': 9.990725572224521e-05, 'epoch': 0.02}
{'loss': 1.0299, 'learning_rate': 9.990577821342561e-05, 'epoch': 0.02}
{'loss': 1.0779, 'learning_rate': 9.99042890394338e-05, 'epoch': 0.02}
7/6500 [23:10<18:45:16, 10.59s/it]                                                       2%|         | 127/6500 [23:10<18:45:16, 10.59s/it]  2%|         | 128/6500 [23:21<18:39:20, 10.54s/it]                                                       2%|         | 128/6500 [23:21<18:39:20, 10.54s/it]  2%|         | 129/6500 [23:31<18:35:31, 10.51s/it]                                                       2%|         | 129/6500 [23:31<18:35:31, 10.51s/it]  2%|         | 130/6500 [23:42<18:39:51, 10.55s/it]                                                       2%|         | 130/6500 [23:42<18:39:51, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0423781871795654, 'eval_runtime': 3.9766, 'eval_samples_per_second': 5.784, 'eval_steps_per_second': 1.509, 'epoch': 0.02}
                                                       2%|         | 130/6500 [23:46<18:39:51, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-130
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-130/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.5358, 'learning_rate': 9.990278820061783e-05, 'epoch': 0.02}
{'loss': 1.0578, 'learning_rate': 9.990127569732855e-05, 'epoch': 0.02}
{'loss': 1.0307, 'learning_rate': 9.989975152991947e-05, 'epoch': 0.02}
{'loss': 1.0013, 'learning_rate': 9.989821569874687e-05, 'epoch': 0.02}
{'loss': 1.0145, 'learning_rate': 9.989666820416974e-05, 'epoch': 0.02}
{'loss': 1.095, 'learning_rate': 9.989510904654979e-05, 'epoch': 0.02}
  2%|         | 131/6500 [23:57<20:57:26, 11.85s/it]                                                       2%|         | 131/6500 [23:57<20:57:26, 11.85s/it]  2%|         | 132/6500 [24:07<20:12:01, 11.42s/it]                                                       2%|         | 132/6500 [24:07<20:12:01, 11.42s/it]  2%|         | 133/6500 [24:17<19:39:32, 11.12s/it]                                                       2%|         | 133/6500 [24:17<19:39:32, 11.12s/it]  2%|         | 134/6500 [24:28<19:16:11, 10.90s/it]                                                       2%|         | 134/6500 [24:28<19:16:11, 10.90s/it]  2%|         | 135/6500 [24:38<19:00:06, 10.75s/it]                                                       2%|         | 135/6500 [24:38<19:00:06, 10.75s/it]  2%|         | 136/6500 [24:49<18:50:26, 10.66s/it]                                                       2%|         | 136/6500 [24:49<18:50:26, 10.66s/it]  2%|         | 13{'loss': 1.0242, 'learning_rate': 9.989353822625146e-05, 'epoch': 0.02}
{'loss': 1.0161, 'learning_rate': 9.989195574364194e-05, 'epoch': 0.02}
{'loss': 1.0259, 'learning_rate': 9.98903615990911e-05, 'epoch': 0.02}
{'loss': 1.0186, 'learning_rate': 9.988875579297159e-05, 'epoch': 0.02}
7/6500 [24:59<18:43:48, 10.60s/it]                                                       2%|         | 137/6500 [24:59<18:43:48, 10.60s/it]  2%|         | 138/6500 [25:10<18:38:14, 10.55s/it]                                                       2%|         | 138/6500 [25:10<18:38:14, 10.55s/it]  2%|         | 139/6500 [25:20<18:33:12, 10.50s/it]                                                       2%|         | 139/6500 [25:20<18:33:12, 10.50s/it]  2%|         | 140/6500 [25:30<18:30:10, 10.47s/it]                                                       2%|         | 140/6500 [25:30<18:30:10, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0352061986923218, 'eval_runtime': 3.9647, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.02}
                                                       2%|         | 140/6500 [25:34<18:30:10, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-140
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-140/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-140/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-140/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0083, 'learning_rate': 9.988713832565874e-05, 'epoch': 0.02}
{'loss': 1.0467, 'learning_rate': 9.988550919753061e-05, 'epoch': 0.02}
{'loss': 1.0841, 'learning_rate': 9.988386840896803e-05, 'epoch': 0.02}
{'loss': 1.0529, 'learning_rate': 9.98822159603545e-05, 'epoch': 0.02}
{'loss': 1.0672, 'learning_rate': 9.988055185207628e-05, 'epoch': 0.02}
{'loss': 1.0611, 'learning_rate': 9.987887608452235e-05, 'epoch': 0.02}
  2%|         | 141/6500 [25:45<20:51:07, 11.80s/it]                                                       2%|         | 141/6500 [25:45<20:51:07, 11.80s/it]  2%|         | 142/6500 [25:56<20:07:01, 11.39s/it]                                                       2%|         | 142/6500 [25:56<20:07:01, 11.39s/it]  2%|         | 143/6500 [26:06<19:35:15, 11.09s/it]                                                       2%|         | 143/6500 [26:06<19:35:15, 11.09s/it]  2%|         | 144/6500 [26:16<19:13:21, 10.89s/it]                                                       2%|         | 144/6500 [26:16<19:13:21, 10.89s/it]  2%|         | 145/6500 [26:27<18:57:32, 10.74s/it]                                                       2%|         | 145/6500 [26:27<18:57:32, 10.74s/it]  2%|         | 146/6500 [26:38<19:16:34, 10.92s/it]                                                       2%|         | 146/6500 [26:38<19:16:34, 10.92s/it]  2%|         | 14{'loss': 1.0508, 'learning_rate': 9.98771886580844e-05, 'epoch': 0.02}
{'loss': 1.0513, 'learning_rate': 9.987548957315685e-05, 'epoch': 0.02}
{'loss': 1.028, 'learning_rate': 9.987377883013687e-05, 'epoch': 0.02}
{'loss': 0.9903, 'learning_rate': 9.987205642942432e-05, 'epoch': 0.02}
7/6500 [26:49<19:00:47, 10.77s/it]                                                       2%|         | 147/6500 [26:49<19:00:47, 10.77s/it]  2%|         | 148/6500 [26:59<18:50:07, 10.67s/it]                                                       2%|         | 148/6500 [26:59<18:50:07, 10.67s/it]  2%|         | 149/6500 [27:10<18:41:45, 10.60s/it]                                                       2%|         | 149/6500 [27:10<18:41:45, 10.60s/it]  2%|         | 150/6500 [27:20<18:36:14, 10.55s/it]                                                       2%|         | 150/6500 [27:20<18:36:14, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0283153057098389, 'eval_runtime': 3.9911, 'eval_samples_per_second': 5.763, 'eval_steps_per_second': 1.503, 'epoch': 0.02}
                                                       2%|         | 150/6500 [27:24<18:36:14, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-150 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-150

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0436, 'learning_rate': 9.98703223714218e-05, 'epoch': 0.02}
{'loss': 1.0444, 'learning_rate': 9.986857665653466e-05, 'epoch': 0.02}
{'loss': 1.0141, 'learning_rate': 9.986681928517092e-05, 'epoch': 0.02}
{'loss': 1.0326, 'learning_rate': 9.986505025774138e-05, 'epoch': 0.02}
{'loss': 0.9862, 'learning_rate': 9.986326957465951e-05, 'epoch': 0.02}
{'loss': 1.0548, 'learning_rate': 9.986147723634156e-05, 'epoch': 0.02}
  2%|         | 151/6500 [27:35<20:54:40, 11.86s/it]                                                       2%|         | 151/6500 [27:35<20:54:40, 11.86s/it]  2%|         | 152/6500 [27:45<20:08:57, 11.43s/it]                                                       2%|         | 152/6500 [27:45<20:08:57, 11.43s/it]  2%|         | 153/6500 [27:56<19:36:38, 11.12s/it]                                                       2%|         | 153/6500 [27:56<19:36:38, 11.12s/it]  2%|         | 154/6500 [28:06<19:14:44, 10.92s/it]                                                       2%|         | 154/6500 [28:06<19:14:44, 10.92s/it]  2%|         | 155/6500 [28:17<18:59:05, 10.77s/it]                                                       2%|         | 155/6500 [28:17<18:59:05, 10.77s/it]  2%|         | 156/6500 [28:27<18:48:33, 10.67s/it]                                                       2%|         | 156/6500 [28:27<18:48:33, 10.67s/it]  2%|         | 15{'loss': 1.0616, 'learning_rate': 9.985967324320646e-05, 'epoch': 0.02}
{'loss': 0.9926, 'learning_rate': 9.985785759567591e-05, 'epoch': 0.02}
{'loss': 1.0554, 'learning_rate': 9.985603029417427e-05, 'epoch': 0.02}
{'loss': 1.0187, 'learning_rate': 9.985419133912869e-05, 'epoch': 0.02}
7/6500 [28:37<18:41:48, 10.61s/it]                                                       2%|         | 157/6500 [28:38<18:41:48, 10.61s/it]  2%|         | 158/6500 [28:48<18:36:40, 10.56s/it]                                                       2%|         | 158/6500 [28:48<18:36:40, 10.56s/it]  2%|         | 159/6500 [28:58<18:32:26, 10.53s/it]                                                       2%|         | 159/6500 [28:58<18:32:26, 10.53s/it]  2%|         | 160/6500 [29:09<18:29:23, 10.50s/it]                                                       2%|         | 160/6500 [29:09<18:29:23, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.021403431892395, 'eval_runtime': 4.0045, 'eval_samples_per_second': 5.744, 'eval_steps_per_second': 1.498, 'epoch': 0.02}
                                                       2%|         | 160/6500 [29:13<18:29:23, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-160the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-160/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.5007, 'learning_rate': 9.9852340730969e-05, 'epoch': 0.02}
{'loss': 1.01, 'learning_rate': 9.985047847012776e-05, 'epoch': 0.02}
{'loss': 1.012, 'learning_rate': 9.984860455704028e-05, 'epoch': 0.03}
{'loss': 0.9735, 'learning_rate': 9.984671899214457e-05, 'epoch': 0.03}
{'loss': 1.0235, 'learning_rate': 9.984482177588138e-05, 'epoch': 0.03}
{'loss': 1.0465, 'learning_rate': 9.984291290869415e-05, 'epoch': 0.03}
  2%|         | 161/6500 [29:24<20:48:26, 11.82s/it]                                                       2%|         | 161/6500 [29:24<20:48:26, 11.82s/it]  2%|         | 162/6500 [29:35<20:17:12, 11.52s/it]                                                       2%|         | 162/6500 [29:35<20:17:12, 11.52s/it]  3%|         | 163/6500 [29:45<19:42:29, 11.20s/it]                                                       3%|         | 163/6500 [29:45<19:42:29, 11.20s/it]  3%|         | 164/6500 [29:55<19:18:15, 10.97s/it]                                                       3%|         | 164/6500 [29:55<19:18:15, 10.97s/it]  3%|         | 165/6500 [30:06<19:01:41, 10.81s/it]                                                       3%|         | 165/6500 [30:06<19:01:41, 10.81s/it]  3%|         | 166/6500 [30:16<18:49:49, 10.70s/it]                                                       3%|         | 166/6500 [30:16<18:49:49, 10.70s/it]  3%|         | 16{'loss': 0.9713, 'learning_rate': 9.984099239102909e-05, 'epoch': 0.03}
{'loss': 1.0072, 'learning_rate': 9.983906022333507e-05, 'epoch': 0.03}
{'loss': 0.9871, 'learning_rate': 9.983711640606377e-05, 'epoch': 0.03}
{'loss': 0.9901, 'learning_rate': 9.983516093966952e-05, 'epoch': 0.03}
7/6500 [30:27<18:41:28, 10.63s/it]                                                       3%|         | 167/6500 [30:27<18:41:28, 10.63s/it]  3%|         | 168/6500 [30:37<18:35:12, 10.57s/it]                                                       3%|         | 168/6500 [30:37<18:35:12, 10.57s/it]  3%|         | 169/6500 [30:48<18:30:40, 10.53s/it]                                                       3%|         | 169/6500 [30:48<18:30:40, 10.53s/it]  3%|         | 170/6500 [30:58<18:27:54, 10.50s/it]                                                       3%|         | 170/6500 [30:58<18:27:54, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0151759386062622, 'eval_runtime': 3.9723, 'eval_samples_per_second': 5.79, 'eval_steps_per_second': 1.51, 'epoch': 0.03}
                                                       3%|         | 170/6500 [31:02<18:27:54, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-170
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-170/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-170/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-170/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9934, 'learning_rate': 9.98331938246094e-05, 'epoch': 0.03}
{'loss': 1.0356, 'learning_rate': 9.983121506134322e-05, 'epoch': 0.03}
{'loss': 1.038, 'learning_rate': 9.98292246503335e-05, 'epoch': 0.03}
{'loss': 1.0169, 'learning_rate': 9.982722259204548e-05, 'epoch': 0.03}
{'loss': 1.0371, 'learning_rate': 9.982520888694713e-05, 'epoch': 0.03}
{'loss': 1.0661, 'learning_rate': 9.982318353550915e-05, 'epoch': 0.03}
  3%|         | 171/6500 [31:13<20:45:56, 11.81s/it]                                                       3%|         | 171/6500 [31:13<20:45:56, 11.81s/it]  3%|         | 172/6500 [31:23<20:01:13, 11.39s/it]                                                       3%|         | 172/6500 [31:23<20:01:13, 11.39s/it]  3%|         | 173/6500 [31:34<19:29:41, 11.09s/it]                                                       3%|         | 173/6500 [31:34<19:29:41, 11.09s/it]  3%|         | 174/6500 [31:44<19:08:01, 10.89s/it]                                                       3%|         | 174/6500 [31:44<19:08:01, 10.89s/it]  3%|         | 175/6500 [31:55<18:52:02, 10.74s/it]                                                       3%|         | 175/6500 [31:55<18:52:02, 10.74s/it]  3%|         | 176/6500 [32:05<18:41:26, 10.64s/it]                                                       3%|         | 176/6500 [32:05<18:41:26, 10.64s/it]  3%|         | 17{'loss': 1.0193, 'learning_rate': 9.982114653820494e-05, 'epoch': 0.03}
{'loss': 1.0137, 'learning_rate': 9.981909789551065e-05, 'epoch': 0.03}
{'loss': 0.9697, 'learning_rate': 9.981703760790515e-05, 'epoch': 0.03}
{'loss': 0.9783, 'learning_rate': 9.981496567586997e-05, 'epoch': 0.03}
7/6500 [32:15<18:33:32, 10.57s/it]                                                       3%|         | 177/6500 [32:15<18:33:32, 10.57s/it]  3%|         | 178/6500 [32:26<18:36:03, 10.59s/it]                                                       3%|         | 178/6500 [32:26<18:36:03, 10.59s/it]  3%|         | 179/6500 [32:36<18:30:09, 10.54s/it]                                                       3%|         | 179/6500 [32:36<18:30:09, 10.54s/it]  3%|         | 180/6500 [32:47<18:26:27, 10.50s/it]                                                       3%|         | 180/6500 [32:47<18:26:27, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0099982023239136, 'eval_runtime': 4.4539, 'eval_samples_per_second': 5.164, 'eval_steps_per_second': 1.347, 'epoch': 0.03}
                                                       3%|         | 180/6500 [32:51<18:26:27, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-180I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-180
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0292, 'learning_rate': 9.981288209988946e-05, 'epoch': 0.03}
{'loss': 1.0149, 'learning_rate': 9.981078688045062e-05, 'epoch': 0.03}
{'loss': 1.01, 'learning_rate': 9.980868001804322e-05, 'epoch': 0.03}
{'loss': 0.9849, 'learning_rate': 9.980656151315969e-05, 'epoch': 0.03}
{'loss': 0.9958, 'learning_rate': 9.980443136629525e-05, 'epoch': 0.03}
{'loss': 1.0363, 'learning_rate': 9.980228957794777e-05, 'epoch': 0.03}
  3%|         | 181/6500 [33:02<21:00:20, 11.97s/it]                                                       3%|         | 181/6500 [33:02<21:00:20, 11.97s/it]  3%|         | 182/6500 [33:13<20:13:30, 11.52s/it]                                                       3%|         | 182/6500 [33:13<20:13:30, 11.52s/it]  3%|         | 183/6500 [33:23<19:41:29, 11.22s/it]                                                       3%|         | 183/6500 [33:23<19:41:29, 11.22s/it]  3%|         | 184/6500 [33:34<19:16:18, 10.98s/it]                                                       3%|         | 184/6500 [33:34<19:16:18, 10.98s/it]  3%|         | 185/6500 [33:44<18:59:01, 10.82s/it]                                                       3%|         | 185/6500 [33:44<18:59:01, 10.82s/it]  3%|         | 186/6500 [33:55<18:46:48, 10.71s/it]                                                       3%|         | 186/6500 [33:55<18:46:48, 10.71s/it]  3%|         | 18{'loss': 1.0101, 'learning_rate': 9.980013614861792e-05, 'epoch': 0.03}
{'loss': 0.9709, 'learning_rate': 9.979797107880903e-05, 'epoch': 0.03}
{'loss': 1.0167, 'learning_rate': 9.979579436902717e-05, 'epoch': 0.03}
{'loss': 1.4901, 'learning_rate': 9.979360601978116e-05, 'epoch': 0.03}
7/6500 [34:05<18:36:54, 10.62s/it]                                                       3%|         | 187/6500 [34:05<18:36:54, 10.62s/it]  3%|         | 188/6500 [34:15<18:30:22, 10.55s/it]                                                       3%|         | 188/6500 [34:15<18:30:22, 10.55s/it]  3%|         | 189/6500 [34:26<18:25:54, 10.51s/it]                                                       3%|         | 189/6500 [34:26<18:25:54, 10.51s/it]  3%|         | 190/6500 [34:36<18:24:40, 10.50s/it]                                                       3%|         | 190/6500 [34:36<18:24:40, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0040289163589478, 'eval_runtime': 3.9977, 'eval_samples_per_second': 5.753, 'eval_steps_per_second': 1.501, 'epoch': 0.03}
                                                       3%|         | 190/6500 [34:40<18:24:40, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-190
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0149, 'learning_rate': 9.979140603158248e-05, 'epoch': 0.03}
{'loss': 0.9601, 'learning_rate': 9.978919440494539e-05, 'epoch': 0.03}
{'loss': 0.994, 'learning_rate': 9.978697114038681e-05, 'epoch': 0.03}
{'loss': 0.9484, 'learning_rate': 9.978473623842644e-05, 'epoch': 0.03}
{'loss': 1.0192, 'learning_rate': 9.978248969958668e-05, 'epoch': 0.03}
{'loss': 0.9924, 'learning_rate': 9.978023152439263e-05, 'epoch': 0.03}
  3%|         | 191/6500 [34:51<20:42:52, 11.82s/it]                                                       3%|         | 191/6500 [34:51<20:42:52, 11.82s/it]  3%|         | 192/6500 [35:02<19:58:37, 11.40s/it]                                                       3%|         | 192/6500 [35:02<19:58:37, 11.40s/it]  3%|         | 193/6500 [35:12<19:27:35, 11.11s/it]                                                       3%|         | 193/6500 [35:12<19:27:35, 11.11s/it]  3%|         | 194/6500 [35:23<19:16:47, 11.01s/it]                                                       3%|         | 194/6500 [35:23<19:16:47, 11.01s/it]  3%|         | 195/6500 [35:33<18:58:28, 10.83s/it]                                                       3%|         | 195/6500 [35:33<18:58:28, 10.83s/it]  3%|         | 196/6500 [35:44<18:45:09, 10.71s/it]                                                       3%|         | 196/6500 [35:44<18:45:09, 10.71s/it]  3%|         | 19{'loss': 0.9647, 'learning_rate': 9.977796171337212e-05, 'epoch': 0.03}
{'loss': 0.9769, 'learning_rate': 9.977568026705574e-05, 'epoch': 0.03}
{'loss': 0.9715, 'learning_rate': 9.977338718597672e-05, 'epoch': 0.03}
{'loss': 0.9615, 'learning_rate': 9.977108247067108e-05, 'epoch': 0.03}
7/6500 [35:54<18:37:14, 10.64s/it]                                                       3%|         | 197/6500 [35:54<18:37:14, 10.64s/it]  3%|         | 198/6500 [36:04<18:30:46, 10.58s/it]                                                       3%|         | 198/6500 [36:04<18:30:46, 10.58s/it]  3%|         | 199/6500 [36:15<18:25:59, 10.53s/it]                                                       3%|         | 199/6500 [36:15<18:25:59, 10.53s/it]  3%|         | 200/6500 [36:25<18:23:01, 10.50s/it]                                                       3%|         | 200/6500 [36:25<18:23:01, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9995927214622498, 'eval_runtime': 4.0152, 'eval_samples_per_second': 5.728, 'eval_steps_per_second': 1.494, 'epoch': 0.03}
                                                       3%|         | 200/6500 [36:29<18:23:01, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-200the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-200

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-200/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9813, 'learning_rate': 9.976876612167752e-05, 'epoch': 0.03}
{'loss': 1.0313, 'learning_rate': 9.976643813953747e-05, 'epoch': 0.03}
{'loss': 1.0121, 'learning_rate': 9.976409852479511e-05, 'epoch': 0.03}
{'loss': 1.012, 'learning_rate': 9.976174727799728e-05, 'epoch': 0.03}
{'loss': 1.005, 'learning_rate': 9.975938439969357e-05, 'epoch': 0.03}
{'loss': 1.0058, 'learning_rate': 9.975700989043633e-05, 'epoch': 0.03}
  3%|         | 201/6500 [36:40<20:43:52, 11.85s/it]                                                       3%|         | 201/6500 [36:40<20:43:52, 11.85s/it]  3%|         | 202/6500 [36:51<19:59:01, 11.42s/it]                                                       3%|         | 202/6500 [36:51<19:59:01, 11.42s/it]  3%|         | 203/6500 [37:01<19:27:25, 11.12s/it]                                                       3%|         | 203/6500 [37:01<19:27:25, 11.12s/it]  3%|         | 204/6500 [37:12<19:04:32, 10.91s/it]                                                       3%|         | 204/6500 [37:12<19:04:32, 10.91s/it]  3%|         | 205/6500 [37:22<18:49:03, 10.76s/it]                                                       3%|         | 205/6500 [37:22<18:49:03, 10.76s/it]  3%|         | 206/6500 [37:32<18:38:11, 10.66s/it]                                                       3%|         | 206/6500 [37:32<18:38:11, 10.66s/it]  3%|         | 20{'loss': 1.0051, 'learning_rate': 9.975462375078053e-05, 'epoch': 0.03}
{'loss': 0.9943, 'learning_rate': 9.975222598128394e-05, 'epoch': 0.03}
{'loss': 0.9274, 'learning_rate': 9.974981658250704e-05, 'epoch': 0.03}
{'loss': 0.9877, 'learning_rate': 9.974739555501298e-05, 'epoch': 0.03}
7/6500 [37:43<18:30:21, 10.59s/it]                                                       3%|         | 207/6500 [37:43<18:30:21, 10.59s/it]  3%|         | 208/6500 [37:53<18:26:47, 10.55s/it]                                                       3%|         | 208/6500 [37:53<18:26:47, 10.55s/it]  3%|         | 209/6500 [38:04<18:22:29, 10.51s/it]                                                       3%|         | 209/6500 [38:04<18:22:29, 10.51s/it]  3%|         | 210/6500 [38:14<18:19:48, 10.49s/it]                                                       3%|         | 210/6500 [38:14<18:19:48, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.995161235332489, 'eval_runtime': 4.0646, 'eval_samples_per_second': 5.659, 'eval_steps_per_second': 1.476, 'epoch': 0.03}
                                                       3%|         | 210/6500 [38:18<18:19:48, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-210
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-210
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0168, 'learning_rate': 9.974496289936769e-05, 'epoch': 0.03}
{'loss': 0.9825, 'learning_rate': 9.974251861613977e-05, 'epoch': 0.03}
{'loss': 0.9859, 'learning_rate': 9.974006270590058e-05, 'epoch': 0.03}
{'loss': 0.9468, 'learning_rate': 9.973759516922414e-05, 'epoch': 0.03}
{'loss': 1.0182, 'learning_rate': 9.973511600668724e-05, 'epoch': 0.03}
{'loss': 1.0112, 'learning_rate': 9.973262521886937e-05, 'epoch': 0.03}
  3%|         | 211/6500 [38:30<20:58:17, 12.00s/it]                                                       3%|         | 211/6500 [38:30<20:58:17, 12.00s/it]  3%|         | 212/6500 [38:40<20:08:36, 11.53s/it]                                                       3%|         | 212/6500 [38:40<20:08:36, 11.53s/it]  3%|         | 213/6500 [38:51<19:34:41, 11.21s/it]                                                       3%|         | 213/6500 [38:51<19:34:41, 11.21s/it]  3%|         | 214/6500 [39:01<19:09:38, 10.97s/it]                                                       3%|         | 214/6500 [39:01<19:09:38, 10.97s/it]  3%|         | 215/6500 [39:11<18:51:57, 10.81s/it]                                                       3%|         | 215/6500 [39:11<18:51:57, 10.81s/it]  3%|         | 216/6500 [39:22<18:39:38, 10.69s/it]                                                       3%|         | 216/6500 [39:22<18:39:38, 10.69s/it]  3%|         | 21{'loss': 0.9791, 'learning_rate': 9.973012280635273e-05, 'epoch': 0.03}
{'loss': 0.9901, 'learning_rate': 9.972760876972226e-05, 'epoch': 0.03}
{'loss': 0.9758, 'learning_rate': 9.972508310956557e-05, 'epoch': 0.03}
{'loss': 1.4627, 'learning_rate': 9.972254582647305e-05, 'epoch': 0.03}
7/6500 [39:32<18:30:57, 10.61s/it]                                                       3%|         | 217/6500 [39:32<18:30:57, 10.61s/it]  3%|         | 218/6500 [39:43<18:25:16, 10.56s/it]                                                       3%|         | 218/6500 [39:43<18:25:16, 10.56s/it]  3%|         | 219/6500 [39:53<18:24:06, 10.55s/it]                                                       3%|         | 219/6500 [39:53<18:24:06, 10.55s/it]  3%|         | 220/6500 [40:04<18:20:30, 10.51s/it]                                                       3%|         | 220/6500 [40:04<18:20:30, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.990726888179779, 'eval_runtime': 3.9894, 'eval_samples_per_second': 5.765, 'eval_steps_per_second': 1.504, 'epoch': 0.03}
                                                       3%|         | 220/6500 [40:08<18:20:30, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-220the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-220

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-220
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-220/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9819, 'learning_rate': 9.971999692103777e-05, 'epoch': 0.03}
{'loss': 0.9659, 'learning_rate': 9.971743639385551e-05, 'epoch': 0.03}
{'loss': 0.9181, 'learning_rate': 9.971486424552477e-05, 'epoch': 0.03}
{'loss': 0.9536, 'learning_rate': 9.971228047664677e-05, 'epoch': 0.03}
{'loss': 1.0351, 'learning_rate': 9.970968508782549e-05, 'epoch': 0.03}
{'loss': 0.9353, 'learning_rate': 9.970707807966755e-05, 'epoch': 0.03}
  3%|         | 221/6500 [40:19<20:38:41, 11.84s/it]                                                       3%|         | 221/6500 [40:19<20:38:41, 11.84s/it]  3%|         | 222/6500 [40:29<19:54:24, 11.42s/it]                                                       3%|         | 222/6500 [40:29<19:54:24, 11.42s/it]  3%|         | 223/6500 [40:39<19:23:15, 11.12s/it]                                                       3%|         | 223/6500 [40:39<19:23:15, 11.12s/it]  3%|         | 224/6500 [40:50<19:02:57, 10.93s/it]                                                       3%|         | 224/6500 [40:50<19:02:57, 10.93s/it]  3%|         | 225/6500 [41:00<18:46:55, 10.78s/it]                                                       3%|         | 225/6500 [41:00<18:46:55, 10.78s/it]  3%|         | 226/6500 [41:11<18:35:40, 10.67s/it]                                                       3%|         | 226/6500 [41:11<18:35:40, 10.67s/it]  3%|         | 22{'loss': 0.9513, 'learning_rate': 9.970445945278233e-05, 'epoch': 0.03}
{'loss': 0.9445, 'learning_rate': 9.970182920778193e-05, 'epoch': 0.04}
{'loss': 0.9452, 'learning_rate': 9.969918734528114e-05, 'epoch': 0.04}
{'loss': 0.9581, 'learning_rate': 9.969653386589748e-05, 'epoch': 0.04}
7/6500 [41:22<18:40:08, 10.71s/it]                                                       3%|         | 227/6500 [41:22<18:40:08, 10.71s/it]  4%|         | 228/6500 [41:32<18:31:01, 10.63s/it]                                                       4%|         | 228/6500 [41:32<18:31:01, 10.63s/it]  4%|         | 229/6500 [41:43<18:25:16, 10.58s/it]                                                       4%|         | 229/6500 [41:43<18:25:16, 10.58s/it]  4%|         | 230/6500 [41:53<18:20:28, 10.53s/it]                                                       4%|         | 230/6500 [41:53<18:20:28, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9868087768554688, 'eval_runtime': 3.9908, 'eval_samples_per_second': 5.763, 'eval_steps_per_second': 1.503, 'epoch': 0.04}
                                                       4%|         | 230/6500 [41:57<18:20:28, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-230
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-230/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.981, 'learning_rate': 9.96938687702512e-05, 'epoch': 0.04}
{'loss': 0.9969, 'learning_rate': 9.969119205896523e-05, 'epoch': 0.04}
{'loss': 0.9489, 'learning_rate': 9.968850373266522e-05, 'epoch': 0.04}
{'loss': 1.0146, 'learning_rate': 9.968580379197961e-05, 'epoch': 0.04}
{'loss': 0.9954, 'learning_rate': 9.968309223753944e-05, 'epoch': 0.04}
{'loss': 0.9817, 'learning_rate': 9.968036906997855e-05, 'epoch': 0.04}
  4%|         | 231/6500 [42:08<20:39:37, 11.86s/it]                                                       4%|         | 231/6500 [42:08<20:39:37, 11.86s/it]  4%|         | 232/6500 [42:18<19:54:59, 11.44s/it]                                                       4%|         | 232/6500 [42:18<19:54:59, 11.44s/it]  4%|         | 233/6500 [42:29<19:23:17, 11.14s/it]                                                       4%|         | 233/6500 [42:29<19:23:17, 11.14s/it]  4%|         | 234/6500 [42:39<19:01:09, 10.93s/it]                                                       4%|         | 234/6500 [42:39<19:01:09, 10.93s/it]  4%|         | 235/6500 [42:50<18:45:11, 10.78s/it]                                                       4%|         | 235/6500 [42:50<18:45:11, 10.78s/it]  4%|         | 236/6500 [43:00<18:34:23, 10.67s/it]                                                       4%|         | 236/6500 [43:00<18:34:23, 10.67s/it]  4%|         | 23{'loss': 0.9821, 'learning_rate': 9.967763428993344e-05, 'epoch': 0.04}
{'loss': 0.9467, 'learning_rate': 9.967488789804337e-05, 'epoch': 0.04}
{'loss': 0.9465, 'learning_rate': 9.967212989495028e-05, 'epoch': 0.04}
{'loss': 0.9813, 'learning_rate': 9.966936028129882e-05, 'epoch': 0.04}
7/6500 [43:11<18:26:52, 10.60s/it]                                                       4%|         | 237/6500 [43:11<18:26:52, 10.60s/it]  4%|         | 238/6500 [43:21<18:21:29, 10.55s/it]                                                       4%|         | 238/6500 [43:21<18:21:29, 10.55s/it]  4%|         | 239/6500 [43:31<18:17:32, 10.52s/it]                                                       4%|         | 239/6500 [43:31<18:17:32, 10.52s/it]  4%|         | 240/6500 [43:42<18:14:31, 10.49s/it]                                                       4%|         | 240/6500 [43:42<18:14:31, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9826543927192688, 'eval_runtime': 3.9794, 'eval_samples_per_second': 5.78, 'eval_steps_per_second': 1.508, 'epoch': 0.04}
                                                       4%|         | 240/6500 [43:46<18:14:31, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-240
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9755, 'learning_rate': 9.966657905773642e-05, 'epoch': 0.04}
{'loss': 0.9596, 'learning_rate': 9.966378622491312e-05, 'epoch': 0.04}
{'loss': 0.9605, 'learning_rate': 9.966098178348176e-05, 'epoch': 0.04}
{'loss': 0.9371, 'learning_rate': 9.965816573409785e-05, 'epoch': 0.04}
{'loss': 1.0454, 'learning_rate': 9.965533807741964e-05, 'epoch': 0.04}
{'loss': 0.96, 'learning_rate': 9.965249881410805e-05, 'epoch': 0.04}
  4%|         | 241/6500 [43:57<20:30:54, 11.80s/it]                                                       4%|         | 241/6500 [43:57<20:30:54, 11.80s/it]  4%|         | 242/6500 [44:07<19:47:38, 11.39s/it]                                                       4%|         | 242/6500 [44:07<19:47:38, 11.39s/it]  4%|         | 243/6500 [44:18<19:25:29, 11.18s/it]                                                       4%|         | 243/6500 [44:18<19:25:29, 11.18s/it]  4%|         | 244/6500 [44:28<19:02:07, 10.95s/it]                                                       4%|         | 244/6500 [44:28<19:02:07, 10.95s/it]  4%|         | 245/6500 [44:39<18:45:46, 10.80s/it]                                                       4%|         | 245/6500 [44:39<18:45:46, 10.80s/it]  4%|         | 246/6500 [44:49<18:33:41, 10.68s/it]                                                       4%|         | 246/6500 [44:49<18:33:41, 10.68s/it]  4%|         | 24{'loss': 0.9332, 'learning_rate': 9.964964794482675e-05, 'epoch': 0.04}
{'loss': 0.9995, 'learning_rate': 9.964678547024213e-05, 'epoch': 0.04}
{'loss': 1.4433, 'learning_rate': 9.964391139102325e-05, 'epoch': 0.04}
{'loss': 0.9556, 'learning_rate': 9.964102570784193e-05, 'epoch': 0.04}
7/6500 [45:00<18:26:01, 10.61s/it]                                                       4%|         | 247/6500 [45:00<18:26:01, 10.61s/it]  4%|         | 248/6500 [45:10<18:20:21, 10.56s/it]                                                       4%|         | 248/6500 [45:10<18:20:21, 10.56s/it]  4%|         | 249/6500 [45:20<18:15:59, 10.52s/it]                                                       4%|         | 249/6500 [45:20<18:15:59, 10.52s/it]  4%|         | 250/6500 [45:31<18:13:09, 10.49s/it]                                                       4%|         | 250/6500 [45:31<18:13:09, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9785353541374207, 'eval_runtime': 4.2449, 'eval_samples_per_second': 5.418, 'eval_steps_per_second': 1.413, 'epoch': 0.04}
                                                       4%|         | 250/6500 [45:35<18:13:09, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-250
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9659, 'learning_rate': 9.963812842137267e-05, 'epoch': 0.04}
{'loss': 0.9351, 'learning_rate': 9.963521953229268e-05, 'epoch': 0.04}
{'loss': 0.9057, 'learning_rate': 9.963229904128196e-05, 'epoch': 0.04}
{'loss': 0.9736, 'learning_rate': 9.962936694902307e-05, 'epoch': 0.04}
{'loss': 0.9908, 'learning_rate': 9.96264232562014e-05, 'epoch': 0.04}
{'loss': 0.9047, 'learning_rate': 9.962346796350504e-05, 'epoch': 0.04}
  4%|         | 251/6500 [45:46<20:37:17, 11.88s/it]                                                       4%|         | 251/6500 [45:46<20:37:17, 11.88s/it]  4%|         | 252/6500 [45:56<19:51:48, 11.45s/it]                                                       4%|         | 252/6500 [45:56<19:51:48, 11.45s/it]  4%|         | 253/6500 [46:07<19:19:50, 11.14s/it]                                                       4%|         | 253/6500 [46:07<19:19:50, 11.14s/it]  4%|         | 254/6500 [46:17<18:57:48, 10.93s/it]                                                       4%|         | 254/6500 [46:17<18:57:48, 10.93s/it]  4%|         | 255/6500 [46:28<18:42:37, 10.79s/it]                                                       4%|         | 255/6500 [46:28<18:42:37, 10.79s/it]  4%|         | 256/6500 [46:38<18:31:40, 10.68s/it]                                                       4%|         | 256/6500 [46:38<18:31:40, 10.68s/it]  4%|         | 25{'loss': 0.9251, 'learning_rate': 9.962050107162477e-05, 'epoch': 0.04}
{'loss': 0.9283, 'learning_rate': 9.961752258125406e-05, 'epoch': 0.04}
{'loss': 0.9399, 'learning_rate': 9.961453249308914e-05, 'epoch': 0.04}
{'loss': 0.9315, 'learning_rate': 9.96115308078289e-05, 'epoch': 0.04}
7/6500 [46:49<18:24:29, 10.62s/it]                                                       4%|         | 257/6500 [46:49<18:24:29, 10.62s/it]  4%|         | 258/6500 [46:59<18:18:28, 10.56s/it]                                                       4%|         | 258/6500 [46:59<18:18:28, 10.56s/it]  4%|         | 259/6500 [47:10<18:26:27, 10.64s/it]                                                       4%|         | 259/6500 [47:10<18:26:27, 10.64s/it]  4%|         | 260/6500 [47:20<18:19:13, 10.57s/it]                                                       4%|         | 260/6500 [47:20<18:19:13, 10.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9742714166641235, 'eval_runtime': 3.9989, 'eval_samples_per_second': 5.752, 'eval_steps_per_second': 1.5, 'epoch': 0.04}
                                                       4%|         | 260/6500 [47:24<18:19:13, 10.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-260the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-260

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-260/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-260/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9663, 'learning_rate': 9.960851752617498e-05, 'epoch': 0.04}
{'loss': 0.9657, 'learning_rate': 9.96054926488317e-05, 'epoch': 0.04}
{'loss': 0.9774, 'learning_rate': 9.960245617650613e-05, 'epoch': 0.04}
{'loss': 0.9675, 'learning_rate': 9.959940810990802e-05, 'epoch': 0.04}
{'loss': 1.0107, 'learning_rate': 9.959634844974983e-05, 'epoch': 0.04}
{'loss': 0.9475, 'learning_rate': 9.959327719674674e-05, 'epoch': 0.04}
  4%|         | 261/6500 [47:35<20:33:05, 11.86s/it]                                                       4%|         | 261/6500 [47:35<20:33:05, 11.86s/it]  4%|         | 262/6500 [47:45<19:47:13, 11.42s/it]                                                       4%|         | 262/6500 [47:45<19:47:13, 11.42s/it]  4%|         | 263/6500 [47:56<19:15:38, 11.12s/it]                                                       4%|         | 263/6500 [47:56<19:15:38, 11.12s/it]  4%|         | 264/6500 [48:06<18:53:00, 10.90s/it]                                                       4%|         | 264/6500 [48:06<18:53:00, 10.90s/it]  4%|         | 265/6500 [48:17<18:41:57, 10.80s/it]                                                       4%|         | 265/6500 [48:17<18:41:57, 10.80s/it]  4%|         | 266/6500 [48:27<18:28:44, 10.67s/it]                                                       4%|         | 266/6500 [48:27<18:28:44, 10.67s/it]  4%|         | 26{'loss': 0.9547, 'learning_rate': 9.959019435161664e-05, 'epoch': 0.04}
{'loss': 0.9134, 'learning_rate': 9.958709991508012e-05, 'epoch': 0.04}
{'loss': 0.9103, 'learning_rate': 9.958399388786049e-05, 'epoch': 0.04}
{'loss': 0.9868, 'learning_rate': 9.958087627068376e-05, 'epoch': 0.04}
7/6500 [48:38<18:20:01, 10.59s/it]                                                       4%|         | 267/6500 [48:38<18:20:01, 10.59s/it]  4%|         | 268/6500 [48:48<18:13:53, 10.53s/it]                                                       4%|         | 268/6500 [48:48<18:13:53, 10.53s/it]  4%|         | 269/6500 [48:58<18:09:31, 10.49s/it]                                                       4%|         | 269/6500 [48:58<18:09:31, 10.49s/it]  4%|         | 270/6500 [49:09<18:06:24, 10.46s/it]                                                       4%|         | 270/6500 [49:09<18:06:24, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9686020016670227, 'eval_runtime': 3.9849, 'eval_samples_per_second': 5.772, 'eval_steps_per_second': 1.506, 'epoch': 0.04}
                                                       4%|         | 270/6500 [49:13<18:06:24, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-270I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-270

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-270/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9473, 'learning_rate': 9.957774706427867e-05, 'epoch': 0.04}
{'loss': 0.9466, 'learning_rate': 9.957460626937664e-05, 'epoch': 0.04}
{'loss': 0.9242, 'learning_rate': 9.957145388671181e-05, 'epoch': 0.04}
{'loss': 0.9697, 'learning_rate': 9.956828991702103e-05, 'epoch': 0.04}
{'loss': 0.9739, 'learning_rate': 9.956511436104385e-05, 'epoch': 0.04}
{'loss': 0.9546, 'learning_rate': 9.956192721952257e-05, 'epoch': 0.04}
  4%|         | 271/6500 [49:24<20:25:27, 11.80s/it]                                                       4%|         | 271/6500 [49:24<20:25:27, 11.80s/it]  4%|         | 272/6500 [49:34<19:41:46, 11.39s/it]                                                       4%|         | 272/6500 [49:34<19:41:46, 11.39s/it]  4%|         | 273/6500 [49:45<19:10:58, 11.09s/it]                                                       4%|         | 273/6500 [49:45<19:10:58, 11.09s/it]  4%|         | 274/6500 [49:55<18:49:22, 10.88s/it]                                                       4%|         | 274/6500 [49:55<18:49:22, 10.88s/it]  4%|         | 275/6500 [50:06<18:41:30, 10.81s/it]                                                       4%|         | 275/6500 [50:06<18:41:30, 10.81s/it]  4%|         | 276/6500 [50:16<18:28:41, 10.69s/it]                                                       4%|         | 276/6500 [50:16<18:28:41, 10.69s/it]  4%|         | 27{'loss': 0.9276, 'learning_rate': 9.955872849320213e-05, 'epoch': 0.04}
{'loss': 0.9516, 'learning_rate': 9.955551818283024e-05, 'epoch': 0.04}
{'loss': 1.4247, 'learning_rate': 9.955229628915727e-05, 'epoch': 0.04}
{'loss': 0.9572, 'learning_rate': 9.954906281293634e-05, 'epoch': 0.04}
7/6500 [50:26<18:20:23, 10.61s/it]                                                       4%|         | 277/6500 [50:26<18:20:23, 10.61s/it]  4%|         | 278/6500 [50:37<18:13:44, 10.55s/it]                                                       4%|         | 278/6500 [50:37<18:13:44, 10.55s/it]  4%|         | 279/6500 [50:47<18:13:44, 10.55s/it]                                                       4%|         | 279/6500 [50:47<18:13:44, 10.55s/it]  4%|         | 280/6500 [50:58<18:08:55, 10.50s/it]                                                       4%|         | 280/6500 [50:58<18:08:55, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.965909481048584, 'eval_runtime': 4.2342, 'eval_samples_per_second': 5.432, 'eval_steps_per_second': 1.417, 'epoch': 0.04}
                                                       4%|         | 280/6500 [51:02<18:08:55, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-280
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-280
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9169, 'learning_rate': 9.954581775492322e-05, 'epoch': 0.04}
{'loss': 0.889, 'learning_rate': 9.954256111587645e-05, 'epoch': 0.04}
{'loss': 0.926, 'learning_rate': 9.953929289655724e-05, 'epoch': 0.04}
{'loss': 0.9821, 'learning_rate': 9.953601309772953e-05, 'epoch': 0.04}
{'loss': 0.9175, 'learning_rate': 9.953272172015992e-05, 'epoch': 0.04}
{'loss': 0.9282, 'learning_rate': 9.952941876461779e-05, 'epoch': 0.04}
  4%|         | 281/6500 [51:13<20:31:02, 11.88s/it]                                                       4%|         | 281/6500 [51:13<20:31:02, 11.88s/it]  4%|         | 282/6500 [51:23<19:45:30, 11.44s/it]                                                       4%|         | 282/6500 [51:23<19:45:30, 11.44s/it]  4%|         | 283/6500 [51:34<19:12:18, 11.12s/it]                                                       4%|         | 283/6500 [51:34<19:12:18, 11.12s/it]  4%|         | 284/6500 [51:44<18:49:53, 10.91s/it]                                                       4%|         | 284/6500 [51:44<18:49:53, 10.91s/it]  4%|         | 285/6500 [51:54<18:33:28, 10.75s/it]                                                       4%|         | 285/6500 [51:54<18:33:28, 10.75s/it]  4%|         | 286/6500 [52:05<18:23:00, 10.65s/it]                                                       4%|         | 286/6500 [52:05<18:23:00, 10.65s/it]  4%|         | 28{'loss': 0.898, 'learning_rate': 9.952610423187516e-05, 'epoch': 0.04}
{'loss': 0.9253, 'learning_rate': 9.952277812270681e-05, 'epoch': 0.04}
{'loss': 0.8991, 'learning_rate': 9.951944043789016e-05, 'epoch': 0.04}
{'loss': 0.9418, 'learning_rate': 9.951609117820538e-05, 'epoch': 0.04}
7/6500 [52:15<18:15:00, 10.57s/it]                                                       4%|         | 287/6500 [52:15<18:15:00, 10.57s/it]  4%|         | 288/6500 [52:26<18:09:31, 10.52s/it]                                                       4%|         | 288/6500 [52:26<18:09:31, 10.52s/it]  4%|         | 289/6500 [52:36<18:05:54, 10.49s/it]                                                       4%|         | 289/6500 [52:36<18:05:54, 10.49s/it]  4%|         | 290/6500 [52:46<18:03:04, 10.46s/it]                                                       4%|         | 290/6500 [52:46<18:03:04, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9620429873466492, 'eval_runtime': 4.0122, 'eval_samples_per_second': 5.733, 'eval_steps_per_second': 1.495, 'epoch': 0.04}
                                                       4%|         | 290/6500 [52:51<18:03:04, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-290
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-290

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-290/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9886, 'learning_rate': 9.951273034443537e-05, 'epoch': 0.04}
{'loss': 0.949, 'learning_rate': 9.950935793736567e-05, 'epoch': 0.04}
{'loss': 0.9539, 'learning_rate': 9.950597395778458e-05, 'epoch': 0.05}
{'loss': 0.95, 'learning_rate': 9.950257840648307e-05, 'epoch': 0.05}
{'loss': 0.9395, 'learning_rate': 9.949917128425485e-05, 'epoch': 0.05}
{'loss': 0.9499, 'learning_rate': 9.94957525918963e-05, 'epoch': 0.05}
  4%|         | 291/6500 [53:02<20:34:11, 11.93s/it]                                                       4%|         | 291/6500 [53:02<20:34:11, 11.93s/it]  4%|         | 292/6500 [53:12<19:48:16, 11.48s/it]                                                       4%|         | 292/6500 [53:12<19:48:16, 11.48s/it]  5%|         | 293/6500 [53:23<19:15:54, 11.17s/it]                                                       5%|         | 293/6500 [53:23<19:15:54, 11.17s/it]  5%|         | 294/6500 [53:33<18:55:37, 10.98s/it]                                                       5%|         | 294/6500 [53:33<18:55:37, 10.98s/it]  5%|         | 295/6500 [53:44<18:38:12, 10.81s/it]                                                       5%|         | 295/6500 [53:44<18:38:12, 10.81s/it]  5%|         | 296/6500 [53:54<18:27:08, 10.71s/it]                                                       5%|         | 296/6500 [53:54<18:27:08, 10.71s/it]  5%|         | 29{'loss': 0.9326, 'learning_rate': 9.949232233020653e-05, 'epoch': 0.05}
{'loss': 0.8748, 'learning_rate': 9.948888049998731e-05, 'epoch': 0.05}
{'loss': 0.9504, 'learning_rate': 9.948542710204319e-05, 'epoch': 0.05}
{'loss': 0.9515, 'learning_rate': 9.948196213718135e-05, 'epoch': 0.05}
7/6500 [54:05<18:18:48, 10.63s/it]                                                       5%|         | 297/6500 [54:05<18:18:48, 10.63s/it]  5%|         | 298/6500 [54:15<18:12:31, 10.57s/it]                                                       5%|         | 298/6500 [54:15<18:12:31, 10.57s/it]  5%|         | 299/6500 [54:25<18:08:44, 10.53s/it]                                                       5%|         | 299/6500 [54:25<18:08:44, 10.53s/it]  5%|         | 300/6500 [54:36<18:05:39, 10.51s/it]                                                       5%|         | 300/6500 [54:36<18:05:39, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9582197666168213, 'eval_runtime': 3.9939, 'eval_samples_per_second': 5.759, 'eval_steps_per_second': 1.502, 'epoch': 0.05}
                                                       5%|         | 300/6500 [54:40<18:05:39, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-300I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-300

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-300/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9172, 'learning_rate': 9.947848560621172e-05, 'epoch': 0.05}
{'loss': 0.9517, 'learning_rate': 9.94749975099469e-05, 'epoch': 0.05}
{'loss': 0.8767, 'learning_rate': 9.947149784920225e-05, 'epoch': 0.05}
{'loss': 0.9844, 'learning_rate': 9.946798662479577e-05, 'epoch': 0.05}
{'loss': 0.9483, 'learning_rate': 9.946446383754817e-05, 'epoch': 0.05}
{'loss': 0.9205, 'learning_rate': 9.946092948828289e-05, 'epoch': 0.05}
  5%|         | 301/6500 [54:51<20:22:30, 11.83s/it]                                                       5%|         | 301/6500 [54:51<20:22:30, 11.83s/it]  5%|         | 302/6500 [55:01<19:38:51, 11.41s/it]                                                       5%|         | 302/6500 [55:01<19:38:51, 11.41s/it]  5%|         | 303/6500 [55:12<19:08:26, 11.12s/it]                                                       5%|         | 303/6500 [55:12<19:08:26, 11.12s/it]  5%|         | 304/6500 [55:22<18:47:06, 10.91s/it]                                                       5%|         | 304/6500 [55:22<18:47:06, 10.91s/it]  5%|         | 305/6500 [55:33<18:32:29, 10.77s/it]                                                       5%|         | 305/6500 [55:33<18:32:29, 10.77s/it]  5%|         | 306/6500 [55:43<18:22:25, 10.68s/it]                                                       5%|         | 306/6500 [55:43<18:22:25, 10.68s/it]  5%|         | 30{'loss': 0.9357, 'learning_rate': 9.94573835778261e-05, 'epoch': 0.05}
{'loss': 0.9289, 'learning_rate': 9.945382610700657e-05, 'epoch': 0.05}
{'loss': 1.396, 'learning_rate': 9.94502570766559e-05, 'epoch': 0.05}
{'loss': 0.9208, 'learning_rate': 9.944667648760828e-05, 'epoch': 0.05}
7/6500 [55:54<18:23:13, 10.69s/it]                                                       5%|         | 307/6500 [55:54<18:23:13, 10.69s/it]  5%|         | 308/6500 [56:04<18:15:45, 10.62s/it]                                                       5%|         | 308/6500 [56:04<18:15:45, 10.62s/it]  5%|         | 309/6500 [56:15<18:10:37, 10.57s/it]                                                       5%|         | 309/6500 [56:15<18:10:37, 10.57s/it]  5%|         | 310/6500 [56:25<18:06:39, 10.53s/it]                                                       5%|         | 310/6500 [56:25<18:06:39, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9547909498214722, 'eval_runtime': 4.2648, 'eval_samples_per_second': 5.393, 'eval_steps_per_second': 1.407, 'epoch': 0.05}
                                                       5%|         | 310/6500 [56:29<18:06:39, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-310/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-310/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9286, 'learning_rate': 9.944308434070069e-05, 'epoch': 0.05}
{'loss': 0.8663, 'learning_rate': 9.943948063677274e-05, 'epoch': 0.05}
{'loss': 0.9333, 'learning_rate': 9.94358653766668e-05, 'epoch': 0.05}
{'loss': 0.9347, 'learning_rate': 9.943223856122788e-05, 'epoch': 0.05}
{'loss': 0.8962, 'learning_rate': 9.942860019130377e-05, 'epoch': 0.05}
{'loss': 0.9012, 'learning_rate': 9.942495026774489e-05, 'epoch': 0.05}
  5%|         | 311/6500 [56:40<20:31:40, 11.94s/it]                                                       5%|         | 311/6500 [56:40<20:31:40, 11.94s/it]  5%|         | 312/6500 [56:51<19:46:08, 11.50s/it]                                                       5%|         | 312/6500 [56:51<19:46:08, 11.50s/it]  5%|         | 313/6500 [57:01<19:12:59, 11.18s/it]                                                       5%|         | 313/6500 [57:01<19:12:59, 11.18s/it]  5%|         | 314/6500 [57:12<18:49:12, 10.95s/it]                                                       5%|         | 314/6500 [57:12<18:49:12, 10.95s/it]  5%|         | 315/6500 [57:22<18:32:28, 10.79s/it]                                                       5%|         | 315/6500 [57:22<18:32:28, 10.79s/it]  5%|         | 316/6500 [57:33<18:20:48, 10.68s/it]                                                       5%|         | 316/6500 [57:33<18:20:48, 10.68s/it]  5%|         | 31{'loss': 0.8937, 'learning_rate': 9.94212887914044e-05, 'epoch': 0.05}
{'loss': 0.9051, 'learning_rate': 9.941761576313812e-05, 'epoch': 0.05}
{'loss': 0.901, 'learning_rate': 9.941393118380466e-05, 'epoch': 0.05}
{'loss': 0.9344, 'learning_rate': 9.94102350542652e-05, 'epoch': 0.05}
7/6500 [57:43<18:12:52, 10.61s/it]                                                       5%|         | 317/6500 [57:43<18:12:52, 10.61s/it]  5%|         | 318/6500 [57:53<18:08:26, 10.56s/it]                                                       5%|         | 318/6500 [57:53<18:08:26, 10.56s/it]  5%|         | 319/6500 [58:04<18:04:31, 10.53s/it]                                                       5%|         | 319/6500 [58:04<18:04:31, 10.53s/it]  5%|         | 320/6500 [58:14<18:01:39, 10.50s/it]                                                       5%|         | 320/6500 [58:14<18:01:39, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9518771171569824, 'eval_runtime': 3.9909, 'eval_samples_per_second': 5.763, 'eval_steps_per_second': 1.503, 'epoch': 0.05}
                                                       5%|         | 320/6500 [58:18<18:01:39, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-320
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-320/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-320/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.928, 'learning_rate': 9.940652737538372e-05, 'epoch': 0.05}
{'loss': 0.9103, 'learning_rate': 9.940280814802685e-05, 'epoch': 0.05}
{'loss': 0.968, 'learning_rate': 9.939907737306397e-05, 'epoch': 0.05}
{'loss': 0.9459, 'learning_rate': 9.939533505136708e-05, 'epoch': 0.05}
{'loss': 0.9511, 'learning_rate': 9.939158118381098e-05, 'epoch': 0.05}
{'loss': 0.943, 'learning_rate': 9.938781577127306e-05, 'epoch': 0.05}
  5%|         | 321/6500 [58:29<20:17:07, 11.82s/it]                                                       5%|         | 321/6500 [58:29<20:17:07, 11.82s/it]  5%|         | 322/6500 [58:40<19:34:41, 11.41s/it]                                                       5%|         | 322/6500 [58:40<19:34:41, 11.41s/it]  5%|         | 323/6500 [58:50<19:04:47, 11.12s/it]                                                       5%|         | 323/6500 [58:50<19:04:47, 11.12s/it]  5%|         | 324/6500 [59:01<18:55:55, 11.04s/it]                                                       5%|         | 324/6500 [59:01<18:55:55, 11.04s/it]  5%|         | 325/6500 [59:11<18:37:06, 10.85s/it]                                                       5%|         | 325/6500 [59:11<18:37:06, 10.85s/it]  5%|         | 326/6500 [59:22<18:24:53, 10.74s/it]                                                       5%|         | 326/6500 [59:22<18:24:53, 10.74s/it]  5%|         | 32{'loss': 0.8936, 'learning_rate': 9.93840388146335e-05, 'epoch': 0.05}
{'loss': 0.8752, 'learning_rate': 9.938025031477512e-05, 'epoch': 0.05}
{'loss': 0.9476, 'learning_rate': 9.937645027258347e-05, 'epoch': 0.05}
{'loss': 0.9347, 'learning_rate': 9.937263868894678e-05, 'epoch': 0.05}
7/6500 [59:32<18:16:04, 10.65s/it]                                                       5%|         | 327/6500 [59:32<18:16:04, 10.65s/it]  5%|         | 328/6500 [59:43<18:09:35, 10.59s/it]                                                       5%|         | 328/6500 [59:43<18:09:35, 10.59s/it]  5%|         | 329/6500 [59:53<18:04:39, 10.55s/it]                                                       5%|         | 329/6500 [59:53<18:04:39, 10.55s/it]  5%|         | 330/6500 [1:00:04<18:02:07, 10.52s/it]                                                         5%|         | 330/6500 [1:00:04<18:02:07, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.947989284992218, 'eval_runtime': 3.9952, 'eval_samples_per_second': 5.757, 'eval_steps_per_second': 1.502, 'epoch': 0.05}
                                                         5%|         | 330/6500 [1:00:08<18:02:07, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-330
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-330/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9052, 'learning_rate': 9.936881556475599e-05, 'epoch': 0.05}
{'loss': 0.9066, 'learning_rate': 9.936498090090474e-05, 'epoch': 0.05}
{'loss': 0.8828, 'learning_rate': 9.936113469828933e-05, 'epoch': 0.05}
{'loss': 0.9897, 'learning_rate': 9.935727695780881e-05, 'epoch': 0.05}
{'loss': 0.9038, 'learning_rate': 9.93534076803649e-05, 'epoch': 0.05}
{'loss': 0.8977, 'learning_rate': 9.934952686686201e-05, 'epoch': 0.05}
  5%|         | 331/6500 [1:00:19<20:17:52, 11.85s/it]                                                         5%|         | 331/6500 [1:00:19<20:17:52, 11.85s/it]  5%|         | 332/6500 [1:00:29<19:34:13, 11.42s/it]                                                         5%|         | 332/6500 [1:00:29<19:34:13, 11.42s/it]  5%|         | 333/6500 [1:00:39<19:04:31, 11.14s/it]                                                         5%|         | 333/6500 [1:00:39<19:04:31, 11.14s/it]  5%|         | 334/6500 [1:00:50<18:44:05, 10.94s/it]                                                         5%|         | 334/6500 [1:00:50<18:44:05, 10.94s/it]  5%|         | 335/6500 [1:01:00<18:28:42, 10.79s/it]                                                         5%|         | 335/6500 [1:01:00<18:28:42, 10.79s/it]  5%|         | 336/6500 [1:01:11<18:18:25, 10.69s/it]                                                         5%|         | 336/6500 [1:01:11<18:18:{'loss': 0.932, 'learning_rate': 9.934563451820728e-05, 'epoch': 0.05}
{'loss': 1.4055, 'learning_rate': 9.93417306353105e-05, 'epoch': 0.05}
{'loss': 0.9078, 'learning_rate': 9.933781521908419e-05, 'epoch': 0.05}
{'loss': 0.8963, 'learning_rate': 9.933388827044355e-05, 'epoch': 0.05}
25, 10.69s/it]  5%|         | 337/6500 [1:01:21<18:11:34, 10.63s/it]                                                         5%|         | 337/6500 [1:01:21<18:11:34, 10.63s/it]  5%|         | 338/6500 [1:01:32<18:05:47, 10.57s/it]                                                         5%|         | 338/6500 [1:01:32<18:05:47, 10.57s/it]  5%|         | 339/6500 [1:01:42<18:02:01, 10.54s/it]                                                         5%|         | 339/6500 [1:01:42<18:02:01, 10.54s/it]  5%|         | 340/6500 [1:01:53<18:11:51, 10.63s/it]                                                         5%|         | 340/6500 [1:01:53<18:11:51, 10.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9458469152450562, 'eval_runtime': 4.2385, 'eval_samples_per_second': 5.426, 'eval_steps_per_second': 1.416, 'epoch': 0.05}
                                                         5%|         | 340/6500 [1:01:57<18:11:51, 10.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-340
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-340/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8973, 'learning_rate': 9.932994979030647e-05, 'epoch': 0.05}
{'loss': 0.8578, 'learning_rate': 9.932599977959356e-05, 'epoch': 0.05}
{'loss': 0.9364, 'learning_rate': 9.932203823922812e-05, 'epoch': 0.05}
{'loss': 0.9056, 'learning_rate': 9.931806517013612e-05, 'epoch': 0.05}
{'loss': 0.8816, 'learning_rate': 9.931408057324625e-05, 'epoch': 0.05}
{'loss': 0.8648, 'learning_rate': 9.931008444948988e-05, 'epoch': 0.05}
  5%|         | 341/6500 [1:02:08<20:30:24, 11.99s/it]                                                         5%|         | 341/6500 [1:02:08<20:30:24, 11.99s/it]  5%|         | 342/6500 [1:02:19<19:43:05, 11.53s/it]                                                         5%|         | 342/6500 [1:02:19<19:43:05, 11.53s/it]  5%|         | 343/6500 [1:02:29<19:09:08, 11.20s/it]                                                         5%|         | 343/6500 [1:02:29<19:09:08, 11.20s/it]  5%|         | 344/6500 [1:02:40<18:44:52, 10.96s/it]                                                         5%|         | 344/6500 [1:02:40<18:44:52, 10.96s/it]  5%|         | 345/6500 [1:02:50<18:29:12, 10.81s/it]                                                         5%|         | 345/6500 [1:02:50<18:29:12, 10.81s/it]  5%|         | 346/6500 [1:03:00<18:17:00, 10.70s/it]                                                         5%|         | 346/6500 [1:03:00<18:17:{'loss': 0.9032, 'learning_rate': 9.930607679980107e-05, 'epoch': 0.05}
{'loss': 0.8775, 'learning_rate': 9.93020576251166e-05, 'epoch': 0.05}
{'loss': 0.8747, 'learning_rate': 9.929802692637593e-05, 'epoch': 0.05}
{'loss': 0.9407, 'learning_rate': 9.929398470452118e-05, 'epoch': 0.05}
00, 10.70s/it]  5%|         | 347/6500 [1:03:11<18:08:45, 10.62s/it]                                                         5%|         | 347/6500 [1:03:11<18:08:45, 10.62s/it]  5%|         | 348/6500 [1:03:21<18:03:03, 10.56s/it]                                                         5%|         | 348/6500 [1:03:21<18:03:03, 10.56s/it]  5%|         | 349/6500 [1:03:32<17:58:52, 10.52s/it]                                                         5%|         | 349/6500 [1:03:32<17:58:52, 10.52s/it]  5%|         | 350/6500 [1:03:42<17:56:11, 10.50s/it]                                                         5%|         | 350/6500 [1:03:42<17:56:11, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.942787230014801, 'eval_runtime': 3.9855, 'eval_samples_per_second': 5.771, 'eval_steps_per_second': 1.505, 'epoch': 0.05}
                                                         5%|         | 350/6500 [1:03:46<17:56:11, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-350the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-350

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9183, 'learning_rate': 9.928993096049724e-05, 'epoch': 0.05}
{'loss': 0.9311, 'learning_rate': 9.928586569525162e-05, 'epoch': 0.05}
{'loss': 0.9203, 'learning_rate': 9.928178890973455e-05, 'epoch': 0.05}
{'loss': 0.9416, 'learning_rate': 9.927770060489897e-05, 'epoch': 0.05}
{'loss': 0.9047, 'learning_rate': 9.927360078170048e-05, 'epoch': 0.05}
{'loss': 0.9119, 'learning_rate': 9.92694894410974e-05, 'epoch': 0.05}
  5%|         | 351/6500 [1:03:57<20:10:30, 11.81s/it]                                                         5%|         | 351/6500 [1:03:57<20:10:30, 11.81s/it]  5%|         | 352/6500 [1:04:07<19:27:47, 11.40s/it]                                                         5%|         | 352/6500 [1:04:07<19:27:47, 11.40s/it]  5%|         | 353/6500 [1:04:18<18:57:53, 11.11s/it]                                                         5%|         | 353/6500 [1:04:18<18:57:53, 11.11s/it]  5%|         | 354/6500 [1:04:28<18:37:13, 10.91s/it]                                                         5%|         | 354/6500 [1:04:28<18:37:13, 10.91s/it]  5%|         | 355/6500 [1:04:39<18:22:24, 10.76s/it]                                                         5%|         | 355/6500 [1:04:39<18:22:24, 10.76s/it]  5%|         | 356/6500 [1:04:50<18:21:59, 10.76s/it]                                                         5%|         | 356/6500 [1:04:50<18:21:{'loss': 0.8736, 'learning_rate': 9.926536658405072e-05, 'epoch': 0.05}
{'loss': 0.8655, 'learning_rate': 9.926123221152415e-05, 'epoch': 0.06}
{'loss': 0.9459, 'learning_rate': 9.925708632448405e-05, 'epoch': 0.06}
{'loss': 0.8917, 'learning_rate': 9.925292892389953e-05, 'epoch': 0.06}
59, 10.76s/it]  5%|         | 357/6500 [1:05:00<18:14:29, 10.69s/it]                                                         5%|         | 357/6500 [1:05:00<18:14:29, 10.69s/it]  6%|         | 358/6500 [1:05:12<18:38:12, 10.92s/it]                                                         6%|         | 358/6500 [1:05:12<18:38:12, 10.92s/it]  6%|         | 359/6500 [1:05:22<18:22:54, 10.78s/it]                                                         6%|         | 359/6500 [1:05:22<18:22:54, 10.78s/it]  6%|         | 360/6500 [1:05:32<18:11:48, 10.67s/it]                                                         6%|         | 360/6500 [1:05:32<18:11:48, 10.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9384322762489319, 'eval_runtime': 4.1129, 'eval_samples_per_second': 5.592, 'eval_steps_per_second': 1.459, 'epoch': 0.06}
                                                         6%|         | 360/6500 [1:05:36<18:11:48, 10.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-360I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-360/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-360/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9131, 'learning_rate': 9.924876001074231e-05, 'epoch': 0.06}
{'loss': 0.859, 'learning_rate': 9.92445795859869e-05, 'epoch': 0.06}
{'loss': 0.9201, 'learning_rate': 9.924038765061042e-05, 'epoch': 0.06}
{'loss': 0.9442, 'learning_rate': 9.923618420559268e-05, 'epoch': 0.06}
{'loss': 0.897, 'learning_rate': 9.923196925191629e-05, 'epoch': 0.06}
{'loss': 0.8935, 'learning_rate': 9.922774279056639e-05, 'epoch': 0.06}
  6%|         | 361/6500 [1:05:47<20:28:05, 12.00s/it]                                                         6%|         | 361/6500 [1:05:47<20:28:05, 12.00s/it]  6%|         | 362/6500 [1:05:58<19:39:24, 11.53s/it]                                                         6%|         | 362/6500 [1:05:58<19:39:24, 11.53s/it]  6%|         | 363/6500 [1:06:08<19:05:51, 11.20s/it]                                                         6%|         | 363/6500 [1:06:08<19:05:51, 11.20s/it]  6%|         | 364/6500 [1:06:19<18:41:40, 10.97s/it]                                                         6%|         | 364/6500 [1:06:19<18:41:40, 10.97s/it]  6%|         | 365/6500 [1:06:29<18:24:45, 10.80s/it]                                                         6%|         | 365/6500 [1:06:29<18:24:45, 10.80s/it]  6%|         | 366/6500 [1:06:40<18:13:02, 10.69s/it]                                                         6%|         | 366/6500 [1:06:40<18:13:{'loss': 0.8961, 'learning_rate': 9.922350482253093e-05, 'epoch': 0.06}
{'loss': 1.3972, 'learning_rate': 9.921925534880051e-05, 'epoch': 0.06}
{'loss': 0.9081, 'learning_rate': 9.921499437036841e-05, 'epoch': 0.06}
{'loss': 0.8696, 'learning_rate': 9.92107218882306e-05, 'epoch': 0.06}
02, 10.69s/it]  6%|         | 367/6500 [1:06:50<18:06:15, 10.63s/it]                                                         6%|         | 367/6500 [1:06:50<18:06:15, 10.63s/it]  6%|         | 368/6500 [1:07:01<18:00:42, 10.57s/it]                                                         6%|         | 368/6500 [1:07:01<18:00:42, 10.57s/it]  6%|         | 369/6500 [1:07:11<17:56:31, 10.54s/it]                                                         6%|         | 369/6500 [1:07:11<17:56:31, 10.54s/it]  6%|         | 370/6500 [1:07:21<17:53:52, 10.51s/it]                                                         6%|         | 370/6500 [1:07:21<17:53:52, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9362258315086365, 'eval_runtime': 3.9765, 'eval_samples_per_second': 5.784, 'eval_steps_per_second': 1.509, 'epoch': 0.06}
                                                         6%|         | 370/6500 [1:07:25<17:53:52, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-370the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-370

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-370/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-370/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8448, 'learning_rate': 9.920643790338575e-05, 'epoch': 0.06}
{'loss': 0.876, 'learning_rate': 9.920214241683523e-05, 'epoch': 0.06}
{'loss': 0.9324, 'learning_rate': 9.919783542958308e-05, 'epoch': 0.06}
{'loss': 0.8826, 'learning_rate': 9.9193516942636e-05, 'epoch': 0.06}
{'loss': 0.869, 'learning_rate': 9.918918695700348e-05, 'epoch': 0.06}
{'loss': 0.8493, 'learning_rate': 9.918484547369755e-05, 'epoch': 0.06}
  6%|         | 371/6500 [1:07:36<20:10:35, 11.85s/it]                                                         6%|         | 371/6500 [1:07:36<20:10:35, 11.85s/it]  6%|         | 372/6500 [1:07:48<19:53:21, 11.68s/it]                                                         6%|         | 372/6500 [1:07:48<19:53:21, 11.68s/it]  6%|         | 373/6500 [1:07:58<19:13:58, 11.30s/it]                                                         6%|         | 373/6500 [1:07:58<19:13:58, 11.30s/it]  6%|         | 374/6500 [1:08:09<18:46:31, 11.03s/it]                                                         6%|         | 374/6500 [1:08:09<18:46:31, 11.03s/it]  6%|         | 375/6500 [1:08:19<18:27:36, 10.85s/it]                                                         6%|         | 375/6500 [1:08:19<18:27:36, 10.85s/it]  6%|         | 376/6500 [1:08:29<18:13:48, 10.72s/it]                                                         6%|         | 376/6500 [1:08:29<18:13:{'loss': 0.8789, 'learning_rate': 9.918049249373305e-05, 'epoch': 0.06}
{'loss': 0.8618, 'learning_rate': 9.917612801812744e-05, 'epoch': 0.06}
{'loss': 0.8847, 'learning_rate': 9.917175204790093e-05, 'epoch': 0.06}
{'loss': 0.9349, 'learning_rate': 9.916736458407632e-05, 'epoch': 0.06}
48, 10.72s/it]  6%|         | 377/6500 [1:08:40<18:04:22, 10.63s/it]                                                         6%|         | 377/6500 [1:08:40<18:04:22, 10.63s/it]  6%|         | 378/6500 [1:08:50<17:57:25, 10.56s/it]                                                         6%|         | 378/6500 [1:08:50<17:57:25, 10.56s/it]  6%|         | 379/6500 [1:09:01<17:53:27, 10.52s/it]                                                         6%|         | 379/6500 [1:09:01<17:53:27, 10.52s/it]  6%|         | 380/6500 [1:09:11<17:50:03, 10.49s/it]                                                         6%|         | 380/6500 [1:09:11<17:50:03, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9330518245697021, 'eval_runtime': 3.966, 'eval_samples_per_second': 5.799, 'eval_steps_per_second': 1.513, 'epoch': 0.06}
                                                         6%|         | 380/6500 [1:09:15<17:50:03, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-380the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-380

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.889, 'learning_rate': 9.91629656276792e-05, 'epoch': 0.06}
{'loss': 0.9184, 'learning_rate': 9.915855517973776e-05, 'epoch': 0.06}
{'loss': 0.909, 'learning_rate': 9.915413324128295e-05, 'epoch': 0.06}
{'loss': 0.8953, 'learning_rate': 9.914969981334834e-05, 'epoch': 0.06}
{'loss': 0.9066, 'learning_rate': 9.914525489697026e-05, 'epoch': 0.06}
{'loss': 0.8843, 'learning_rate': 9.914079849318764e-05, 'epoch': 0.06}
  6%|         | 381/6500 [1:09:26<20:03:16, 11.80s/it]                                                         6%|         | 381/6500 [1:09:26<20:03:16, 11.80s/it]  6%|         | 382/6500 [1:09:36<19:20:18, 11.38s/it]                                                         6%|         | 382/6500 [1:09:36<19:20:18, 11.38s/it]  6%|         | 383/6500 [1:09:47<18:50:41, 11.09s/it]                                                         6%|         | 383/6500 [1:09:47<18:50:41, 11.09s/it]  6%|         | 384/6500 [1:09:57<18:29:47, 10.89s/it]                                                         6%|         | 384/6500 [1:09:57<18:29:47, 10.89s/it]  6%|         | 385/6500 [1:10:08<18:14:52, 10.74s/it]                                                         6%|         | 385/6500 [1:10:08<18:14:52, 10.74s/it]  6%|         | 386/6500 [1:10:18<18:04:56, 10.65s/it]                                                         6%|         | 386/6500 [1:10:18<18:04:{'loss': 0.8603, 'learning_rate': 9.913633060304214e-05, 'epoch': 0.06}
{'loss': 0.8982, 'learning_rate': 9.913185122757814e-05, 'epoch': 0.06}
{'loss': 0.9063, 'learning_rate': 9.912736036784264e-05, 'epoch': 0.06}
{'loss': 0.8798, 'learning_rate': 9.912285802488534e-05, 'epoch': 0.06}
56, 10.65s/it]  6%|         | 387/6500 [1:10:28<17:57:28, 10.58s/it]                                                         6%|         | 387/6500 [1:10:28<17:57:28, 10.58s/it]  6%|         | 388/6500 [1:10:39<17:59:52, 10.60s/it]                                                         6%|         | 388/6500 [1:10:39<17:59:52, 10.60s/it]  6%|         | 389/6500 [1:10:49<17:54:22, 10.55s/it]                                                         6%|         | 389/6500 [1:10:49<17:54:22, 10.55s/it]  6%|         | 390/6500 [1:11:00<17:50:21, 10.51s/it]                                                         6%|         | 390/6500 [1:11:00<17:50:21, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9296810030937195, 'eval_runtime': 3.968, 'eval_samples_per_second': 5.796, 'eval_steps_per_second': 1.512, 'epoch': 0.06}
                                                         6%|         | 390/6500 [1:11:04<17:50:21, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-390the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-390

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-390
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8959, 'learning_rate': 9.911834419975866e-05, 'epoch': 0.06}
{'loss': 0.8485, 'learning_rate': 9.911381889351765e-05, 'epoch': 0.06}
{'loss': 0.941, 'learning_rate': 9.91092821072201e-05, 'epoch': 0.06}
{'loss': 0.9049, 'learning_rate': 9.910473384192647e-05, 'epoch': 0.06}
{'loss': 0.8641, 'learning_rate': 9.910017409869984e-05, 'epoch': 0.06}
{'loss': 0.9097, 'learning_rate': 9.909560287860606e-05, 'epoch': 0.06}
  6%|         | 391/6500 [1:11:15<20:01:55, 11.80s/it]                                                         6%|         | 391/6500 [1:11:15<20:01:55, 11.80s/it]  6%|         | 392/6500 [1:11:25<19:18:47, 11.38s/it]                                                         6%|         | 392/6500 [1:11:25<19:18:47, 11.38s/it]  6%|         | 393/6500 [1:11:36<18:53:27, 11.14s/it]                                                         6%|         | 393/6500 [1:11:36<18:53:27, 11.14s/it]  6%|         | 394/6500 [1:11:46<18:31:09, 10.92s/it]                                                         6%|         | 394/6500 [1:11:46<18:31:09, 10.92s/it]  6%|         | 395/6500 [1:11:57<18:16:40, 10.78s/it]                                                         6%|         | 395/6500 [1:11:57<18:16:40, 10.78s/it]  6%|         | 396/6500 [1:12:07<18:05:26, 10.67s/it]                                                         6%|         | 396/6500 [1:12:07<18:05:{'loss': 0.8719, 'learning_rate': 9.90910201827136e-05, 'epoch': 0.06}
{'loss': 1.392, 'learning_rate': 9.908642601209366e-05, 'epoch': 0.06}
{'loss': 0.8751, 'learning_rate': 9.908182036782009e-05, 'epoch': 0.06}
{'loss': 0.8721, 'learning_rate': 9.907720325096943e-05, 'epoch': 0.06}
26, 10.67s/it]  6%|         | 397/6500 [1:12:17<17:57:50, 10.60s/it]                                                         6%|         | 397/6500 [1:12:17<17:57:50, 10.60s/it]  6%|         | 398/6500 [1:12:28<17:54:19, 10.56s/it]                                                         6%|         | 398/6500 [1:12:28<17:54:19, 10.56s/it]  6%|         | 399/6500 [1:12:38<17:49:10, 10.51s/it]                                                         6%|         | 399/6500 [1:12:38<17:49:10, 10.51s/it]  6%|         | 400/6500 [1:12:49<17:45:36, 10.48s/it]                                                         6%|         | 400/6500 [1:12:49<17:45:36, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9279502630233765, 'eval_runtime': 3.9746, 'eval_samples_per_second': 5.787, 'eval_steps_per_second': 1.51, 'epoch': 0.06}
                                                         6%|         | 400/6500 [1:12:53<17:45:36, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-400 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-400

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-400/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8329, 'learning_rate': 9.90725746626209e-05, 'epoch': 0.06}
{'loss': 0.8861, 'learning_rate': 9.906793460385642e-05, 'epoch': 0.06}
{'loss': 0.9166, 'learning_rate': 9.906328307576056e-05, 'epoch': 0.06}
{'loss': 0.8237, 'learning_rate': 9.905862007942058e-05, 'epoch': 0.06}
{'loss': 0.864, 'learning_rate': 9.905394561592645e-05, 'epoch': 0.06}
{'loss': 0.8484, 'learning_rate': 9.904925968637078e-05, 'epoch': 0.06}
  6%|         | 401/6500 [1:13:04<19:59:12, 11.80s/it]                                                         6%|         | 401/6500 [1:13:04<19:59:12, 11.80s/it]  6%|         | 402/6500 [1:13:14<19:17:07, 11.39s/it]                                                         6%|         | 402/6500 [1:13:14<19:17:07, 11.39s/it]  6%|         | 403/6500 [1:13:24<18:47:47, 11.10s/it]                                                         6%|         | 403/6500 [1:13:24<18:47:47, 11.10s/it]  6%|         | 404/6500 [1:13:35<18:44:00, 11.06s/it]                                                         6%|         | 404/6500 [1:13:35<18:44:00, 11.06s/it]  6%|         | 405/6500 [1:13:46<18:24:11, 10.87s/it]                                                         6%|         | 405/6500 [1:13:46<18:24:11, 10.87s/it]  6%|         | 406/6500 [1:13:56<18:09:29, 10.73s/it]                                                         6%|         | 406/6500 [1:13:56<18:09:{'loss': 0.8501, 'learning_rate': 9.904456229184887e-05, 'epoch': 0.06}
{'loss': 0.869, 'learning_rate': 9.903985343345873e-05, 'epoch': 0.06}
{'loss': 0.883, 'learning_rate': 9.903513311230104e-05, 'epoch': 0.06}
{'loss': 0.8749, 'learning_rate': 9.90304013294791e-05, 'epoch': 0.06}
29, 10.73s/it]  6%|         | 407/6500 [1:14:07<17:59:27, 10.63s/it]                                                         6%|         | 407/6500 [1:14:07<17:59:27, 10.63s/it]  6%|         | 408/6500 [1:14:17<17:52:29, 10.56s/it]                                                         6%|         | 408/6500 [1:14:17<17:52:29, 10.56s/it]  6%|         | 409/6500 [1:14:27<17:47:10, 10.51s/it]                                                         6%|         | 409/6500 [1:14:27<17:47:10, 10.51s/it]  6%|         | 410/6500 [1:14:38<17:43:47, 10.48s/it]                                                         6%|         | 410/6500 [1:14:38<17:43:47, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9248113036155701, 'eval_runtime': 3.9649, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.06}
                                                         6%|         | 410/6500 [1:14:42<17:43:47, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-410 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-410

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-410/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8748, 'learning_rate': 9.902565808609896e-05, 'epoch': 0.06}
{'loss': 0.9085, 'learning_rate': 9.902090338326932e-05, 'epoch': 0.06}
{'loss': 0.9201, 'learning_rate': 9.901613722210158e-05, 'epoch': 0.06}
{'loss': 0.8747, 'learning_rate': 9.901135960370977e-05, 'epoch': 0.06}
{'loss': 0.8892, 'learning_rate': 9.900657052921066e-05, 'epoch': 0.06}
{'loss': 0.8586, 'learning_rate': 9.900176999972366e-05, 'epoch': 0.06}
  6%|         | 411/6500 [1:14:53<19:55:52, 11.78s/it]                                                         6%|         | 411/6500 [1:14:53<19:55:52, 11.78s/it]  6%|         | 412/6500 [1:15:03<19:13:23, 11.37s/it]                                                         6%|         | 412/6500 [1:15:03<19:13:23, 11.37s/it]  6%|         | 413/6500 [1:15:13<18:43:20, 11.07s/it]                                                         6%|         | 413/6500 [1:15:13<18:43:20, 11.07s/it]  6%|         | 414/6500 [1:15:24<18:22:16, 10.87s/it]                                                         6%|         | 414/6500 [1:15:24<18:22:16, 10.87s/it]  6%|         | 415/6500 [1:15:34<18:07:49, 10.73s/it]                                                         6%|         | 415/6500 [1:15:34<18:07:49, 10.73s/it]  6%|         | 416/6500 [1:15:45<17:57:41, 10.63s/it]                                                         6%|         | 416/6500 [1:15:45<17:57:{'loss': 0.8491, 'learning_rate': 9.899695801637085e-05, 'epoch': 0.06}
{'loss': 0.9008, 'learning_rate': 9.899213458027701e-05, 'epoch': 0.06}
{'loss': 0.8764, 'learning_rate': 9.898729969256958e-05, 'epoch': 0.06}
{'loss': 0.875, 'learning_rate': 9.89824533543787e-05, 'epoch': 0.06}
41, 10.63s/it]  6%|         | 417/6500 [1:15:55<17:50:11, 10.56s/it]                                                         6%|         | 417/6500 [1:15:55<17:50:11, 10.56s/it]  6%|         | 418/6500 [1:16:05<17:45:29, 10.51s/it]                                                         6%|         | 418/6500 [1:16:05<17:45:29, 10.51s/it]  6%|         | 419/6500 [1:16:16<17:42:16, 10.48s/it]                                                         6%|         | 419/6500 [1:16:16<17:42:16, 10.48s/it]  6%|         | 420/6500 [1:16:26<17:39:42, 10.46s/it]                                                         6%|         | 420/6500 [1:16:26<17:39:42, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9212849736213684, 'eval_runtime': 3.9716, 'eval_samples_per_second': 5.791, 'eval_steps_per_second': 1.511, 'epoch': 0.06}
                                                         6%|         | 420/6500 [1:16:30<17:39:42, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-420
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-420/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8541, 'learning_rate': 9.897759556683716e-05, 'epoch': 0.06}
{'loss': 0.8731, 'learning_rate': 9.897272633108046e-05, 'epoch': 0.06}
{'loss': 0.9358, 'learning_rate': 9.896784564824673e-05, 'epoch': 0.07}
{'loss': 0.8602, 'learning_rate': 9.896295351947681e-05, 'epoch': 0.07}
{'loss': 0.8616, 'learning_rate': 9.895804994591421e-05, 'epoch': 0.07}
{'loss': 0.8892, 'learning_rate': 9.89531349287051e-05, 'epoch': 0.07}
  6%|         | 421/6500 [1:16:41<20:00:17, 11.85s/it]                                                         6%|         | 421/6500 [1:16:41<20:00:17, 11.85s/it]  6%|         | 422/6500 [1:16:52<19:15:39, 11.41s/it]                                                         6%|         | 422/6500 [1:16:52<19:15:39, 11.41s/it]  7%|         | 423/6500 [1:17:02<18:45:01, 11.11s/it]                                                         7%|         | 423/6500 [1:17:02<18:45:01, 11.11s/it]  7%|         | 424/6500 [1:17:12<18:23:09, 10.89s/it]                                                         7%|         | 424/6500 [1:17:12<18:23:09, 10.89s/it]  7%|         | 425/6500 [1:17:23<18:07:49, 10.74s/it]                                                         7%|         | 425/6500 [1:17:23<18:07:49, 10.74s/it]  7%|         | 426/6500 [1:17:33<17:56:52, 10.64s/it]                                                         7%|         | 426/6500 [1:17:33<17:56:{'loss': 1.3614, 'learning_rate': 9.894820846899835e-05, 'epoch': 0.07}
{'loss': 0.8819, 'learning_rate': 9.894327056794547e-05, 'epoch': 0.07}
{'loss': 0.8456, 'learning_rate': 9.893832122670068e-05, 'epoch': 0.07}
{'loss': 0.8565, 'learning_rate': 9.893336044642085e-05, 'epoch': 0.07}
52, 10.64s/it]  7%|         | 427/6500 [1:17:44<17:49:01, 10.56s/it]                                                         7%|         | 427/6500 [1:17:44<17:49:01, 10.56s/it]  7%|         | 428/6500 [1:17:54<17:43:49, 10.51s/it]                                                         7%|         | 428/6500 [1:17:54<17:43:49, 10.51s/it]  7%|         | 429/6500 [1:18:04<17:40:01, 10.48s/it]                                                         7%|         | 429/6500 [1:18:04<17:40:01, 10.48s/it]  7%|         | 430/6500 [1:18:15<17:37:16, 10.45s/it]                                                         7%|         | 430/6500 [1:18:15<17:37:16, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9196316003799438, 'eval_runtime': 3.9822, 'eval_samples_per_second': 5.776, 'eval_steps_per_second': 1.507, 'epoch': 0.07}
                                                         7%|         | 430/6500 [1:18:19<17:37:16, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-430 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-430

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-430/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8137, 'learning_rate': 9.892838822826553e-05, 'epoch': 0.07}
{'loss': 0.8903, 'learning_rate': 9.892340457339695e-05, 'epoch': 0.07}
{'loss': 0.8582, 'learning_rate': 9.891840948298003e-05, 'epoch': 0.07}
{'loss': 0.8342, 'learning_rate': 9.89134029581823e-05, 'epoch': 0.07}
{'loss': 0.8275, 'learning_rate': 9.890838500017403e-05, 'epoch': 0.07}
{'loss': 0.8489, 'learning_rate': 9.890335561012815e-05, 'epoch': 0.07}
  7%|         | 431/6500 [1:18:30<19:49:54, 11.76s/it]                                                         7%|         | 431/6500 [1:18:30<19:49:54, 11.76s/it]  7%|         | 432/6500 [1:18:40<19:08:34, 11.36s/it]                                                         7%|         | 432/6500 [1:18:40<19:08:34, 11.36s/it]  7%|         | 433/6500 [1:18:50<18:39:08, 11.07s/it]                                                         7%|         | 433/6500 [1:18:50<18:39:08, 11.07s/it]  7%|         | 434/6500 [1:19:01<18:18:47, 10.87s/it]                                                         7%|         | 434/6500 [1:19:01<18:18:47, 10.87s/it]  7%|         | 435/6500 [1:19:11<18:04:16, 10.73s/it]                                                         7%|         | 435/6500 [1:19:11<18:04:16, 10.73s/it]  7%|         | 436/6500 [1:19:22<17:53:33, 10.62s/it]                                                         7%|         | 436/6500 [1:19:22<17:53:{'loss': 0.8412, 'learning_rate': 9.889831478922023e-05, 'epoch': 0.07}
{'loss': 0.8426, 'learning_rate': 9.889326253862852e-05, 'epoch': 0.07}
{'loss': 0.8999, 'learning_rate': 9.888819885953398e-05, 'epoch': 0.07}
{'loss': 0.8629, 'learning_rate': 9.888312375312019e-05, 'epoch': 0.07}
33, 10.62s/it]  7%|         | 437/6500 [1:19:33<18:04:52, 10.74s/it]                                                         7%|         | 437/6500 [1:19:33<18:04:52, 10.74s/it]  7%|         | 438/6500 [1:19:43<17:54:28, 10.63s/it]                                                         7%|         | 438/6500 [1:19:43<17:54:28, 10.63s/it]  7%|         | 439/6500 [1:19:53<17:48:47, 10.58s/it]                                                         7%|         | 439/6500 [1:19:53<17:48:47, 10.58s/it]  7%|         | 440/6500 [1:20:04<17:43:00, 10.52s/it]                                                         7%|         | 440/6500 [1:20:04<17:43:00, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9159906506538391, 'eval_runtime': 3.9468, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.07}
                                                         7%|         | 440/6500 [1:20:08<17:43:00, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-440
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8866, 'learning_rate': 9.887803722057344e-05, 'epoch': 0.07}
{'loss': 0.8903, 'learning_rate': 9.887293926308267e-05, 'epoch': 0.07}
{'loss': 0.8911, 'learning_rate': 9.886782988183952e-05, 'epoch': 0.07}
{'loss': 0.8668, 'learning_rate': 9.886270907803823e-05, 'epoch': 0.07}
{'loss': 0.8551, 'learning_rate': 9.88575768528758e-05, 'epoch': 0.07}
{'loss': 0.8151, 'learning_rate': 9.885243320755184e-05, 'epoch': 0.07}
  7%|         | 441/6500 [1:20:19<19:54:02, 11.82s/it]                                                         7%|         | 441/6500 [1:20:19<19:54:02, 11.82s/it]  7%|         | 442/6500 [1:20:29<19:10:51, 11.40s/it]                                                         7%|         | 442/6500 [1:20:29<19:10:51, 11.40s/it]  7%|         | 443/6500 [1:20:39<18:40:28, 11.10s/it]                                                         7%|         | 443/6500 [1:20:40<18:40:28, 11.10s/it]  7%|         | 444/6500 [1:20:50<18:18:36, 10.88s/it]                                                         7%|         | 444/6500 [1:20:50<18:18:36, 10.88s/it]  7%|         | 445/6500 [1:21:00<18:04:07, 10.74s/it]                                                         7%|         | 445/6500 [1:21:00<18:04:07, 10.74s/it]  7%|         | 446/6500 [1:21:11<17:53:28, 10.64s/it]                                                         7%|         | 446/6500 [1:21:11<17:53:{'loss': 0.8667, 'learning_rate': 9.884727814326864e-05, 'epoch': 0.07}
{'loss': 0.8808, 'learning_rate': 9.884211166123116e-05, 'epoch': 0.07}
{'loss': 0.8419, 'learning_rate': 9.883693376264707e-05, 'epoch': 0.07}
{'loss': 0.8649, 'learning_rate': 9.883174444872663e-05, 'epoch': 0.07}
28, 10.64s/it]  7%|         | 447/6500 [1:21:21<17:45:43, 10.56s/it]                                                         7%|         | 447/6500 [1:21:21<17:45:43, 10.56s/it]  7%|         | 448/6500 [1:21:31<17:40:22, 10.51s/it]                                                         7%|         | 448/6500 [1:21:31<17:40:22, 10.51s/it]  7%|         | 449/6500 [1:21:42<17:36:56, 10.48s/it]                                                         7%|         | 449/6500 [1:21:42<17:36:56, 10.48s/it]  7%|         | 450/6500 [1:21:52<17:34:40, 10.46s/it]                                                         7%|         | 450/6500 [1:21:52<17:34:40, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9135812520980835, 'eval_runtime': 3.961, 'eval_samples_per_second': 5.807, 'eval_steps_per_second': 1.515, 'epoch': 0.07}
                                                         7%|         | 450/6500 [1:21:56<17:34:40, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-450I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-450

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-450
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8295, 'learning_rate': 9.882654372068284e-05, 'epoch': 0.07}
{'loss': 0.8992, 'learning_rate': 9.88213315797313e-05, 'epoch': 0.07}
{'loss': 0.8989, 'learning_rate': 9.881610802709036e-05, 'epoch': 0.07}
{'loss': 0.8432, 'learning_rate': 9.881087306398097e-05, 'epoch': 0.07}
{'loss': 0.8652, 'learning_rate': 9.880562669162677e-05, 'epoch': 0.07}
{'loss': 0.8363, 'learning_rate': 9.880036891125409e-05, 'epoch': 0.07}
  7%|         | 451/6500 [1:22:07<19:47:39, 11.78s/it]                                                         7%|         | 451/6500 [1:22:07<19:47:39, 11.78s/it]  7%|         | 452/6500 [1:22:18<19:06:57, 11.38s/it]                                                         7%|         | 452/6500 [1:22:18<19:06:57, 11.38s/it]  7%|         | 453/6500 [1:22:28<18:43:46, 11.15s/it]                                                         7%|         | 453/6500 [1:22:28<18:43:46, 11.15s/it]  7%|         | 454/6500 [1:22:39<18:20:42, 10.92s/it]                                                         7%|         | 454/6500 [1:22:39<18:20:42, 10.92s/it]  7%|         | 455/6500 [1:22:49<18:04:45, 10.77s/it]                                                         7%|         | 455/6500 [1:22:49<18:04:45, 10.77s/it]  7%|         | 456/6500 [1:22:59<17:54:11, 10.66s/it]                                                         7%|         | 456/6500 [1:22:59<17:54:{'loss': 1.3486, 'learning_rate': 9.879509972409188e-05, 'epoch': 0.07}
{'loss': 0.8606, 'learning_rate': 9.878981913137179e-05, 'epoch': 0.07}
{'loss': 0.8502, 'learning_rate': 9.878452713432813e-05, 'epoch': 0.07}
{'loss': 0.791, 'learning_rate': 9.877922373419786e-05, 'epoch': 0.07}
11, 10.66s/it]  7%|         | 457/6500 [1:23:10<17:46:13, 10.59s/it]                                                         7%|         | 457/6500 [1:23:10<17:46:13, 10.59s/it]  7%|         | 458/6500 [1:23:20<17:40:38, 10.53s/it]                                                         7%|         | 458/6500 [1:23:20<17:40:38, 10.53s/it]  7%|         | 459/6500 [1:23:31<17:37:00, 10.50s/it]                                                         7%|         | 459/6500 [1:23:31<17:37:00, 10.50s/it]  7%|         | 460/6500 [1:23:41<17:34:08, 10.47s/it]                                                         7%|         | 460/6500 [1:23:41<17:34:08, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9119995832443237, 'eval_runtime': 3.9832, 'eval_samples_per_second': 5.774, 'eval_steps_per_second': 1.506, 'epoch': 0.07}
                                                         7%|         | 460/6500 [1:23:45<17:34:08, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-460
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-460

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-460
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8409, 'learning_rate': 9.877390893222061e-05, 'epoch': 0.07}
{'loss': 0.8982, 'learning_rate': 9.876858272963871e-05, 'epoch': 0.07}
{'loss': 0.823, 'learning_rate': 9.876324512769713e-05, 'epoch': 0.07}
{'loss': 0.8342, 'learning_rate': 9.875789612764346e-05, 'epoch': 0.07}
{'loss': 0.8175, 'learning_rate': 9.875253573072804e-05, 'epoch': 0.07}
{'loss': 0.8326, 'learning_rate': 9.874716393820383e-05, 'epoch': 0.07}
  7%|         | 461/6500 [1:23:56<19:46:52, 11.79s/it]                                                         7%|         | 461/6500 [1:23:56<19:46:52, 11.79s/it]  7%|         | 462/6500 [1:24:06<19:04:46, 11.38s/it]                                                         7%|         | 462/6500 [1:24:06<19:04:46, 11.38s/it]  7%|         | 463/6500 [1:24:17<18:35:10, 11.08s/it]                                                         7%|         | 463/6500 [1:24:17<18:35:10, 11.08s/it]  7%|         | 464/6500 [1:24:27<18:14:39, 10.88s/it]                                                         7%|         | 464/6500 [1:24:27<18:14:39, 10.88s/it]  7%|         | 465/6500 [1:24:38<17:59:50, 10.74s/it]                                                         7%|         | 465/6500 [1:24:38<17:59:50, 10.74s/it]  7%|         | 466/6500 [1:24:48<17:49:52, 10.64s/it]                                                         7%|         | 466/6500 [1:24:48<17:49:{'loss': 0.8287, 'learning_rate': 9.87417807513264e-05, 'epoch': 0.07}
{'loss': 0.851, 'learning_rate': 9.87363861713541e-05, 'epoch': 0.07}
{'loss': 0.8878, 'learning_rate': 9.873098019954786e-05, 'epoch': 0.07}
{'loss': 0.8303, 'learning_rate': 9.872556283717125e-05, 'epoch': 0.07}
52, 10.64s/it]  7%|         | 467/6500 [1:24:58<17:42:50, 10.57s/it]                                                         7%|         | 467/6500 [1:24:58<17:42:50, 10.57s/it]  7%|         | 468/6500 [1:25:09<17:37:21, 10.52s/it]                                                         7%|         | 468/6500 [1:25:09<17:37:21, 10.52s/it]  7%|         | 469/6500 [1:25:20<17:45:08, 10.60s/it]                                                         7%|         | 469/6500 [1:25:20<17:45:08, 10.60s/it]  7%|         | 470/6500 [1:25:30<17:39:11, 10.54s/it]                                                         7%|         | 470/6500 [1:25:30<17:39:11, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9090346693992615, 'eval_runtime': 4.2015, 'eval_samples_per_second': 5.474, 'eval_steps_per_second': 1.428, 'epoch': 0.07}
                                                         7%|         | 470/6500 [1:25:34<17:39:11, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-470I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-470

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-470/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-470/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9011, 'learning_rate': 9.872013408549061e-05, 'epoch': 0.07}
{'loss': 0.8592, 'learning_rate': 9.871469394577484e-05, 'epoch': 0.07}
{'loss': 0.8565, 'learning_rate': 9.870924241929558e-05, 'epoch': 0.07}
{'loss': 0.8621, 'learning_rate': 9.870377950732703e-05, 'epoch': 0.07}
{'loss': 0.8263, 'learning_rate': 9.869830521114616e-05, 'epoch': 0.07}
{'loss': 0.8362, 'learning_rate': 9.869281953203254e-05, 'epoch': 0.07}
  7%|         | 471/6500 [1:25:45<19:56:30, 11.91s/it]                                                         7%|         | 471/6500 [1:25:45<19:56:30, 11.91s/it]  7%|         | 472/6500 [1:25:56<19:12:42, 11.47s/it]                                                         7%|         | 472/6500 [1:25:56<19:12:42, 11.47s/it]  7%|         | 473/6500 [1:26:06<18:39:53, 11.15s/it]                                                         7%|         | 473/6500 [1:26:06<18:39:53, 11.15s/it]  7%|         | 474/6500 [1:26:16<18:17:08, 10.92s/it]                                                         7%|         | 474/6500 [1:26:16<18:17:08, 10.92s/it]  7%|         | 475/6500 [1:26:27<18:01:16, 10.77s/it]                                                         7%|         | 475/6500 [1:26:27<18:01:16, 10.77s/it]  7%|         | 476/6500 [1:26:37<17:49:28, 10.65s/it]                                                         7%|         | 476/6500 [1:26:37<17:49:{'loss': 0.844, 'learning_rate': 9.86873224712684e-05, 'epoch': 0.07}
{'loss': 0.8647, 'learning_rate': 9.868181403013865e-05, 'epoch': 0.07}
{'loss': 0.8443, 'learning_rate': 9.867629420993086e-05, 'epoch': 0.07}
{'loss': 0.8392, 'learning_rate': 9.867076301193528e-05, 'epoch': 0.07}
28, 10.65s/it]  7%|         | 477/6500 [1:26:47<17:41:25, 10.57s/it]                                                         7%|         | 477/6500 [1:26:47<17:41:25, 10.57s/it]  7%|         | 478/6500 [1:26:58<17:37:25, 10.54s/it]                                                         7%|         | 478/6500 [1:26:58<17:37:25, 10.54s/it]  7%|         | 479/6500 [1:27:08<17:33:11, 10.50s/it]                                                         7%|         | 479/6500 [1:27:08<17:33:11, 10.50s/it]  7%|         | 480/6500 [1:27:19<17:30:23, 10.47s/it]                                                         7%|         | 480/6500 [1:27:19<17:30:23, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9056978821754456, 'eval_runtime': 3.9555, 'eval_samples_per_second': 5.815, 'eval_steps_per_second': 1.517, 'epoch': 0.07}
                                                         7%|         | 480/6500 [1:27:23<17:30:23, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-480I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8268, 'learning_rate': 9.866522043744475e-05, 'epoch': 0.07}
{'loss': 0.9351, 'learning_rate': 9.865966648775483e-05, 'epoch': 0.07}
{'loss': 0.8333, 'learning_rate': 9.865410116416374e-05, 'epoch': 0.07}
{'loss': 0.8155, 'learning_rate': 9.86485244679723e-05, 'epoch': 0.07}
{'loss': 0.8615, 'learning_rate': 9.864293640048407e-05, 'epoch': 0.07}
{'loss': 1.3275, 'learning_rate': 9.863733696300521e-05, 'epoch': 0.07}
  7%|         | 481/6500 [1:27:34<19:46:01, 11.82s/it]                                                         7%|         | 481/6500 [1:27:34<19:46:01, 11.82s/it]  7%|         | 482/6500 [1:27:44<19:03:05, 11.40s/it]                                                         7%|         | 482/6500 [1:27:44<19:03:05, 11.40s/it]  7%|         | 483/6500 [1:27:55<18:32:53, 11.10s/it]                                                         7%|         | 483/6500 [1:27:55<18:32:53, 11.10s/it]  7%|         | 484/6500 [1:28:05<18:12:00, 10.89s/it]                                                         7%|         | 484/6500 [1:28:05<18:12:00, 10.89s/it]  7%|         | 485/6500 [1:28:16<18:03:27, 10.81s/it]                                                         7%|         | 485/6500 [1:28:16<18:03:27, 10.81s/it]  7%|         | 486/6500 [1:28:26<17:51:13, 10.69s/it]                                                         7%|         | 486/6500 [1:28:26<17:51:{'loss': 0.8534, 'learning_rate': 9.863172615684455e-05, 'epoch': 0.07}
{'loss': 0.8482, 'learning_rate': 9.86261039833136e-05, 'epoch': 0.08}
{'loss': 0.8249, 'learning_rate': 9.862047044372648e-05, 'epoch': 0.08}
{'loss': 0.7812, 'learning_rate': 9.861482553940003e-05, 'epoch': 0.08}
13, 10.69s/it]  7%|         | 487/6500 [1:28:36<17:42:54, 10.61s/it]                                                         7%|         | 487/6500 [1:28:36<17:42:54, 10.61s/it]  8%|         | 488/6500 [1:28:47<17:36:28, 10.54s/it]                                                         8%|         | 488/6500 [1:28:47<17:36:28, 10.54s/it]  8%|         | 489/6500 [1:28:57<17:31:33, 10.50s/it]                                                         8%|         | 489/6500 [1:28:57<17:31:33, 10.50s/it]  8%|         | 490/6500 [1:29:08<17:28:47, 10.47s/it]                                                         8%|         | 490/6500 [1:29:08<17:28:47, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9035063982009888, 'eval_runtime': 3.9508, 'eval_samples_per_second': 5.822, 'eval_steps_per_second': 1.519, 'epoch': 0.08}
                                                         8%|         | 490/6500 [1:29:12<17:28:47, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-490/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-490/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.868, 'learning_rate': 9.860916927165366e-05, 'epoch': 0.08}
{'loss': 0.8563, 'learning_rate': 9.860350164180954e-05, 'epoch': 0.08}
{'loss': 0.793, 'learning_rate': 9.859782265119244e-05, 'epoch': 0.08}
{'loss': 0.8164, 'learning_rate': 9.859213230112976e-05, 'epoch': 0.08}
{'loss': 0.8084, 'learning_rate': 9.85864305929516e-05, 'epoch': 0.08}
{'loss': 0.8201, 'learning_rate': 9.85807175279907e-05, 'epoch': 0.08}
  8%|         | 491/6500 [1:29:22<19:40:48, 11.79s/it]                                                         8%|         | 491/6500 [1:29:22<19:40:48, 11.79s/it]  8%|         | 492/6500 [1:29:33<18:58:37, 11.37s/it]                                                         8%|         | 492/6500 [1:29:33<18:58:37, 11.37s/it]  8%|         | 493/6500 [1:29:43<18:28:52, 11.08s/it]                                                         8%|         | 493/6500 [1:29:43<18:28:52, 11.08s/it]  8%|         | 494/6500 [1:29:54<18:09:19, 10.88s/it]                                                         8%|         | 494/6500 [1:29:54<18:09:19, 10.88s/it]  8%|         | 495/6500 [1:30:04<17:54:29, 10.74s/it]                                                         8%|         | 495/6500 [1:30:04<17:54:29, 10.74s/it]  8%|         | 496/6500 [1:30:14<17:43:29, 10.63s/it]                                                         8%|         | 496/6500 [1:30:14<17:43:{'loss': 0.8089, 'learning_rate': 9.857499310758245e-05, 'epoch': 0.08}
{'loss': 0.8481, 'learning_rate': 9.85692573330649e-05, 'epoch': 0.08}
{'loss': 0.8313, 'learning_rate': 9.856351020577876e-05, 'epoch': 0.08}
{'loss': 0.8448, 'learning_rate': 9.855775172706738e-05, 'epoch': 0.08}
29, 10.63s/it]  8%|         | 497/6500 [1:30:25<17:36:06, 10.56s/it]                                                         8%|         | 497/6500 [1:30:25<17:36:06, 10.56s/it]  8%|         | 498/6500 [1:30:35<17:31:05, 10.51s/it]                                                         8%|         | 498/6500 [1:30:35<17:31:05, 10.51s/it]  8%|         | 499/6500 [1:30:46<17:26:59, 10.47s/it]                                                         8%|         | 499/6500 [1:30:46<17:26:59, 10.47s/it]  8%|         | 500/6500 [1:30:56<17:25:14, 10.45s/it]                                                         8%|         | 500/6500 [1:30:56<17:25:14, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9028367400169373, 'eval_runtime': 3.9594, 'eval_samples_per_second': 5.809, 'eval_steps_per_second': 1.515, 'epoch': 0.08}
                                                         8%|         | 500/6500 [1:31:00<17:25:14, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8763, 'learning_rate': 9.855198189827677e-05, 'epoch': 0.08}
{'loss': 0.8801, 'learning_rate': 9.85462007207556e-05, 'epoch': 0.08}
{'loss': 0.842, 'learning_rate': 9.854040819585517e-05, 'epoch': 0.08}
{'loss': 0.8497, 'learning_rate': 9.853460432492944e-05, 'epoch': 0.08}
{'loss': 0.8129, 'learning_rate': 9.852878910933507e-05, 'epoch': 0.08}
{'loss': 0.8018, 'learning_rate': 9.852296255043129e-05, 'epoch': 0.08}
  8%|         | 501/6500 [1:31:11<19:54:44, 11.95s/it]                                                         8%|         | 501/6500 [1:31:11<19:54:44, 11.95s/it]  8%|         | 502/6500 [1:31:22<19:10:01, 11.50s/it]                                                         8%|         | 502/6500 [1:31:22<19:10:01, 11.50s/it]  8%|         | 503/6500 [1:31:32<18:38:00, 11.19s/it]                                                         8%|         | 503/6500 [1:31:32<18:38:00, 11.19s/it]  8%|         | 504/6500 [1:31:43<18:16:21, 10.97s/it]                                                         8%|         | 504/6500 [1:31:43<18:16:21, 10.97s/it]  8%|         | 505/6500 [1:31:53<18:01:04, 10.82s/it]                                                         8%|         | 505/6500 [1:31:53<18:01:04, 10.82s/it]  8%|         | 506/6500 [1:32:04<17:50:17, 10.71s/it]                                                         8%|         | 506/6500 [1:32:04<17:50:{'loss': 0.8625, 'learning_rate': 9.851712464958005e-05, 'epoch': 0.08}
{'loss': 0.8392, 'learning_rate': 9.85112754081459e-05, 'epoch': 0.08}
{'loss': 0.8319, 'learning_rate': 9.850541482749608e-05, 'epoch': 0.08}
{'loss': 0.8062, 'learning_rate': 9.849954290900046e-05, 'epoch': 0.08}
17, 10.71s/it]  8%|         | 507/6500 [1:32:14<17:42:33, 10.64s/it]                                                         8%|         | 507/6500 [1:32:14<17:42:33, 10.64s/it]  8%|         | 508/6500 [1:32:25<17:37:16, 10.59s/it]                                                         8%|         | 508/6500 [1:32:25<17:37:16, 10.59s/it]  8%|         | 509/6500 [1:32:35<17:33:31, 10.55s/it]                                                         8%|         | 509/6500 [1:32:35<17:33:31, 10.55s/it]  8%|         | 510/6500 [1:32:46<17:30:48, 10.53s/it]                                                         8%|         | 510/6500 [1:32:46<17:30:48, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8985500931739807, 'eval_runtime': 4.079, 'eval_samples_per_second': 5.639, 'eval_steps_per_second': 1.471, 'epoch': 0.08}
                                                         8%|         | 510/6500 [1:32:50<17:30:48, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-510I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-510/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8494, 'learning_rate': 9.849365965403157e-05, 'epoch': 0.08}
{'loss': 0.871, 'learning_rate': 9.848776506396458e-05, 'epoch': 0.08}
{'loss': 0.8411, 'learning_rate': 9.848185914017733e-05, 'epoch': 0.08}
{'loss': 0.8161, 'learning_rate': 9.847594188405027e-05, 'epoch': 0.08}
{'loss': 0.8414, 'learning_rate': 9.847001329696653e-05, 'epoch': 0.08}
{'loss': 1.3159, 'learning_rate': 9.846407338031189e-05, 'epoch': 0.08}
  8%|         | 511/6500 [1:33:01<19:46:05, 11.88s/it]                                                         8%|         | 511/6500 [1:33:01<19:46:05, 11.88s/it]  8%|         | 512/6500 [1:33:11<19:03:47, 11.46s/it]                                                         8%|         | 512/6500 [1:33:11<19:03:47, 11.46s/it]  8%|         | 513/6500 [1:33:22<18:33:40, 11.16s/it]                                                         8%|         | 513/6500 [1:33:22<18:33:40, 11.16s/it]  8%|         | 514/6500 [1:33:32<18:12:10, 10.95s/it]                                                         8%|         | 514/6500 [1:33:32<18:12:10, 10.95s/it]  8%|         | 515/6500 [1:33:43<17:57:33, 10.80s/it]                                                         8%|         | 515/6500 [1:33:43<17:57:33, 10.80s/it]  8%|         | 516/6500 [1:33:53<17:47:28, 10.70s/it]                                                         8%|         | 516/6500 [1:33:53<17:47:{'loss': 0.8503, 'learning_rate': 9.845812213547475e-05, 'epoch': 0.08}
{'loss': 0.799, 'learning_rate': 9.84521595638462e-05, 'epoch': 0.08}
{'loss': 0.786, 'learning_rate': 9.844618566681996e-05, 'epoch': 0.08}
{'loss': 0.7998, 'learning_rate': 9.844020044579237e-05, 'epoch': 0.08}
28, 10.70s/it]  8%|         | 517/6500 [1:34:03<17:40:11, 10.63s/it]                                                         8%|         | 517/6500 [1:34:03<17:40:11, 10.63s/it]  8%|         | 518/6500 [1:34:14<17:46:24, 10.70s/it]                                                         8%|         | 518/6500 [1:34:14<17:46:24, 10.70s/it]  8%|         | 519/6500 [1:34:25<17:39:04, 10.62s/it]                                                         8%|         | 519/6500 [1:34:25<17:39:04, 10.62s/it]  8%|         | 520/6500 [1:34:37<18:16:58, 11.01s/it]                                                         8%|         | 520/6500 [1:34:37<18:16:58, 11.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8955709934234619, 'eval_runtime': 4.0859, 'eval_samples_per_second': 5.629, 'eval_steps_per_second': 1.468, 'epoch': 0.08}
                                                         8%|         | 520/6500 [1:34:41<18:16:58, 11.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-520
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-520

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-520
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8672, 'learning_rate': 9.843420390216242e-05, 'epoch': 0.08}
{'loss': 0.8131, 'learning_rate': 9.842819603733182e-05, 'epoch': 0.08}
{'loss': 0.8147, 'learning_rate': 9.842217685270484e-05, 'epoch': 0.08}
{'loss': 0.7725, 'learning_rate': 9.841614634968843e-05, 'epoch': 0.08}
{'loss': 0.8201, 'learning_rate': 9.84101045296922e-05, 'epoch': 0.08}
{'loss': 0.7953, 'learning_rate': 9.840405139412836e-05, 'epoch': 0.08}
  8%|         | 521/6500 [1:34:52<20:19:43, 12.24s/it]                                                         8%|         | 521/6500 [1:34:52<20:19:43, 12.24s/it]  8%|         | 522/6500 [1:35:02<19:26:30, 11.71s/it]                                                         8%|         | 522/6500 [1:35:02<19:26:30, 11.71s/it]  8%|         | 523/6500 [1:35:13<18:49:23, 11.34s/it]                                                         8%|         | 523/6500 [1:35:13<18:49:23, 11.34s/it]  8%|         | 524/6500 [1:35:23<18:23:05, 11.08s/it]                                                         8%|         | 524/6500 [1:35:23<18:23:05, 11.08s/it]  8%|         | 525/6500 [1:35:34<18:04:34, 10.89s/it]                                                         8%|         | 525/6500 [1:35:34<18:04:34, 10.89s/it]  8%|         | 526/6500 [1:35:44<17:51:37, 10.76s/it]                                                         8%|         | 526/6500 [1:35:44<17:51:{'loss': 0.8125, 'learning_rate': 9.83979869444118e-05, 'epoch': 0.08}
{'loss': 0.867, 'learning_rate': 9.839191118196007e-05, 'epoch': 0.08}
{'loss': 0.8224, 'learning_rate': 9.838582410819332e-05, 'epoch': 0.08}
{'loss': 0.8439, 'learning_rate': 9.83797257245344e-05, 'epoch': 0.08}
37, 10.76s/it]  8%|         | 527/6500 [1:35:55<17:43:16, 10.68s/it]                                                         8%|         | 527/6500 [1:35:55<17:43:16, 10.68s/it]  8%|         | 528/6500 [1:36:05<17:36:51, 10.62s/it]                                                         8%|         | 528/6500 [1:36:05<17:36:51, 10.62s/it]  8%|         | 529/6500 [1:36:16<17:32:26, 10.58s/it]                                                         8%|         | 529/6500 [1:36:16<17:32:26, 10.58s/it]  8%|         | 530/6500 [1:36:26<17:29:40, 10.55s/it]                                                         8%|         | 530/6500 [1:36:26<17:29:40, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8955502510070801, 'eval_runtime': 3.9706, 'eval_samples_per_second': 5.793, 'eval_steps_per_second': 1.511, 'epoch': 0.08}
                                                         8%|         | 530/6500 [1:36:30<17:29:40, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-530I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-530
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8465, 'learning_rate': 9.837361603240872e-05, 'epoch': 0.08}
{'loss': 0.8332, 'learning_rate': 9.836749503324442e-05, 'epoch': 0.08}
{'loss': 0.8325, 'learning_rate': 9.836136272847223e-05, 'epoch': 0.08}
{'loss': 0.8165, 'learning_rate': 9.835521911952555e-05, 'epoch': 0.08}
{'loss': 0.7741, 'learning_rate': 9.83490642078404e-05, 'epoch': 0.08}
{'loss': 0.835, 'learning_rate': 9.834289799485545e-05, 'epoch': 0.08}
  8%|         | 531/6500 [1:36:41<19:41:12, 11.87s/it]                                                         8%|         | 531/6500 [1:36:41<19:41:12, 11.87s/it]  8%|         | 532/6500 [1:36:51<18:58:58, 11.45s/it]                                                         8%|         | 532/6500 [1:36:51<18:58:58, 11.45s/it]  8%|         | 533/6500 [1:37:02<18:29:10, 11.15s/it]                                                         8%|         | 533/6500 [1:37:02<18:29:10, 11.15s/it]  8%|         | 534/6500 [1:37:13<18:24:45, 11.11s/it]                                                         8%|         | 534/6500 [1:37:13<18:24:45, 11.11s/it]  8%|         | 535/6500 [1:37:23<18:05:32, 10.92s/it]                                                         8%|         | 535/6500 [1:37:23<18:05:32, 10.92s/it]  8%|         | 536/6500 [1:37:34<17:51:40, 10.78s/it]                                                         8%|         | 536/6500 [1:37:34<17:51:{'loss': 0.8536, 'learning_rate': 9.833672048201204e-05, 'epoch': 0.08}
{'loss': 0.8028, 'learning_rate': 9.83305316707541e-05, 'epoch': 0.08}
{'loss': 0.8376, 'learning_rate': 9.832433156252822e-05, 'epoch': 0.08}
{'loss': 0.783, 'learning_rate': 9.831812015878368e-05, 'epoch': 0.08}
40, 10.78s/it]  8%|         | 537/6500 [1:37:44<17:42:01, 10.69s/it]                                                         8%|         | 537/6500 [1:37:44<17:42:01, 10.69s/it]  8%|         | 538/6500 [1:37:55<17:35:03, 10.62s/it]                                                         8%|         | 538/6500 [1:37:55<17:35:03, 10.62s/it]  8%|         | 539/6500 [1:38:05<17:30:03, 10.57s/it]                                                         8%|         | 539/6500 [1:38:05<17:30:03, 10.57s/it]  8%|         | 540/6500 [1:38:16<17:26:42, 10.54s/it]                                                         8%|         | 540/6500 [1:38:16<17:26:42, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8910630941390991, 'eval_runtime': 3.9805, 'eval_samples_per_second': 5.778, 'eval_steps_per_second': 1.507, 'epoch': 0.08}
                                                         8%|         | 540/6500 [1:38:20<17:26:42, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-540I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-540/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-540/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8693, 'learning_rate': 9.831189746097232e-05, 'epoch': 0.08}
{'loss': 0.8438, 'learning_rate': 9.830566347054868e-05, 'epoch': 0.08}
{'loss': 0.8079, 'learning_rate': 9.82994181889699e-05, 'epoch': 0.08}
{'loss': 0.8357, 'learning_rate': 9.829316161769578e-05, 'epoch': 0.08}
{'loss': 0.7926, 'learning_rate': 9.828689375818877e-05, 'epoch': 0.08}
{'loss': 1.3103, 'learning_rate': 9.828061461191392e-05, 'epoch': 0.08}
  8%|         | 541/6500 [1:38:31<19:36:45, 11.85s/it]                                                         8%|         | 541/6500 [1:38:31<19:36:45, 11.85s/it]  8%|         | 542/6500 [1:38:41<18:55:20, 11.43s/it]                                                         8%|         | 542/6500 [1:38:41<18:55:20, 11.43s/it]  8%|         | 543/6500 [1:38:52<18:26:12, 11.14s/it]                                                         8%|         | 543/6500 [1:38:52<18:26:12, 11.14s/it]  8%|         | 544/6500 [1:39:02<18:09:25, 10.97s/it]                                                         8%|         | 544/6500 [1:39:02<18:09:25, 10.97s/it]  8%|         | 545/6500 [1:39:13<17:54:46, 10.83s/it]                                                         8%|         | 545/6500 [1:39:13<17:54:46, 10.83s/it]  8%|         | 546/6500 [1:39:23<17:44:03, 10.72s/it]                                                         8%|         | 546/6500 [1:39:23<17:44:{'loss': 0.8145, 'learning_rate': 9.827432418033897e-05, 'epoch': 0.08}
{'loss': 0.8143, 'learning_rate': 9.826802246493425e-05, 'epoch': 0.08}
{'loss': 0.7759, 'learning_rate': 9.826170946717274e-05, 'epoch': 0.08}
{'loss': 0.8261, 'learning_rate': 9.825538518853009e-05, 'epoch': 0.08}
03, 10.72s/it]  8%|         | 547/6500 [1:39:34<17:36:14, 10.65s/it]                                                         8%|         | 547/6500 [1:39:34<17:36:14, 10.65s/it]  8%|         | 548/6500 [1:39:44<17:31:17, 10.60s/it]                                                         8%|         | 548/6500 [1:39:44<17:31:17, 10.60s/it]  8%|         | 549/6500 [1:39:55<17:27:41, 10.56s/it]                                                         8%|         | 549/6500 [1:39:55<17:27:41, 10.56s/it]  8%|         | 550/6500 [1:40:05<17:35:52, 10.65s/it]                                                         8%|         | 550/6500 [1:40:05<17:35:52, 10.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8895776867866516, 'eval_runtime': 3.984, 'eval_samples_per_second': 5.773, 'eval_steps_per_second': 1.506, 'epoch': 0.08}
                                                         8%|         | 550/6500 [1:40:09<17:35:52, 10.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-550I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-550

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-550
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-550/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8345, 'learning_rate': 9.824904963048455e-05, 'epoch': 0.08}
{'loss': 0.7712, 'learning_rate': 9.824270279451701e-05, 'epoch': 0.08}
{'loss': 0.7953, 'learning_rate': 9.823634468211103e-05, 'epoch': 0.09}
{'loss': 0.7823, 'learning_rate': 9.822997529475275e-05, 'epoch': 0.09}
{'loss': 0.7999, 'learning_rate': 9.822359463393099e-05, 'epoch': 0.09}
{'loss': 0.7867, 'learning_rate': 9.821720270113718e-05, 'epoch': 0.09}
  8%|         | 551/6500 [1:40:20<19:44:35, 11.95s/it]                                                         8%|         | 551/6500 [1:40:20<19:44:35, 11.95s/it]  8%|         | 552/6500 [1:40:31<18:59:49, 11.50s/it]                                                         8%|         | 552/6500 [1:40:31<18:59:49, 11.50s/it]  9%|         | 553/6500 [1:40:41<18:29:01, 11.19s/it]                                                         9%|         | 553/6500 [1:40:41<18:29:01, 11.19s/it]  9%|         | 554/6500 [1:40:52<18:06:43, 10.97s/it]                                                         9%|         | 554/6500 [1:40:52<18:06:43, 10.97s/it]  9%|         | 555/6500 [1:41:02<17:51:19, 10.81s/it]                                                         9%|         | 555/6500 [1:41:02<17:51:19, 10.81s/it]  9%|         | 556/6500 [1:41:13<17:40:56, 10.71s/it]                                                         9%|         | 556/6500 [1:41:13<17:40:{'loss': 0.8229, 'learning_rate': 9.821079949786541e-05, 'epoch': 0.09}
{'loss': 0.8223, 'learning_rate': 9.820438502561238e-05, 'epoch': 0.09}
{'loss': 0.792, 'learning_rate': 9.819795928587745e-05, 'epoch': 0.09}
{'loss': 0.856, 'learning_rate': 9.819152228016257e-05, 'epoch': 0.09}
56, 10.71s/it]  9%|         | 557/6500 [1:41:23<17:33:05, 10.63s/it]                                                         9%|         | 557/6500 [1:41:23<17:33:05, 10.63s/it]  9%|         | 558/6500 [1:41:34<17:28:39, 10.59s/it]                                                         9%|         | 558/6500 [1:41:34<17:28:39, 10.59s/it]  9%|         | 559/6500 [1:41:44<17:24:28, 10.55s/it]                                                         9%|         | 559/6500 [1:41:44<17:24:28, 10.55s/it]  9%|         | 560/6500 [1:41:55<17:22:39, 10.53s/it]                                                         9%|         | 560/6500 [1:41:55<17:22:39, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8891347646713257, 'eval_runtime': 3.9653, 'eval_samples_per_second': 5.8, 'eval_steps_per_second': 1.513, 'epoch': 0.09}
                                                         9%|         | 560/6500 [1:41:58<17:22:39, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-560I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-560

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8238, 'learning_rate': 9.81850740099724e-05, 'epoch': 0.09}
{'loss': 0.8311, 'learning_rate': 9.817861447681411e-05, 'epoch': 0.09}
{'loss': 0.8383, 'learning_rate': 9.817214368219763e-05, 'epoch': 0.09}
{'loss': 0.7836, 'learning_rate': 9.816566162763546e-05, 'epoch': 0.09}
{'loss': 0.7757, 'learning_rate': 9.815916831464273e-05, 'epoch': 0.09}
{'loss': 0.8382, 'learning_rate': 9.815266374473721e-05, 'epoch': 0.09}
  9%|         | 561/6500 [1:42:09<19:32:56, 11.85s/it]                                                         9%|         | 561/6500 [1:42:09<19:32:56, 11.85s/it]  9%|         | 562/6500 [1:42:20<18:50:56, 11.43s/it]                                                         9%|         | 562/6500 [1:42:20<18:50:56, 11.43s/it]  9%|         | 563/6500 [1:42:30<18:22:12, 11.14s/it]                                                         9%|         | 563/6500 [1:42:30<18:22:12, 11.14s/it]  9%|         | 564/6500 [1:42:41<18:02:24, 10.94s/it]                                                         9%|         | 564/6500 [1:42:41<18:02:24, 10.94s/it]  9%|         | 565/6500 [1:42:51<17:48:08, 10.80s/it]                                                         9%|         | 565/6500 [1:42:51<17:48:08, 10.80s/it]  9%|         | 566/6500 [1:43:02<17:44:09, 10.76s/it]                                                         9%|         | 566/6500 [1:43:02<17:44:{'loss': 0.8256, 'learning_rate': 9.814614791943933e-05, 'epoch': 0.09}
{'loss': 0.8029, 'learning_rate': 9.813962084027211e-05, 'epoch': 0.09}
{'loss': 0.7957, 'learning_rate': 9.813308250876121e-05, 'epoch': 0.09}
{'loss': 0.7888, 'learning_rate': 9.812653292643492e-05, 'epoch': 0.09}
09, 10.76s/it]  9%|         | 567/6500 [1:43:12<17:34:51, 10.67s/it]                                                         9%|         | 567/6500 [1:43:12<17:34:51, 10.67s/it]  9%|         | 568/6500 [1:43:23<17:28:18, 10.60s/it]                                                         9%|         | 568/6500 [1:43:23<17:28:18, 10.60s/it]  9%|         | 569/6500 [1:43:33<17:23:57, 10.56s/it]                                                         9%|         | 569/6500 [1:43:33<17:23:57, 10.56s/it]  9%|         | 570/6500 [1:43:44<17:21:49, 10.54s/it]                                                         9%|         | 570/6500 [1:43:44<17:21:49, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8863860964775085, 'eval_runtime': 4.2158, 'eval_samples_per_second': 5.456, 'eval_steps_per_second': 1.423, 'epoch': 0.09}
                                                         9%|         | 570/6500 [1:43:48<17:21:49, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-570
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8917, 'learning_rate': 9.811997209482418e-05, 'epoch': 0.09}
{'loss': 0.7928, 'learning_rate': 9.811340001546251e-05, 'epoch': 0.09}
{'loss': 0.7905, 'learning_rate': 9.810681668988615e-05, 'epoch': 0.09}
{'loss': 0.8172, 'learning_rate': 9.810022211963388e-05, 'epoch': 0.09}
{'loss': 1.3092, 'learning_rate': 9.809361630624714e-05, 'epoch': 0.09}
{'loss': 0.8175, 'learning_rate': 9.808699925127001e-05, 'epoch': 0.09}
  9%|         | 571/6500 [1:43:59<19:38:04, 11.92s/it]                                                         9%|         | 571/6500 [1:43:59<19:38:04, 11.92s/it]  9%|         | 572/6500 [1:44:09<18:53:42, 11.47s/it]                                                         9%|         | 572/6500 [1:44:09<18:53:42, 11.47s/it]  9%|         | 573/6500 [1:44:20<18:22:56, 11.17s/it]                                                         9%|         | 573/6500 [1:44:20<18:22:56, 11.17s/it]  9%|         | 574/6500 [1:44:31<18:09:47, 11.03s/it]                                                         9%|         | 574/6500 [1:44:31<18:09:47, 11.03s/it]  9%|         | 575/6500 [1:44:41<17:51:56, 10.86s/it]                                                         9%|         | 575/6500 [1:44:41<17:51:56, 10.86s/it]  9%|         | 576/6500 [1:44:51<17:39:58, 10.74s/it]                                                         9%|         | 576/6500 [1:44:51<17:39:{'loss': 0.7979, 'learning_rate': 9.808037095624917e-05, 'epoch': 0.09}
{'loss': 0.7954, 'learning_rate': 9.807373142273395e-05, 'epoch': 0.09}
{'loss': 0.7588, 'learning_rate': 9.80670806522763e-05, 'epoch': 0.09}
{'loss': 0.8413, 'learning_rate': 9.80604186464308e-05, 'epoch': 0.09}
58, 10.74s/it]  9%|         | 577/6500 [1:45:02<17:31:29, 10.65s/it]                                                         9%|         | 577/6500 [1:45:02<17:31:29, 10.65s/it]  9%|         | 578/6500 [1:45:12<17:25:12, 10.59s/it]                                                         9%|         | 578/6500 [1:45:12<17:25:12, 10.59s/it]  9%|         | 579/6500 [1:45:23<17:21:00, 10.55s/it]                                                         9%|         | 579/6500 [1:45:23<17:21:00, 10.55s/it]  9%|         | 580/6500 [1:45:33<17:17:35, 10.52s/it]                                                         9%|         | 580/6500 [1:45:33<17:17:35, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8837559819221497, 'eval_runtime': 3.979, 'eval_samples_per_second': 5.78, 'eval_steps_per_second': 1.508, 'epoch': 0.09}
                                                         9%|         | 580/6500 [1:45:37<17:17:35, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-580
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-580
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-580/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-580/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8025, 'learning_rate': 9.805374540675468e-05, 'epoch': 0.09}
{'loss': 0.7774, 'learning_rate': 9.804706093480771e-05, 'epoch': 0.09}
{'loss': 0.7605, 'learning_rate': 9.804036523215239e-05, 'epoch': 0.09}
{'loss': 0.7939, 'learning_rate': 9.803365830035379e-05, 'epoch': 0.09}
{'loss': 0.776, 'learning_rate': 9.80269401409796e-05, 'epoch': 0.09}
{'loss': 0.781, 'learning_rate': 9.802021075560017e-05, 'epoch': 0.09}
  9%|         | 581/6500 [1:45:48<19:29:34, 11.86s/it]                                                         9%|         | 581/6500 [1:45:48<19:29:34, 11.86s/it]  9%|         | 582/6500 [1:45:59<18:56:24, 11.52s/it]                                                         9%|         | 582/6500 [1:45:59<18:56:24, 11.52s/it]  9%|         | 583/6500 [1:46:09<18:23:34, 11.19s/it]                                                         9%|         | 583/6500 [1:46:09<18:23:34, 11.19s/it]  9%|         | 584/6500 [1:46:20<18:00:18, 10.96s/it]                                                         9%|         | 584/6500 [1:46:20<18:00:18, 10.96s/it]  9%|         | 585/6500 [1:46:30<17:44:27, 10.80s/it]                                                         9%|         | 585/6500 [1:46:30<17:44:27, 10.80s/it]  9%|         | 586/6500 [1:46:41<17:33:19, 10.69s/it]                                                         9%|         | 586/6500 [1:46:41<17:33:{'loss': 0.8287, 'learning_rate': 9.801347014578846e-05, 'epoch': 0.09}
{'loss': 0.785, 'learning_rate': 9.800671831312e-05, 'epoch': 0.09}
{'loss': 0.8218, 'learning_rate': 9.799995525917304e-05, 'epoch': 0.09}
{'loss': 0.816, 'learning_rate': 9.799318098552837e-05, 'epoch': 0.09}
19, 10.69s/it]  9%|         | 587/6500 [1:46:51<17:24:53, 10.60s/it]                                                         9%|         | 587/6500 [1:46:51<17:24:53, 10.60s/it]  9%|         | 588/6500 [1:47:01<17:18:59, 10.54s/it]                                                         9%|         | 588/6500 [1:47:01<17:18:59, 10.54s/it]  9%|         | 589/6500 [1:47:12<17:15:01, 10.51s/it]                                                         9%|         | 589/6500 [1:47:12<17:15:01, 10.51s/it]  9%|         | 590/6500 [1:47:22<17:11:42, 10.47s/it]                                                         9%|         | 590/6500 [1:47:22<17:11:42, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8819578289985657, 'eval_runtime': 3.9813, 'eval_samples_per_second': 5.777, 'eval_steps_per_second': 1.507, 'epoch': 0.09}
                                                         9%|         | 590/6500 [1:47:26<17:11:42, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-590/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8331, 'learning_rate': 9.798639549376945e-05, 'epoch': 0.09}
{'loss': 0.797, 'learning_rate': 9.797959878548236e-05, 'epoch': 0.09}
{'loss': 0.8166, 'learning_rate': 9.797279086225576e-05, 'epoch': 0.09}
{'loss': 0.7838, 'learning_rate': 9.796597172568099e-05, 'epoch': 0.09}
{'loss': 0.7739, 'learning_rate': 9.795914137735194e-05, 'epoch': 0.09}
{'loss': 0.83, 'learning_rate': 9.795229981886521e-05, 'epoch': 0.09}
  9%|         | 591/6500 [1:47:37<19:23:12, 11.81s/it]                                                         9%|         | 591/6500 [1:47:37<19:23:12, 11.81s/it]  9%|         | 592/6500 [1:47:48<18:40:49, 11.38s/it]                                                         9%|         | 592/6500 [1:47:48<18:40:49, 11.38s/it]  9%|         | 593/6500 [1:47:58<18:12:09, 11.09s/it]                                                         9%|         | 593/6500 [1:47:58<18:12:09, 11.09s/it]  9%|         | 594/6500 [1:48:08<17:51:15, 10.88s/it]                                                         9%|         | 594/6500 [1:48:08<17:51:15, 10.88s/it]  9%|         | 595/6500 [1:48:19<17:36:45, 10.74s/it]                                                         9%|         | 595/6500 [1:48:19<17:36:45, 10.74s/it]  9%|         | 596/6500 [1:48:29<17:26:08, 10.63s/it]                                                         9%|         | 596/6500 [1:48:29<17:26:{'loss': 0.7937, 'learning_rate': 9.794544705181995e-05, 'epoch': 0.09}
{'loss': 0.8059, 'learning_rate': 9.793858307781796e-05, 'epoch': 0.09}
{'loss': 0.7725, 'learning_rate': 9.793170789846364e-05, 'epoch': 0.09}
{'loss': 0.8216, 'learning_rate': 9.792482151536402e-05, 'epoch': 0.09}
08, 10.63s/it]  9%|         | 597/6500 [1:48:40<17:19:12, 10.56s/it]                                                         9%|         | 597/6500 [1:48:40<17:19:12, 10.56s/it]  9%|         | 598/6500 [1:48:50<17:20:44, 10.58s/it]                                                         9%|         | 598/6500 [1:48:50<17:20:44, 10.58s/it]  9%|         | 599/6500 [1:49:01<17:15:25, 10.53s/it]                                                         9%|         | 599/6500 [1:49:01<17:15:25, 10.53s/it]  9%|         | 600/6500 [1:49:11<17:11:30, 10.49s/it]                                                         9%|         | 600/6500 [1:49:11<17:11:30, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8792667984962463, 'eval_runtime': 4.2215, 'eval_samples_per_second': 5.448, 'eval_steps_per_second': 1.421, 'epoch': 0.09}
                                                         9%|         | 600/6500 [1:49:15<17:11:30, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-600
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8502, 'learning_rate': 9.791792393012877e-05, 'epoch': 0.09}
{'loss': 0.7886, 'learning_rate': 9.791101514437014e-05, 'epoch': 0.09}
{'loss': 0.7903, 'learning_rate': 9.790409515970302e-05, 'epoch': 0.09}
{'loss': 0.793, 'learning_rate': 9.789716397774493e-05, 'epoch': 0.09}
{'loss': 1.2887, 'learning_rate': 9.789022160011597e-05, 'epoch': 0.09}
{'loss': 0.8196, 'learning_rate': 9.78832680284389e-05, 'epoch': 0.09}
  9%|         | 601/6500 [1:49:26<19:28:58, 11.89s/it]                                                         9%|         | 601/6500 [1:49:26<19:28:58, 11.89s/it]  9%|         | 602/6500 [1:49:37<18:43:56, 11.43s/it]                                                         9%|         | 602/6500 [1:49:37<18:43:56, 11.43s/it]  9%|         | 603/6500 [1:49:47<18:12:40, 11.12s/it]                                                         9%|         | 603/6500 [1:49:47<18:12:40, 11.12s/it]  9%|         | 604/6500 [1:49:57<17:51:03, 10.90s/it]                                                         9%|         | 604/6500 [1:49:57<17:51:03, 10.90s/it]  9%|         | 605/6500 [1:50:08<17:35:54, 10.75s/it]                                                         9%|         | 605/6500 [1:50:08<17:35:54, 10.75s/it]  9%|         | 606/6500 [1:50:18<17:25:41, 10.65s/it]                                                         9%|         | 606/6500 [1:50:18<17:25:{'loss': 0.7774, 'learning_rate': 9.787630326433905e-05, 'epoch': 0.09}
{'loss': 0.7418, 'learning_rate': 9.786932730944441e-05, 'epoch': 0.09}
{'loss': 0.7752, 'learning_rate': 9.786234016538557e-05, 'epoch': 0.09}
{'loss': 0.8196, 'learning_rate': 9.785534183379572e-05, 'epoch': 0.09}
41, 10.65s/it]  9%|         | 607/6500 [1:50:29<17:18:32, 10.57s/it]                                                         9%|         | 607/6500 [1:50:29<17:18:32, 10.57s/it]  9%|         | 608/6500 [1:50:39<17:13:11, 10.52s/it]                                                         9%|         | 608/6500 [1:50:39<17:13:11, 10.52s/it]  9%|         | 609/6500 [1:50:49<17:09:54, 10.49s/it]                                                         9%|         | 609/6500 [1:50:49<17:09:54, 10.49s/it]  9%|         | 610/6500 [1:51:00<17:07:07, 10.46s/it]                                                         9%|         | 610/6500 [1:51:00<17:07:07, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8772718906402588, 'eval_runtime': 3.9839, 'eval_samples_per_second': 5.773, 'eval_steps_per_second': 1.506, 'epoch': 0.09}
                                                         9%|         | 610/6500 [1:51:04<17:07:07, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-610
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-610/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-610/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-610/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-610/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7848, 'learning_rate': 9.784833231631068e-05, 'epoch': 0.09}
{'loss': 0.765, 'learning_rate': 9.784131161456888e-05, 'epoch': 0.09}
{'loss': 0.744, 'learning_rate': 9.783427973021136e-05, 'epoch': 0.09}
{'loss': 0.7829, 'learning_rate': 9.782723666488181e-05, 'epoch': 0.09}
{'loss': 0.7723, 'learning_rate': 9.782018242022648e-05, 'epoch': 0.09}
{'loss': 0.7733, 'learning_rate': 9.781311699789426e-05, 'epoch': 0.09}
  9%|         | 611/6500 [1:51:15<19:17:09, 11.79s/it]                                                         9%|         | 611/6500 [1:51:15<19:17:09, 11.79s/it]  9%|         | 612/6500 [1:51:25<18:36:35, 11.38s/it]                                                         9%|         | 612/6500 [1:51:25<18:36:35, 11.38s/it]  9%|         | 613/6500 [1:51:35<18:07:41, 11.09s/it]                                                         9%|         | 613/6500 [1:51:35<18:07:41, 11.09s/it]  9%|         | 614/6500 [1:51:46<17:46:54, 10.88s/it]                                                         9%|         | 614/6500 [1:51:46<17:46:54, 10.88s/it]  9%|         | 615/6500 [1:51:57<17:43:18, 10.84s/it]                                                         9%|         | 615/6500 [1:51:57<17:43:18, 10.84s/it]  9%|         | 616/6500 [1:52:07<17:29:53, 10.71s/it]                                                         9%|         | 616/6500 [1:52:07<17:29:{'loss': 0.8294, 'learning_rate': 9.780604039953665e-05, 'epoch': 0.09}
{'loss': 0.7805, 'learning_rate': 9.779895262680775e-05, 'epoch': 0.1}
{'loss': 0.8258, 'learning_rate': 9.77918536813643e-05, 'epoch': 0.1}
{'loss': 0.8109, 'learning_rate': 9.778474356486564e-05, 'epoch': 0.1}
53, 10.71s/it]  9%|         | 617/6500 [1:52:17<17:20:39, 10.61s/it]                                                         9%|         | 617/6500 [1:52:17<17:20:39, 10.61s/it] 10%|         | 618/6500 [1:52:28<17:14:25, 10.55s/it]                                                        10%|         | 618/6500 [1:52:28<17:14:25, 10.55s/it] 10%|         | 619/6500 [1:52:38<17:09:51, 10.51s/it]                                                        10%|         | 619/6500 [1:52:38<17:09:51, 10.51s/it] 10%|         | 620/6500 [1:52:49<17:06:39, 10.48s/it]                                                        10%|         | 620/6500 [1:52:49<17:06:39, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8760520219802856, 'eval_runtime': 4.0899, 'eval_samples_per_second': 5.624, 'eval_steps_per_second': 1.467, 'epoch': 0.1}
                                                        10%|         | 620/6500 [1:52:53<17:06:39, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-620
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-620
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-620/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-620/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7986, 'learning_rate': 9.777762227897371e-05, 'epoch': 0.1}
{'loss': 0.7949, 'learning_rate': 9.777048982535306e-05, 'epoch': 0.1}
{'loss': 0.7828, 'learning_rate': 9.776334620567085e-05, 'epoch': 0.1}
{'loss': 0.7613, 'learning_rate': 9.77561914215969e-05, 'epoch': 0.1}
{'loss': 0.794, 'learning_rate': 9.774902547480353e-05, 'epoch': 0.1}
{'loss': 0.8025, 'learning_rate': 9.77418483669658e-05, 'epoch': 0.1}
 10%|         | 621/6500 [1:53:04<19:18:05, 11.82s/it]                                                        10%|         | 621/6500 [1:53:04<19:18:05, 11.82s/it] 10%|         | 622/6500 [1:53:14<18:36:27, 11.40s/it]                                                        10%|         | 622/6500 [1:53:14<18:36:27, 11.40s/it] 10%|         | 623/6500 [1:53:24<18:08:02, 11.11s/it]                                                        10%|         | 623/6500 [1:53:24<18:08:02, 11.11s/it] 10%|         | 624/6500 [1:53:35<17:51:33, 10.94s/it]                                                        10%|         | 624/6500 [1:53:35<17:51:33, 10.94s/it] 10%|         | 625/6500 [1:53:45<17:36:01, 10.79s/it]                                                        10%|         | 625/6500 [1:53:45<17:36:01, 10.79s/it] 10%|         | 626/6500 [1:53:56<17:25:12, 10.68s/it]                                                        10%|         | 626/6500 [1:53:56<17:25:{'loss': 0.7675, 'learning_rate': 9.773466009976129e-05, 'epoch': 0.1}
{'loss': 0.8039, 'learning_rate': 9.77274606748702e-05, 'epoch': 0.1}
{'loss': 0.7586, 'learning_rate': 9.772025009397537e-05, 'epoch': 0.1}
{'loss': 0.8392, 'learning_rate': 9.771302835876224e-05, 'epoch': 0.1}
12, 10.68s/it] 10%|         | 627/6500 [1:54:06<17:17:02, 10.59s/it]                                                        10%|         | 627/6500 [1:54:06<17:17:02, 10.59s/it] 10%|         | 628/6500 [1:54:17<17:11:34, 10.54s/it]                                                        10%|         | 628/6500 [1:54:17<17:11:34, 10.54s/it] 10%|         | 629/6500 [1:54:27<17:08:11, 10.51s/it]                                                        10%|         | 629/6500 [1:54:27<17:08:11, 10.51s/it] 10%|         | 630/6500 [1:54:37<17:05:44, 10.48s/it]                                                        10%|         | 630/6500 [1:54:37<17:05:44, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.873172402381897, 'eval_runtime': 4.0237, 'eval_samples_per_second': 5.716, 'eval_steps_per_second': 1.491, 'epoch': 0.1}
                                                        10%|         | 630/6500 [1:54:41<17:05:44, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-630
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-630

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-630
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-630/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8063, 'learning_rate': 9.77057954709188e-05, 'epoch': 0.1}
{'loss': 0.7617, 'learning_rate': 9.769855143213575e-05, 'epoch': 0.1}
{'loss': 0.7995, 'learning_rate': 9.769129624410631e-05, 'epoch': 0.1}
{'loss': 0.7684, 'learning_rate': 9.768402990852635e-05, 'epoch': 0.1}
{'loss': 1.287, 'learning_rate': 9.76767524270943e-05, 'epoch': 0.1}
{'loss': 0.787, 'learning_rate': 9.766946380151125e-05, 'epoch': 0.1}
 10%|         | 631/6500 [1:54:53<19:34:45, 12.01s/it]                                                        10%|         | 631/6500 [1:54:53<19:34:45, 12.01s/it] 10%|         | 632/6500 [1:55:03<18:47:25, 11.53s/it]                                                        10%|         | 632/6500 [1:55:03<18:47:25, 11.53s/it] 10%|         | 633/6500 [1:55:14<18:14:12, 11.19s/it]                                                        10%|         | 633/6500 [1:55:14<18:14:12, 11.19s/it] 10%|         | 634/6500 [1:55:24<17:50:53, 10.95s/it]                                                        10%|         | 634/6500 [1:55:24<17:50:53, 10.95s/it] 10%|         | 635/6500 [1:55:35<17:34:17, 10.79s/it]                                                        10%|         | 635/6500 [1:55:35<17:34:17, 10.79s/it] 10%|         | 636/6500 [1:55:45<17:22:42, 10.67s/it]                                                        10%|         | 636/6500 [1:55:45<17:22:{'loss': 0.7744, 'learning_rate': 9.766216403348089e-05, 'epoch': 0.1}
{'loss': 0.7316, 'learning_rate': 9.765485312470946e-05, 'epoch': 0.1}
{'loss': 0.7935, 'learning_rate': 9.764753107690588e-05, 'epoch': 0.1}
{'loss': 0.8022, 'learning_rate': 9.76401978917816e-05, 'epoch': 0.1}
42, 10.67s/it] 10%|         | 637/6500 [1:55:55<17:15:58, 10.60s/it]                                                        10%|         | 637/6500 [1:55:55<17:15:58, 10.60s/it] 10%|         | 638/6500 [1:56:06<17:09:32, 10.54s/it]                                                        10%|         | 638/6500 [1:56:06<17:09:32, 10.54s/it] 10%|         | 639/6500 [1:56:16<17:05:45, 10.50s/it]                                                        10%|         | 639/6500 [1:56:16<17:05:45, 10.50s/it] 10%|         | 640/6500 [1:56:27<17:03:00, 10.47s/it]                                                        10%|         | 640/6500 [1:56:27<17:03:00, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8730981349945068, 'eval_runtime': 3.9875, 'eval_samples_per_second': 5.768, 'eval_steps_per_second': 1.505, 'epoch': 0.1}
                                                        10%|         | 640/6500 [1:56:31<17:03:00, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-640
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-640

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-640
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-640/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.737, 'learning_rate': 9.763285357105072e-05, 'epoch': 0.1}
{'loss': 0.766, 'learning_rate': 9.762549811642991e-05, 'epoch': 0.1}
{'loss': 0.7466, 'learning_rate': 9.761813152963853e-05, 'epoch': 0.1}
{'loss': 0.7671, 'learning_rate': 9.761075381239839e-05, 'epoch': 0.1}
{'loss': 0.7619, 'learning_rate': 9.760336496643403e-05, 'epoch': 0.1}
{'loss': 0.7848, 'learning_rate': 9.759596499347254e-05, 'epoch': 0.1}
 10%|         | 641/6500 [1:56:42<19:12:53, 11.81s/it]                                                        10%|         | 641/6500 [1:56:42<19:12:53, 11.81s/it] 10%|         | 642/6500 [1:56:52<18:31:02, 11.38s/it]                                                        10%|         | 642/6500 [1:56:52<18:31:02, 11.38s/it] 10%|         | 643/6500 [1:57:02<18:01:34, 11.08s/it]                                                        10%|         | 643/6500 [1:57:02<18:01:34, 11.08s/it] 10%|         | 644/6500 [1:57:13<17:40:41, 10.87s/it]                                                        10%|         | 644/6500 [1:57:13<17:40:41, 10.87s/it] 10%|         | 645/6500 [1:57:23<17:26:02, 10.72s/it]                                                        10%|         | 645/6500 [1:57:23<17:26:02, 10.72s/it] 10%|         | 646/6500 [1:57:33<17:15:51, 10.62s/it]                                                        10%|         | 646/6500 [1:57:34<17:15:{'loss': 0.786, 'learning_rate': 9.758855389524364e-05, 'epoch': 0.1}
{'loss': 0.7689, 'learning_rate': 9.75811316734796e-05, 'epoch': 0.1}
{'loss': 0.8159, 'learning_rate': 9.757369832991532e-05, 'epoch': 0.1}
{'loss': 0.802, 'learning_rate': 9.756625386628832e-05, 'epoch': 0.1}
51, 10.62s/it] 10%|         | 647/6500 [1:57:44<17:19:45, 10.66s/it]                                                        10%|         | 647/6500 [1:57:44<17:19:45, 10.66s/it] 10%|         | 648/6500 [1:57:55<17:11:20, 10.57s/it]                                                        10%|         | 648/6500 [1:57:55<17:11:20, 10.57s/it] 10%|         | 649/6500 [1:58:05<17:05:04, 10.51s/it]                                                        10%|         | 649/6500 [1:58:05<17:05:04, 10.51s/it] 10%|         | 650/6500 [1:58:15<17:01:16, 10.47s/it]                                                        10%|         | 650/6500 [1:58:15<17:01:16, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8693976402282715, 'eval_runtime': 4.1144, 'eval_samples_per_second': 5.59, 'eval_steps_per_second': 1.458, 'epoch': 0.1}
                                                        10%|         | 650/6500 [1:58:20<17:01:16, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-650/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-650/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7764, 'learning_rate': 9.755879828433869e-05, 'epoch': 0.1}
{'loss': 0.7986, 'learning_rate': 9.755133158580912e-05, 'epoch': 0.1}
{'loss': 0.7485, 'learning_rate': 9.75438537724449e-05, 'epoch': 0.1}
{'loss': 0.7484, 'learning_rate': 9.753636484599393e-05, 'epoch': 0.1}
{'loss': 0.7998, 'learning_rate': 9.752886480820671e-05, 'epoch': 0.1}
{'loss': 0.7825, 'learning_rate': 9.752135366083632e-05, 'epoch': 0.1}
 10%|         | 651/6500 [1:58:30<19:12:43, 11.82s/it]                                                        10%|         | 651/6500 [1:58:30<19:12:43, 11.82s/it] 10%|         | 652/6500 [1:58:41<18:31:26, 11.40s/it]                                                        10%|         | 652/6500 [1:58:41<18:31:26, 11.40s/it] 10%|         | 653/6500 [1:58:51<18:01:58, 11.10s/it]                                                        10%|         | 653/6500 [1:58:51<18:01:58, 11.10s/it] 10%|         | 654/6500 [1:59:02<17:41:03, 10.89s/it]                                                        10%|         | 654/6500 [1:59:02<17:41:03, 10.89s/it] 10%|         | 655/6500 [1:59:12<17:26:26, 10.74s/it]                                                        10%|         | 655/6500 [1:59:12<17:26:26, 10.74s/it] 10%|         | 656/6500 [1:59:22<17:16:17, 10.64s/it]                                                        10%|         | 656/6500 [1:59:22<17:16:{'loss': 0.7806, 'learning_rate': 9.751383140563845e-05, 'epoch': 0.1}
{'loss': 0.7525, 'learning_rate': 9.750629804437137e-05, 'epoch': 0.1}
{'loss': 0.8011, 'learning_rate': 9.749875357879597e-05, 'epoch': 0.1}
{'loss': 0.8245, 'learning_rate': 9.749119801067572e-05, 'epoch': 0.1}
17, 10.64s/it] 10%|         | 657/6500 [1:59:33<17:08:35, 10.56s/it]                                                        10%|         | 657/6500 [1:59:33<17:08:35, 10.56s/it] 10%|         | 658/6500 [1:59:43<17:03:31, 10.51s/it]                                                        10%|         | 658/6500 [1:59:43<17:03:31, 10.51s/it] 10%|         | 659/6500 [1:59:54<16:59:59, 10.48s/it]                                                        10%|         | 659/6500 [1:59:54<16:59:59, 10.48s/it] 10%|         | 660/6500 [2:00:04<16:57:32, 10.45s/it]                                                        10%|         | 660/6500 [2:00:04<16:57:32, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8675874471664429, 'eval_runtime': 3.9834, 'eval_samples_per_second': 5.774, 'eval_steps_per_second': 1.506, 'epoch': 0.1}
                                                        10%|         | 660/6500 [2:00:08<16:57:32, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-660I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-660

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-660
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7581, 'learning_rate': 9.74836313417767e-05, 'epoch': 0.1}
{'loss': 0.7501, 'learning_rate': 9.747605357386754e-05, 'epoch': 0.1}
{'loss': 0.791, 'learning_rate': 9.746846470871951e-05, 'epoch': 0.1}
{'loss': 1.2722, 'learning_rate': 9.746086474810649e-05, 'epoch': 0.1}
{'loss': 0.7898, 'learning_rate': 9.745325369380489e-05, 'epoch': 0.1}
{'loss': 0.7619, 'learning_rate': 9.744563154759375e-05, 'epoch': 0.1}
 10%|         | 661/6500 [2:00:19<19:05:32, 11.77s/it]                                                        10%|         | 661/6500 [2:00:19<19:05:32, 11.77s/it] 10%|         | 662/6500 [2:00:29<18:25:18, 11.36s/it]                                                        10%|         | 662/6500 [2:00:29<18:25:18, 11.36s/it] 10%|         | 663/6500 [2:00:40<18:09:49, 11.20s/it]                                                        10%|         | 663/6500 [2:00:40<18:09:49, 11.20s/it] 10%|         | 664/6500 [2:00:50<17:45:55, 10.96s/it]                                                        10%|         | 664/6500 [2:00:50<17:45:55, 10.96s/it] 10%|         | 665/6500 [2:01:01<17:29:19, 10.79s/it]                                                        10%|         | 665/6500 [2:01:01<17:29:19, 10.79s/it] 10%|         | 666/6500 [2:01:11<17:17:53, 10.67s/it]                                                        10%|         | 666/6500 [2:01:11<17:17:{'loss': 0.7524, 'learning_rate': 9.743799831125472e-05, 'epoch': 0.1}
{'loss': 0.7283, 'learning_rate': 9.743035398657201e-05, 'epoch': 0.1}
{'loss': 0.7917, 'learning_rate': 9.742269857533244e-05, 'epoch': 0.1}
{'loss': 0.7668, 'learning_rate': 9.74150320793254e-05, 'epoch': 0.1}
53, 10.67s/it] 10%|         | 667/6500 [2:01:22<17:10:02, 10.60s/it]                                                        10%|         | 667/6500 [2:01:22<17:10:02, 10.60s/it] 10%|         | 668/6500 [2:01:32<17:04:00, 10.53s/it]                                                        10%|         | 668/6500 [2:01:32<17:04:00, 10.53s/it] 10%|         | 669/6500 [2:01:42<16:59:38, 10.49s/it]                                                        10%|         | 669/6500 [2:01:42<16:59:38, 10.49s/it] 10%|         | 670/6500 [2:01:53<16:56:57, 10.47s/it]                                                        10%|         | 670/6500 [2:01:53<16:56:57, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8661310076713562, 'eval_runtime': 4.0661, 'eval_samples_per_second': 5.657, 'eval_steps_per_second': 1.476, 'epoch': 0.1}
                                                        10%|         | 670/6500 [2:01:57<16:56:57, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-670/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-670/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7471, 'learning_rate': 9.740735450034292e-05, 'epoch': 0.1}
{'loss': 0.7338, 'learning_rate': 9.739966584017956e-05, 'epoch': 0.1}
{'loss': 0.7475, 'learning_rate': 9.739196610063251e-05, 'epoch': 0.1}
{'loss': 0.7476, 'learning_rate': 9.738425528350152e-05, 'epoch': 0.1}
{'loss': 0.7455, 'learning_rate': 9.737653339058896e-05, 'epoch': 0.1}
{'loss': 0.8029, 'learning_rate': 9.736880042369978e-05, 'epoch': 0.1}
 10%|         | 671/6500 [2:02:08<19:09:32, 11.83s/it]                                                        10%|         | 671/6500 [2:02:08<19:09:32, 11.83s/it] 10%|         | 672/6500 [2:02:18<18:28:39, 11.41s/it]                                                        10%|         | 672/6500 [2:02:18<18:28:39, 11.41s/it] 10%|         | 673/6500 [2:02:29<17:59:57, 11.12s/it]                                                        10%|         | 673/6500 [2:02:29<17:59:57, 11.12s/it] 10%|         | 674/6500 [2:02:39<17:39:52, 10.92s/it]                                                        10%|         | 674/6500 [2:02:39<17:39:52, 10.92s/it] 10%|         | 675/6500 [2:02:50<17:25:01, 10.76s/it]                                                        10%|         | 675/6500 [2:02:50<17:25:01, 10.76s/it] 10%|         | 676/6500 [2:03:00<17:15:00, 10.66s/it]                                                        10%|         | 676/6500 [2:03:00<17:15:{'loss': 0.7548, 'learning_rate': 9.736105638464151e-05, 'epoch': 0.1}
{'loss': 0.7861, 'learning_rate': 9.735330127522425e-05, 'epoch': 0.1}
{'loss': 0.795, 'learning_rate': 9.734553509726074e-05, 'epoch': 0.1}
{'loss': 0.7866, 'learning_rate': 9.733775785256629e-05, 'epoch': 0.1}
00, 10.66s/it] 10%|         | 677/6500 [2:03:10<17:08:15, 10.60s/it]                                                        10%|         | 677/6500 [2:03:10<17:08:15, 10.60s/it] 10%|         | 678/6500 [2:03:21<17:03:28, 10.55s/it]                                                        10%|         | 678/6500 [2:03:21<17:03:28, 10.55s/it] 10%|         | 679/6500 [2:03:32<17:10:57, 10.63s/it]                                                        10%|         | 679/6500 [2:03:32<17:10:57, 10.63s/it] 10%|         | 680/6500 [2:03:42<17:05:19, 10.57s/it]                                                        10%|         | 680/6500 [2:03:42<17:05:19, 10.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8635719418525696, 'eval_runtime': 3.9769, 'eval_samples_per_second': 5.783, 'eval_steps_per_second': 1.509, 'epoch': 0.1}
                                                        10%|         | 680/6500 [2:03:46<17:05:19, 10.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-680I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-680

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7728, 'learning_rate': 9.732996954295874e-05, 'epoch': 0.1}
{'loss': 0.7741, 'learning_rate': 9.732217017025858e-05, 'epoch': 0.1}
{'loss': 0.7432, 'learning_rate': 9.731435973628886e-05, 'epoch': 0.11}
{'loss': 0.7629, 'learning_rate': 9.730653824287523e-05, 'epoch': 0.11}
{'loss': 0.7824, 'learning_rate': 9.729870569184593e-05, 'epoch': 0.11}
{'loss': 0.7534, 'learning_rate': 9.729086208503174e-05, 'epoch': 0.11}
 10%|         | 681/6500 [2:03:57<19:13:30, 11.89s/it]                                                        10%|         | 681/6500 [2:03:57<19:13:30, 11.89s/it] 10%|         | 682/6500 [2:04:08<18:30:46, 11.46s/it]                                                        10%|         | 682/6500 [2:04:08<18:30:46, 11.46s/it] 11%|         | 683/6500 [2:04:18<17:59:51, 11.14s/it]                                                        11%|         | 683/6500 [2:04:18<17:59:51, 11.14s/it] 11%|         | 684/6500 [2:04:28<17:38:19, 10.92s/it]                                                        11%|         | 684/6500 [2:04:28<17:38:19, 10.92s/it] 11%|         | 685/6500 [2:04:39<17:23:39, 10.77s/it]                                                        11%|         | 685/6500 [2:04:39<17:23:39, 10.77s/it] 11%|         | 686/6500 [2:04:49<17:13:19, 10.66s/it]                                                        11%|         | 686/6500 [2:04:49<17:13:{'loss': 0.7675, 'learning_rate': 9.728300742426609e-05, 'epoch': 0.11}
{'loss': 0.7337, 'learning_rate': 9.727514171138492e-05, 'epoch': 0.11}
{'loss': 0.7978, 'learning_rate': 9.726726494822681e-05, 'epoch': 0.11}
{'loss': 0.8101, 'learning_rate': 9.725937713663292e-05, 'epoch': 0.11}
19, 10.66s/it] 11%|         | 687/6500 [2:05:00<17:05:36, 10.59s/it]                                                        11%|         | 687/6500 [2:05:00<17:05:36, 10.59s/it] 11%|         | 688/6500 [2:05:10<17:01:00, 10.54s/it]                                                        11%|         | 688/6500 [2:05:10<17:01:00, 10.54s/it] 11%|         | 689/6500 [2:05:20<16:57:27, 10.51s/it]                                                        11%|         | 689/6500 [2:05:20<16:57:27, 10.51s/it] 11%|         | 690/6500 [2:05:31<16:55:48, 10.49s/it]                                                        11%|         | 690/6500 [2:05:31<16:55:48, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8631210327148438, 'eval_runtime': 4.0911, 'eval_samples_per_second': 5.622, 'eval_steps_per_second': 1.467, 'epoch': 0.11}
                                                        11%|         | 690/6500 [2:05:35<16:55:48, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-690I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7595, 'learning_rate': 9.725147827844697e-05, 'epoch': 0.11}
{'loss': 0.7521, 'learning_rate': 9.724356837551525e-05, 'epoch': 0.11}
{'loss': 0.7645, 'learning_rate': 9.723564742968667e-05, 'epoch': 0.11}
{'loss': 1.2683, 'learning_rate': 9.722771544281271e-05, 'epoch': 0.11}
{'loss': 0.761, 'learning_rate': 9.721977241674742e-05, 'epoch': 0.11}
{'loss': 0.7622, 'learning_rate': 9.721181835334741e-05, 'epoch': 0.11}
 11%|         | 691/6500 [2:05:47<19:27:09, 12.06s/it]                                                        11%|         | 691/6500 [2:05:47<19:27:09, 12.06s/it] 11%|         | 692/6500 [2:05:57<18:41:39, 11.59s/it]                                                        11%|         | 692/6500 [2:05:57<18:41:39, 11.59s/it] 11%|         | 693/6500 [2:06:08<18:08:15, 11.24s/it]                                                        11%|         | 693/6500 [2:06:08<18:08:15, 11.24s/it] 11%|         | 694/6500 [2:06:18<17:44:31, 11.00s/it]                                                        11%|         | 694/6500 [2:06:18<17:44:31, 11.00s/it] 11%|         | 695/6500 [2:06:29<17:45:44, 11.02s/it]                                                        11%|         | 695/6500 [2:06:29<17:45:44, 11.02s/it] 11%|         | 696/6500 [2:06:39<17:28:50, 10.84s/it]                                                        11%|         | 696/6500 [2:06:39<17:28:{'loss': 0.6992, 'learning_rate': 9.720385325447192e-05, 'epoch': 0.11}
{'loss': 0.7495, 'learning_rate': 9.719587712198275e-05, 'epoch': 0.11}
{'loss': 0.8063, 'learning_rate': 9.718788995774423e-05, 'epoch': 0.11}
{'loss': 0.7282, 'learning_rate': 9.717989176362337e-05, 'epoch': 0.11}
50, 10.84s/it] 11%|         | 697/6500 [2:06:50<17:16:49, 10.72s/it]                                                        11%|         | 697/6500 [2:06:50<17:16:49, 10.72s/it] 11%|         | 698/6500 [2:07:00<17:08:47, 10.64s/it]                                                        11%|         | 698/6500 [2:07:00<17:08:47, 10.64s/it] 11%|         | 699/6500 [2:07:11<17:03:28, 10.59s/it]                                                        11%|         | 699/6500 [2:07:11<17:03:28, 10.59s/it] 11%|         | 700/6500 [2:07:21<16:59:13, 10.54s/it]                                                        11%|         | 700/6500 [2:07:21<16:59:13, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8626193404197693, 'eval_runtime': 4.177, 'eval_samples_per_second': 5.506, 'eval_steps_per_second': 1.436, 'epoch': 0.11}
                                                        11%|         | 700/6500 [2:07:25<16:59:13, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-700
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-700
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-700/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-700/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7374, 'learning_rate': 9.717188254148966e-05, 'epoch': 0.11}
{'loss': 0.7207, 'learning_rate': 9.71638622932152e-05, 'epoch': 0.11}
{'loss': 0.7517, 'learning_rate': 9.715583102067469e-05, 'epoch': 0.11}
{'loss': 0.7332, 'learning_rate': 9.714778872574541e-05, 'epoch': 0.11}
{'loss': 0.7459, 'learning_rate': 9.713973541030716e-05, 'epoch': 0.11}
{'loss': 0.7877, 'learning_rate': 9.713167107624239e-05, 'epoch': 0.11}
 11%|         | 701/6500 [2:07:36<19:12:30, 11.92s/it]                                                        11%|         | 701/6500 [2:07:36<19:12:30, 11.92s/it] 11%|         | 702/6500 [2:07:47<18:31:43, 11.50s/it]                                                        11%|         | 702/6500 [2:07:47<18:31:43, 11.50s/it] 11%|         | 703/6500 [2:07:57<18:04:06, 11.22s/it]                                                        11%|         | 703/6500 [2:07:57<18:04:06, 11.22s/it] 11%|         | 704/6500 [2:08:08<17:42:21, 11.00s/it]                                                        11%|         | 704/6500 [2:08:08<17:42:21, 11.00s/it] 11%|         | 705/6500 [2:08:19<17:29:48, 10.87s/it]                                                        11%|         | 705/6500 [2:08:19<17:29:48, 10.87s/it] 11%|         | 706/6500 [2:08:29<17:19:02, 10.76s/it]                                                        11%|         | 706/6500 [2:08:29<17:19:{'loss': 0.7339, 'learning_rate': 9.712359572543606e-05, 'epoch': 0.11}
{'loss': 0.8032, 'learning_rate': 9.711550935977576e-05, 'epoch': 0.11}
{'loss': 0.7702, 'learning_rate': 9.71074119811516e-05, 'epoch': 0.11}
{'loss': 0.756, 'learning_rate': 9.709930359145631e-05, 'epoch': 0.11}
02, 10.76s/it] 11%|         | 707/6500 [2:08:39<17:10:22, 10.67s/it]                                                        11%|         | 707/6500 [2:08:39<17:10:22, 10.67s/it] 11%|         | 708/6500 [2:08:50<17:04:05, 10.61s/it]                                                        11%|         | 708/6500 [2:08:50<17:04:05, 10.61s/it] 11%|         | 709/6500 [2:09:00<17:00:10, 10.57s/it]                                                        11%|         | 709/6500 [2:09:00<17:00:10, 10.57s/it] 11%|         | 710/6500 [2:09:11<16:57:04, 10.54s/it]                                                        11%|         | 710/6500 [2:09:11<16:57:04, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8594504594802856, 'eval_runtime': 3.9763, 'eval_samples_per_second': 5.784, 'eval_steps_per_second': 1.509, 'epoch': 0.11}
                                                        11%|         | 710/6500 [2:09:15<16:57:04, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-710
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7711, 'learning_rate': 9.709118419258518e-05, 'epoch': 0.11}
{'loss': 0.7308, 'learning_rate': 9.708305378643604e-05, 'epoch': 0.11}
{'loss': 0.741, 'learning_rate': 9.707491237490937e-05, 'epoch': 0.11}
{'loss': 0.7474, 'learning_rate': 9.706675995990815e-05, 'epoch': 0.11}
{'loss': 0.7779, 'learning_rate': 9.705859654333797e-05, 'epoch': 0.11}
{'loss': 0.7512, 'learning_rate': 9.705042212710695e-05, 'epoch': 0.11}
 11%|         | 711/6500 [2:09:26<19:09:13, 11.91s/it]                                                        11%|         | 711/6500 [2:09:26<19:09:13, 11.91s/it] 11%|         | 712/6500 [2:09:36<18:27:19, 11.48s/it]                                                        11%|         | 712/6500 [2:09:36<18:27:19, 11.48s/it] 11%|         | 713/6500 [2:09:47<17:58:29, 11.18s/it]                                                        11%|         | 713/6500 [2:09:47<17:58:29, 11.18s/it] 11%|         | 714/6500 [2:09:57<17:38:56, 10.98s/it]                                                        11%|         | 714/6500 [2:09:57<17:38:56, 10.98s/it] 11%|         | 715/6500 [2:10:08<17:24:10, 10.83s/it]                                                        11%|         | 715/6500 [2:10:08<17:24:10, 10.83s/it] 11%|         | 716/6500 [2:10:20<18:01:54, 11.22s/it]                                                        11%|         | 716/6500 [2:10:20<18:01:{'loss': 0.7578, 'learning_rate': 9.704223671312584e-05, 'epoch': 0.11}
{'loss': 0.7324, 'learning_rate': 9.703404030330791e-05, 'epoch': 0.11}
{'loss': 0.8409, 'learning_rate': 9.702583289956903e-05, 'epoch': 0.11}
{'loss': 0.7343, 'learning_rate': 9.701761450382765e-05, 'epoch': 0.11}
54, 11.22s/it] 11%|         | 717/6500 [2:10:31<17:43:12, 11.03s/it]                                                        11%|         | 717/6500 [2:10:31<17:43:12, 11.03s/it] 11%|         | 718/6500 [2:10:41<17:27:32, 10.87s/it]                                                        11%|         | 718/6500 [2:10:41<17:27:32, 10.87s/it] 11%|         | 719/6500 [2:10:52<17:20:33, 10.80s/it]                                                        11%|         | 719/6500 [2:10:52<17:20:33, 10.80s/it] 11%|         | 720/6500 [2:11:02<17:10:32, 10.70s/it]                                                        11%|         | 720/6500 [2:11:02<17:10:32, 10.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8585978746414185, 'eval_runtime': 4.8816, 'eval_samples_per_second': 4.712, 'eval_steps_per_second': 1.229, 'epoch': 0.11}
                                                        11%|         | 720/6500 [2:11:07<17:10:32, 10.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7266, 'learning_rate': 9.700938511800474e-05, 'epoch': 0.11}
{'loss': 0.7629, 'learning_rate': 9.700114474402387e-05, 'epoch': 0.11}
{'loss': 1.1598, 'learning_rate': 9.69928933838112e-05, 'epoch': 0.11}
{'loss': 0.8382, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.11}
{'loss': 0.7579, 'learning_rate': 9.69763577124078e-05, 'epoch': 0.11}
{'loss': 0.7355, 'learning_rate': 9.696807340508221e-05, 'epoch': 0.11}
 11%|         | 721/6500 [2:11:18<19:38:30, 12.24s/it]                                                        11%|         | 721/6500 [2:11:18<19:38:30, 12.24s/it] 11%|         | 722/6500 [2:11:29<18:58:00, 11.82s/it]                                                        11%|         | 722/6500 [2:11:29<18:58:00, 11.82s/it] 11%|         | 723/6500 [2:11:39<18:18:27, 11.41s/it]                                                        11%|         | 723/6500 [2:11:39<18:18:27, 11.41s/it] 11%|         | 724/6500 [2:11:50<17:50:40, 11.12s/it]                                                        11%|         | 724/6500 [2:11:50<17:50:40, 11.12s/it] 11%|         | 725/6500 [2:12:00<17:30:41, 10.92s/it]                                                        11%|         | 725/6500 [2:12:00<17:30:41, 10.92s/it] 11%|         | 726/6500 [2:12:11<17:16:18, 10.77s/it]                                                        11%|         | 726/6500 [2:12:11<17:16:{'loss': 0.7038, 'learning_rate': 9.6959778119255e-05, 'epoch': 0.11}
{'loss': 0.771, 'learning_rate': 9.69514718568652e-05, 'epoch': 0.11}
{'loss': 0.7694, 'learning_rate': 9.69431546198543e-05, 'epoch': 0.11}
{'loss': 0.7018, 'learning_rate': 9.693482641016645e-05, 'epoch': 0.11}
18, 10.77s/it] 11%|         | 727/6500 [2:12:21<17:05:38, 10.66s/it]                                                        11%|         | 727/6500 [2:12:21<17:05:38, 10.66s/it] 11%|         | 728/6500 [2:12:33<17:53:33, 11.16s/it]                                                        11%|         | 728/6500 [2:12:33<17:53:33, 11.16s/it] 11%|         | 729/6500 [2:12:44<17:32:13, 10.94s/it]                                                        11%|         | 729/6500 [2:12:44<17:32:13, 10.94s/it] 11%|         | 730/6500 [2:12:54<17:17:32, 10.79s/it]                                                        11%|         | 730/6500 [2:12:54<17:17:32, 10.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8582333922386169, 'eval_runtime': 3.9656, 'eval_samples_per_second': 5.8, 'eval_steps_per_second': 1.513, 'epoch': 0.11}
                                                        11%|         | 730/6500 [2:12:58<17:17:32, 10.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-730I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-730

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7271, 'learning_rate': 9.692648722974829e-05, 'epoch': 0.11}
{'loss': 0.717, 'learning_rate': 9.691813708054904e-05, 'epoch': 0.11}
{'loss': 0.7311, 'learning_rate': 9.690977596452053e-05, 'epoch': 0.11}
{'loss': 0.7183, 'learning_rate': 9.69014038836171e-05, 'epoch': 0.11}
{'loss': 0.7479, 'learning_rate': 9.689302083979568e-05, 'epoch': 0.11}
{'loss': 0.7494, 'learning_rate': 9.688462683501574e-05, 'epoch': 0.11}
 11%|         | 731/6500 [2:13:09<19:17:04, 12.03s/it]                                                        11%|         | 731/6500 [2:13:09<19:17:04, 12.03s/it] 11%|        | 732/6500 [2:13:20<18:30:41, 11.55s/it]                                                        11%|        | 732/6500 [2:13:20<18:30:41, 11.55s/it] 11%|        | 733/6500 [2:13:30<17:58:16, 11.22s/it]                                                        11%|        | 733/6500 [2:13:30<17:58:16, 11.22s/it] 11%|        | 734/6500 [2:13:41<17:35:44, 10.99s/it]                                                        11%|        | 734/6500 [2:13:41<17:35:44, 10.99s/it] 11%|        | 735/6500 [2:13:51<17:20:09, 10.83s/it]                                                        11%|        | 735/6500 [2:13:51<17:20:09, 10.83s/it] 11%|        | 736/6500 [2:14:01<17:08:39, 10.71s/it]                                                        11%|        | 736/{'loss': 0.7352, 'learning_rate': 9.687622187123936e-05, 'epoch': 0.11}
{'loss': 0.7798, 'learning_rate': 9.686780595043113e-05, 'epoch': 0.11}
{'loss': 0.7749, 'learning_rate': 9.68593790745582e-05, 'epoch': 0.11}
{'loss': 0.7525, 'learning_rate': 9.685094124559034e-05, 'epoch': 0.11}
6500 [2:14:01<17:08:39, 10.71s/it] 11%|        | 737/6500 [2:14:12<17:00:48, 10.63s/it]                                                        11%|        | 737/6500 [2:14:12<17:00:48, 10.63s/it] 11%|        | 738/6500 [2:14:22<16:54:49, 10.57s/it]                                                        11%|        | 738/6500 [2:14:22<16:54:49, 10.57s/it] 11%|        | 739/6500 [2:14:33<16:51:05, 10.53s/it]                                                        11%|        | 739/6500 [2:14:33<16:51:05, 10.53s/it] 11%|        | 740/6500 [2:14:43<16:48:07, 10.50s/it]                                                        11%|        | 740/6500 [2:14:43<16:48:07, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8558531999588013, 'eval_runtime': 3.9737, 'eval_samples_per_second': 5.788, 'eval_steps_per_second': 1.51, 'epoch': 0.11}
                                                        11%|        | 740/6500 [2:14:47<16:48:07, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-740the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-740

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-740/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-740/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7656, 'learning_rate': 9.684249246549981e-05, 'epoch': 0.11}
{'loss': 0.7164, 'learning_rate': 9.683403273626148e-05, 'epoch': 0.11}
{'loss': 0.7226, 'learning_rate': 9.682556205985274e-05, 'epoch': 0.11}
{'loss': 0.7728, 'learning_rate': 9.68170804382536e-05, 'epoch': 0.11}
{'loss': 0.7522, 'learning_rate': 9.680858787344654e-05, 'epoch': 0.11}
{'loss': 0.7415, 'learning_rate': 9.680008436741665e-05, 'epoch': 0.11}
 11%|        | 741/6500 [2:14:58<18:57:06, 11.85s/it]                                                        11%|        | 741/6500 [2:14:58<18:57:06, 11.85s/it] 11%|        | 742/6500 [2:15:09<18:16:02, 11.42s/it]                                                        11%|        | 742/6500 [2:15:09<18:16:02, 11.42s/it] 11%|        | 743/6500 [2:15:19<17:47:05, 11.12s/it]                                                        11%|        | 743/6500 [2:15:19<17:47:05, 11.12s/it] 11%|        | 744/6500 [2:15:30<17:33:46, 10.98s/it]                                                        11%|        | 744/6500 [2:15:30<17:33:46, 10.98s/it] 11%|        | 745/6500 [2:15:40<17:17:51, 10.82s/it]                                                        11%|        | 745/6500 [2:15:40<17:17:51, 10.82s/it] 11%|        | 746/6500 [2:15:51<17:06:56, 10.71s/it]                                                        11%|        | {'loss': 0.7157, 'learning_rate': 9.679156992215162e-05, 'epoch': 0.11}
{'loss': 0.7649, 'learning_rate': 9.67830445396416e-05, 'epoch': 0.12}
{'loss': 0.7908, 'learning_rate': 9.677450822187937e-05, 'epoch': 0.12}
{'loss': 0.7339, 'learning_rate': 9.676596097086023e-05, 'epoch': 0.12}
746/6500 [2:15:51<17:06:56, 10.71s/it] 11%|        | 747/6500 [2:16:01<16:58:43, 10.62s/it]                                                        11%|        | 747/6500 [2:16:01<16:58:43, 10.62s/it] 12%|        | 748/6500 [2:16:11<16:53:15, 10.57s/it]                                                        12%|        | 748/6500 [2:16:11<16:53:15, 10.57s/it] 12%|        | 749/6500 [2:16:22<16:49:19, 10.53s/it]                                                        12%|        | 749/6500 [2:16:22<16:49:19, 10.53s/it] 12%|        | 750/6500 [2:16:32<16:46:32, 10.50s/it]                                                        12%|        | 750/6500 [2:16:32<16:46:32, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8545582890510559, 'eval_runtime': 3.9825, 'eval_samples_per_second': 5.775, 'eval_steps_per_second': 1.507, 'epoch': 0.12}
                                                        12%|        | 750/6500 [2:16:36<16:46:32, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-750
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-750
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-750/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-750/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7254, 'learning_rate': 9.675740278858208e-05, 'epoch': 0.12}
{'loss': 0.7475, 'learning_rate': 9.674883367704529e-05, 'epoch': 0.12}
{'loss': 1.2372, 'learning_rate': 9.674025363825287e-05, 'epoch': 0.12}
{'loss': 0.7671, 'learning_rate': 9.673166267421037e-05, 'epoch': 0.12}
{'loss': 0.7236, 'learning_rate': 9.672306078692583e-05, 'epoch': 0.12}
{'loss': 0.7152, 'learning_rate': 9.671444797840991e-05, 'epoch': 0.12}
 12%|        | 751/6500 [2:16:47<18:53:24, 11.83s/it]                                                        12%|        | 751/6500 [2:16:47<18:53:24, 11.83s/it] 12%|        | 752/6500 [2:16:58<18:13:44, 11.42s/it]                                                        12%|        | 752/6500 [2:16:58<18:13:44, 11.42s/it] 12%|        | 753/6500 [2:17:08<17:45:07, 11.12s/it]                                                        12%|        | 753/6500 [2:17:08<17:45:07, 11.12s/it] 12%|        | 754/6500 [2:17:19<17:25:54, 10.92s/it]                                                        12%|        | 754/6500 [2:17:19<17:25:54, 10.92s/it] 12%|        | 755/6500 [2:17:29<17:12:08, 10.78s/it]                                                        12%|        | 755/6500 [2:17:29<17:12:08, 10.78s/it] 12%|        | 756/6500 [2:17:39<17:01:57, 10.67s/it]                                                        12%|        | {'loss': 0.7089, 'learning_rate': 9.670582425067581e-05, 'epoch': 0.12}
{'loss': 0.7748, 'learning_rate': 9.669718960573927e-05, 'epoch': 0.12}
{'loss': 0.7263, 'learning_rate': 9.668854404561858e-05, 'epoch': 0.12}
{'loss': 0.7211, 'learning_rate': 9.66798875723346e-05, 'epoch': 0.12}
756/6500 [2:17:39<17:01:57, 10.67s/it] 12%|        | 757/6500 [2:17:50<16:54:30, 10.60s/it]                                                        12%|        | 757/6500 [2:17:50<16:54:30, 10.60s/it] 12%|        | 758/6500 [2:18:00<16:50:02, 10.55s/it]                                                        12%|        | 758/6500 [2:18:00<16:50:02, 10.55s/it] 12%|        | 759/6500 [2:18:11<16:46:35, 10.52s/it]                                                        12%|        | 759/6500 [2:18:11<16:46:35, 10.52s/it] 12%|        | 760/6500 [2:18:22<16:53:53, 10.60s/it]                                                        12%|        | 760/6500 [2:18:22<16:53:53, 10.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8524038791656494, 'eval_runtime': 3.9749, 'eval_samples_per_second': 5.786, 'eval_steps_per_second': 1.509, 'epoch': 0.12}
                                                        12%|        | 760/6500 [2:18:26<16:53:53, 10.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-760I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-760

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-760/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6937, 'learning_rate': 9.667122018791071e-05, 'epoch': 0.12}
{'loss': 0.7302, 'learning_rate': 9.666254189437286e-05, 'epoch': 0.12}
{'loss': 0.7176, 'learning_rate': 9.665385269374956e-05, 'epoch': 0.12}
{'loss': 0.7239, 'learning_rate': 9.664515258807185e-05, 'epoch': 0.12}
{'loss': 0.7797, 'learning_rate': 9.663644157937336e-05, 'epoch': 0.12}
{'loss': 0.7175, 'learning_rate': 9.662771966969017e-05, 'epoch': 0.12}
 12%|        | 761/6500 [2:18:36<18:56:36, 11.88s/it]                                                        12%|        | 761/6500 [2:18:36<18:56:36, 11.88s/it] 12%|        | 762/6500 [2:18:47<18:14:40, 11.45s/it]                                                        12%|        | 762/6500 [2:18:47<18:14:40, 11.45s/it] 12%|        | 763/6500 [2:18:57<17:45:01, 11.14s/it]                                                        12%|        | 763/6500 [2:18:57<17:45:01, 11.14s/it] 12%|        | 764/6500 [2:19:08<17:23:37, 10.92s/it]                                                        12%|        | 764/6500 [2:19:08<17:23:37, 10.92s/it] 12%|        | 765/6500 [2:19:18<17:09:14, 10.77s/it]                                                        12%|        | 765/6500 [2:19:18<17:09:14, 10.77s/it] 12%|        | 766/6500 [2:19:29<16:59:06, 10.66s/it]                                                        12%|        | {'loss': 0.7615, 'learning_rate': 9.661898686106101e-05, 'epoch': 0.12}
{'loss': 0.7551, 'learning_rate': 9.661024315552714e-05, 'epoch': 0.12}
{'loss': 0.7323, 'learning_rate': 9.66014885551323e-05, 'epoch': 0.12}
{'loss': 0.7428, 'learning_rate': 9.659272306192286e-05, 'epoch': 0.12}
766/6500 [2:19:29<16:59:06, 10.66s/it] 12%|        | 767/6500 [2:19:39<16:51:57, 10.59s/it]                                                        12%|        | 767/6500 [2:19:39<16:51:57, 10.59s/it] 12%|        | 768/6500 [2:19:49<16:46:40, 10.54s/it]                                                        12%|        | 768/6500 [2:19:49<16:46:40, 10.54s/it] 12%|        | 769/6500 [2:20:00<16:43:04, 10.50s/it]                                                        12%|        | 769/6500 [2:20:00<16:43:04, 10.50s/it] 12%|        | 770/6500 [2:20:10<16:41:00, 10.48s/it]                                                        12%|        | 770/6500 [2:20:10<16:41:00, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8507624268531799, 'eval_runtime': 4.2163, 'eval_samples_per_second': 5.455, 'eval_steps_per_second': 1.423, 'epoch': 0.12}
                                                        12%|        | 770/6500 [2:20:14<16:41:00, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-770
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-770/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-770/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7529, 'learning_rate': 9.658394667794771e-05, 'epoch': 0.12}
{'loss': 0.695, 'learning_rate': 9.657515940525826e-05, 'epoch': 0.12}
{'loss': 0.7456, 'learning_rate': 9.656636124590845e-05, 'epoch': 0.12}
{'loss': 0.7588, 'learning_rate': 9.655755220195486e-05, 'epoch': 0.12}
{'loss': 0.7171, 'learning_rate': 9.65487322754565e-05, 'epoch': 0.12}
{'loss': 0.7432, 'learning_rate': 9.653990146847499e-05, 'epoch': 0.12}
 12%|        | 771/6500 [2:20:25<18:51:55, 11.85s/it]                                                        12%|        | 771/6500 [2:20:25<18:51:55, 11.85s/it] 12%|        | 772/6500 [2:20:36<18:10:17, 11.42s/it]                                                        12%|        | 772/6500 [2:20:36<18:10:17, 11.42s/it] 12%|        | 773/6500 [2:20:46<17:41:18, 11.12s/it]                                                        12%|        | 773/6500 [2:20:46<17:41:18, 11.12s/it] 12%|        | 774/6500 [2:20:57<17:21:21, 10.91s/it]                                                        12%|        | 774/6500 [2:20:57<17:21:21, 10.91s/it] 12%|        | 775/6500 [2:21:07<17:07:39, 10.77s/it]                                                        12%|        | 775/6500 [2:21:07<17:07:39, 10.77s/it] 12%|        | 776/6500 [2:21:18<17:03:56, 10.73s/it]                                                        12%|        | {'loss': 0.7079, 'learning_rate': 9.653105978307449e-05, 'epoch': 0.12}
{'loss': 0.7794, 'learning_rate': 9.652220722132167e-05, 'epoch': 0.12}
{'loss': 0.7686, 'learning_rate': 9.651334378528578e-05, 'epoch': 0.12}
{'loss': 0.7165, 'learning_rate': 9.650446947703857e-05, 'epoch': 0.12}
776/6500 [2:21:18<17:03:56, 10.73s/it] 12%|        | 777/6500 [2:21:28<16:55:28, 10.65s/it]                                                        12%|        | 777/6500 [2:21:28<16:55:28, 10.65s/it] 12%|        | 778/6500 [2:21:38<16:49:22, 10.58s/it]                                                        12%|        | 778/6500 [2:21:38<16:49:22, 10.58s/it] 12%|        | 779/6500 [2:21:49<16:44:58, 10.54s/it]                                                        12%|        | 779/6500 [2:21:49<16:44:58, 10.54s/it] 12%|        | 780/6500 [2:21:59<16:42:05, 10.51s/it]                                                        12%|        | 780/6500 [2:21:59<16:42:05, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8525749444961548, 'eval_runtime': 3.9915, 'eval_samples_per_second': 5.762, 'eval_steps_per_second': 1.503, 'epoch': 0.12}
                                                        12%|        | 780/6500 [2:22:03<16:42:05, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-780I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-780

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-780
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-780/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-780/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.751, 'learning_rate': 9.64955842986544e-05, 'epoch': 0.12}
{'loss': 0.7167, 'learning_rate': 9.648668825221006e-05, 'epoch': 0.12}
{'loss': 1.2407, 'learning_rate': 9.647778133978502e-05, 'epoch': 0.12}
{'loss': 0.7357, 'learning_rate': 9.646886356346116e-05, 'epoch': 0.12}
{'loss': 0.7418, 'learning_rate': 9.645993492532298e-05, 'epoch': 0.12}
{'loss': 0.6808, 'learning_rate': 9.64509954274575e-05, 'epoch': 0.12}
 12%|        | 781/6500 [2:22:14<18:50:26, 11.86s/it]                                                        12%|        | 781/6500 [2:22:14<18:50:26, 11.86s/it] 12%|        | 782/6500 [2:22:25<18:09:06, 11.43s/it]                                                        12%|        | 782/6500 [2:22:25<18:09:06, 11.43s/it] 12%|        | 783/6500 [2:22:35<17:39:28, 11.12s/it]                                                        12%|        | 783/6500 [2:22:35<17:39:28, 11.12s/it] 12%|        | 784/6500 [2:22:46<17:19:35, 10.91s/it]                                                        12%|        | 784/6500 [2:22:46<17:19:35, 10.91s/it] 12%|        | 785/6500 [2:22:56<17:05:09, 10.76s/it]                                                        12%|        | 785/6500 [2:22:56<17:05:09, 10.76s/it] 12%|        | 786/6500 [2:23:06<16:54:45, 10.66s/it]                                                        12%|        | {'loss': 0.7423, 'learning_rate': 9.644204507195426e-05, 'epoch': 0.12}
{'loss': 0.7432, 'learning_rate': 9.643308386090537e-05, 'epoch': 0.12}
{'loss': 0.6844, 'learning_rate': 9.642411179640542e-05, 'epoch': 0.12}
{'loss': 0.7113, 'learning_rate': 9.641512888055162e-05, 'epoch': 0.12}
786/6500 [2:23:06<16:54:45, 10.66s/it] 12%|        | 787/6500 [2:23:17<16:47:51, 10.58s/it]                                                        12%|        | 787/6500 [2:23:17<16:47:51, 10.58s/it] 12%|        | 788/6500 [2:23:27<16:43:04, 10.54s/it]                                                        12%|        | 788/6500 [2:23:27<16:43:04, 10.54s/it] 12%|        | 789/6500 [2:23:38<16:39:31, 10.50s/it]                                                        12%|        | 789/6500 [2:23:38<16:39:31, 10.50s/it] 12%|        | 790/6500 [2:23:48<16:37:37, 10.48s/it]                                                        12%|        | 790/6500 [2:23:48<16:37:37, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8506575226783752, 'eval_runtime': 3.9678, 'eval_samples_per_second': 5.797, 'eval_steps_per_second': 1.512, 'epoch': 0.12}
                                                        12%|        | 790/6500 [2:23:52<16:37:37, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-790/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-790/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6961, 'learning_rate': 9.640613511544365e-05, 'epoch': 0.12}
{'loss': 0.7146, 'learning_rate': 9.639713050318375e-05, 'epoch': 0.12}
{'loss': 0.7053, 'learning_rate': 9.638811504587669e-05, 'epoch': 0.12}
{'loss': 0.7286, 'learning_rate': 9.637908874562978e-05, 'epoch': 0.12}
{'loss': 0.7434, 'learning_rate': 9.637005160455287e-05, 'epoch': 0.12}
{'loss': 0.7072, 'learning_rate': 9.636100362475832e-05, 'epoch': 0.12}
 12%|        | 791/6500 [2:24:03<18:43:34, 11.81s/it]                                                        12%|        | 791/6500 [2:24:03<18:43:34, 11.81s/it] 12%|        | 792/6500 [2:24:14<18:12:42, 11.49s/it]                                                        12%|        | 792/6500 [2:24:14<18:12:42, 11.49s/it] 12%|        | 793/6500 [2:24:24<17:42:23, 11.17s/it]                                                        12%|        | 793/6500 [2:24:24<17:42:23, 11.17s/it] 12%|        | 794/6500 [2:24:35<17:20:30, 10.94s/it]                                                        12%|        | 794/6500 [2:24:35<17:20:30, 10.94s/it] 12%|        | 795/6500 [2:24:45<17:05:29, 10.79s/it]                                                        12%|        | 795/6500 [2:24:45<17:05:29, 10.79s/it] 12%|        | 796/6500 [2:24:56<16:58:19, 10.71s/it]                                                        12%|        | {'loss': 0.7823, 'learning_rate': 9.635194480836108e-05, 'epoch': 0.12}
{'loss': 0.7404, 'learning_rate': 9.634287515747856e-05, 'epoch': 0.12}
{'loss': 0.7492, 'learning_rate': 9.633379467423072e-05, 'epoch': 0.12}
{'loss': 0.7409, 'learning_rate': 9.632470336074009e-05, 'epoch': 0.12}
796/6500 [2:24:56<16:58:19, 10.71s/it] 12%|        | 797/6500 [2:25:06<16:49:48, 10.62s/it]                                                        12%|        | 797/6500 [2:25:06<16:49:48, 10.62s/it] 12%|        | 798/6500 [2:25:17<16:56:16, 10.69s/it]                                                        12%|        | 798/6500 [2:25:17<16:56:16, 10.69s/it] 12%|        | 799/6500 [2:25:27<16:49:08, 10.62s/it]                                                        12%|        | 799/6500 [2:25:27<16:49:08, 10.62s/it] 12%|        | 800/6500 [2:25:38<16:43:09, 10.56s/it]                                                        12%|        | 800/6500 [2:25:38<16:43:09, 10.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8465900421142578, 'eval_runtime': 4.2269, 'eval_samples_per_second': 5.441, 'eval_steps_per_second': 1.419, 'epoch': 0.12}
                                                        12%|        | 800/6500 [2:25:42<16:43:09, 10.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6946, 'learning_rate': 9.631560121913172e-05, 'epoch': 0.12}
{'loss': 0.694, 'learning_rate': 9.630648825153317e-05, 'epoch': 0.12}
{'loss': 0.7459, 'learning_rate': 9.629736446007454e-05, 'epoch': 0.12}
{'loss': 0.736, 'learning_rate': 9.628822984688845e-05, 'epoch': 0.12}
{'loss': 0.7104, 'learning_rate': 9.627908441411008e-05, 'epoch': 0.12}
{'loss': 0.7192, 'learning_rate': 9.62699281638771e-05, 'epoch': 0.12}
 12%|        | 801/6500 [2:25:53<18:54:49, 11.95s/it]                                                        12%|        | 801/6500 [2:25:53<18:54:49, 11.95s/it] 12%|        | 802/6500 [2:26:03<18:10:50, 11.49s/it]                                                        12%|        | 802/6500 [2:26:03<18:10:50, 11.49s/it] 12%|        | 803/6500 [2:26:14<17:39:41, 11.16s/it]                                                        12%|        | 803/6500 [2:26:14<17:39:41, 11.16s/it] 12%|        | 804/6500 [2:26:24<17:18:25, 10.94s/it]                                                        12%|        | 804/6500 [2:26:24<17:18:25, 10.94s/it] 12%|        | 805/6500 [2:26:35<17:03:18, 10.78s/it]                                                        12%|        | 805/6500 [2:26:35<17:03:18, 10.78s/it] 12%|        | 806/6500 [2:26:45<16:52:08, 10.67s/it]                                                        12%|        | {'loss': 0.7132, 'learning_rate': 9.626076109832975e-05, 'epoch': 0.12}
{'loss': 0.8149, 'learning_rate': 9.625158321961075e-05, 'epoch': 0.12}
{'loss': 0.6947, 'learning_rate': 9.624239452986539e-05, 'epoch': 0.12}
{'loss': 0.7107, 'learning_rate': 9.623319503124148e-05, 'epoch': 0.12}
806/6500 [2:26:45<16:52:08, 10.67s/it] 12%|        | 807/6500 [2:26:55<16:46:00, 10.60s/it]                                                        12%|        | 807/6500 [2:26:55<16:46:00, 10.60s/it] 12%|        | 808/6500 [2:27:06<16:46:43, 10.61s/it]                                                        12%|        | 808/6500 [2:27:06<16:46:43, 10.61s/it] 12%|        | 809/6500 [2:27:16<16:41:06, 10.55s/it]                                                        12%|        | 809/6500 [2:27:16<16:41:06, 10.55s/it] 12%|        | 810/6500 [2:27:27<16:37:33, 10.52s/it]                                                        12%|        | 810/6500 [2:27:27<16:37:33, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.846269965171814, 'eval_runtime': 3.9814, 'eval_samples_per_second': 5.777, 'eval_steps_per_second': 1.507, 'epoch': 0.12}
                                                        12%|        | 810/6500 [2:27:31<16:37:33, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7288, 'learning_rate': 9.622398472588932e-05, 'epoch': 0.12}
{'loss': 1.2191, 'learning_rate': 9.621476361596177e-05, 'epoch': 0.12}
{'loss': 0.7319, 'learning_rate': 9.620553170361423e-05, 'epoch': 0.13}
{'loss': 0.7272, 'learning_rate': 9.619628899100459e-05, 'epoch': 0.13}
{'loss': 0.7041, 'learning_rate': 9.618703548029327e-05, 'epoch': 0.13}
{'loss': 0.6838, 'learning_rate': 9.617777117364322e-05, 'epoch': 0.13}
 12%|        | 811/6500 [2:27:42<18:40:19, 11.82s/it]                                                        12%|        | 811/6500 [2:27:42<18:40:19, 11.82s/it] 12%|        | 812/6500 [2:27:52<18:01:24, 11.41s/it]                                                        12%|        | 812/6500 [2:27:52<18:01:24, 11.41s/it] 13%|        | 813/6500 [2:28:03<17:33:13, 11.11s/it]                                                        13%|        | 813/6500 [2:28:03<17:33:13, 11.11s/it] 13%|        | 814/6500 [2:28:13<17:13:24, 10.90s/it]                                                        13%|        | 814/6500 [2:28:13<17:13:24, 10.90s/it] 13%|        | 815/6500 [2:28:23<16:59:14, 10.76s/it]                                                        13%|        | 815/6500 [2:28:23<16:59:14, 10.76s/it] 13%|        | 816/6500 [2:28:34<16:52:58, 10.69s/it]                                                        13%|        | {'loss': 0.7508, 'learning_rate': 9.616849607321994e-05, 'epoch': 0.13}
{'loss': 0.7257, 'learning_rate': 9.61592101811914e-05, 'epoch': 0.13}
{'loss': 0.6915, 'learning_rate': 9.614991349972815e-05, 'epoch': 0.13}
{'loss': 0.6883, 'learning_rate': 9.614060603100318e-05, 'epoch': 0.13}
816/6500 [2:28:34<16:52:58, 10.69s/it] 13%|        | 817/6500 [2:28:44<16:45:18, 10.61s/it]                                                        13%|        | 817/6500 [2:28:44<16:45:18, 10.61s/it] 13%|        | 818/6500 [2:28:55<16:40:35, 10.57s/it]                                                        13%|        | 818/6500 [2:28:55<16:40:35, 10.57s/it] 13%|        | 819/6500 [2:29:05<16:36:35, 10.53s/it]                                                        13%|        | 819/6500 [2:29:05<16:36:35, 10.53s/it] 13%|        | 820/6500 [2:29:16<16:33:07, 10.49s/it]                                                        13%|        | 820/6500 [2:29:16<16:33:07, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8467252850532532, 'eval_runtime': 4.0093, 'eval_samples_per_second': 5.737, 'eval_steps_per_second': 1.497, 'epoch': 0.13}
                                                        13%|        | 820/6500 [2:29:20<16:33:07, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-820
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-820/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-820/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-820/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6935, 'learning_rate': 9.613128777719214e-05, 'epoch': 0.13}
{'loss': 0.7134, 'learning_rate': 9.612195874047302e-05, 'epoch': 0.13}
{'loss': 0.6918, 'learning_rate': 9.61126189230265e-05, 'epoch': 0.13}
{'loss': 0.7445, 'learning_rate': 9.610326832703565e-05, 'epoch': 0.13}
{'loss': 0.7001, 'learning_rate': 9.609390695468616e-05, 'epoch': 0.13}
{'loss': 0.724, 'learning_rate': 9.608453480816617e-05, 'epoch': 0.13}
 13%|        | 821/6500 [2:29:31<18:37:08, 11.80s/it]                                                        13%|        | 821/6500 [2:29:31<18:37:08, 11.80s/it] 13%|        | 822/6500 [2:29:41<17:58:20, 11.39s/it]                                                        13%|        | 822/6500 [2:29:41<17:58:20, 11.39s/it] 13%|        | 823/6500 [2:29:51<17:30:36, 11.10s/it]                                                        13%|        | 823/6500 [2:29:51<17:30:36, 11.10s/it] 13%|        | 824/6500 [2:30:02<17:11:41, 10.91s/it]                                                        13%|        | 824/6500 [2:30:02<17:11:41, 10.91s/it] 13%|        | 825/6500 [2:30:13<17:08:38, 10.88s/it]                                                        13%|        | 825/6500 [2:30:13<17:08:38, 10.88s/it] 13%|        | 826/6500 [2:30:23<16:56:09, 10.75s/it]                                                        13%|        | {'loss': 0.7412, 'learning_rate': 9.607515188966638e-05, 'epoch': 0.13}
{'loss': 0.7384, 'learning_rate': 9.606575820137996e-05, 'epoch': 0.13}
{'loss': 0.7124, 'learning_rate': 9.605635374550263e-05, 'epoch': 0.13}
{'loss': 0.7237, 'learning_rate': 9.604693852423268e-05, 'epoch': 0.13}
826/6500 [2:30:23<16:56:09, 10.75s/it] 13%|        | 827/6500 [2:30:34<16:46:57, 10.65s/it]                                                        13%|        | 827/6500 [2:30:34<16:46:57, 10.65s/it] 13%|        | 828/6500 [2:30:44<16:40:20, 10.58s/it]                                                        13%|        | 828/6500 [2:30:44<16:40:20, 10.58s/it] 13%|        | 829/6500 [2:30:54<16:36:07, 10.54s/it]                                                        13%|        | 829/6500 [2:30:54<16:36:07, 10.54s/it] 13%|        | 830/6500 [2:31:05<16:33:06, 10.51s/it]                                                        13%|        | 830/6500 [2:31:05<16:33:06, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8416059017181396, 'eval_runtime': 3.9852, 'eval_samples_per_second': 5.771, 'eval_steps_per_second': 1.506, 'epoch': 0.13}
                                                        13%|        | 830/6500 [2:31:09<16:33:06, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-830
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-830
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-830/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-830/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6936, 'learning_rate': 9.603751253977079e-05, 'epoch': 0.13}
{'loss': 0.6951, 'learning_rate': 9.602807579432027e-05, 'epoch': 0.13}
{'loss': 0.752, 'learning_rate': 9.601862829008688e-05, 'epoch': 0.13}
{'loss': 0.7017, 'learning_rate': 9.600917002927893e-05, 'epoch': 0.13}
{'loss': 0.7265, 'learning_rate': 9.599970101410722e-05, 'epoch': 0.13}
{'loss': 0.6911, 'learning_rate': 9.59902212467851e-05, 'epoch': 0.13}
 13%|        | 831/6500 [2:31:20<18:37:39, 11.83s/it]                                                        13%|        | 831/6500 [2:31:20<18:37:39, 11.83s/it] 13%|        | 832/6500 [2:31:30<17:57:36, 11.41s/it]                                                        13%|        | 832/6500 [2:31:30<17:57:36, 11.41s/it] 13%|        | 833/6500 [2:31:41<17:29:37, 11.11s/it]                                                        13%|        | 833/6500 [2:31:41<17:29:37, 11.11s/it] 13%|        | 834/6500 [2:31:51<17:10:58, 10.92s/it]                                                        13%|        | 834/6500 [2:31:51<17:10:58, 10.92s/it] 13%|        | 835/6500 [2:32:01<16:55:08, 10.75s/it]                                                        13%|        | 835/6500 [2:32:01<16:55:08, 10.75s/it] 13%|        | 836/6500 [2:32:12<16:44:13, 10.64s/it]                                                        13%|        | {'loss': 0.7273, 'learning_rate': 9.598073072952836e-05, 'epoch': 0.13}
{'loss': 0.765, 'learning_rate': 9.59712294645554e-05, 'epoch': 0.13}
{'loss': 0.7051, 'learning_rate': 9.596171745408705e-05, 'epoch': 0.13}
{'loss': 0.6838, 'learning_rate': 9.595219470034671e-05, 'epoch': 0.13}
836/6500 [2:32:12<16:44:13, 10.64s/it] 13%|        | 837/6500 [2:32:22<16:36:41, 10.56s/it]                                                        13%|        | 837/6500 [2:32:22<16:36:41, 10.56s/it] 13%|        | 838/6500 [2:32:33<16:31:20, 10.51s/it]                                                        13%|        | 838/6500 [2:32:33<16:31:20, 10.51s/it] 13%|        | 839/6500 [2:32:43<16:27:33, 10.47s/it]                                                        13%|        | 839/6500 [2:32:43<16:27:33, 10.47s/it] 13%|        | 840/6500 [2:32:53<16:25:24, 10.45s/it]                                                        13%|        | 840/6500 [2:32:53<16:25:24, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8429563641548157, 'eval_runtime': 4.2101, 'eval_samples_per_second': 5.463, 'eval_steps_per_second': 1.425, 'epoch': 0.13}
                                                        13%|        | 840/6500 [2:32:58<16:25:24, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-840
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-840/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-840/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7206, 'learning_rate': 9.594266120556023e-05, 'epoch': 0.13}
{'loss': 1.2005, 'learning_rate': 9.593311697195605e-05, 'epoch': 0.13}
{'loss': 0.745, 'learning_rate': 9.592356200176504e-05, 'epoch': 0.13}
{'loss': 0.7004, 'learning_rate': 9.591399629722066e-05, 'epoch': 0.13}
{'loss': 0.6625, 'learning_rate': 9.590441986055878e-05, 'epoch': 0.13}
{'loss': 0.7003, 'learning_rate': 9.589483269401786e-05, 'epoch': 0.13}
 13%|        | 841/6500 [2:33:09<18:43:25, 11.91s/it]                                                        13%|        | 841/6500 [2:33:09<18:43:25, 11.91s/it] 13%|        | 842/6500 [2:33:19<17:58:41, 11.44s/it]                                                        13%|        | 842/6500 [2:33:19<17:58:41, 11.44s/it] 13%|        | 843/6500 [2:33:29<17:28:06, 11.12s/it]                                                        13%|        | 843/6500 [2:33:29<17:28:06, 11.12s/it] 13%|        | 844/6500 [2:33:40<17:06:21, 10.89s/it]                                                        13%|        | 844/6500 [2:33:40<17:06:21, 10.89s/it] 13%|        | 845/6500 [2:33:50<16:50:47, 10.72s/it]                                                        13%|        | 845/6500 [2:33:50<16:50:47, 10.72s/it] 13%|        | 846/6500 [2:34:00<16:40:24, 10.62s/it]                                                        13%|        | {'loss': 0.7329, 'learning_rate': 9.588523479983887e-05, 'epoch': 0.13}
{'loss': 0.706, 'learning_rate': 9.58756261802652e-05, 'epoch': 0.13}
{'loss': 0.6969, 'learning_rate': 9.586600683754287e-05, 'epoch': 0.13}
{'loss': 0.6686, 'learning_rate': 9.58563767739203e-05, 'epoch': 0.13}
846/6500 [2:34:00<16:40:24, 10.62s/it] 13%|        | 847/6500 [2:34:11<16:33:02, 10.54s/it]                                                        13%|        | 847/6500 [2:34:11<16:33:02, 10.54s/it] 13%|        | 848/6500 [2:34:21<16:27:52, 10.49s/it]                                                        13%|        | 848/6500 [2:34:21<16:27:52, 10.49s/it] 13%|        | 849/6500 [2:34:32<16:23:55, 10.45s/it]                                                        13%|        | 849/6500 [2:34:32<16:23:55, 10.45s/it] 13%|        | 850/6500 [2:34:42<16:20:45, 10.42s/it]                                                        13%|        | 850/6500 [2:34:42<16:20:45, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8435727953910828, 'eval_runtime': 3.9547, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.13}
                                                        13%|        | 850/6500 [2:34:46<16:20:45, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-850
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-850/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-850/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6962, 'learning_rate': 9.584673599164846e-05, 'epoch': 0.13}
{'loss': 0.6873, 'learning_rate': 9.583708449298083e-05, 'epoch': 0.13}
{'loss': 0.6942, 'learning_rate': 9.582742228017342e-05, 'epoch': 0.13}
{'loss': 0.7374, 'learning_rate': 9.581774935548467e-05, 'epoch': 0.13}
{'loss': 0.6881, 'learning_rate': 9.580806572117557e-05, 'epoch': 0.13}
{'loss': 0.7393, 'learning_rate': 9.579837137950966e-05, 'epoch': 0.13}
 13%|        | 851/6500 [2:34:57<18:23:12, 11.72s/it]                                                        13%|        | 851/6500 [2:34:57<18:23:12, 11.72s/it] 13%|        | 852/6500 [2:35:07<17:44:38, 11.31s/it]                                                        13%|        | 852/6500 [2:35:07<17:44:38, 11.31s/it] 13%|        | 853/6500 [2:35:17<17:17:36, 11.02s/it]                                                        13%|        | 853/6500 [2:35:17<17:17:36, 11.02s/it] 13%|        | 854/6500 [2:35:28<16:58:51, 10.83s/it]                                                        13%|        | 854/6500 [2:35:28<16:58:51, 10.83s/it] 13%|        | 855/6500 [2:35:38<16:45:57, 10.69s/it]                                                        13%|        | 855/6500 [2:35:38<16:45:57, 10.69s/it] 13%|        | 856/6500 [2:35:48<16:36:41, 10.60s/it]                                                        13%|        | {'loss': 0.7308, 'learning_rate': 9.578866633275288e-05, 'epoch': 0.13}
{'loss': 0.7091, 'learning_rate': 9.577895058317374e-05, 'epoch': 0.13}
{'loss': 0.7169, 'learning_rate': 9.576922413304326e-05, 'epoch': 0.13}
{'loss': 0.7107, 'learning_rate': 9.575948698463491e-05, 'epoch': 0.13}
856/6500 [2:35:48<16:36:41, 10.60s/it] 13%|        | 857/6500 [2:35:59<16:40:15, 10.64s/it]                                                        13%|        | 857/6500 [2:35:59<16:40:15, 10.64s/it] 13%|        | 858/6500 [2:36:10<16:32:34, 10.56s/it]                                                        13%|        | 858/6500 [2:36:10<16:32:34, 10.56s/it] 13%|        | 859/6500 [2:36:20<16:27:18, 10.50s/it]                                                        13%|        | 859/6500 [2:36:20<16:27:18, 10.50s/it] 13%|        | 860/6500 [2:36:30<16:23:29, 10.46s/it]                                                        13%|        | 860/6500 [2:36:30<16:23:29, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8430450558662415, 'eval_runtime': 3.9584, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.13}
                                                        13%|        | 860/6500 [2:36:34<16:23:29, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-860I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-860/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-860/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6896, 'learning_rate': 9.574973914022469e-05, 'epoch': 0.13}
{'loss': 0.7022, 'learning_rate': 9.57399806020911e-05, 'epoch': 0.13}
{'loss': 0.7225, 'learning_rate': 9.573021137251516e-05, 'epoch': 0.13}
{'loss': 0.6957, 'learning_rate': 9.572043145378038e-05, 'epoch': 0.13}
{'loss': 0.7088, 'learning_rate': 9.571064084817271e-05, 'epoch': 0.13}
{'loss': 0.6765, 'learning_rate': 9.570083955798065e-05, 'epoch': 0.13}
 13%|        | 861/6500 [2:36:45<18:28:01, 11.79s/it]                                                        13%|        | 861/6500 [2:36:45<18:28:01, 11.79s/it] 13%|        | 862/6500 [2:36:56<17:48:33, 11.37s/it]                                                        13%|        | 862/6500 [2:36:56<17:48:33, 11.37s/it] 13%|        | 863/6500 [2:37:06<17:20:43, 11.08s/it]                                                        13%|        | 863/6500 [2:37:06<17:20:43, 11.08s/it] 13%|        | 864/6500 [2:37:16<17:01:25, 10.87s/it]                                                        13%|        | 864/6500 [2:37:16<17:01:25, 10.87s/it] 13%|        | 865/6500 [2:37:27<16:50:04, 10.76s/it]                                                        13%|        | 865/6500 [2:37:27<16:50:04, 10.76s/it] 13%|        | 866/6500 [2:37:37<16:45:14, 10.71s/it]                                                        13%|        | {'loss': 0.7512, 'learning_rate': 9.569102758549524e-05, 'epoch': 0.13}
{'loss': 0.7362, 'learning_rate': 9.568120493300993e-05, 'epoch': 0.13}
{'loss': 0.6925, 'learning_rate': 9.567137160282071e-05, 'epoch': 0.13}
{'loss': 0.7125, 'learning_rate': 9.566152759722606e-05, 'epoch': 0.13}
866/6500 [2:37:37<16:45:14, 10.71s/it] 13%|        | 867/6500 [2:37:48<16:35:44, 10.61s/it]                                                        13%|        | 867/6500 [2:37:48<16:35:44, 10.61s/it] 13%|        | 868/6500 [2:37:58<16:29:28, 10.54s/it]                                                        13%|        | 868/6500 [2:37:58<16:29:28, 10.54s/it] 13%|        | 869/6500 [2:38:09<16:24:50, 10.49s/it]                                                        13%|        | 869/6500 [2:38:09<16:24:50, 10.49s/it] 13%|        | 870/6500 [2:38:19<16:21:35, 10.46s/it]                                                        13%|        | 870/6500 [2:38:19<16:21:35, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.841886043548584, 'eval_runtime': 4.0331, 'eval_samples_per_second': 5.703, 'eval_steps_per_second': 1.488, 'epoch': 0.13}
                                                        13%|        | 870/6500 [2:38:23<16:21:35, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-870I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-870
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-870/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-870/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-870/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-870/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7021, 'learning_rate': 9.565167291852697e-05, 'epoch': 0.13}
{'loss': 1.2104, 'learning_rate': 9.56418075690269e-05, 'epoch': 0.13}
{'loss': 0.7017, 'learning_rate': 9.563193155103181e-05, 'epoch': 0.13}
{'loss': 0.71, 'learning_rate': 9.562204486685017e-05, 'epoch': 0.13}
{'loss': 0.6533, 'learning_rate': 9.561214751879292e-05, 'epoch': 0.13}
{'loss': 0.7173, 'learning_rate': 9.560223950917353e-05, 'epoch': 0.13}
 13%|        | 871/6500 [2:38:34<18:32:22, 11.86s/it]                                                        13%|        | 871/6500 [2:38:34<18:32:22, 11.86s/it] 13%|        | 872/6500 [2:38:44<17:50:44, 11.42s/it]                                                        13%|        | 872/6500 [2:38:44<17:50:44, 11.42s/it] 13%|        | 873/6500 [2:38:55<17:31:46, 11.21s/it]                                                        13%|        | 873/6500 [2:38:55<17:31:46, 11.21s/it] 13%|        | 874/6500 [2:39:06<17:08:04, 10.96s/it]                                                        13%|        | 874/6500 [2:39:06<17:08:04, 10.96s/it] 13%|        | 875/6500 [2:39:16<16:51:30, 10.79s/it]                                                        13%|        | 875/6500 [2:39:16<16:51:30, 10.79s/it] 13%|        | 876/6500 [2:39:26<16:39:36, 10.66s/it]                                                        13%|        | {'loss': 0.7204, 'learning_rate': 9.559232084030791e-05, 'epoch': 0.13}
{'loss': 0.6503, 'learning_rate': 9.558239151451451e-05, 'epoch': 0.14}
{'loss': 0.6883, 'learning_rate': 9.557245153411423e-05, 'epoch': 0.14}
{'loss': 0.6708, 'learning_rate': 9.556250090143049e-05, 'epoch': 0.14}
876/6500 [2:39:26<16:39:36, 10.66s/it] 13%|        | 877/6500 [2:39:37<16:31:41, 10.58s/it]                                                        13%|        | 877/6500 [2:39:37<16:31:41, 10.58s/it] 14%|        | 878/6500 [2:39:47<16:26:21, 10.53s/it]                                                        14%|        | 878/6500 [2:39:47<16:26:21, 10.53s/it] 14%|        | 879/6500 [2:39:58<16:25:22, 10.52s/it]                                                        14%|        | 879/6500 [2:39:58<16:25:22, 10.52s/it] 14%|        | 880/6500 [2:40:08<16:21:10, 10.48s/it]                                                        14%|        | 880/6500 [2:40:08<16:21:10, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8414161801338196, 'eval_runtime': 3.9752, 'eval_samples_per_second': 5.786, 'eval_steps_per_second': 1.509, 'epoch': 0.14}
                                                        14%|        | 880/6500 [2:40:12<16:21:10, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-880
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-880
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.691, 'learning_rate': 9.55525396187892e-05, 'epoch': 0.14}
{'loss': 0.6869, 'learning_rate': 9.554256768851873e-05, 'epoch': 0.14}
{'loss': 0.6977, 'learning_rate': 9.553258511294996e-05, 'epoch': 0.14}
{'loss': 0.7039, 'learning_rate': 9.552259189441626e-05, 'epoch': 0.14}
{'loss': 0.6797, 'learning_rate': 9.55125880352535e-05, 'epoch': 0.14}
{'loss': 0.7441, 'learning_rate': 9.55025735378e-05, 'epoch': 0.14}
 14%|        | 881/6500 [2:40:23<18:24:13, 11.79s/it]                                                        14%|        | 881/6500 [2:40:23<18:24:13, 11.79s/it] 14%|        | 882/6500 [2:40:33<17:44:26, 11.37s/it]                                                        14%|        | 882/6500 [2:40:33<17:44:26, 11.37s/it] 14%|        | 883/6500 [2:40:44<17:16:39, 11.07s/it]                                                        14%|        | 883/6500 [2:40:44<17:16:39, 11.07s/it] 14%|        | 884/6500 [2:40:54<16:58:12, 10.88s/it]                                                        14%|        | 884/6500 [2:40:54<16:58:12, 10.88s/it] 14%|        | 885/6500 [2:41:04<16:44:01, 10.73s/it]                                                        14%|        | 885/6500 [2:41:04<16:44:01, 10.73s/it] 14%|        | 886/6500 [2:41:15<16:34:08, 10.62s/it]                                                        14%|        | {'loss': 0.7158, 'learning_rate': 9.549254840439659e-05, 'epoch': 0.14}
{'loss': 0.6972, 'learning_rate': 9.54825126373866e-05, 'epoch': 0.14}
{'loss': 0.721, 'learning_rate': 9.547246623911582e-05, 'epoch': 0.14}
{'loss': 0.669, 'learning_rate': 9.546240921193253e-05, 'epoch': 0.14}
886/6500 [2:41:15<16:34:08, 10.62s/it] 14%|        | 887/6500 [2:41:25<16:27:39, 10.56s/it]                                                        14%|        | 887/6500 [2:41:25<16:27:39, 10.56s/it] 14%|        | 888/6500 [2:41:36<16:22:48, 10.51s/it]                                                        14%|        | 888/6500 [2:41:36<16:22:48, 10.51s/it] 14%|        | 889/6500 [2:41:46<16:26:38, 10.55s/it]                                                        14%|        | 889/6500 [2:41:46<16:26:38, 10.55s/it] 14%|        | 890/6500 [2:41:57<16:22:23, 10.51s/it]                                                        14%|        | 890/6500 [2:41:57<16:22:23, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8361510634422302, 'eval_runtime': 3.9781, 'eval_samples_per_second': 5.782, 'eval_steps_per_second': 1.508, 'epoch': 0.14}
                                                        14%|        | 890/6500 [2:42:01<16:22:23, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-890
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.669, 'learning_rate': 9.54523415581875e-05, 'epoch': 0.14}
{'loss': 0.7194, 'learning_rate': 9.544226328023398e-05, 'epoch': 0.14}
{'loss': 0.706, 'learning_rate': 9.543217438042773e-05, 'epoch': 0.14}
{'loss': 0.693, 'learning_rate': 9.542207486112694e-05, 'epoch': 0.14}
{'loss': 0.6752, 'learning_rate': 9.541196472469233e-05, 'epoch': 0.14}
{'loss': 0.7215, 'learning_rate': 9.540184397348706e-05, 'epoch': 0.14}
 14%|        | 891/6500 [2:42:12<18:23:30, 11.80s/it]                                                        14%|        | 891/6500 [2:42:12<18:23:30, 11.80s/it] 14%|        | 892/6500 [2:42:22<17:43:28, 11.38s/it]                                                        14%|        | 892/6500 [2:42:22<17:43:28, 11.38s/it] 14%|        | 893/6500 [2:42:33<17:28:41, 11.22s/it]                                                        14%|        | 893/6500 [2:42:33<17:28:41, 11.22s/it] 14%|        | 894/6500 [2:42:43<17:06:31, 10.99s/it]                                                        14%|        | 894/6500 [2:42:43<17:06:31, 10.99s/it] 14%|        | 895/6500 [2:42:54<16:49:32, 10.81s/it]                                                        14%|        | 895/6500 [2:42:54<16:49:32, 10.81s/it] 14%|        | 896/6500 [2:43:04<16:37:59, 10.69s/it]                                                        14%|        | {'loss': 0.747, 'learning_rate': 9.539171260987681e-05, 'epoch': 0.14}
{'loss': 0.6729, 'learning_rate': 9.538157063622974e-05, 'epoch': 0.14}
{'loss': 0.6777, 'learning_rate': 9.537141805491646e-05, 'epoch': 0.14}
{'loss': 0.6988, 'learning_rate': 9.536125486831005e-05, 'epoch': 0.14}
896/6500 [2:43:04<16:37:59, 10.69s/it] 14%|        | 897/6500 [2:43:14<16:30:09, 10.60s/it]                                                        14%|        | 897/6500 [2:43:14<16:30:09, 10.60s/it] 14%|        | 898/6500 [2:43:25<16:24:37, 10.55s/it]                                                        14%|        | 898/6500 [2:43:25<16:24:37, 10.55s/it] 14%|        | 899/6500 [2:43:35<16:21:02, 10.51s/it]                                                        14%|        | 899/6500 [2:43:35<16:21:02, 10.51s/it] 14%|        | 900/6500 [2:43:46<16:18:43, 10.49s/it]                                                        14%|        | 900/6500 [2:43:46<16:18:43, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.837685763835907, 'eval_runtime': 4.2552, 'eval_samples_per_second': 5.405, 'eval_steps_per_second': 1.41, 'epoch': 0.14}
                                                        14%|        | 900/6500 [2:43:50<16:18:43, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-900
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-900/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-900/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1993, 'learning_rate': 9.535108107878612e-05, 'epoch': 0.14}
{'loss': 0.7006, 'learning_rate': 9.534089668872274e-05, 'epoch': 0.14}
{'loss': 0.6853, 'learning_rate': 9.533070170050042e-05, 'epoch': 0.14}
{'loss': 0.6744, 'learning_rate': 9.53204961165022e-05, 'epoch': 0.14}
{'loss': 0.6667, 'learning_rate': 9.531027993911356e-05, 'epoch': 0.14}
{'loss': 0.7084, 'learning_rate': 9.53000531707225e-05, 'epoch': 0.14}
 14%|        | 901/6500 [2:44:01<18:29:54, 11.89s/it]                                                        14%|        | 901/6500 [2:44:01<18:29:54, 11.89s/it] 14%|        | 902/6500 [2:44:11<17:48:22, 11.45s/it]                                                        14%|        | 902/6500 [2:44:11<17:48:22, 11.45s/it] 14%|        | 903/6500 [2:44:22<17:19:02, 11.14s/it]                                                        14%|        | 903/6500 [2:44:22<17:19:02, 11.14s/it] 14%|        | 904/6500 [2:44:32<16:58:05, 10.92s/it]                                                        14%|        | 904/6500 [2:44:32<16:58:05, 10.92s/it] 14%|        | 905/6500 [2:44:43<16:53:37, 10.87s/it]                                                        14%|        | 905/6500 [2:44:43<16:53:37, 10.87s/it] 14%|        | 906/6500 [2:44:53<16:40:35, 10.73s/it]                                                        14%|        | {'loss': 0.6976, 'learning_rate': 9.528981581371942e-05, 'epoch': 0.14}
{'loss': 0.6659, 'learning_rate': 9.527956787049727e-05, 'epoch': 0.14}
{'loss': 0.6517, 'learning_rate': 9.526930934345142e-05, 'epoch': 0.14}
{'loss': 0.6738, 'learning_rate': 9.525904023497975e-05, 'epoch': 0.14}
906/6500 [2:44:53<16:40:35, 10.73s/it] 14%|        | 907/6500 [2:45:04<16:30:53, 10.63s/it]                                                        14%|        | 907/6500 [2:45:04<16:30:53, 10.63s/it] 14%|        | 908/6500 [2:45:14<16:24:16, 10.56s/it]                                                        14%|        | 908/6500 [2:45:14<16:24:16, 10.56s/it] 14%|        | 909/6500 [2:45:24<16:19:13, 10.51s/it]                                                        14%|        | 909/6500 [2:45:24<16:19:13, 10.51s/it] 14%|        | 910/6500 [2:45:35<16:15:36, 10.47s/it]                                                        14%|        | 910/6500 [2:45:35<16:15:36, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8418816328048706, 'eval_runtime': 3.9561, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.14}
                                                        14%|        | 910/6500 [2:45:39<16:15:36, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-910I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-910

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6733, 'learning_rate': 9.524876054748262e-05, 'epoch': 0.14}
{'loss': 0.6615, 'learning_rate': 9.523847028336283e-05, 'epoch': 0.14}
{'loss': 0.7171, 'learning_rate': 9.522816944502565e-05, 'epoch': 0.14}
{'loss': 0.6814, 'learning_rate': 9.521785803487889e-05, 'epoch': 0.14}
{'loss': 0.692, 'learning_rate': 9.52075360553327e-05, 'epoch': 0.14}
{'loss': 0.7108, 'learning_rate': 9.519720350879985e-05, 'epoch': 0.14}
 14%|        | 911/6500 [2:45:50<18:16:11, 11.77s/it]                                                        14%|        | 911/6500 [2:45:50<18:16:11, 11.77s/it] 14%|        | 912/6500 [2:46:00<17:36:52, 11.35s/it]                                                        14%|        | 912/6500 [2:46:00<17:36:52, 11.35s/it] 14%|        | 913/6500 [2:46:10<17:09:40, 11.06s/it]                                                        14%|        | 913/6500 [2:46:10<17:09:40, 11.06s/it] 14%|        | 914/6500 [2:46:21<16:50:23, 10.85s/it]                                                        14%|        | 914/6500 [2:46:21<16:50:23, 10.85s/it] 14%|        | 915/6500 [2:46:31<16:37:01, 10.71s/it]                                                        14%|        | 915/6500 [2:46:31<16:37:01, 10.71s/it] 14%|        | 916/6500 [2:46:41<16:27:54, 10.62s/it]                                                        14%|        | {'loss': 0.6949, 'learning_rate': 9.518686039769548e-05, 'epoch': 0.14}
{'loss': 0.6942, 'learning_rate': 9.517650672443722e-05, 'epoch': 0.14}
{'loss': 0.6959, 'learning_rate': 9.51661424914452e-05, 'epoch': 0.14}
{'loss': 0.6653, 'learning_rate': 9.515576770114199e-05, 'epoch': 0.14}
916/6500 [2:46:41<16:27:54, 10.62s/it] 14%|        | 917/6500 [2:46:52<16:21:32, 10.55s/it]                                                        14%|        | 917/6500 [2:46:52<16:21:32, 10.55s/it] 14%|        | 918/6500 [2:47:02<16:16:53, 10.50s/it]                                                        14%|        | 918/6500 [2:47:02<16:16:53, 10.50s/it] 14%|        | 919/6500 [2:47:13<16:13:40, 10.47s/it]                                                        14%|        | 919/6500 [2:47:13<16:13:40, 10.47s/it] 14%|        | 920/6500 [2:47:23<16:11:49, 10.45s/it]                                                        14%|        | 920/6500 [2:47:23<16:11:49, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.834083616733551, 'eval_runtime': 3.9769, 'eval_samples_per_second': 5.783, 'eval_steps_per_second': 1.509, 'epoch': 0.14}
                                                        14%|        | 920/6500 [2:47:27<16:11:49, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-920
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-920/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.686, 'learning_rate': 9.514538235595262e-05, 'epoch': 0.14}
{'loss': 0.7166, 'learning_rate': 9.513498645830462e-05, 'epoch': 0.14}
{'loss': 0.679, 'learning_rate': 9.512458001062797e-05, 'epoch': 0.14}
{'loss': 0.6829, 'learning_rate': 9.511416301535508e-05, 'epoch': 0.14}
{'loss': 0.6618, 'learning_rate': 9.51037354749209e-05, 'epoch': 0.14}
{'loss': 0.7148, 'learning_rate': 9.509329739176278e-05, 'epoch': 0.14}
 14%|        | 921/6500 [2:47:38<18:13:40, 11.76s/it]                                                        14%|        | 921/6500 [2:47:38<18:13:40, 11.76s/it] 14%|        | 922/6500 [2:47:49<17:43:04, 11.43s/it]                                                        14%|        | 922/6500 [2:47:49<17:43:04, 11.43s/it] 14%|        | 923/6500 [2:47:59<17:23:01, 11.22s/it]                                                        14%|        | 923/6500 [2:47:59<17:23:01, 11.22s/it] 14%|        | 924/6500 [2:48:10<17:08:15, 11.06s/it]                                                        14%|        | 924/6500 [2:48:10<17:08:15, 11.06s/it] 14%|        | 925/6500 [2:48:20<16:50:34, 10.88s/it]                                                        14%|        | 925/6500 [2:48:20<16:50:34, 10.88s/it] 14%|        | 926/6500 [2:48:31<16:38:21, 10.75s/it]                                                        14%|        | {'loss': 0.7398, 'learning_rate': 9.508284876832058e-05, 'epoch': 0.14}
{'loss': 0.6725, 'learning_rate': 9.507238960703659e-05, 'epoch': 0.14}
{'loss': 0.6781, 'learning_rate': 9.506191991035556e-05, 'epoch': 0.14}
{'loss': 0.6813, 'learning_rate': 9.505143968072474e-05, 'epoch': 0.14}
926/6500 [2:48:31<16:38:21, 10.75s/it] 14%|        | 927/6500 [2:48:41<16:30:02, 10.66s/it]                                                        14%|        | 927/6500 [2:48:41<16:30:02, 10.66s/it] 14%|        | 928/6500 [2:48:52<16:24:03, 10.60s/it]                                                        14%|        | 928/6500 [2:48:52<16:24:03, 10.60s/it] 14%|        | 929/6500 [2:49:02<16:19:53, 10.55s/it]                                                        14%|        | 929/6500 [2:49:02<16:19:53, 10.55s/it] 14%|        | 930/6500 [2:49:13<16:16:52, 10.52s/it]                                                        14%|        | 930/6500 [2:49:13<16:16:52, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8345983028411865, 'eval_runtime': 4.0147, 'eval_samples_per_second': 5.729, 'eval_steps_per_second': 1.495, 'epoch': 0.14}
                                                        14%|        | 930/6500 [2:49:17<16:16:52, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-930
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-930
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-930/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-930/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1985, 'learning_rate': 9.50409489205938e-05, 'epoch': 0.14}
{'loss': 0.7017, 'learning_rate': 9.50304476324149e-05, 'epoch': 0.14}
{'loss': 0.6762, 'learning_rate': 9.501993581864268e-05, 'epoch': 0.14}
{'loss': 0.6347, 'learning_rate': 9.500941348173417e-05, 'epoch': 0.14}
{'loss': 0.675, 'learning_rate': 9.499888062414893e-05, 'epoch': 0.14}
{'loss': 0.7442, 'learning_rate': 9.498833724834895e-05, 'epoch': 0.14}
 14%|        | 931/6500 [2:49:28<18:18:52, 11.84s/it]                                                        14%|        | 931/6500 [2:49:28<18:18:52, 11.84s/it] 14%|        | 932/6500 [2:49:38<17:39:40, 11.42s/it]                                                        14%|        | 932/6500 [2:49:38<17:39:40, 11.42s/it] 14%|        | 933/6500 [2:49:48<17:12:01, 11.12s/it]                                                        14%|        | 933/6500 [2:49:48<17:12:01, 11.12s/it] 14%|        | 934/6500 [2:49:59<16:52:34, 10.92s/it]                                                        14%|        | 934/6500 [2:49:59<16:52:34, 10.92s/it] 14%|        | 935/6500 [2:50:09<16:39:08, 10.77s/it]                                                        14%|        | 935/6500 [2:50:09<16:39:08, 10.77s/it] 14%|        | 936/6500 [2:50:20<16:29:12, 10.67s/it]                                                        14%|        | {'loss': 0.6525, 'learning_rate': 9.497778335679865e-05, 'epoch': 0.14}
{'loss': 0.6625, 'learning_rate': 9.496721895196497e-05, 'epoch': 0.14}
{'loss': 0.6519, 'learning_rate': 9.495664403631727e-05, 'epoch': 0.14}
{'loss': 0.6745, 'learning_rate': 9.494605861232736e-05, 'epoch': 0.14}
936/6500 [2:50:20<16:29:12, 10.67s/it] 14%|        | 937/6500 [2:50:30<16:22:37, 10.60s/it]                                                        14%|        | 937/6500 [2:50:30<16:22:37, 10.60s/it] 14%|        | 938/6500 [2:50:41<16:35:16, 10.74s/it]                                                        14%|        | 938/6500 [2:50:41<16:35:16, 10.74s/it] 14%|        | 939/6500 [2:50:52<16:26:33, 10.64s/it]                                                        14%|        | 939/6500 [2:50:52<16:26:33, 10.64s/it] 14%|        | 940/6500 [2:51:02<16:20:32, 10.58s/it]                                                        14%|        | 940/6500 [2:51:02<16:20:32, 10.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8387199640274048, 'eval_runtime': 3.9821, 'eval_samples_per_second': 5.776, 'eval_steps_per_second': 1.507, 'epoch': 0.14}
                                                        14%|        | 940/6500 [2:51:06<16:20:32, 10.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-940
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6624, 'learning_rate': 9.493546268246954e-05, 'epoch': 0.14}
{'loss': 0.6776, 'learning_rate': 9.492485624922051e-05, 'epoch': 0.14}
{'loss': 0.7067, 'learning_rate': 9.491423931505949e-05, 'epoch': 0.15}
{'loss': 0.6613, 'learning_rate': 9.49036118824681e-05, 'epoch': 0.15}
{'loss': 0.7146, 'learning_rate': 9.489297395393047e-05, 'epoch': 0.15}
{'loss': 0.6967, 'learning_rate': 9.488232553193312e-05, 'epoch': 0.15}
 14%|        | 941/6500 [2:51:17<18:21:48, 11.89s/it]                                                        14%|        | 941/6500 [2:51:17<18:21:48, 11.89s/it] 14%|        | 942/6500 [2:51:27<17:40:21, 11.45s/it]                                                        14%|        | 942/6500 [2:51:27<17:40:21, 11.45s/it] 15%|        | 943/6500 [2:51:38<17:11:50, 11.14s/it]                                                        15%|        | 943/6500 [2:51:38<17:11:50, 11.14s/it] 15%|        | 944/6500 [2:51:48<16:51:28, 10.92s/it]                                                        15%|        | 944/6500 [2:51:48<16:51:28, 10.92s/it] 15%|        | 945/6500 [2:51:59<16:37:03, 10.77s/it]                                                        15%|        | 945/6500 [2:51:59<16:37:03, 10.77s/it] 15%|        | 946/6500 [2:52:09<16:26:56, 10.66s/it]                                                        15%|        | {'loss': 0.675, 'learning_rate': 9.487166661896507e-05, 'epoch': 0.15}
{'loss': 0.6991, 'learning_rate': 9.486099721751777e-05, 'epoch': 0.15}
{'loss': 0.6714, 'learning_rate': 9.485031733008514e-05, 'epoch': 0.15}
{'loss': 0.6734, 'learning_rate': 9.48396269591635e-05, 'epoch': 0.15}
946/6500 [2:52:09<16:26:56, 10.66s/it] 15%|        | 947/6500 [2:52:20<16:20:00, 10.59s/it]                                                        15%|        | 947/6500 [2:52:20<16:20:00, 10.59s/it] 15%|        | 948/6500 [2:52:30<16:15:13, 10.54s/it]                                                        15%|        | 948/6500 [2:52:30<16:15:13, 10.54s/it] 15%|        | 949/6500 [2:52:40<16:11:41, 10.50s/it]                                                        15%|        | 949/6500 [2:52:40<16:11:41, 10.50s/it] 15%|        | 950/6500 [2:52:51<16:09:36, 10.48s/it]                                                        15%|        | 950/6500 [2:52:51<16:09:36, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8296633958816528, 'eval_runtime': 3.9801, 'eval_samples_per_second': 5.779, 'eval_steps_per_second': 1.508, 'epoch': 0.15}
                                                        15%|        | 950/6500 [2:52:55<16:09:36, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-950/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6736, 'learning_rate': 9.482892610725171e-05, 'epoch': 0.15}
{'loss': 0.7037, 'learning_rate': 9.481821477685101e-05, 'epoch': 0.15}
{'loss': 0.6878, 'learning_rate': 9.48074929704651e-05, 'epoch': 0.15}
{'loss': 0.6754, 'learning_rate': 9.479676069060016e-05, 'epoch': 0.15}
{'loss': 0.6645, 'learning_rate': 9.478601793976474e-05, 'epoch': 0.15}
{'loss': 0.7635, 'learning_rate': 9.477526472046995e-05, 'epoch': 0.15}
 15%|        | 951/6500 [2:53:06<18:13:51, 11.83s/it]                                                        15%|        | 951/6500 [2:53:06<18:13:51, 11.83s/it] 15%|        | 952/6500 [2:53:16<17:34:40, 11.41s/it]                                                        15%|        | 952/6500 [2:53:16<17:34:40, 11.41s/it] 15%|        | 953/6500 [2:53:27<17:07:38, 11.12s/it]                                                        15%|        | 953/6500 [2:53:27<17:07:38, 11.12s/it] 15%|        | 954/6500 [2:53:37<16:54:53, 10.98s/it]                                                        15%|        | 954/6500 [2:53:37<16:54:53, 10.98s/it] 15%|        | 955/6500 [2:53:48<16:40:09, 10.82s/it]                                                        15%|        | 955/6500 [2:53:48<16:40:09, 10.82s/it] 15%|        | 956/6500 [2:53:58<16:29:16, 10.71s/it]                                                        15%|        | {'loss': 0.6655, 'learning_rate': 9.476450103522927e-05, 'epoch': 0.15}
{'loss': 0.6588, 'learning_rate': 9.475372688655864e-05, 'epoch': 0.15}
{'loss': 0.6892, 'learning_rate': 9.474294227697647e-05, 'epoch': 0.15}
{'loss': 0.9433, 'learning_rate': 9.473214720900356e-05, 'epoch': 0.15}
956/6500 [2:53:58<16:29:16, 10.71s/it] 15%|        | 957/6500 [2:54:09<16:21:20, 10.62s/it]                                                        15%|        | 957/6500 [2:54:09<16:21:20, 10.62s/it] 15%|        | 958/6500 [2:54:19<16:16:01, 10.57s/it]                                                        15%|        | 958/6500 [2:54:19<16:16:01, 10.57s/it] 15%|        | 959/6500 [2:54:29<16:11:54, 10.52s/it]                                                        15%|        | 959/6500 [2:54:29<16:11:54, 10.52s/it] 15%|        | 960/6500 [2:54:40<16:09:30, 10.50s/it]                                                        15%|        | 960/6500 [2:54:40<16:09:30, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8310864567756653, 'eval_runtime': 4.2209, 'eval_samples_per_second': 5.449, 'eval_steps_per_second': 1.422, 'epoch': 0.15}
                                                        15%|        | 960/6500 [2:54:44<16:09:30, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-960
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9258, 'learning_rate': 9.472134168516324e-05, 'epoch': 0.15}
{'loss': 0.6936, 'learning_rate': 9.47105257079812e-05, 'epoch': 0.15}
{'loss': 0.6603, 'learning_rate': 9.469969927998564e-05, 'epoch': 0.15}
{'loss': 0.6315, 'learning_rate': 9.468886240370712e-05, 'epoch': 0.15}
{'loss': 0.6837, 'learning_rate': 9.467801508167875e-05, 'epoch': 0.15}
{'loss': 0.6965, 'learning_rate': 9.466715731643598e-05, 'epoch': 0.15}
 15%|        | 961/6500 [2:54:56<18:34:31, 12.07s/it]                                                        15%|        | 961/6500 [2:54:56<18:34:31, 12.07s/it] 15%|        | 962/6500 [2:55:06<17:55:45, 11.65s/it]                                                        15%|        | 962/6500 [2:55:06<17:55:45, 11.65s/it] 15%|        | 963/6500 [2:55:17<17:21:58, 11.29s/it]                                                        15%|        | 963/6500 [2:55:17<17:21:58, 11.29s/it] 15%|        | 964/6500 [2:55:27<16:58:04, 11.03s/it]                                                        15%|        | 964/6500 [2:55:27<16:58:04, 11.03s/it] 15%|        | 965/6500 [2:55:38<16:41:35, 10.86s/it]                                                        15%|        | 965/6500 [2:55:38<16:41:35, 10.86s/it] 15%|        | 966/6500 [2:55:48<16:30:14, 10.74s/it]                                                        15%|        | {'loss': 0.6218, 'learning_rate': 9.465628911051679e-05, 'epoch': 0.15}
{'loss': 0.6654, 'learning_rate': 9.464541046646152e-05, 'epoch': 0.15}
{'loss': 0.6363, 'learning_rate': 9.463452138681301e-05, 'epoch': 0.15}
{'loss': 0.665, 'learning_rate': 9.462362187411651e-05, 'epoch': 0.15}
966/6500 [2:55:48<16:30:14, 10.74s/it] 15%|        | 967/6500 [2:55:59<16:22:02, 10.65s/it]                                                        15%|        | 967/6500 [2:55:59<16:22:02, 10.65s/it] 15%|        | 968/6500 [2:56:09<16:16:22, 10.59s/it]                                                        15%|        | 968/6500 [2:56:09<16:16:22, 10.59s/it] 15%|        | 969/6500 [2:56:19<16:11:55, 10.54s/it]                                                        15%|        | 969/6500 [2:56:19<16:11:55, 10.54s/it] 15%|        | 970/6500 [2:56:30<16:20:02, 10.63s/it]                                                        15%|        | 970/6500 [2:56:30<16:20:02, 10.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8378238677978516, 'eval_runtime': 3.9777, 'eval_samples_per_second': 5.782, 'eval_steps_per_second': 1.508, 'epoch': 0.15}
                                                        15%|        | 970/6500 [2:56:34<16:20:02, 10.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-970
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-970/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6523, 'learning_rate': 9.46127119309197e-05, 'epoch': 0.15}
{'loss': 0.6824, 'learning_rate': 9.460179155977274e-05, 'epoch': 0.15}
{'loss': 0.6704, 'learning_rate': 9.459086076322818e-05, 'epoch': 0.15}
{'loss': 0.6745, 'learning_rate': 9.457991954384105e-05, 'epoch': 0.15}
{'loss': 0.7177, 'learning_rate': 9.456896790416875e-05, 'epoch': 0.15}
{'loss': 0.709, 'learning_rate': 9.455800584677119e-05, 'epoch': 0.15}
 15%|        | 971/6500 [2:56:45<18:19:19, 11.93s/it]                                                        15%|        | 971/6500 [2:56:45<18:19:19, 11.93s/it] 15%|        | 972/6500 [2:56:56<17:39:18, 11.50s/it]                                                        15%|        | 972/6500 [2:56:56<17:39:18, 11.50s/it] 15%|        | 973/6500 [2:57:06<17:09:45, 11.18s/it]                                                        15%|        | 973/6500 [2:57:06<17:09:45, 11.18s/it] 15%|        | 974/6500 [2:57:17<16:48:23, 10.95s/it]                                                        15%|        | 974/6500 [2:57:17<16:48:23, 10.95s/it] 15%|        | 975/6500 [2:57:27<16:33:24, 10.79s/it]                                                        15%|        | 975/6500 [2:57:27<16:33:24, 10.79s/it] 15%|        | 976/6500 [2:57:37<16:23:25, 10.68s/it]                                                        15%|        | {'loss': 0.6796, 'learning_rate': 9.454703337421069e-05, 'epoch': 0.15}
{'loss': 0.6905, 'learning_rate': 9.453605048905199e-05, 'epoch': 0.15}
{'loss': 0.6427, 'learning_rate': 9.452505719386227e-05, 'epoch': 0.15}
{'loss': 0.6598, 'learning_rate': 9.451405349121115e-05, 'epoch': 0.15}
976/6500 [2:57:37<16:23:25, 10.68s/it] 15%|        | 977/6500 [2:57:48<16:16:08, 10.60s/it]                                                        15%|        | 977/6500 [2:57:48<16:16:08, 10.60s/it] 15%|        | 978/6500 [2:57:58<16:11:24, 10.56s/it]                                                        15%|        | 978/6500 [2:57:58<16:11:24, 10.56s/it] 15%|        | 979/6500 [2:58:09<16:08:08, 10.52s/it]                                                        15%|        | 979/6500 [2:58:09<16:08:08, 10.52s/it] 15%|        | 980/6500 [2:58:19<16:06:00, 10.50s/it]                                                        15%|        | 980/6500 [2:58:19<16:06:00, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8265735507011414, 'eval_runtime': 3.9609, 'eval_samples_per_second': 5.807, 'eval_steps_per_second': 1.515, 'epoch': 0.15}
                                                        15%|        | 980/6500 [2:58:23<16:06:00, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-980/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6988, 'learning_rate': 9.450303938367067e-05, 'epoch': 0.15}
{'loss': 0.672, 'learning_rate': 9.449201487381532e-05, 'epoch': 0.15}
{'loss': 0.7501, 'learning_rate': 9.448097996422201e-05, 'epoch': 0.15}
{'loss': 0.6572, 'learning_rate': 9.446993465747006e-05, 'epoch': 0.15}
{'loss': 0.6969, 'learning_rate': 9.44588789561413e-05, 'epoch': 0.15}
{'loss': 0.7221, 'learning_rate': 9.44478128628199e-05, 'epoch': 0.15}
 15%|        | 981/6500 [2:58:34<18:06:31, 11.81s/it]                                                        15%|        | 981/6500 [2:58:34<18:06:31, 11.81s/it] 15%|        | 982/6500 [2:58:45<17:28:18, 11.40s/it]                                                        15%|        | 982/6500 [2:58:45<17:28:18, 11.40s/it] 15%|        | 983/6500 [2:58:56<17:19:14, 11.30s/it]                                                        15%|        | 983/6500 [2:58:56<17:19:14, 11.30s/it] 15%|        | 984/6500 [2:59:06<16:57:22, 11.07s/it]                                                        15%|        | 984/6500 [2:59:06<16:57:22, 11.07s/it] 15%|        | 985/6500 [2:59:17<16:39:32, 10.87s/it]                                                        15%|        | 985/6500 [2:59:17<16:39:32, 10.87s/it] 15%|        | 986/6500 [2:59:27<16:31:39, 10.79s/it]                                                        15%|        | {'loss': 0.6579, 'learning_rate': 9.443673638009247e-05, 'epoch': 0.15}
{'loss': 0.6574, 'learning_rate': 9.442564951054809e-05, 'epoch': 0.15}
{'loss': 0.6723, 'learning_rate': 9.441455225677827e-05, 'epoch': 0.15}
{'loss': 1.1698, 'learning_rate': 9.440344462137689e-05, 'epoch': 0.15}
986/6500 [2:59:27<16:31:39, 10.79s/it] 15%|        | 987/6500 [2:59:38<16:21:44, 10.68s/it]                                                        15%|        | 987/6500 [2:59:38<16:21:44, 10.68s/it] 15%|        | 988/6500 [2:59:48<16:14:40, 10.61s/it]                                                        15%|        | 988/6500 [2:59:48<16:14:40, 10.61s/it] 15%|        | 989/6500 [2:59:59<16:15:08, 10.62s/it]                                                        15%|        | 989/6500 [2:59:59<16:15:08, 10.62s/it] 15%|        | 990/6500 [3:00:09<16:08:45, 10.55s/it]                                                        15%|        | 990/6500 [3:00:09<16:08:45, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.827791690826416, 'eval_runtime': 4.2891, 'eval_samples_per_second': 5.362, 'eval_steps_per_second': 1.399, 'epoch': 0.15}
                                                        15%|        | 990/6500 [3:00:13<16:08:45, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7167, 'learning_rate': 9.43923266069403e-05, 'epoch': 0.15}
{'loss': 0.6596, 'learning_rate': 9.438119821606727e-05, 'epoch': 0.15}
{'loss': 0.6493, 'learning_rate': 9.437005945135903e-05, 'epoch': 0.15}
{'loss': 0.642, 'learning_rate': 9.435891031541915e-05, 'epoch': 0.15}
{'loss': 0.7005, 'learning_rate': 9.434775081085368e-05, 'epoch': 0.15}
{'loss': 0.6555, 'learning_rate': 9.433658094027111e-05, 'epoch': 0.15}
 15%|        | 991/6500 [3:00:24<18:16:20, 11.94s/it]                                                        15%|        | 991/6500 [3:00:24<18:16:20, 11.94s/it] 15%|        | 992/6500 [3:00:35<17:33:44, 11.48s/it]                                                        15%|        | 992/6500 [3:00:35<17:33:44, 11.48s/it] 15%|        | 993/6500 [3:00:45<17:04:31, 11.16s/it]                                                        15%|        | 993/6500 [3:00:45<17:04:31, 11.16s/it] 15%|        | 994/6500 [3:00:55<16:43:36, 10.94s/it]                                                        15%|        | 994/6500 [3:00:55<16:43:36, 10.94s/it] 15%|        | 995/6500 [3:01:06<16:31:13, 10.80s/it]                                                        15%|        | 995/6500 [3:01:06<16:31:13, 10.80s/it] 15%|        | 996/6500 [3:01:16<16:20:25, 10.69s/it]                                                        15%|        | {'loss': 0.6565, 'learning_rate': 9.432540070628231e-05, 'epoch': 0.15}
{'loss': 0.6235, 'learning_rate': 9.431421011150062e-05, 'epoch': 0.15}
{'loss': 0.6502, 'learning_rate': 9.430300915854172e-05, 'epoch': 0.15}
{'loss': 0.6559, 'learning_rate': 9.42917978500238e-05, 'epoch': 0.15}
996/6500 [3:01:16<16:20:25, 10.69s/it] 15%|        | 997/6500 [3:01:27<16:13:20, 10.61s/it]                                                        15%|        | 997/6500 [3:01:27<16:13:20, 10.61s/it] 15%|        | 998/6500 [3:01:37<16:07:16, 10.55s/it]                                                        15%|        | 998/6500 [3:01:37<16:07:16, 10.55s/it] 15%|        | 999/6500 [3:01:48<16:03:31, 10.51s/it]                                                        15%|        | 999/6500 [3:01:48<16:03:31, 10.51s/it] 15%|        | 1000/6500 [3:01:58<16:01:06, 10.48s/it]                                                         15%|        | 1000/6500 [3:01:58<16:01:06, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8356145620346069, 'eval_runtime': 3.9723, 'eval_samples_per_second': 5.79, 'eval_steps_per_second': 1.51, 'epoch': 0.15}
                                                         15%|        | 1000/6500 [3:02:02<16:01:06, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1000/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6424, 'learning_rate': 9.428057618856745e-05, 'epoch': 0.15}
{'loss': 0.6999, 'learning_rate': 9.426934417679563e-05, 'epoch': 0.15}
{'loss': 0.6503, 'learning_rate': 9.425810181733377e-05, 'epoch': 0.15}
{'loss': 0.6904, 'learning_rate': 9.424684911280972e-05, 'epoch': 0.15}
{'loss': 0.6769, 'learning_rate': 9.423558606585369e-05, 'epoch': 0.15}
{'loss': 0.655, 'learning_rate': 9.42243126790984e-05, 'epoch': 0.15}
 15%|        | 1001/6500 [3:02:13<18:03:43, 11.82s/it]                                                         15%|        | 1001/6500 [3:02:13<18:03:43, 11.82s/it] 15%|        | 1002/6500 [3:02:24<17:35:59, 11.52s/it]                                                         15%|        | 1002/6500 [3:02:24<17:35:59, 11.52s/it] 15%|        | 1003/6500 [3:02:34<17:05:11, 11.19s/it]                                                         15%|        | 1003/6500 [3:02:34<17:05:11, 11.19s/it] 15%|        | 1004/6500 [3:02:45<16:43:31, 10.96s/it]                                                         15%|        | 1004/6500 [3:02:45<16:43:31, 10.96s/it] 15%|        | 1005/6500 [3:02:55<16:29:28, 10.80s/it]                                                         15%|        | 1005/6500 [3:02:55<16:29:28, 10.80s/it] 15%|        | 1006/6500 [3:03:05<16:18:21, 10.68s/it]                                                         15%{'loss': 0.6712, 'learning_rate': 9.42130289551789e-05, 'epoch': 0.15}
{'loss': 0.6685, 'learning_rate': 9.420173489673269e-05, 'epoch': 0.16}
{'loss': 0.6278, 'learning_rate': 9.419043050639973e-05, 'epoch': 0.16}
{'loss': 0.6637, 'learning_rate': 9.417911578682229e-05, 'epoch': 0.16}
|        | 1006/6500 [3:03:05<16:18:21, 10.68s/it] 15%|        | 1007/6500 [3:03:16<16:10:36, 10.60s/it]                                                         15%|        | 1007/6500 [3:03:16<16:10:36, 10.60s/it] 16%|        | 1008/6500 [3:03:26<16:05:50, 10.55s/it]                                                         16%|        | 1008/6500 [3:03:26<16:05:50, 10.55s/it] 16%|        | 1009/6500 [3:03:37<16:01:54, 10.51s/it]                                                         16%|        | 1009/6500 [3:03:37<16:01:54, 10.51s/it] 16%|        | 1010/6500 [3:03:47<15:59:36, 10.49s/it]                                                         16%|        | 1010/6500 [3:03:47<15:59:36, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.824267566204071, 'eval_runtime': 4.0447, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.483, 'epoch': 0.16}
                                                         16%|        | 1010/6500 [3:03:51<15:59:36, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1010
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6984, 'learning_rate': 9.416779074064517e-05, 'epoch': 0.16}
{'loss': 0.6539, 'learning_rate': 9.415645537051549e-05, 'epoch': 0.16}
{'loss': 0.6738, 'learning_rate': 9.414510967908286e-05, 'epoch': 0.16}
{'loss': 0.634, 'learning_rate': 9.413375366899923e-05, 'epoch': 0.16}
{'loss': 0.706, 'learning_rate': 9.412238734291903e-05, 'epoch': 0.16}
{'loss': 0.6943, 'learning_rate': 9.411101070349905e-05, 'epoch': 0.16}
 16%|        | 1011/6500 [3:04:03<18:13:40, 11.95s/it]                                                         16%|        | 1011/6500 [3:04:03<18:13:40, 11.95s/it] 16%|        | 1012/6500 [3:04:13<17:32:13, 11.50s/it]                                                         16%|        | 1012/6500 [3:04:13<17:32:13, 11.50s/it] 16%|        | 1013/6500 [3:04:23<17:02:44, 11.18s/it]                                                         16%|        | 1013/6500 [3:04:23<17:02:44, 11.18s/it] 16%|        | 1014/6500 [3:04:34<16:41:16, 10.95s/it]                                                         16%|        | 1014/6500 [3:04:34<16:41:16, 10.95s/it] 16%|        | 1015/6500 [3:04:44<16:26:18, 10.79s/it]                                                         16%|        | 1015/6500 [3:04:44<16:26:18, 10.79s/it] 16%|        | 1016/6500 [3:04:55<16:19:05, 10.71s/it]                                                         16%{'loss': 0.6511, 'learning_rate': 9.409962375339851e-05, 'epoch': 0.16}
{'loss': 0.6553, 'learning_rate': 9.408822649527906e-05, 'epoch': 0.16}
{'loss': 0.6469, 'learning_rate': 9.407681893180473e-05, 'epoch': 0.16}
{'loss': 1.1681, 'learning_rate': 9.406540106564196e-05, 'epoch': 0.16}
|        | 1016/6500 [3:04:55<16:19:05, 10.71s/it] 16%|        | 1017/6500 [3:05:05<16:10:49, 10.62s/it]                                                         16%|        | 1017/6500 [3:05:05<16:10:49, 10.62s/it] 16%|        | 1018/6500 [3:05:16<16:05:20, 10.57s/it]                                                         16%|        | 1018/6500 [3:05:16<16:05:20, 10.57s/it] 16%|        | 1019/6500 [3:05:26<16:06:54, 10.58s/it]                                                         16%|        | 1019/6500 [3:05:26<16:06:54, 10.58s/it] 16%|        | 1020/6500 [3:05:37<16:01:59, 10.53s/it]                                                         16%|        | 1020/6500 [3:05:37<16:01:59, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8297044634819031, 'eval_runtime': 4.3233, 'eval_samples_per_second': 5.32, 'eval_steps_per_second': 1.388, 'epoch': 0.16}
                                                         16%|        | 1020/6500 [3:05:41<16:01:59, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1020
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1020/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.676, 'learning_rate': 9.405397289945963e-05, 'epoch': 0.16}
{'loss': 0.6725, 'learning_rate': 9.404253443592896e-05, 'epoch': 0.16}
{'loss': 0.6031, 'learning_rate': 9.403108567772367e-05, 'epoch': 0.16}
{'loss': 0.6656, 'learning_rate': 9.40196266275198e-05, 'epoch': 0.16}
{'loss': 0.68, 'learning_rate': 9.400815728799586e-05, 'epoch': 0.16}
{'loss': 0.6294, 'learning_rate': 9.399667766183274e-05, 'epoch': 0.16}
 16%|        | 1021/6500 [3:05:52<18:13:40, 11.98s/it]                                                         16%|        | 1021/6500 [3:05:52<18:13:40, 11.98s/it] 16%|        | 1022/6500 [3:06:02<17:30:28, 11.51s/it]                                                         16%|        | 1022/6500 [3:06:02<17:30:28, 11.51s/it] 16%|        | 1023/6500 [3:06:13<17:00:06, 11.18s/it]                                                         16%|        | 1023/6500 [3:06:13<17:00:06, 11.18s/it] 16%|        | 1024/6500 [3:06:23<16:38:58, 10.95s/it]                                                         16%|        | 1024/6500 [3:06:23<16:38:58, 10.95s/it] 16%|        | 1025/6500 [3:06:34<16:23:56, 10.78s/it]                                                         16%|        | 1025/6500 [3:06:34<16:23:56, 10.78s/it] 16%|        | 1026/6500 [3:06:44<16:13:10, 10.67s/it]                                                         16%{'loss': 0.6421, 'learning_rate': 9.39851877517137e-05, 'epoch': 0.16}
{'loss': 0.641, 'learning_rate': 9.397368756032445e-05, 'epoch': 0.16}
{'loss': 0.6335, 'learning_rate': 9.396217709035312e-05, 'epoch': 0.16}
{'loss': 0.6435, 'learning_rate': 9.395065634449018e-05, 'epoch': 0.16}
|        | 1026/6500 [3:06:44<16:13:10, 10.67s/it] 16%|        | 1027/6500 [3:06:54<16:06:57, 10.60s/it]                                                         16%|        | 1027/6500 [3:06:54<16:06:57, 10.60s/it] 16%|        | 1028/6500 [3:07:05<16:01:23, 10.54s/it]                                                         16%|        | 1028/6500 [3:07:05<16:01:23, 10.54s/it] 16%|        | 1029/6500 [3:07:15<15:57:22, 10.50s/it]                                                         16%|        | 1029/6500 [3:07:15<15:57:22, 10.50s/it] 16%|        | 1030/6500 [3:07:26<15:54:42, 10.47s/it]                                                         16%|        | 1030/6500 [3:07:26<15:54:42, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8292645812034607, 'eval_runtime': 3.9736, 'eval_samples_per_second': 5.788, 'eval_steps_per_second': 1.51, 'epoch': 0.16}
                                                         16%|        | 1030/6500 [3:07:30<15:54:42, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1030/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1030/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6537, 'learning_rate': 9.393912532542854e-05, 'epoch': 0.16}
{'loss': 0.6684, 'learning_rate': 9.392758403586352e-05, 'epoch': 0.16}
{'loss': 0.627, 'learning_rate': 9.391603247849281e-05, 'epoch': 0.16}
{'loss': 0.7185, 'learning_rate': 9.390447065601651e-05, 'epoch': 0.16}
{'loss': 0.661, 'learning_rate': 9.389289857113715e-05, 'epoch': 0.16}
{'loss': 0.6765, 'learning_rate': 9.388131622655962e-05, 'epoch': 0.16}
 16%|        | 1031/6500 [3:07:41<17:54:55, 11.79s/it]                                                         16%|        | 1031/6500 [3:07:41<17:54:55, 11.79s/it] 16%|        | 1032/6500 [3:07:51<17:17:19, 11.38s/it]                                                         16%|        | 1032/6500 [3:07:51<17:17:19, 11.38s/it] 16%|        | 1033/6500 [3:08:01<16:51:10, 11.10s/it]                                                         16%|        | 1033/6500 [3:08:01<16:51:10, 11.10s/it] 16%|        | 1034/6500 [3:08:12<16:32:13, 10.89s/it]                                                         16%|        | 1034/6500 [3:08:12<16:32:13, 10.89s/it] 16%|        | 1035/6500 [3:08:23<16:29:26, 10.86s/it]                                                         16%|        | 1035/6500 [3:08:23<16:29:26, 10.86s/it] 16%|        | 1036/6500 [3:08:33<16:16:34, 10.72s/it]                                                         16%{'loss': 0.6714, 'learning_rate': 9.386972362499123e-05, 'epoch': 0.16}
{'loss': 0.641, 'learning_rate': 9.385812076914167e-05, 'epoch': 0.16}
{'loss': 0.6549, 'learning_rate': 9.384650766172305e-05, 'epoch': 0.16}
{'loss': 0.6493, 'learning_rate': 9.383488430544984e-05, 'epoch': 0.16}
|        | 1036/6500 [3:08:33<16:16:34, 10.72s/it] 16%|        | 1037/6500 [3:08:43<16:07:56, 10.63s/it]                                                         16%|        | 1037/6500 [3:08:43<16:07:56, 10.63s/it] 16%|        | 1038/6500 [3:08:54<16:01:40, 10.56s/it]                                                         16%|        | 1038/6500 [3:08:54<16:01:40, 10.56s/it] 16%|        | 1039/6500 [3:09:04<15:57:17, 10.52s/it]                                                         16%|        | 1039/6500 [3:09:04<15:57:17, 10.52s/it] 16%|        | 1040/6500 [3:09:15<16:01:08, 10.56s/it]                                                         16%|        | 1040/6500 [3:09:15<16:01:08, 10.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8213202357292175, 'eval_runtime': 4.0211, 'eval_samples_per_second': 5.72, 'eval_steps_per_second': 1.492, 'epoch': 0.16}
                                                         16%|        | 1040/6500 [3:09:19<16:01:08, 10.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1040I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1040

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1040/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1040/pytorch_model.bin
 the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1040/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6672, 'learning_rate': 9.382325070303896e-05, 'epoch': 0.16}
{'loss': 0.6551, 'learning_rate': 9.381160685720967e-05, 'epoch': 0.16}
{'loss': 0.6514, 'learning_rate': 9.379995277068365e-05, 'epoch': 0.16}
{'loss': 0.6296, 'learning_rate': 9.378828844618499e-05, 'epoch': 0.16}
{'loss': 0.7441, 'learning_rate': 9.377661388644014e-05, 'epoch': 0.16}
{'loss': 0.6411, 'learning_rate': 9.376492909417795e-05, 'epoch': 0.16}
 16%|        | 1041/6500 [3:09:30<18:03:40, 11.91s/it]                                                         16%|        | 1041/6500 [3:09:30<18:03:40, 11.91s/it] 16%|        | 1042/6500 [3:09:40<17:22:43, 11.46s/it]                                                         16%|        | 1042/6500 [3:09:40<17:22:43, 11.46s/it] 16%|        | 1043/6500 [3:09:51<16:53:43, 11.15s/it]                                                         16%|        | 1043/6500 [3:09:51<16:53:43, 11.15s/it] 16%|        | 1044/6500 [3:10:01<16:33:36, 10.93s/it]                                                         16%|        | 1044/6500 [3:10:01<16:33:36, 10.93s/it] 16%|        | 1045/6500 [3:10:12<16:19:28, 10.77s/it]                                                         16%|        | 1045/6500 [3:10:12<16:19:28, 10.77s/it] 16%|        | 1046/6500 [3:10:22<16:09:17, 10.66s/it]                                                         16%{'loss': 0.6419, 'learning_rate': 9.375323407212969e-05, 'epoch': 0.16}
{'loss': 0.6568, 'learning_rate': 9.374152882302898e-05, 'epoch': 0.16}
{'loss': 1.1716, 'learning_rate': 9.372981334961187e-05, 'epoch': 0.16}
{'loss': 0.6689, 'learning_rate': 9.371808765461677e-05, 'epoch': 0.16}
|        | 1046/6500 [3:10:22<16:09:17, 10.66s/it] 16%|        | 1047/6500 [3:10:33<16:09:44, 10.67s/it]                                                         16%|        | 1047/6500 [3:10:33<16:09:44, 10.67s/it] 16%|        | 1048/6500 [3:10:43<16:03:26, 10.60s/it]                                                         16%|        | 1048/6500 [3:10:43<16:03:26, 10.60s/it] 16%|        | 1049/6500 [3:10:54<15:59:17, 10.56s/it]                                                         16%|        | 1049/6500 [3:10:54<15:59:17, 10.56s/it] 16%|        | 1050/6500 [3:11:04<15:55:15, 10.52s/it]                                                         16%|        | 1050/6500 [3:11:04<15:55:15, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.82624751329422, 'eval_runtime': 4.0334, 'eval_samples_per_second': 5.702, 'eval_steps_per_second': 1.488, 'epoch': 0.16}
                                                         16%|        | 1050/6500 [3:11:08<15:55:15, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1050/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1050/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1050/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.66, 'learning_rate': 9.370635174078448e-05, 'epoch': 0.16}
{'loss': 0.6405, 'learning_rate': 9.369460561085823e-05, 'epoch': 0.16}
{'loss': 0.6111, 'learning_rate': 9.368284926758357e-05, 'epoch': 0.16}
{'loss': 0.6808, 'learning_rate': 9.36710827137085e-05, 'epoch': 0.16}
{'loss': 0.6502, 'learning_rate': 9.365930595198336e-05, 'epoch': 0.16}
{'loss': 0.6102, 'learning_rate': 9.364751898516091e-05, 'epoch': 0.16}
 16%|        | 1051/6500 [3:11:20<18:15:54, 12.07s/it]                                                         16%|        | 1051/6500 [3:11:20<18:15:54, 12.07s/it] 16%|        | 1052/6500 [3:11:30<17:31:38, 11.58s/it]                                                         16%|        | 1052/6500 [3:11:30<17:31:38, 11.58s/it] 16%|        | 1053/6500 [3:11:41<17:00:14, 11.24s/it]                                                         16%|        | 1053/6500 [3:11:41<17:00:14, 11.24s/it] 16%|        | 1054/6500 [3:11:51<16:38:04, 11.00s/it]                                                         16%|        | 1054/6500 [3:11:51<16:38:04, 11.00s/it] 16%|        | 1055/6500 [3:12:02<16:22:52, 10.83s/it]                                                         16%|        | 1055/6500 [3:12:02<16:22:52, 10.83s/it] 16%|        | 1056/6500 [3:12:12<16:12:01, 10.71s/it]                                                         16%{'loss': 0.6263, 'learning_rate': 9.363572181599628e-05, 'epoch': 0.16}
{'loss': 0.6304, 'learning_rate': 9.362391444724699e-05, 'epoch': 0.16}
{'loss': 0.6409, 'learning_rate': 9.361209688167292e-05, 'epoch': 0.16}
{'loss': 0.6245, 'learning_rate': 9.36002691220364e-05, 'epoch': 0.16}
|        | 1056/6500 [3:12:12<16:12:01, 10.71s/it] 16%|        | 1057/6500 [3:12:22<16:04:16, 10.63s/it]                                                         16%|        | 1057/6500 [3:12:22<16:04:16, 10.63s/it] 16%|        | 1058/6500 [3:12:33<16:03:10, 10.62s/it]                                                         16%|        | 1058/6500 [3:12:33<16:03:10, 10.62s/it] 16%|        | 1059/6500 [3:12:43<15:58:43, 10.57s/it]                                                         16%|        | 1059/6500 [3:12:43<15:58:43, 10.57s/it] 16%|        | 1060/6500 [3:12:54<15:55:00, 10.53s/it]                                                         16%|        | 1060/6500 [3:12:54<15:55:00, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.82488614320755, 'eval_runtime': 4.0471, 'eval_samples_per_second': 5.683, 'eval_steps_per_second': 1.483, 'epoch': 0.16}
                                                         16%|        | 1060/6500 [3:12:58<15:55:00, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1060I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1060/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6662, 'learning_rate': 9.358843117110204e-05, 'epoch': 0.16}
{'loss': 0.6308, 'learning_rate': 9.357658303163693e-05, 'epoch': 0.16}
{'loss': 0.6559, 'learning_rate': 9.356472470641047e-05, 'epoch': 0.16}
{'loss': 0.678, 'learning_rate': 9.35528561981945e-05, 'epoch': 0.16}
{'loss': 0.6646, 'learning_rate': 9.354097750976319e-05, 'epoch': 0.16}
{'loss': 0.6502, 'learning_rate': 9.352908864389312e-05, 'epoch': 0.16}
 16%|        | 1061/6500 [3:13:09<17:59:23, 11.91s/it]                                                         16%|        | 1061/6500 [3:13:09<17:59:23, 11.91s/it] 16%|        | 1062/6500 [3:13:19<17:19:21, 11.47s/it]                                                         16%|        | 1062/6500 [3:13:19<17:19:21, 11.47s/it] 16%|        | 1063/6500 [3:13:30<16:51:46, 11.17s/it]                                                         16%|        | 1063/6500 [3:13:30<16:51:46, 11.17s/it] 16%|        | 1064/6500 [3:13:40<16:31:36, 10.94s/it]                                                         16%|        | 1064/6500 [3:13:40<16:31:36, 10.94s/it] 16%|        | 1065/6500 [3:13:51<16:17:12, 10.79s/it]                                                         16%|        | 1065/6500 [3:13:51<16:17:12, 10.79s/it] 16%|        | 1066/6500 [3:14:01<16:07:15, 10.68s/it]                                                         16%{'loss': 0.6492, 'learning_rate': 9.351718960336325e-05, 'epoch': 0.16}
{'loss': 0.6319, 'learning_rate': 9.35052803909549e-05, 'epoch': 0.16}
{'loss': 0.6203, 'learning_rate': 9.349336100945176e-05, 'epoch': 0.16}
{'loss': 0.6924, 'learning_rate': 9.348143146163994e-05, 'epoch': 0.16}
|        | 1066/6500 [3:14:01<16:07:15, 10.68s/it] 16%|        | 1067/6500 [3:14:12<16:07:24, 10.68s/it]                                                         16%|        | 1067/6500 [3:14:12<16:07:24, 10.68s/it] 16%|        | 1068/6500 [3:14:22<16:00:21, 10.61s/it]                                                         16%|        | 1068/6500 [3:14:22<16:00:21, 10.61s/it] 16%|        | 1069/6500 [3:14:33<15:55:41, 10.56s/it]                                                         16%|        | 1069/6500 [3:14:33<15:55:41, 10.56s/it] 16%|        | 1070/6500 [3:14:43<15:52:36, 10.53s/it]                                                         16%|        | 1070/6500 [3:14:43<15:52:36, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8171853423118591, 'eval_runtime': 3.9886, 'eval_samples_per_second': 5.766, 'eval_steps_per_second': 1.504, 'epoch': 0.16}
                                                         16%|        | 1070/6500 [3:14:47<15:52:36, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1070/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1070/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6414, 'learning_rate': 9.346949175030791e-05, 'epoch': 0.16}
{'loss': 0.6559, 'learning_rate': 9.345754187824644e-05, 'epoch': 0.16}
{'loss': 0.6307, 'learning_rate': 9.34455818482488e-05, 'epoch': 0.17}
{'loss': 0.6548, 'learning_rate': 9.343361166311057e-05, 'epoch': 0.17}
{'loss': 0.7023, 'learning_rate': 9.342163132562967e-05, 'epoch': 0.17}
{'loss': 0.638, 'learning_rate': 9.340964083860648e-05, 'epoch': 0.17}
 16%|        | 1071/6500 [3:14:58<17:51:15, 11.84s/it]                                                         16%|        | 1071/6500 [3:14:58<17:51:15, 11.84s/it] 16%|        | 1072/6500 [3:15:09<17:12:54, 11.42s/it]                                                         16%|        | 1072/6500 [3:15:09<17:12:54, 11.42s/it] 17%|        | 1073/6500 [3:15:19<16:46:12, 11.12s/it]                                                         17%|        | 1073/6500 [3:15:19<16:46:12, 11.12s/it] 17%|        | 1074/6500 [3:15:29<16:27:50, 10.92s/it]                                                         17%|        | 1074/6500 [3:15:29<16:27:50, 10.92s/it] 17%|        | 1075/6500 [3:15:40<16:14:35, 10.78s/it]                                                         17%|        | 1075/6500 [3:15:40<16:14:35, 10.78s/it] 17%|        | 1076/6500 [3:15:50<16:06:41, 10.69s/it]                                                         17%{'loss': 0.6249, 'learning_rate': 9.339764020484366e-05, 'epoch': 0.17}
{'loss': 0.642, 'learning_rate': 9.338562942714631e-05, 'epoch': 0.17}
{'loss': 1.1533, 'learning_rate': 9.337360850832187e-05, 'epoch': 0.17}
{'loss': 0.6768, 'learning_rate': 9.336157745118016e-05, 'epoch': 0.17}
|        | 1076/6500 [3:15:50<16:06:41, 10.69s/it] 17%|        | 1077/6500 [3:16:01<16:00:00, 10.62s/it]                                                         17%|        | 1077/6500 [3:16:01<16:00:00, 10.62s/it] 17%|        | 1078/6500 [3:16:11<15:56:12, 10.58s/it]                                                         17%|        | 1078/6500 [3:16:11<15:56:12, 10.58s/it] 17%|        | 1079/6500 [3:16:22<15:51:58, 10.54s/it]                                                         17%|        | 1079/6500 [3:16:22<15:51:58, 10.54s/it] 17%|        | 1080/6500 [3:16:32<15:48:49, 10.50s/it]                                                         17%|        | 1080/6500 [3:16:32<15:48:49, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8218197226524353, 'eval_runtime': 4.2055, 'eval_samples_per_second': 5.469, 'eval_steps_per_second': 1.427, 'epoch': 0.17}
                                                         17%|        | 1080/6500 [3:16:36<15:48:49, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1080I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1080/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1080/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1080/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.635, 'learning_rate': 9.334953625853335e-05, 'epoch': 0.17}
{'loss': 0.6034, 'learning_rate': 9.333748493319603e-05, 'epoch': 0.17}
{'loss': 0.6392, 'learning_rate': 9.332542347798509e-05, 'epoch': 0.17}
{'loss': 0.672, 'learning_rate': 9.331335189571984e-05, 'epoch': 0.17}
{'loss': 0.6389, 'learning_rate': 9.330127018922194e-05, 'epoch': 0.17}
{'loss': 0.6303, 'learning_rate': 9.32891783613154e-05, 'epoch': 0.17}
 17%|        | 1081/6500 [3:16:47<17:53:57, 11.89s/it]                                                         17%|        | 1081/6500 [3:16:47<17:53:57, 11.89s/it] 17%|        | 1082/6500 [3:16:58<17:14:48, 11.46s/it]                                                         17%|        | 1082/6500 [3:16:58<17:14:48, 11.46s/it] 17%|        | 1083/6500 [3:17:09<16:59:04, 11.29s/it]                                                         17%|        | 1083/6500 [3:17:09<16:59:04, 11.29s/it] 17%|        | 1084/6500 [3:17:19<16:35:21, 11.03s/it]                                                         17%|        | 1084/6500 [3:17:19<16:35:21, 11.03s/it] 17%|        | 1085/6500 [3:17:30<16:21:40, 10.88s/it]                                                         17%|        | 1085/6500 [3:17:30<16:21:40, 10.88s/it] 17%|        | 1086/6500 [3:17:40<16:09:36, 10.75s/it]                                                         17%{'loss': 0.6028, 'learning_rate': 9.327707641482662e-05, 'epoch': 0.17}
{'loss': 0.6324, 'learning_rate': 9.326496435258437e-05, 'epoch': 0.17}
{'loss': 0.6172, 'learning_rate': 9.325284217741974e-05, 'epoch': 0.17}
{'loss': 0.6182, 'learning_rate': 9.324070989216625e-05, 'epoch': 0.17}
|        | 1086/6500 [3:17:40<16:09:36, 10.75s/it] 17%|        | 1087/6500 [3:17:51<16:02:23, 10.67s/it]                                                         17%|        | 1087/6500 [3:17:51<16:02:23, 10.67s/it] 17%|        | 1088/6500 [3:18:01<16:03:38, 10.68s/it]                                                         17%|        | 1088/6500 [3:18:01<16:03:38, 10.68s/it] 17%|        | 1089/6500 [3:18:12<16:13:27, 10.79s/it]                                                         17%|        | 1089/6500 [3:18:12<16:13:27, 10.79s/it] 17%|        | 1090/6500 [3:18:23<16:03:30, 10.69s/it]                                                         17%|        | 1090/6500 [3:18:23<16:03:30, 10.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8245472311973572, 'eval_runtime': 4.0836, 'eval_samples_per_second': 5.632, 'eval_steps_per_second': 1.469, 'epoch': 0.17}
                                                         17%|        | 1090/6500 [3:18:27<16:03:30, 10.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.675, 'learning_rate': 9.322856749965971e-05, 'epoch': 0.17}
{'loss': 0.6219, 'learning_rate': 9.321641500273836e-05, 'epoch': 0.17}
{'loss': 0.67, 'learning_rate': 9.320425240424277e-05, 'epoch': 0.17}
{'loss': 0.6542, 'learning_rate': 9.319207970701586e-05, 'epoch': 0.17}
{'loss': 0.629, 'learning_rate': 9.317989691390291e-05, 'epoch': 0.17}
{'loss': 0.6513, 'learning_rate': 9.316770402775164e-05, 'epoch': 0.17}
 17%|        | 1091/6500 [3:18:39<18:25:11, 12.26s/it]                                                         17%|        | 1091/6500 [3:18:39<18:25:11, 12.26s/it] 17%|        | 1092/6500 [3:18:49<17:35:33, 11.71s/it]                                                         17%|        | 1092/6500 [3:18:49<17:35:33, 11.71s/it] 17%|        | 1093/6500 [3:19:00<17:00:37, 11.33s/it]                                                         17%|        | 1093/6500 [3:19:00<17:00:37, 11.33s/it] 17%|        | 1094/6500 [3:19:10<16:36:11, 11.06s/it]                                                         17%|        | 1094/6500 [3:19:10<16:36:11, 11.06s/it] 17%|        | 1095/6500 [3:19:20<16:19:13, 10.87s/it]                                                         17%|        | 1095/6500 [3:19:20<16:19:13, 10.87s/it] 17%|        | 1096/6500 [3:19:31<16:07:19, 10.74s/it]                                                         17%{'loss': 0.648, 'learning_rate': 9.315550105141199e-05, 'epoch': 0.17}
{'loss': 0.6236, 'learning_rate': 9.314328798773636e-05, 'epoch': 0.17}
{'loss': 0.6306, 'learning_rate': 9.313106483957948e-05, 'epoch': 0.17}
{'loss': 0.6671, 'learning_rate': 9.311883160979844e-05, 'epoch': 0.17}
|        | 1096/6500 [3:19:31<16:07:19, 10.74s/it] 17%|        | 1097/6500 [3:19:41<15:59:29, 10.66s/it]                                                         17%|        | 1097/6500 [3:19:41<15:59:29, 10.66s/it] 17%|        | 1098/6500 [3:19:52<15:53:26, 10.59s/it]                                                         17%|        | 1098/6500 [3:19:52<15:53:26, 10.59s/it] 17%|        | 1099/6500 [3:20:02<15:55:50, 10.62s/it]                                                         17%|        | 1099/6500 [3:20:02<15:55:50, 10.62s/it] 17%|        | 1100/6500 [3:20:13<15:50:29, 10.56s/it]                                                         17%|        | 1100/6500 [3:20:13<15:50:29, 10.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8164730668067932, 'eval_runtime': 3.9874, 'eval_samples_per_second': 5.768, 'eval_steps_per_second': 1.505, 'epoch': 0.17}
                                                         17%|        | 1100/6500 [3:20:17<15:50:29, 10.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1100I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1100

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1100/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6329, 'learning_rate': 9.310658830125267e-05, 'epoch': 0.17}
{'loss': 0.6377, 'learning_rate': 9.309433491680398e-05, 'epoch': 0.17}
{'loss': 0.6096, 'learning_rate': 9.308207145931653e-05, 'epoch': 0.17}
{'loss': 0.6832, 'learning_rate': 9.306979793165681e-05, 'epoch': 0.17}
{'loss': 0.6657, 'learning_rate': 9.305751433669369e-05, 'epoch': 0.17}
{'loss': 0.6248, 'learning_rate': 9.304522067729839e-05, 'epoch': 0.17}
 17%|        | 1101/6500 [3:20:28<17:46:45, 11.86s/it]                                                         17%|        | 1101/6500 [3:20:28<17:46:45, 11.86s/it] 17%|        | 1102/6500 [3:20:38<17:06:48, 11.41s/it]                                                         17%|        | 1102/6500 [3:20:38<17:06:48, 11.41s/it] 17%|        | 1103/6500 [3:20:48<16:39:45, 11.11s/it]                                                         17%|        | 1103/6500 [3:20:48<16:39:45, 11.11s/it] 17%|        | 1104/6500 [3:20:59<16:19:37, 10.89s/it]                                                         17%|        | 1104/6500 [3:20:59<16:19:37, 10.89s/it] 17%|        | 1105/6500 [3:21:09<16:05:26, 10.74s/it]                                                         17%|        | 1105/6500 [3:21:09<16:05:26, 10.74s/it] 17%|        | 1106/6500 [3:21:20<15:55:35, 10.63s/it]                                                         17%{'loss': 0.6419, 'learning_rate': 9.303291695634449e-05, 'epoch': 0.17}
{'loss': 0.6334, 'learning_rate': 9.302060317670787e-05, 'epoch': 0.17}
{'loss': 1.1624, 'learning_rate': 9.300827934126683e-05, 'epoch': 0.17}
{'loss': 0.6432, 'learning_rate': 9.299594545290202e-05, 'epoch': 0.17}
|        | 1106/6500 [3:21:20<15:55:35, 10.63s/it] 17%|        | 1107/6500 [3:21:30<15:48:26, 10.55s/it]                                                         17%|        | 1107/6500 [3:21:30<15:48:26, 10.55s/it] 17%|        | 1108/6500 [3:21:40<15:43:56, 10.50s/it]                                                         17%|        | 1108/6500 [3:21:40<15:43:56, 10.50s/it] 17%|        | 1109/6500 [3:21:51<15:39:52, 10.46s/it]                                                         17%|        | 1109/6500 [3:21:51<15:39:52, 10.46s/it] 17%|        | 1110/6500 [3:22:01<15:37:50, 10.44s/it]                                                         17%|        | 1110/6500 [3:22:01<15:37:50, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8197090029716492, 'eval_runtime': 3.9789, 'eval_samples_per_second': 5.78, 'eval_steps_per_second': 1.508, 'epoch': 0.17}
                                                         17%|        | 1110/6500 [3:22:05<15:37:50, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1110I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1110

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6447, 'learning_rate': 9.298360151449635e-05, 'epoch': 0.17}
{'loss': 0.5976, 'learning_rate': 9.297124752893518e-05, 'epoch': 0.17}
{'loss': 0.6517, 'learning_rate': 9.295888349910618e-05, 'epoch': 0.17}
{'loss': 0.6524, 'learning_rate': 9.294650942789933e-05, 'epoch': 0.17}
{'loss': 0.5921, 'learning_rate': 9.293412531820704e-05, 'epoch': 0.17}
{'loss': 0.629, 'learning_rate': 9.292173117292399e-05, 'epoch': 0.17}
 17%|        | 1111/6500 [3:22:16<17:35:04, 11.75s/it]                                                         17%|        | 1111/6500 [3:22:16<17:35:04, 11.75s/it] 17%|        | 1112/6500 [3:22:26<16:58:04, 11.34s/it]                                                         17%|        | 1112/6500 [3:22:26<16:58:04, 11.34s/it] 17%|        | 1113/6500 [3:22:37<16:32:06, 11.05s/it]                                                         17%|        | 1113/6500 [3:22:37<16:32:06, 11.05s/it] 17%|        | 1114/6500 [3:22:47<16:13:48, 10.85s/it]                                                         17%|        | 1114/6500 [3:22:47<16:13:48, 10.85s/it] 17%|        | 1115/6500 [3:22:57<16:00:39, 10.70s/it]                                                         17%|        | 1115/6500 [3:22:57<16:00:39, 10.70s/it] 17%|        | 1116/6500 [3:23:08<16:01:53, 10.72s/it]                                                         17%{'loss': 0.6069, 'learning_rate': 9.290932699494726e-05, 'epoch': 0.17}
{'loss': 0.6333, 'learning_rate': 9.289691278717623e-05, 'epoch': 0.17}
{'loss': 0.6238, 'learning_rate': 9.288448855251265e-05, 'epoch': 0.17}
{'loss': 0.6395, 'learning_rate': 9.287205429386063e-05, 'epoch': 0.17}
|        | 1116/6500 [3:23:08<16:01:53, 10.72s/it] 17%|        | 1117/6500 [3:23:19<15:52:56, 10.62s/it]                                                         17%|        | 1117/6500 [3:23:19<15:52:56, 10.62s/it] 17%|        | 1118/6500 [3:23:29<15:46:43, 10.55s/it]                                                         17%|        | 1118/6500 [3:23:29<15:46:43, 10.55s/it] 17%|        | 1119/6500 [3:23:39<15:41:59, 10.50s/it]                                                         17%|        | 1119/6500 [3:23:39<15:41:59, 10.50s/it] 17%|        | 1120/6500 [3:23:50<15:38:49, 10.47s/it]                                                         17%|        | 1120/6500 [3:23:50<15:38:49, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8251902461051941, 'eval_runtime': 4.2303, 'eval_samples_per_second': 5.437, 'eval_steps_per_second': 1.418, 'epoch': 0.17}
                                                         17%|        | 1120/6500 [3:23:54<15:38:49, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1120/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1120/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6289, 'learning_rate': 9.285961001412657e-05, 'epoch': 0.17}
{'loss': 0.6112, 'learning_rate': 9.284715571621927e-05, 'epoch': 0.17}
{'loss': 0.6908, 'learning_rate': 9.283469140304983e-05, 'epoch': 0.17}
{'loss': 0.6405, 'learning_rate': 9.28222170775317e-05, 'epoch': 0.17}
{'loss': 0.6428, 'learning_rate': 9.280973274258071e-05, 'epoch': 0.17}
{'loss': 0.6583, 'learning_rate': 9.279723840111496e-05, 'epoch': 0.17}
 17%|        | 1121/6500 [3:24:05<17:44:16, 11.87s/it]                                                         17%|        | 1121/6500 [3:24:05<17:44:16, 11.87s/it] 17%|        | 1122/6500 [3:24:15<17:04:10, 11.43s/it]                                                         17%|        | 1122/6500 [3:24:15<17:04:10, 11.43s/it] 17%|        | 1123/6500 [3:24:26<16:36:20, 11.12s/it]                                                         17%|        | 1123/6500 [3:24:26<16:36:20, 11.12s/it] 17%|        | 1124/6500 [3:24:36<16:16:21, 10.90s/it]                                                         17%|        | 1124/6500 [3:24:36<16:16:21, 10.90s/it] 17%|        | 1125/6500 [3:24:46<16:02:30, 10.74s/it]                                                         17%|        | 1125/6500 [3:24:46<16:02:30, 10.74s/it] 17%|        | 1126/6500 [3:24:57<15:53:48, 10.65s/it]                                                         17%{'loss': 0.6173, 'learning_rate': 9.278473405605497e-05, 'epoch': 0.17}
{'loss': 0.6168, 'learning_rate': 9.277221971032351e-05, 'epoch': 0.17}
{'loss': 0.6525, 'learning_rate': 9.275969536684577e-05, 'epoch': 0.17}
{'loss': 0.6466, 'learning_rate': 9.274716102854922e-05, 'epoch': 0.17}
|        | 1126/6500 [3:24:57<15:53:48, 10.65s/it] 17%|        | 1127/6500 [3:25:07<15:46:37, 10.57s/it]                                                         17%|        | 1127/6500 [3:25:07<15:46:37, 10.57s/it] 17%|        | 1128/6500 [3:25:18<15:41:45, 10.52s/it]                                                         17%|        | 1128/6500 [3:25:18<15:41:45, 10.52s/it] 17%|        | 1129/6500 [3:25:28<15:38:03, 10.48s/it]                                                         17%|        | 1129/6500 [3:25:28<15:38:03, 10.48s/it] 17%|        | 1130/6500 [3:25:38<15:35:16, 10.45s/it]                                                         17%|        | 1130/6500 [3:25:38<15:35:16, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8126811385154724, 'eval_runtime': 3.9879, 'eval_samples_per_second': 5.767, 'eval_steps_per_second': 1.505, 'epoch': 0.17}
                                                         17%|        | 1130/6500 [3:25:42<15:35:16, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1130I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1130/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6253, 'learning_rate': 9.273461669836366e-05, 'epoch': 0.17}
{'loss': 0.6212, 'learning_rate': 9.272206237922129e-05, 'epoch': 0.17}
{'loss': 0.6211, 'learning_rate': 9.270949807405662e-05, 'epoch': 0.17}
{'loss': 0.7234, 'learning_rate': 9.269692378580642e-05, 'epoch': 0.17}
{'loss': 0.599, 'learning_rate': 9.26843395174099e-05, 'epoch': 0.17}
{'loss': 0.63, 'learning_rate': 9.267174527180853e-05, 'epoch': 0.17}
 17%|        | 1131/6500 [3:25:53<17:33:19, 11.77s/it]                                                         17%|        | 1131/6500 [3:25:53<17:33:19, 11.77s/it] 17%|        | 1132/6500 [3:26:04<17:02:42, 11.43s/it]                                                         17%|        | 1132/6500 [3:26:04<17:02:42, 11.43s/it] 17%|        | 1133/6500 [3:26:14<16:34:30, 11.12s/it]                                                         17%|        | 1133/6500 [3:26:14<16:34:30, 11.12s/it] 17%|        | 1134/6500 [3:26:25<16:14:33, 10.90s/it]                                                         17%|        | 1134/6500 [3:26:25<16:14:33, 10.90s/it] 17%|        | 1135/6500 [3:26:35<16:00:25, 10.74s/it]                                                         17%|        | 1135/6500 [3:26:35<16:00:25, 10.74s/it] 17%|        | 1136/6500 [3:26:45<15:50:59, 10.64s/it]                                                         17%{'loss': 0.6378, 'learning_rate': 9.265914105194617e-05, 'epoch': 0.17}
{'loss': 1.1491, 'learning_rate': 9.264652686076895e-05, 'epoch': 0.18}
{'loss': 0.6465, 'learning_rate': 9.263390270122538e-05, 'epoch': 0.18}
{'loss': 0.634, 'learning_rate': 9.262126857626627e-05, 'epoch': 0.18}
|        | 1136/6500 [3:26:45<15:50:59, 10.64s/it] 17%|        | 1137/6500 [3:26:56<15:44:07, 10.56s/it]                                                         17%|        | 1137/6500 [3:26:56<15:44:07, 10.56s/it] 18%|        | 1138/6500 [3:27:06<15:39:01, 10.51s/it]                                                         18%|        | 1138/6500 [3:27:06<15:39:01, 10.51s/it] 18%|        | 1139/6500 [3:27:17<15:35:29, 10.47s/it]                                                         18%|        | 1139/6500 [3:27:17<15:35:29, 10.47s/it] 18%|        | 1140/6500 [3:27:27<15:33:40, 10.45s/it]                                                         18%|        | 1140/6500 [3:27:27<15:33:40, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8221741318702698, 'eval_runtime': 3.9939, 'eval_samples_per_second': 5.759, 'eval_steps_per_second': 1.502, 'epoch': 0.18}
                                                         18%|        | 1140/6500 [3:27:31<15:33:40, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1140I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1140/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1140/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6161, 'learning_rate': 9.260862448884477e-05, 'epoch': 0.18}
{'loss': 0.5931, 'learning_rate': 9.259597044191636e-05, 'epoch': 0.18}
{'loss': 0.6488, 'learning_rate': 9.258330643843884e-05, 'epoch': 0.18}
{'loss': 0.6251, 'learning_rate': 9.257063248137236e-05, 'epoch': 0.18}
{'loss': 0.5987, 'learning_rate': 9.255794857367936e-05, 'epoch': 0.18}
{'loss': 0.5982, 'learning_rate': 9.254525471832464e-05, 'epoch': 0.18}
 18%|        | 1141/6500 [3:27:42<17:33:52, 11.80s/it]                                                         18%|        | 1141/6500 [3:27:42<17:33:52, 11.80s/it] 18%|        | 1142/6500 [3:27:52<16:55:28, 11.37s/it]                                                         18%|        | 1142/6500 [3:27:52<16:55:28, 11.37s/it] 18%|        | 1143/6500 [3:28:03<16:28:43, 11.07s/it]                                                         18%|        | 1143/6500 [3:28:03<16:28:43, 11.07s/it] 18%|        | 1144/6500 [3:28:13<16:09:35, 10.86s/it]                                                         18%|        | 1144/6500 [3:28:13<16:09:35, 10.86s/it] 18%|        | 1145/6500 [3:28:23<15:56:36, 10.72s/it]                                                         18%|        | 1145/6500 [3:28:23<15:56:36, 10.72s/it] 18%|        | 1146/6500 [3:28:34<15:47:27, 10.62s/it]                                                         18%{'loss': 0.6117, 'learning_rate': 9.253255091827533e-05, 'epoch': 0.18}
{'loss': 0.618, 'learning_rate': 9.251983717650084e-05, 'epoch': 0.18}
{'loss': 0.5976, 'learning_rate': 9.250711349597291e-05, 'epoch': 0.18}
{'loss': 0.6608, 'learning_rate': 9.249437987966567e-05, 'epoch': 0.18}
|        | 1146/6500 [3:28:34<15:47:27, 10.62s/it] 18%|        | 1147/6500 [3:28:44<15:40:57, 10.55s/it]                                                         18%|        | 1147/6500 [3:28:44<15:40:57, 10.55s/it] 18%|        | 1148/6500 [3:28:55<15:46:50, 10.61s/it]                                                         18%|        | 1148/6500 [3:28:55<15:46:50, 10.61s/it] 18%|        | 1149/6500 [3:29:05<15:40:17, 10.54s/it]                                                         18%|        | 1149/6500 [3:29:05<15:40:17, 10.54s/it] 18%|        | 1150/6500 [3:29:16<15:35:36, 10.49s/it]                                                         18%|        | 1150/6500 [3:29:16<15:35:36, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8215101361274719, 'eval_runtime': 4.2187, 'eval_samples_per_second': 5.452, 'eval_steps_per_second': 1.422, 'epoch': 0.18}
                                                         18%|        | 1150/6500 [3:29:20<15:35:36, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1150 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1150

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6191, 'learning_rate': 9.248163633055549e-05, 'epoch': 0.18}
{'loss': 0.6495, 'learning_rate': 9.246888285162112e-05, 'epoch': 0.18}
{'loss': 0.6521, 'learning_rate': 9.24561194458436e-05, 'epoch': 0.18}
{'loss': 0.6465, 'learning_rate': 9.244334611620629e-05, 'epoch': 0.18}
{'loss': 0.6368, 'learning_rate': 9.243056286569488e-05, 'epoch': 0.18}
{'loss': 0.6266, 'learning_rate': 9.241776969729739e-05, 'epoch': 0.18}
 18%|        | 1151/6500 [3:29:31<17:41:54, 11.91s/it]                                                         18%|        | 1151/6500 [3:29:31<17:41:54, 11.91s/it] 18%|        | 1152/6500 [3:29:41<17:01:02, 11.46s/it]                                                         18%|        | 1152/6500 [3:29:41<17:01:02, 11.46s/it] 18%|        | 1153/6500 [3:29:52<16:32:34, 11.14s/it]                                                         18%|        | 1153/6500 [3:29:52<16:32:34, 11.14s/it] 18%|        | 1154/6500 [3:30:02<16:12:33, 10.92s/it]                                                         18%|        | 1154/6500 [3:30:02<16:12:33, 10.92s/it] 18%|        | 1155/6500 [3:30:13<15:58:17, 10.76s/it]                                                         18%|        | 1155/6500 [3:30:13<15:58:17, 10.76s/it] 18%|        | 1156/6500 [3:30:24<16:20:58, 11.01s/it]                                                         18%{'loss': 0.6133, 'learning_rate': 9.240496661400414e-05, 'epoch': 0.18}
{'loss': 0.6166, 'learning_rate': 9.239215361880776e-05, 'epoch': 0.18}
{'loss': 0.6523, 'learning_rate': 9.237933071470323e-05, 'epoch': 0.18}
{'loss': 0.6093, 'learning_rate': 9.23664979046878e-05, 'epoch': 0.18}
|        | 1156/6500 [3:30:24<16:20:58, 11.01s/it] 18%|        | 1157/6500 [3:30:35<16:06:17, 10.85s/it]                                                         18%|        | 1157/6500 [3:30:35<16:06:17, 10.85s/it] 18%|        | 1158/6500 [3:30:45<15:54:12, 10.72s/it]                                                         18%|        | 1158/6500 [3:30:45<15:54:12, 10.72s/it] 18%|        | 1159/6500 [3:30:55<15:45:15, 10.62s/it]                                                         18%|        | 1159/6500 [3:30:55<15:45:15, 10.62s/it] 18%|        | 1160/6500 [3:31:06<15:38:59, 10.55s/it]                                                         18%|        | 1160/6500 [3:31:06<15:38:59, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8123040199279785, 'eval_runtime': 4.7383, 'eval_samples_per_second': 4.854, 'eval_steps_per_second': 1.266, 'epoch': 0.18}
                                                         18%|        | 1160/6500 [3:31:11<15:38:59, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6337, 'learning_rate': 9.23536551917611e-05, 'epoch': 0.18}
{'loss': 0.6025, 'learning_rate': 9.2340802578925e-05, 'epoch': 0.18}
{'loss': 0.6514, 'learning_rate': 9.232794006918375e-05, 'epoch': 0.18}
{'loss': 0.6835, 'learning_rate': 9.231506766554384e-05, 'epoch': 0.18}
{'loss': 0.6069, 'learning_rate': 9.230218537101416e-05, 'epoch': 0.18}
{'loss': 0.6152, 'learning_rate': 9.228929318860584e-05, 'epoch': 0.18}
 18%|        | 1161/6500 [3:31:22<17:56:42, 12.10s/it]                                                         18%|        | 1161/6500 [3:31:22<17:56:42, 12.10s/it] 18%|        | 1162/6500 [3:31:32<17:11:02, 11.59s/it]                                                         18%|        | 1162/6500 [3:31:32<17:11:02, 11.59s/it] 18%|        | 1163/6500 [3:31:43<16:45:00, 11.30s/it]                                                         18%|        | 1163/6500 [3:31:43<16:45:00, 11.30s/it] 18%|        | 1164/6500 [3:31:53<16:26:46, 11.10s/it]                                                         18%|        | 1164/6500 [3:31:53<16:26:46, 11.10s/it] 18%|        | 1165/6500 [3:32:04<16:07:42, 10.88s/it]                                                         18%|        | 1165/6500 [3:32:04<16:07:42, 10.88s/it] 18%|        | 1166/6500 [3:32:14<15:54:32, 10.74s/it]                                                         18%{'loss': 0.6173, 'learning_rate': 9.227639112133238e-05, 'epoch': 0.18}
{'loss': 1.1392, 'learning_rate': 9.226347917220953e-05, 'epoch': 0.18}
{'loss': 0.6474, 'learning_rate': 9.225055734425539e-05, 'epoch': 0.18}
{'loss': 0.61, 'learning_rate': 9.223762564049035e-05, 'epoch': 0.18}
|        | 1166/6500 [3:32:14<15:54:32, 10.74s/it] 18%|        | 1167/6500 [3:32:24<15:45:27, 10.64s/it]                                                         18%|        | 1167/6500 [3:32:24<15:45:27, 10.64s/it] 18%|        | 1168/6500 [3:32:35<15:38:22, 10.56s/it]                                                         18%|        | 1168/6500 [3:32:35<15:38:22, 10.56s/it] 18%|        | 1169/6500 [3:32:45<15:33:57, 10.51s/it]                                                         18%|        | 1169/6500 [3:32:45<15:33:57, 10.51s/it] 18%|        | 1170/6500 [3:32:56<15:30:56, 10.48s/it]                                                         18%|        | 1170/6500 [3:32:56<15:30:56, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8152517080307007, 'eval_runtime': 3.996, 'eval_samples_per_second': 5.756, 'eval_steps_per_second': 1.501, 'epoch': 0.18}
                                                         18%|        | 1170/6500 [3:33:00<15:30:56, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5794, 'learning_rate': 9.222468406393713e-05, 'epoch': 0.18}
{'loss': 0.6168, 'learning_rate': 9.221173261762073e-05, 'epoch': 0.18}
{'loss': 0.6618, 'learning_rate': 9.219877130456851e-05, 'epoch': 0.18}
{'loss': 0.601, 'learning_rate': 9.218580012781005e-05, 'epoch': 0.18}
{'loss': 0.6021, 'learning_rate': 9.217281909037732e-05, 'epoch': 0.18}
{'loss': 0.5869, 'learning_rate': 9.215982819530451e-05, 'epoch': 0.18}
 18%|        | 1171/6500 [3:33:11<17:31:21, 11.84s/it]                                                         18%|        | 1171/6500 [3:33:11<17:31:21, 11.84s/it] 18%|        | 1172/6500 [3:33:21<16:52:53, 11.41s/it]                                                         18%|        | 1172/6500 [3:33:21<16:52:53, 11.41s/it] 18%|        | 1173/6500 [3:33:33<17:15:52, 11.67s/it]                                                         18%|        | 1173/6500 [3:33:33<17:15:52, 11.67s/it] 18%|        | 1174/6500 [3:33:44<16:46:10, 11.34s/it]                                                         18%|        | 1174/6500 [3:33:44<16:46:10, 11.34s/it] 18%|        | 1175/6500 [3:33:54<16:20:51, 11.05s/it]                                                         18%|        | 1175/6500 [3:33:54<16:20:51, 11.05s/it] 18%|        | 1176/6500 [3:34:05<16:03:35, 10.86s/it]                                                         18%{'loss': 0.6142, 'learning_rate': 9.214682744562823e-05, 'epoch': 0.18}
{'loss': 0.6173, 'learning_rate': 9.213381684438726e-05, 'epoch': 0.18}
{'loss': 0.6064, 'learning_rate': 9.212079639462281e-05, 'epoch': 0.18}
{'loss': 0.6411, 'learning_rate': 9.210776609937829e-05, 'epoch': 0.18}
|        | 1176/6500 [3:34:05<16:03:35, 10.86s/it] 18%|        | 1177/6500 [3:34:15<15:50:20, 10.71s/it]                                                         18%|        | 1177/6500 [3:34:15<15:50:20, 10.71s/it] 18%|        | 1178/6500 [3:34:25<15:41:12, 10.61s/it]                                                         18%|        | 1178/6500 [3:34:25<15:41:12, 10.61s/it] 18%|        | 1179/6500 [3:34:36<15:34:30, 10.54s/it]                                                         18%|        | 1179/6500 [3:34:36<15:34:30, 10.54s/it] 18%|        | 1180/6500 [3:34:47<16:00:23, 10.83s/it]                                                         18%|        | 1180/6500 [3:34:47<16:00:23, 10.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8208011984825134, 'eval_runtime': 4.2107, 'eval_samples_per_second': 5.462, 'eval_steps_per_second': 1.425, 'epoch': 0.18}
                                                         18%|        | 1180/6500 [3:34:51<16:00:23, 10.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1180/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6031, 'learning_rate': 9.209472596169946e-05, 'epoch': 0.18}
{'loss': 0.6523, 'learning_rate': 9.208167598463439e-05, 'epoch': 0.18}
{'loss': 0.6302, 'learning_rate': 9.206861617123341e-05, 'epoch': 0.18}
{'loss': 0.6107, 'learning_rate': 9.205554652454918e-05, 'epoch': 0.18}
{'loss': 0.6326, 'learning_rate': 9.204246704763665e-05, 'epoch': 0.18}
{'loss': 0.6082, 'learning_rate': 9.202937774355307e-05, 'epoch': 0.18}
 18%|        | 1181/6500 [3:35:02<17:55:57, 12.14s/it]                                                         18%|        | 1181/6500 [3:35:02<17:55:57, 12.14s/it] 18%|        | 1182/6500 [3:35:13<17:08:22, 11.60s/it]                                                         18%|        | 1182/6500 [3:35:13<17:08:22, 11.60s/it] 18%|        | 1183/6500 [3:35:23<16:35:22, 11.23s/it]                                                         18%|        | 1183/6500 [3:35:23<16:35:22, 11.23s/it] 18%|        | 1184/6500 [3:35:33<16:12:01, 10.97s/it]                                                         18%|        | 1184/6500 [3:35:33<16:12:01, 10.97s/it] 18%|        | 1185/6500 [3:35:44<15:56:18, 10.80s/it]                                                         18%|        | 1185/6500 [3:35:44<15:56:18, 10.80s/it] 18%|        | 1186/6500 [3:35:54<15:45:11, 10.67s/it]                                                         18%{'loss': 0.6072, 'learning_rate': 9.201627861535799e-05, 'epoch': 0.18}
{'loss': 0.605, 'learning_rate': 9.200316966611324e-05, 'epoch': 0.18}
{'loss': 0.6464, 'learning_rate': 9.199005089888297e-05, 'epoch': 0.18}
{'loss': 0.6178, 'learning_rate': 9.197692231673361e-05, 'epoch': 0.18}
|        | 1186/6500 [3:35:54<15:45:11, 10.67s/it] 18%|        | 1187/6500 [3:36:05<15:36:36, 10.58s/it]                                                         18%|        | 1187/6500 [3:36:05<15:36:36, 10.58s/it] 18%|        | 1188/6500 [3:36:15<15:30:46, 10.51s/it]                                                         18%|        | 1188/6500 [3:36:15<15:30:46, 10.51s/it] 18%|        | 1189/6500 [3:36:25<15:26:52, 10.47s/it]                                                         18%|        | 1189/6500 [3:36:25<15:26:52, 10.47s/it] 18%|        | 1190/6500 [3:36:36<15:23:35, 10.44s/it]                                                         18%|        | 1190/6500 [3:36:36<15:23:35, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.809968888759613, 'eval_runtime': 3.9713, 'eval_samples_per_second': 5.792, 'eval_steps_per_second': 1.511, 'epoch': 0.18}
                                                         18%|        | 1190/6500 [3:36:40<15:23:35, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1190
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1190/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6109, 'learning_rate': 9.196378392273387e-05, 'epoch': 0.18}
{'loss': 0.594, 'learning_rate': 9.195063571995479e-05, 'epoch': 0.18}
{'loss': 0.6602, 'learning_rate': 9.193747771146968e-05, 'epoch': 0.18}
{'loss': 0.6518, 'learning_rate': 9.192430990035413e-05, 'epoch': 0.18}
{'loss': 0.5873, 'learning_rate': 9.191113228968604e-05, 'epoch': 0.18}
{'loss': 0.6153, 'learning_rate': 9.189794488254561e-05, 'epoch': 0.18}
 18%|        | 1191/6500 [3:36:51<17:20:07, 11.76s/it]                                                         18%|        | 1191/6500 [3:36:51<17:20:07, 11.76s/it] 18%|        | 1192/6500 [3:37:01<16:43:13, 11.34s/it]                                                         18%|        | 1192/6500 [3:37:01<16:43:13, 11.34s/it] 18%|        | 1193/6500 [3:37:11<16:17:37, 11.05s/it]                                                         18%|        | 1193/6500 [3:37:11<16:17:37, 11.05s/it] 18%|        | 1194/6500 [3:37:22<15:59:31, 10.85s/it]                                                         18%|        | 1194/6500 [3:37:22<15:59:31, 10.85s/it] 18%|        | 1195/6500 [3:37:32<15:47:00, 10.71s/it]                                                         18%|        | 1195/6500 [3:37:32<15:47:00, 10.71s/it] 18%|        | 1196/6500 [3:37:43<15:43:46, 10.68s/it]                                                         18%{'loss': 0.7962, 'learning_rate': 9.188474768201532e-05, 'epoch': 0.18}
{'loss': 0.958, 'learning_rate': 9.18715406911799e-05, 'epoch': 0.18}
{'loss': 0.6435, 'learning_rate': 9.185832391312644e-05, 'epoch': 0.18}
{'loss': 0.6147, 'learning_rate': 9.184509735094427e-05, 'epoch': 0.18}
|        | 1196/6500 [3:37:43<15:43:46, 10.68s/it] 18%|        | 1197/6500 [3:37:53<15:35:31, 10.58s/it]                                                         18%|        | 1197/6500 [3:37:53<15:35:31, 10.58s/it] 18%|        | 1198/6500 [3:38:03<15:30:10, 10.53s/it]                                                         18%|        | 1198/6500 [3:38:03<15:30:10, 10.53s/it] 18%|        | 1199/6500 [3:38:14<15:25:51, 10.48s/it]                                                         18%|        | 1199/6500 [3:38:14<15:25:51, 10.48s/it] 18%|        | 1200/6500 [3:38:24<15:22:54, 10.45s/it]                                                         18%|        | 1200/6500 [3:38:24<15:22:54, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8151071071624756, 'eval_runtime': 3.9703, 'eval_samples_per_second': 5.793, 'eval_steps_per_second': 1.511, 'epoch': 0.18}
                                                         18%|        | 1200/6500 [3:38:28<15:22:54, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.566, 'learning_rate': 9.1831861007725e-05, 'epoch': 0.18}
{'loss': 0.626, 'learning_rate': 9.181861488656256e-05, 'epoch': 0.18}
{'loss': 0.6339, 'learning_rate': 9.180535899055316e-05, 'epoch': 0.19}
{'loss': 0.5741, 'learning_rate': 9.17920933227953e-05, 'epoch': 0.19}
{'loss': 0.6121, 'learning_rate': 9.177881788638969e-05, 'epoch': 0.19}
{'loss': 0.5839, 'learning_rate': 9.176553268443943e-05, 'epoch': 0.19}
 18%|        | 1201/6500 [3:38:39<17:20:52, 11.79s/it]                                                         18%|        | 1201/6500 [3:38:39<17:20:52, 11.79s/it] 18%|        | 1202/6500 [3:38:49<16:43:47, 11.37s/it]                                                         18%|        | 1202/6500 [3:38:49<16:43:47, 11.37s/it] 19%|        | 1203/6500 [3:39:00<16:17:21, 11.07s/it]                                                         19%|        | 1203/6500 [3:39:00<16:17:21, 11.07s/it] 19%|        | 1204/6500 [3:39:10<15:58:19, 10.86s/it]                                                         19%|        | 1204/6500 [3:39:10<15:58:19, 10.86s/it] 19%|        | 1205/6500 [3:39:21<15:45:26, 10.71s/it]                                                         19%|        | 1205/6500 [3:39:21<15:45:26, 10.71s/it] 19%|        | 1206/6500 [3:39:31<15:35:43, 10.61s/it]                                                         19%{'loss': 0.5958, 'learning_rate': 9.175223772004986e-05, 'epoch': 0.19}
{'loss': 0.598, 'learning_rate': 9.173893299632856e-05, 'epoch': 0.19}
{'loss': 0.613, 'learning_rate': 9.172561851638545e-05, 'epoch': 0.19}
{'loss': 0.6068, 'learning_rate': 9.171229428333272e-05, 'epoch': 0.19}
|        | 1206/6500 [3:39:31<15:35:43, 10.61s/it] 19%|        | 1207/6500 [3:39:41<15:29:09, 10.53s/it]                                                         19%|        | 1207/6500 [3:39:41<15:29:09, 10.53s/it] 19%|        | 1208/6500 [3:39:52<15:24:50, 10.49s/it]                                                         19%|        | 1208/6500 [3:39:52<15:24:50, 10.49s/it] 19%|        | 1209/6500 [3:40:02<15:21:46, 10.45s/it]                                                         19%|        | 1209/6500 [3:40:02<15:21:46, 10.45s/it] 19%|        | 1210/6500 [3:40:12<15:19:50, 10.43s/it]                                                         19%|        | 1210/6500 [3:40:12<15:19:50, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8185080885887146, 'eval_runtime': 3.9733, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.19}
                                                         19%|        | 1210/6500 [3:40:16<15:19:50, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1210I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1210

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1210
the pytorch model path isthe pytorch model path is  the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1210/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1210/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6046, 'learning_rate': 9.169896030028482e-05, 'epoch': 0.19}
{'loss': 0.6584, 'learning_rate': 9.168561657035848e-05, 'epoch': 0.19}
{'loss': 0.633, 'learning_rate': 9.16722630966727e-05, 'epoch': 0.19}
{'loss': 0.6209, 'learning_rate': 9.165889988234881e-05, 'epoch': 0.19}
{'loss': 0.6314, 'learning_rate': 9.164552693051035e-05, 'epoch': 0.19}
{'loss': 0.5939, 'learning_rate': 9.16321442442832e-05, 'epoch': 0.19}
 19%|        | 1211/6500 [3:40:27<17:15:08, 11.74s/it]                                                         19%|        | 1211/6500 [3:40:27<17:15:08, 11.74s/it] 19%|        | 1212/6500 [3:40:38<16:53:41, 11.50s/it]                                                         19%|        | 1212/6500 [3:40:38<16:53:41, 11.50s/it] 19%|        | 1213/6500 [3:40:49<16:24:04, 11.17s/it]                                                         19%|        | 1213/6500 [3:40:49<16:24:04, 11.17s/it] 19%|        | 1214/6500 [3:40:59<16:02:45, 10.93s/it]                                                         19%|        | 1214/6500 [3:40:59<16:02:45, 10.93s/it] 19%|        | 1215/6500 [3:41:09<15:48:26, 10.77s/it]                                                         19%|        | 1215/6500 [3:41:09<15:48:26, 10.77s/it] 19%|        | 1216/6500 [3:41:20<15:37:55, 10.65s/it]                                                         19%{'loss': 0.6061, 'learning_rate': 9.161875182679546e-05, 'epoch': 0.19}
{'loss': 0.6247, 'learning_rate': 9.160534968117752e-05, 'epoch': 0.19}
{'loss': 0.6193, 'learning_rate': 9.159193781056203e-05, 'epoch': 0.19}
{'loss': 0.6148, 'learning_rate': 9.1578516218084e-05, 'epoch': 0.19}
|        | 1216/6500 [3:41:20<15:37:55, 10.65s/it] 19%|        | 1217/6500 [3:41:30<15:30:22, 10.57s/it]                                                         19%|        | 1217/6500 [3:41:30<15:30:22, 10.57s/it] 19%|        | 1218/6500 [3:41:40<15:25:20, 10.51s/it]                                                         19%|        | 1218/6500 [3:41:40<15:25:20, 10.51s/it] 19%|        | 1219/6500 [3:41:51<15:21:43, 10.47s/it]                                                         19%|        | 1219/6500 [3:41:51<15:21:43, 10.47s/it] 19%|        | 1220/6500 [3:42:01<15:18:41, 10.44s/it]                                                         19%|        | 1220/6500 [3:42:01<15:18:41, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8071452379226685, 'eval_runtime': 3.9796, 'eval_samples_per_second': 5.779, 'eval_steps_per_second': 1.508, 'epoch': 0.19}
                                                         19%|        | 1220/6500 [3:42:05<15:18:41, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1220/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1220/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5907, 'learning_rate': 9.156508490688058e-05, 'epoch': 0.19}
{'loss': 0.6179, 'learning_rate': 9.15516438800913e-05, 'epoch': 0.19}
{'loss': 0.6688, 'learning_rate': 9.153819314085787e-05, 'epoch': 0.19}
{'loss': 0.6153, 'learning_rate': 9.152473269232437e-05, 'epoch': 0.19}
{'loss': 0.591, 'learning_rate': 9.151126253763708e-05, 'epoch': 0.19}
{'loss': 0.6111, 'learning_rate': 9.149778267994457e-05, 'epoch': 0.19}
 19%|        | 1221/6500 [3:42:16<17:15:50, 11.77s/it]                                                         19%|        | 1221/6500 [3:42:16<17:15:50, 11.77s/it] 19%|        | 1222/6500 [3:42:26<16:38:33, 11.35s/it]                                                         19%|        | 1222/6500 [3:42:26<16:38:33, 11.35s/it] 19%|        | 1223/6500 [3:42:37<16:11:46, 11.05s/it]                                                         19%|        | 1223/6500 [3:42:37<16:11:46, 11.05s/it] 19%|        | 1224/6500 [3:42:47<15:53:07, 10.84s/it]                                                         19%|        | 1224/6500 [3:42:47<15:53:07, 10.84s/it] 19%|        | 1225/6500 [3:42:58<15:41:01, 10.70s/it]                                                         19%|        | 1225/6500 [3:42:58<15:41:01, 10.70s/it] 19%|        | 1226/6500 [3:43:08<15:31:58, 10.60s/it]                                                         19%{'loss': 1.1302, 'learning_rate': 9.148429312239767e-05, 'epoch': 0.19}
{'loss': 0.6378, 'learning_rate': 9.147079386814947e-05, 'epoch': 0.19}
{'loss': 0.6016, 'learning_rate': 9.145728492035536e-05, 'epoch': 0.19}
{'loss': 0.6057, 'learning_rate': 9.144376628217295e-05, 'epoch': 0.19}
|        | 1226/6500 [3:43:08<15:31:58, 10.60s/it] 19%|        | 1227/6500 [3:43:18<15:24:56, 10.52s/it]                                                         19%|        | 1227/6500 [3:43:18<15:24:56, 10.52s/it] 19%|        | 1228/6500 [3:43:29<15:20:32, 10.48s/it]                                                         19%|        | 1228/6500 [3:43:29<15:20:32, 10.48s/it] 19%|        | 1229/6500 [3:43:39<15:27:14, 10.55s/it]                                                         19%|        | 1229/6500 [3:43:39<15:27:14, 10.55s/it] 19%|        | 1230/6500 [3:43:50<15:23:34, 10.52s/it]                                                         19%|        | 1230/6500 [3:43:50<15:23:34, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8158679604530334, 'eval_runtime': 3.9623, 'eval_samples_per_second': 5.805, 'eval_steps_per_second': 1.514, 'epoch': 0.19}
                                                         19%|        | 1230/6500 [3:43:54<15:23:34, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1230/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1230/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5704, 'learning_rate': 9.143023795676217e-05, 'epoch': 0.19}
{'loss': 0.6367, 'learning_rate': 9.141669994728517e-05, 'epoch': 0.19}
{'loss': 0.5965, 'learning_rate': 9.140315225690636e-05, 'epoch': 0.19}
{'loss': 0.5846, 'learning_rate': 9.138959488879244e-05, 'epoch': 0.19}
{'loss': 0.5724, 'learning_rate': 9.137602784611239e-05, 'epoch': 0.19}
{'loss': 0.6041, 'learning_rate': 9.136245113203739e-05, 'epoch': 0.19}
 19%|        | 1231/6500 [3:44:05<17:17:44, 11.82s/it]                                                         19%|        | 1231/6500 [3:44:05<17:17:44, 11.82s/it] 19%|        | 1232/6500 [3:44:15<16:39:27, 11.38s/it]                                                         19%|        | 1232/6500 [3:44:15<16:39:27, 11.38s/it] 19%|        | 1233/6500 [3:44:25<16:12:10, 11.07s/it]                                                         19%|        | 1233/6500 [3:44:25<16:12:10, 11.07s/it] 19%|        | 1234/6500 [3:44:36<15:53:23, 10.86s/it]                                                         19%|        | 1234/6500 [3:44:36<15:53:23, 10.86s/it] 19%|        | 1235/6500 [3:44:46<15:40:03, 10.71s/it]                                                         19%|        | 1235/6500 [3:44:46<15:40:03, 10.71s/it] 19%|        | 1236/6500 [3:44:56<15:30:23, 10.60s/it]                                                         19%{'loss': 0.5986, 'learning_rate': 9.134886474974091e-05, 'epoch': 0.19}
{'loss': 0.5782, 'learning_rate': 9.133526870239873e-05, 'epoch': 0.19}
{'loss': 0.6315, 'learning_rate': 9.132166299318878e-05, 'epoch': 0.19}
{'loss': 0.588, 'learning_rate': 9.130804762529137e-05, 'epoch': 0.19}
|        | 1236/6500 [3:44:56<15:30:23, 10.60s/it] 19%|        | 1237/6500 [3:45:07<15:23:53, 10.53s/it]                                                         19%|        | 1237/6500 [3:45:07<15:23:53, 10.53s/it] 19%|        | 1238/6500 [3:45:17<15:19:16, 10.48s/it]                                                         19%|        | 1238/6500 [3:45:17<15:19:16, 10.48s/it] 19%|        | 1239/6500 [3:45:28<15:16:02, 10.45s/it]                                                         19%|        | 1239/6500 [3:45:28<15:16:02, 10.45s/it] 19%|        | 1240/6500 [3:45:38<15:13:27, 10.42s/it]                                                         19%|        | 1240/6500 [3:45:38<15:13:27, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8228765726089478, 'eval_runtime': 3.9613, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.515, 'epoch': 0.19}
                                                         19%|        | 1240/6500 [3:45:42<15:13:27, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1240/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1240

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1240/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.632, 'learning_rate': 9.129442260188899e-05, 'epoch': 0.19}
{'loss': 0.6163, 'learning_rate': 9.128078792616638e-05, 'epoch': 0.19}
{'loss': 0.6027, 'learning_rate': 9.126714360131059e-05, 'epoch': 0.19}
{'loss': 0.6105, 'learning_rate': 9.12534896305109e-05, 'epoch': 0.19}
{'loss': 0.6072, 'learning_rate': 9.123982601695882e-05, 'epoch': 0.19}
{'loss': 0.5745, 'learning_rate': 9.122615276384816e-05, 'epoch': 0.19}
 19%|        | 1241/6500 [3:45:53<17:08:44, 11.74s/it]                                                         19%|        | 1241/6500 [3:45:53<17:08:44, 11.74s/it] 19%|        | 1242/6500 [3:46:03<16:32:47, 11.33s/it]                                                         19%|        | 1242/6500 [3:46:03<16:32:47, 11.33s/it] 19%|        | 1243/6500 [3:46:13<16:07:20, 11.04s/it]                                                         19%|        | 1243/6500 [3:46:13<16:07:20, 11.04s/it] 19%|        | 1244/6500 [3:46:24<15:49:23, 10.84s/it]                                                         19%|        | 1244/6500 [3:46:24<15:49:23, 10.84s/it] 19%|        | 1245/6500 [3:46:34<15:42:51, 10.77s/it]                                                         19%|        | 1245/6500 [3:46:34<15:42:51, 10.77s/it] 19%|        | 1246/6500 [3:46:45<15:31:34, 10.64s/it]                                                         19%{'loss': 0.6036, 'learning_rate': 9.121246987437496e-05, 'epoch': 0.19}
{'loss': 0.6412, 'learning_rate': 9.119877735173748e-05, 'epoch': 0.19}
{'loss': 0.5904, 'learning_rate': 9.118507519913631e-05, 'epoch': 0.19}
{'loss': 0.6139, 'learning_rate': 9.11713634197742e-05, 'epoch': 0.19}
|        | 1246/6500 [3:46:45<15:31:34, 10.64s/it] 19%|        | 1247/6500 [3:46:55<15:24:04, 10.55s/it]                                                         19%|        | 1247/6500 [3:46:55<15:24:04, 10.55s/it] 19%|        | 1248/6500 [3:47:05<15:18:33, 10.49s/it]                                                         19%|        | 1248/6500 [3:47:05<15:18:33, 10.49s/it] 19%|        | 1249/6500 [3:47:16<15:15:15, 10.46s/it]                                                         19%|        | 1249/6500 [3:47:16<15:15:15, 10.46s/it] 19%|        | 1250/6500 [3:47:26<15:12:50, 10.43s/it]                                                         19%|        | 1250/6500 [3:47:26<15:12:50, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8078341484069824, 'eval_runtime': 4.199, 'eval_samples_per_second': 5.477, 'eval_steps_per_second': 1.429, 'epoch': 0.19}
                                                         19%|        | 1250/6500 [3:47:30<15:12:50, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1250I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1250

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1250/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5822, 'learning_rate': 9.115764201685623e-05, 'epoch': 0.19}
{'loss': 0.6378, 'learning_rate': 9.114391099358968e-05, 'epoch': 0.19}
{'loss': 0.6465, 'learning_rate': 9.113017035318409e-05, 'epoch': 0.19}
{'loss': 0.5888, 'learning_rate': 9.111642009885127e-05, 'epoch': 0.19}
{'loss': 0.6068, 'learning_rate': 9.110266023380523e-05, 'epoch': 0.19}
{'loss': 0.5775, 'learning_rate': 9.108889076126226e-05, 'epoch': 0.19}
 19%|        | 1251/6500 [3:47:41<17:13:01, 11.81s/it]                                                         19%|        | 1251/6500 [3:47:41<17:13:01, 11.81s/it] 19%|        | 1252/6500 [3:47:52<16:35:37, 11.38s/it]                                                         19%|        | 1252/6500 [3:47:52<16:35:37, 11.38s/it] 19%|        | 1253/6500 [3:48:02<16:08:58, 11.08s/it]                                                         19%|        | 1253/6500 [3:48:02<16:08:58, 11.08s/it] 19%|        | 1254/6500 [3:48:12<15:50:18, 10.87s/it]                                                         19%|        | 1254/6500 [3:48:12<15:50:18, 10.87s/it] 19%|        | 1255/6500 [3:48:23<15:37:24, 10.72s/it]                                                         19%|        | 1255/6500 [3:48:23<15:37:24, 10.72s/it] 19%|        | 1256/6500 [3:48:33<15:28:15, 10.62s/it]                                                         19%{'loss': 1.1249, 'learning_rate': 9.107511168444092e-05, 'epoch': 0.19}
{'loss': 0.6161, 'learning_rate': 9.106132300656196e-05, 'epoch': 0.19}
{'loss': 0.6141, 'learning_rate': 9.104752473084838e-05, 'epoch': 0.19}
{'loss': 0.5653, 'learning_rate': 9.103371686052548e-05, 'epoch': 0.19}
|        | 1256/6500 [3:48:33<15:28:15, 10.62s/it] 19%|        | 1257/6500 [3:48:43<15:21:23, 10.54s/it]                                                         19%|        | 1257/6500 [3:48:43<15:21:23, 10.54s/it] 19%|        | 1258/6500 [3:48:54<15:17:20, 10.50s/it]                                                         19%|        | 1258/6500 [3:48:54<15:17:20, 10.50s/it] 19%|        | 1259/6500 [3:49:04<15:14:07, 10.47s/it]                                                         19%|        | 1259/6500 [3:49:04<15:14:07, 10.47s/it] 19%|        | 1260/6500 [3:49:15<15:11:49, 10.44s/it]                                                         19%|        | 1260/6500 [3:49:15<15:11:49, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8136712908744812, 'eval_runtime': 3.9673, 'eval_samples_per_second': 5.797, 'eval_steps_per_second': 1.512, 'epoch': 0.19}
                                                         19%|        | 1260/6500 [3:49:19<15:11:49, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1260/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5968, 'learning_rate': 9.101989939882076e-05, 'epoch': 0.19}
{'loss': 0.6303, 'learning_rate': 9.100607234896397e-05, 'epoch': 0.19}
{'loss': 0.5774, 'learning_rate': 9.099223571418707e-05, 'epoch': 0.19}
{'loss': 0.5822, 'learning_rate': 9.097838949772432e-05, 'epoch': 0.19}
{'loss': 0.5794, 'learning_rate': 9.096453370281219e-05, 'epoch': 0.19}
{'loss': 0.5877, 'learning_rate': 9.095066833268935e-05, 'epoch': 0.19}
 19%|        | 1261/6500 [3:49:30<17:15:40, 11.86s/it]                                                         19%|        | 1261/6500 [3:49:30<17:15:40, 11.86s/it] 19%|        | 1262/6500 [3:49:40<16:36:50, 11.42s/it]                                                         19%|        | 1262/6500 [3:49:40<16:36:50, 11.42s/it] 19%|        | 1263/6500 [3:49:51<16:09:50, 11.11s/it]                                                         19%|        | 1263/6500 [3:49:51<16:09:50, 11.11s/it] 19%|        | 1264/6500 [3:50:01<15:51:08, 10.90s/it]                                                         19%|        | 1264/6500 [3:50:01<15:51:08, 10.90s/it] 19%|        | 1265/6500 [3:50:11<15:37:24, 10.74s/it]                                                         19%|        | 1265/6500 [3:50:11<15:37:24, 10.74s/it] 19%|        | 1266/6500 [3:50:22<15:27:58, 10.64s/it]                                                         19%{'loss': 0.5949, 'learning_rate': 9.093679339059678e-05, 'epoch': 0.19}
{'loss': 0.5894, 'learning_rate': 9.092290887977765e-05, 'epoch': 0.2}
{'loss': 0.6102, 'learning_rate': 9.090901480347739e-05, 'epoch': 0.2}
{'loss': 0.5714, 'learning_rate': 9.089511116494367e-05, 'epoch': 0.2}
|        | 1266/6500 [3:50:22<15:27:58, 10.64s/it] 19%|        | 1267/6500 [3:50:32<15:21:25, 10.56s/it]                                                         19%|        | 1267/6500 [3:50:32<15:21:25, 10.56s/it] 20%|        | 1268/6500 [3:50:43<15:16:28, 10.51s/it]                                                         20%|        | 1268/6500 [3:50:43<15:16:28, 10.51s/it] 20%|        | 1269/6500 [3:50:53<15:13:27, 10.48s/it]                                                         20%|        | 1269/6500 [3:50:53<15:13:27, 10.48s/it] 20%|        | 1270/6500 [3:51:03<15:10:59, 10.45s/it]                                                         20%|        | 1270/6500 [3:51:03<15:10:59, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8239951133728027, 'eval_runtime': 3.9803, 'eval_samples_per_second': 5.778, 'eval_steps_per_second': 1.507, 'epoch': 0.2}
                                                         20%|        | 1270/6500 [3:51:07<15:10:59, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6521, 'learning_rate': 9.088119796742633e-05, 'epoch': 0.2}
{'loss': 0.5904, 'learning_rate': 9.086727521417755e-05, 'epoch': 0.2}
{'loss': 0.6092, 'learning_rate': 9.085334290845164e-05, 'epoch': 0.2}
{'loss': 0.632, 'learning_rate': 9.083940105350524e-05, 'epoch': 0.2}
{'loss': 0.5879, 'learning_rate': 9.082544965259715e-05, 'epoch': 0.2}
{'loss': 0.5944, 'learning_rate': 9.081148870898842e-05, 'epoch': 0.2}
 20%|        | 1271/6500 [3:51:18<17:06:45, 11.78s/it]                                                         20%|        | 1271/6500 [3:51:18<17:06:45, 11.78s/it] 20%|        | 1272/6500 [3:51:29<16:30:09, 11.36s/it]                                                         20%|        | 1272/6500 [3:51:29<16:30:09, 11.36s/it] 20%|        | 1273/6500 [3:51:39<16:04:27, 11.07s/it]                                                         20%|        | 1273/6500 [3:51:39<16:04:27, 11.07s/it] 20%|        | 1274/6500 [3:51:49<15:46:46, 10.87s/it]                                                         20%|        | 1274/6500 [3:51:49<15:46:46, 10.87s/it] 20%|        | 1275/6500 [3:52:00<15:34:12, 10.73s/it]                                                         20%|        | 1275/6500 [3:52:00<15:34:12, 10.73s/it] 20%|        | 1276/6500 [3:52:10<15:24:50, 10.62s/it]                                                         20%{'loss': 0.5957, 'learning_rate': 9.079751822594235e-05, 'epoch': 0.2}
{'loss': 0.6169, 'learning_rate': 9.078353820672443e-05, 'epoch': 0.2}
{'loss': 0.597, 'learning_rate': 9.076954865460243e-05, 'epoch': 0.2}
{'loss': 0.5878, 'learning_rate': 9.075554957284633e-05, 'epoch': 0.2}
|        | 1276/6500 [3:52:10<15:24:50, 10.62s/it] 20%|        | 1277/6500 [3:52:21<15:24:57, 10.63s/it]                                                         20%|        | 1277/6500 [3:52:21<15:24:57, 10.63s/it] 20%|        | 1278/6500 [3:52:31<15:18:41, 10.56s/it]                                                         20%|        | 1278/6500 [3:52:31<15:18:41, 10.56s/it] 20%|        | 1279/6500 [3:52:42<15:14:11, 10.51s/it]                                                         20%|        | 1279/6500 [3:52:42<15:14:11, 10.51s/it] 20%|        | 1280/6500 [3:52:52<15:10:49, 10.47s/it]                                                         20%|        | 1280/6500 [3:52:52<15:10:49, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.80525141954422, 'eval_runtime': 4.2167, 'eval_samples_per_second': 5.455, 'eval_steps_per_second': 1.423, 'epoch': 0.2}
                                                         20%|        | 1280/6500 [3:52:56<15:10:49, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1280/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1280/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1280/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5783, 'learning_rate': 9.07415409647283e-05, 'epoch': 0.2}
{'loss': 0.6958, 'learning_rate': 9.072752283352279e-05, 'epoch': 0.2}
{'loss': 0.5832, 'learning_rate': 9.071349518250643e-05, 'epoch': 0.2}
{'loss': 0.5742, 'learning_rate': 9.069945801495813e-05, 'epoch': 0.2}
{'loss': 0.6014, 'learning_rate': 9.068541133415897e-05, 'epoch': 0.2}
{'loss': 1.1224, 'learning_rate': 9.067135514339229e-05, 'epoch': 0.2}
 20%|        | 1281/6500 [3:53:07<17:13:15, 11.88s/it]                                                         20%|        | 1281/6500 [3:53:07<17:13:15, 11.88s/it] 20%|        | 1282/6500 [3:53:18<16:34:02, 11.43s/it]                                                         20%|        | 1282/6500 [3:53:18<16:34:02, 11.43s/it] 20%|        | 1283/6500 [3:53:28<16:06:26, 11.11s/it]                                                         20%|        | 1283/6500 [3:53:28<16:06:26, 11.11s/it] 20%|        | 1284/6500 [3:53:38<15:47:18, 10.90s/it]                                                         20%|        | 1284/6500 [3:53:38<15:47:18, 10.90s/it] 20%|        | 1285/6500 [3:53:49<15:33:42, 10.74s/it]                                                         20%|        | 1285/6500 [3:53:49<15:33:42, 10.74s/it] 20%|        | 1286/6500 [3:53:59<15:24:06, 10.63s/it]                                                         20%{'loss': 0.6106, 'learning_rate': 9.065728944594362e-05, 'epoch': 0.2}
{'loss': 0.602, 'learning_rate': 9.064321424510074e-05, 'epoch': 0.2}
{'loss': 0.5986, 'learning_rate': 9.062912954415366e-05, 'epoch': 0.2}
{'loss': 0.5649, 'learning_rate': 9.061503534639457e-05, 'epoch': 0.2}
|        | 1286/6500 [3:53:59<15:24:06, 10.63s/it] 20%|        | 1287/6500 [3:54:09<15:17:49, 10.56s/it]                                                         20%|        | 1287/6500 [3:54:09<15:17:49, 10.56s/it] 20%|        | 1288/6500 [3:54:20<15:13:05, 10.51s/it]                                                         20%|        | 1288/6500 [3:54:20<15:13:05, 10.51s/it] 20%|        | 1289/6500 [3:54:30<15:10:07, 10.48s/it]                                                         20%|        | 1289/6500 [3:54:30<15:10:07, 10.48s/it] 20%|        | 1290/6500 [3:54:41<15:07:17, 10.45s/it]                                                         20%|        | 1290/6500 [3:54:41<15:07:17, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8100040555000305, 'eval_runtime': 3.9764, 'eval_samples_per_second': 5.784, 'eval_steps_per_second': 1.509, 'epoch': 0.2}
                                                         20%|        | 1290/6500 [3:54:45<15:07:17, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1290/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1290/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1290/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.621, 'learning_rate': 9.060093165511789e-05, 'epoch': 0.2}
{'loss': 0.5954, 'learning_rate': 9.058681847362032e-05, 'epoch': 0.2}
{'loss': 0.5585, 'learning_rate': 9.05726958052007e-05, 'epoch': 0.2}
{'loss': 0.5819, 'learning_rate': 9.055856365316011e-05, 'epoch': 0.2}
{'loss': 0.5781, 'learning_rate': 9.054442202080188e-05, 'epoch': 0.2}
{'loss': 0.5875, 'learning_rate': 9.053027091143151e-05, 'epoch': 0.2}
 20%|        | 1291/6500 [3:54:56<17:04:09, 11.80s/it]                                                         20%|        | 1291/6500 [3:54:56<17:04:09, 11.80s/it] 20%|        | 1292/6500 [3:55:06<16:27:13, 11.37s/it]                                                         20%|        | 1292/6500 [3:55:06<16:27:13, 11.37s/it] 20%|        | 1293/6500 [3:55:17<16:11:14, 11.19s/it]                                                         20%|        | 1293/6500 [3:55:17<16:11:14, 11.19s/it] 20%|        | 1294/6500 [3:55:27<15:50:30, 10.95s/it]                                                         20%|        | 1294/6500 [3:55:27<15:50:30, 10.95s/it] 20%|        | 1295/6500 [3:55:38<15:35:20, 10.78s/it]                                                         20%|        | 1295/6500 [3:55:38<15:35:20, 10.78s/it] 20%|        | 1296/6500 [3:55:48<15:24:47, 10.66s/it]                                                         20%{'loss': 0.5872, 'learning_rate': 9.051611032835675e-05, 'epoch': 0.2}
{'loss': 0.6081, 'learning_rate': 9.050194027488754e-05, 'epoch': 0.2}
{'loss': 0.5793, 'learning_rate': 9.048776075433604e-05, 'epoch': 0.2}
{'loss': 0.6012, 'learning_rate': 9.047357177001663e-05, 'epoch': 0.2}
|        | 1296/6500 [3:55:48<15:24:47, 10.66s/it] 20%|        | 1297/6500 [3:55:58<15:17:47, 10.58s/it]                                                         20%|        | 1297/6500 [3:55:58<15:17:47, 10.58s/it] 20%|        | 1298/6500 [3:56:09<15:12:47, 10.53s/it]                                                         20%|        | 1298/6500 [3:56:09<15:12:47, 10.53s/it] 20%|        | 1299/6500 [3:56:19<15:09:00, 10.49s/it]                                                         20%|        | 1299/6500 [3:56:19<15:09:00, 10.49s/it] 20%|        | 1300/6500 [3:56:29<15:06:29, 10.46s/it]                                                         20%|        | 1300/6500 [3:56:29<15:06:29, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8210251927375793, 'eval_runtime': 3.9876, 'eval_samples_per_second': 5.768, 'eval_steps_per_second': 1.505, 'epoch': 0.2}
                                                         20%|        | 1300/6500 [3:56:33<15:06:29, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1300I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1300

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6267, 'learning_rate': 9.045937332524592e-05, 'epoch': 0.2}
{'loss': 0.6109, 'learning_rate': 9.044516542334267e-05, 'epoch': 0.2}
{'loss': 0.5994, 'learning_rate': 9.043094806762793e-05, 'epoch': 0.2}
{'loss': 0.6026, 'learning_rate': 9.04167212614249e-05, 'epoch': 0.2}
{'loss': 0.5782, 'learning_rate': 9.0402485008059e-05, 'epoch': 0.2}
{'loss': 0.5866, 'learning_rate': 9.038823931085789e-05, 'epoch': 0.2}
 20%|        | 1301/6500 [3:56:44<17:00:58, 11.78s/it]                                                         20%|        | 1301/6500 [3:56:44<17:00:58, 11.78s/it] 20%|        | 1302/6500 [3:56:55<16:24:45, 11.37s/it]                                                         20%|        | 1302/6500 [3:56:55<16:24:45, 11.37s/it] 20%|        | 1303/6500 [3:57:05<15:59:17, 11.08s/it]                                                         20%|        | 1303/6500 [3:57:05<15:59:17, 11.08s/it] 20%|        | 1304/6500 [3:57:16<15:41:43, 10.87s/it]                                                         20%|        | 1304/6500 [3:57:16<15:41:43, 10.87s/it] 20%|        | 1305/6500 [3:57:26<15:29:06, 10.73s/it]                                                         20%|        | 1305/6500 [3:57:26<15:29:06, 10.73s/it] 20%|        | 1306/6500 [3:57:36<15:20:08, 10.63s/it]                                                         20%{'loss': 0.6208, 'learning_rate': 9.037398417315142e-05, 'epoch': 0.2}
{'loss': 0.5894, 'learning_rate': 9.03597195982716e-05, 'epoch': 0.2}
{'loss': 0.5931, 'learning_rate': 9.034544558955274e-05, 'epoch': 0.2}
{'loss': 0.5807, 'learning_rate': 9.033116215033126e-05, 'epoch': 0.2}
|        | 1306/6500 [3:57:36<15:20:08, 10.63s/it] 20%|        | 1307/6500 [3:57:47<15:13:40, 10.56s/it]                                                         20%|        | 1307/6500 [3:57:47<15:13:40, 10.56s/it] 20%|        | 1308/6500 [3:57:57<15:09:37, 10.51s/it]                                                         20%|        | 1308/6500 [3:57:57<15:09:37, 10.51s/it] 20%|        | 1309/6500 [3:58:08<15:13:13, 10.56s/it]                                                         20%|        | 1309/6500 [3:58:08<15:13:13, 10.56s/it] 20%|        | 1310/6500 [3:58:18<15:09:10, 10.51s/it]                                                         20%|        | 1310/6500 [3:58:18<15:09:10, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8116315007209778, 'eval_runtime': 4.2287, 'eval_samples_per_second': 5.439, 'eval_steps_per_second': 1.419, 'epoch': 0.2}
                                                         20%|        | 1310/6500 [3:58:22<15:09:10, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1310/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1310/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6019, 'learning_rate': 9.031686928394584e-05, 'epoch': 0.2}
{'loss': 0.6527, 'learning_rate': 9.030256699373738e-05, 'epoch': 0.2}
{'loss': 0.5739, 'learning_rate': 9.028825528304892e-05, 'epoch': 0.2}
{'loss': 0.5785, 'learning_rate': 9.027393415522574e-05, 'epoch': 0.2}
{'loss': 0.5943, 'learning_rate': 9.025960361361531e-05, 'epoch': 0.2}
{'loss': 1.105, 'learning_rate': 9.024526366156732e-05, 'epoch': 0.2}
 20%|        | 1311/6500 [3:58:33<17:10:05, 11.91s/it]                                                         20%|        | 1311/6500 [3:58:33<17:10:05, 11.91s/it] 20%|        | 1312/6500 [3:58:44<16:30:05, 11.45s/it]                                                         20%|        | 1312/6500 [3:58:44<16:30:05, 11.45s/it] 20%|        | 1313/6500 [3:58:54<16:02:26, 11.13s/it]                                                         20%|        | 1313/6500 [3:58:54<16:02:26, 11.13s/it] 20%|        | 1314/6500 [3:59:05<15:43:32, 10.92s/it]                                                         20%|        | 1314/6500 [3:59:05<15:43:32, 10.92s/it] 20%|        | 1315/6500 [3:59:15<15:29:33, 10.76s/it]                                                         20%|        | 1315/6500 [3:59:15<15:29:33, 10.76s/it] 20%|        | 1316/6500 [3:59:25<15:19:45, 10.65s/it]                                                         20%{'loss': 0.6164, 'learning_rate': 9.023091430243367e-05, 'epoch': 0.2}
{'loss': 0.5906, 'learning_rate': 9.021655553956839e-05, 'epoch': 0.2}
{'loss': 0.558, 'learning_rate': 9.020218737632778e-05, 'epoch': 0.2}
{'loss': 0.5687, 'learning_rate': 9.018780981607029e-05, 'epoch': 0.2}
|        | 1316/6500 [3:59:25<15:19:45, 10.65s/it] 20%|        | 1317/6500 [3:59:36<15:13:19, 10.57s/it]                                                         20%|        | 1317/6500 [3:59:36<15:13:19, 10.57s/it] 20%|        | 1318/6500 [3:59:46<15:08:26, 10.52s/it]                                                         20%|        | 1318/6500 [3:59:46<15:08:26, 10.52s/it] 20%|        | 1319/6500 [3:59:56<15:04:45, 10.48s/it]                                                         20%|        | 1319/6500 [3:59:56<15:04:45, 10.48s/it] 20%|        | 1320/6500 [4:00:07<15:02:30, 10.45s/it]                                                         20%|        | 1320/6500 [4:00:07<15:02:30, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8068228960037231, 'eval_runtime': 3.9791, 'eval_samples_per_second': 5.78, 'eval_steps_per_second': 1.508, 'epoch': 0.2}
                                                         20%|        | 1320/6500 [4:00:11<15:02:30, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1320/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1320/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1320/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6172, 'learning_rate': 9.01734228621566e-05, 'epoch': 0.2}
{'loss': 0.5705, 'learning_rate': 9.01590265179496e-05, 'epoch': 0.2}
{'loss': 0.577, 'learning_rate': 9.014462078681431e-05, 'epoch': 0.2}
{'loss': 0.5478, 'learning_rate': 9.013020567211799e-05, 'epoch': 0.2}
{'loss': 0.5884, 'learning_rate': 9.01157811772301e-05, 'epoch': 0.2}
{'loss': 0.5641, 'learning_rate': 9.010134730552224e-05, 'epoch': 0.2}
 20%|        | 1321/6500 [4:00:22<16:55:41, 11.77s/it]                                                         20%|        | 1321/6500 [4:00:22<16:55:41, 11.77s/it] 20%|        | 1322/6500 [4:00:32<16:19:44, 11.35s/it]                                                         20%|        | 1322/6500 [4:00:32<16:19:44, 11.35s/it] 20%|        | 1323/6500 [4:00:43<15:54:55, 11.07s/it]                                                         20%|        | 1323/6500 [4:00:43<15:54:55, 11.07s/it] 20%|        | 1324/6500 [4:00:53<15:37:30, 10.87s/it]                                                         20%|        | 1324/6500 [4:00:53<15:37:30, 10.87s/it] 20%|        | 1325/6500 [4:01:03<15:24:56, 10.72s/it]                                                         20%|        | 1325/6500 [4:01:03<15:24:56, 10.72s/it] 20%|        | 1326/6500 [4:01:14<15:25:37, 10.73s/it]                                                         20%{'loss': 0.5638, 'learning_rate': 9.008690406036829e-05, 'epoch': 0.2}
{'loss': 0.623, 'learning_rate': 9.007245144514425e-05, 'epoch': 0.2}
{'loss': 0.5792, 'learning_rate': 9.005798946322832e-05, 'epoch': 0.2}
{'loss': 0.6203, 'learning_rate': 9.004351811800091e-05, 'epoch': 0.2}
|        | 1326/6500 [4:01:14<15:25:37, 10.73s/it] 20%|        | 1327/6500 [4:01:24<15:16:34, 10.63s/it]                                                         20%|        | 1327/6500 [4:01:24<15:16:34, 10.63s/it] 20%|        | 1328/6500 [4:01:35<15:10:06, 10.56s/it]                                                         20%|        | 1328/6500 [4:01:35<15:10:06, 10.56s/it] 20%|        | 1329/6500 [4:01:45<15:05:43, 10.51s/it]                                                         20%|        | 1329/6500 [4:01:45<15:05:43, 10.51s/it] 20%|        | 1330/6500 [4:01:56<15:02:23, 10.47s/it]                                                         20%|        | 1330/6500 [4:01:56<15:02:23, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8119223117828369, 'eval_runtime': 3.9816, 'eval_samples_per_second': 5.777, 'eval_steps_per_second': 1.507, 'epoch': 0.2}
                                                         20%|        | 1330/6500 [4:02:00<15:02:23, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1330/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6127, 'learning_rate': 9.002903741284463e-05, 'epoch': 0.2}
{'loss': 0.5905, 'learning_rate': 9.001454735114421e-05, 'epoch': 0.2}
{'loss': 0.5959, 'learning_rate': 9.000004793628665e-05, 'epoch': 0.21}
{'loss': 0.5849, 'learning_rate': 8.998553917166108e-05, 'epoch': 0.21}
{'loss': 0.5668, 'learning_rate': 8.997102106065884e-05, 'epoch': 0.21}
{'loss': 0.5936, 'learning_rate': 8.995649360667348e-05, 'epoch': 0.21}
 20%|        | 1331/6500 [4:02:10<16:55:45, 11.79s/it]                                                         20%|        | 1331/6500 [4:02:10<16:55:45, 11.79s/it] 20%|        | 1332/6500 [4:02:21<16:19:20, 11.37s/it]                                                         20%|        | 1332/6500 [4:02:21<16:19:20, 11.37s/it] 21%|        | 1333/6500 [4:02:31<15:53:41, 11.07s/it]                                                         21%|        | 1333/6500 [4:02:31<15:53:41, 11.07s/it] 21%|        | 1334/6500 [4:02:42<15:35:26, 10.86s/it]                                                         21%|        | 1334/6500 [4:02:42<15:35:26, 10.86s/it] 21%|        | 1335/6500 [4:02:52<15:22:25, 10.72s/it]                                                         21%|        | 1335/6500 [4:02:52<15:22:25, 10.72s/it] 21%|        | 1336/6500 [4:03:02<15:13:16, 10.61s/it]                                                         21%{'loss': 0.6126, 'learning_rate': 8.994195681310067e-05, 'epoch': 0.21}
{'loss': 0.5672, 'learning_rate': 8.99274106833383e-05, 'epoch': 0.21}
{'loss': 0.5923, 'learning_rate': 8.991285522078644e-05, 'epoch': 0.21}
{'loss': 0.5765, 'learning_rate': 8.989829042884735e-05, 'epoch': 0.21}
|        | 1336/6500 [4:03:02<15:13:16, 10.61s/it] 21%|        | 1337/6500 [4:03:13<15:07:15, 10.54s/it]                                                         21%|        | 1337/6500 [4:03:13<15:07:15, 10.54s/it] 21%|        | 1338/6500 [4:03:23<15:02:59, 10.50s/it]                                                         21%|        | 1338/6500 [4:03:23<15:02:59, 10.50s/it] 21%|        | 1339/6500 [4:03:34<14:59:51, 10.46s/it]                                                         21%|        | 1339/6500 [4:03:34<14:59:51, 10.46s/it] 21%|        | 1340/6500 [4:03:44<14:57:30, 10.44s/it]                                                         21%|        | 1340/6500 [4:03:44<14:57:30, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8058823943138123, 'eval_runtime': 3.9864, 'eval_samples_per_second': 5.77, 'eval_steps_per_second': 1.505, 'epoch': 0.21}
                                                         21%|        | 1340/6500 [4:03:48<14:57:30, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1340the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1340

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6193, 'learning_rate': 8.988371631092547e-05, 'epoch': 0.21}
{'loss': 0.6211, 'learning_rate': 8.986913287042739e-05, 'epoch': 0.21}
{'loss': 0.5694, 'learning_rate': 8.985454011076191e-05, 'epoch': 0.21}
{'loss': 0.5853, 'learning_rate': 8.983993803533999e-05, 'epoch': 0.21}
{'loss': 0.57, 'learning_rate': 8.98253266475748e-05, 'epoch': 0.21}
{'loss': 1.1074, 'learning_rate': 8.981070595088164e-05, 'epoch': 0.21}
 21%|        | 1341/6500 [4:03:59<16:52:52, 11.78s/it]                                                         21%|        | 1341/6500 [4:03:59<16:52:52, 11.78s/it] 21%|        | 1342/6500 [4:04:10<16:29:28, 11.51s/it]                                                         21%|        | 1342/6500 [4:04:10<16:29:28, 11.51s/it] 21%|        | 1343/6500 [4:04:20<15:59:49, 11.17s/it]                                                         21%|        | 1343/6500 [4:04:20<15:59:49, 11.17s/it] 21%|        | 1344/6500 [4:04:30<15:38:36, 10.92s/it]                                                         21%|        | 1344/6500 [4:04:30<15:38:36, 10.92s/it] 21%|        | 1345/6500 [4:04:41<15:23:50, 10.75s/it]                                                         21%|        | 1345/6500 [4:04:41<15:23:50, 10.75s/it] 21%|        | 1346/6500 [4:04:51<15:12:56, 10.63s/it]                                                         21%{'loss': 0.5905, 'learning_rate': 8.979607594867802e-05, 'epoch': 0.21}
{'loss': 0.592, 'learning_rate': 8.978143664438361e-05, 'epoch': 0.21}
{'loss': 0.5468, 'learning_rate': 8.976678804142025e-05, 'epoch': 0.21}
{'loss': 0.6051, 'learning_rate': 8.975213014321198e-05, 'epoch': 0.21}
|        | 1346/6500 [4:04:51<15:12:56, 10.63s/it] 21%|        | 1347/6500 [4:05:01<15:05:39, 10.55s/it]                                                         21%|        | 1347/6500 [4:05:01<15:05:39, 10.55s/it] 21%|        | 1348/6500 [4:05:12<15:00:40, 10.49s/it]                                                         21%|        | 1348/6500 [4:05:12<15:00:40, 10.49s/it] 21%|        | 1349/6500 [4:05:22<14:56:33, 10.44s/it]                                                         21%|        | 1349/6500 [4:05:22<14:56:33, 10.44s/it] 21%|        | 1350/6500 [4:05:33<14:54:03, 10.42s/it]                                                         21%|        | 1350/6500 [4:05:33<14:54:03, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8078441619873047, 'eval_runtime': 3.9654, 'eval_samples_per_second': 5.8, 'eval_steps_per_second': 1.513, 'epoch': 0.21}
                                                         21%|        | 1350/6500 [4:05:36<14:54:03, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1350the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1350

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1350/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5895, 'learning_rate': 8.9737462953185e-05, 'epoch': 0.21}
{'loss': 0.5496, 'learning_rate': 8.972278647476764e-05, 'epoch': 0.21}
{'loss': 0.5617, 'learning_rate': 8.970810071139047e-05, 'epoch': 0.21}
{'loss': 0.5567, 'learning_rate': 8.969340566648619e-05, 'epoch': 0.21}
{'loss': 0.577, 'learning_rate': 8.967870134348966e-05, 'epoch': 0.21}
{'loss': 0.5653, 'learning_rate': 8.966398774583795e-05, 'epoch': 0.21}
 21%|        | 1351/6500 [4:05:47<16:46:56, 11.73s/it]                                                         21%|        | 1351/6500 [4:05:47<16:46:56, 11.73s/it] 21%|        | 1352/6500 [4:05:58<16:10:49, 11.31s/it]                                                         21%|        | 1352/6500 [4:05:58<16:10:49, 11.31s/it] 21%|        | 1353/6500 [4:06:08<15:46:06, 11.03s/it]                                                         21%|        | 1353/6500 [4:06:08<15:46:06, 11.03s/it] 21%|        | 1354/6500 [4:06:18<15:28:31, 10.83s/it]                                                         21%|        | 1354/6500 [4:06:18<15:28:31, 10.83s/it] 21%|        | 1355/6500 [4:06:29<15:16:07, 10.68s/it]                                                         21%|        | 1355/6500 [4:06:29<15:16:07, 10.68s/it] 21%|        | 1356/6500 [4:06:39<15:07:46, 10.59s/it]                                                         21%{'loss': 0.5817, 'learning_rate': 8.964926487697027e-05, 'epoch': 0.21}
{'loss': 0.5808, 'learning_rate': 8.9634532740328e-05, 'epoch': 0.21}
{'loss': 0.5631, 'learning_rate': 8.961979133935468e-05, 'epoch': 0.21}
{'loss': 0.6279, 'learning_rate': 8.960504067749602e-05, 'epoch': 0.21}
|        | 1356/6500 [4:06:39<15:07:46, 10.59s/it] 21%|        | 1357/6500 [4:06:49<15:01:35, 10.52s/it]                                                         21%|        | 1357/6500 [4:06:49<15:01:35, 10.52s/it] 21%|        | 1358/6500 [4:07:00<15:07:37, 10.59s/it]                                                         21%|        | 1358/6500 [4:07:00<15:07:37, 10.59s/it] 21%|        | 1359/6500 [4:07:11<15:01:39, 10.52s/it]                                                         21%|        | 1359/6500 [4:07:11<15:01:39, 10.52s/it] 21%|        | 1360/6500 [4:07:21<14:57:11, 10.47s/it]                                                         21%|        | 1360/6500 [4:07:21<14:57:11, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8135676383972168, 'eval_runtime': 3.973, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.21}
                                                         21%|        | 1360/6500 [4:07:25<14:57:11, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1360the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1360

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5746, 'learning_rate': 8.959028075819992e-05, 'epoch': 0.21}
{'loss': 0.5897, 'learning_rate': 8.957551158491639e-05, 'epoch': 0.21}
{'loss': 0.5953, 'learning_rate': 8.956073316109766e-05, 'epoch': 0.21}
{'loss': 0.5503, 'learning_rate': 8.954594549019808e-05, 'epoch': 0.21}
{'loss': 0.5703, 'learning_rate': 8.953114857567419e-05, 'epoch': 0.21}
{'loss': 0.5971, 'learning_rate': 8.951634242098468e-05, 'epoch': 0.21}
 21%|        | 1361/6500 [4:07:36<16:49:17, 11.78s/it]                                                         21%|        | 1361/6500 [4:07:36<16:49:17, 11.78s/it] 21%|        | 1362/6500 [4:07:46<16:12:31, 11.36s/it]                                                         21%|        | 1362/6500 [4:07:46<16:12:31, 11.36s/it] 21%|        | 1363/6500 [4:07:57<15:47:16, 11.06s/it]                                                         21%|        | 1363/6500 [4:07:57<15:47:16, 11.06s/it] 21%|        | 1364/6500 [4:08:07<15:29:16, 10.86s/it]                                                         21%|        | 1364/6500 [4:08:07<15:29:16, 10.86s/it] 21%|        | 1365/6500 [4:08:17<15:16:32, 10.71s/it]                                                         21%|        | 1365/6500 [4:08:17<15:16:32, 10.71s/it] 21%|        | 1366/6500 [4:08:28<15:07:48, 10.61s/it]                                                         21%{'loss': 0.586, 'learning_rate': 8.950152702959038e-05, 'epoch': 0.21}
{'loss': 0.578, 'learning_rate': 8.94867024049543e-05, 'epoch': 0.21}
{'loss': 0.5724, 'learning_rate': 8.947186855054164e-05, 'epoch': 0.21}
{'loss': 0.5621, 'learning_rate': 8.945702546981969e-05, 'epoch': 0.21}
|        | 1366/6500 [4:08:28<15:07:48, 10.61s/it] 21%|        | 1367/6500 [4:08:38<15:01:19, 10.54s/it]                                                         21%|        | 1367/6500 [4:08:38<15:01:19, 10.54s/it] 21%|        | 1368/6500 [4:08:48<14:56:54, 10.49s/it]                                                         21%|        | 1368/6500 [4:08:48<14:56:54, 10.49s/it] 21%|        | 1369/6500 [4:08:59<14:53:20, 10.45s/it]                                                         21%|        | 1369/6500 [4:08:59<14:53:20, 10.45s/it] 21%|        | 1370/6500 [4:09:09<14:51:36, 10.43s/it]                                                         21%|        | 1370/6500 [4:09:09<14:51:36, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8079096078872681, 'eval_runtime': 3.9498, 'eval_samples_per_second': 5.823, 'eval_steps_per_second': 1.519, 'epoch': 0.21}
                                                         21%|        | 1370/6500 [4:09:13<14:51:36, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6728, 'learning_rate': 8.944217316625793e-05, 'epoch': 0.21}
{'loss': 0.5562, 'learning_rate': 8.942731164332802e-05, 'epoch': 0.21}
{'loss': 0.5526, 'learning_rate': 8.941244090450372e-05, 'epoch': 0.21}
{'loss': 0.5743, 'learning_rate': 8.9397560953261e-05, 'epoch': 0.21}
{'loss': 1.1018, 'learning_rate': 8.938267179307796e-05, 'epoch': 0.21}
{'loss': 0.5901, 'learning_rate': 8.936777342743481e-05, 'epoch': 0.21}
 21%|        | 1371/6500 [4:09:24<16:42:42, 11.73s/it]                                                         21%|        | 1371/6500 [4:09:24<16:42:42, 11.73s/it] 21%|        | 1372/6500 [4:09:34<16:07:14, 11.32s/it]                                                         21%|        | 1372/6500 [4:09:34<16:07:14, 11.32s/it] 21%|        | 1373/6500 [4:09:45<15:42:20, 11.03s/it]                                                         21%|        | 1373/6500 [4:09:45<15:42:20, 11.03s/it] 21%|        | 1374/6500 [4:09:55<15:31:00, 10.90s/it]                                                         21%|        | 1374/6500 [4:09:55<15:31:00, 10.90s/it] 21%|        | 1375/6500 [4:10:06<15:16:45, 10.73s/it]                                                         21%|        | 1375/6500 [4:10:06<15:16:45, 10.73s/it] 21%|        | 1376/6500 [4:10:16<15:08:08, 10.63s/it]                                                         21%{'loss': 0.5906, 'learning_rate': 8.935286585981399e-05, 'epoch': 0.21}
{'loss': 0.5697, 'learning_rate': 8.933794909370006e-05, 'epoch': 0.21}
{'loss': 0.5453, 'learning_rate': 8.93230231325797e-05, 'epoch': 0.21}
{'loss': 0.6022, 'learning_rate': 8.930808797994177e-05, 'epoch': 0.21}
|        | 1376/6500 [4:10:16<15:08:08, 10.63s/it] 21%|        | 1377/6500 [4:10:26<15:00:49, 10.55s/it]                                                         21%|        | 1377/6500 [4:10:26<15:00:49, 10.55s/it] 21%|        | 1378/6500 [4:10:37<14:55:43, 10.49s/it]                                                         21%|        | 1378/6500 [4:10:37<14:55:43, 10.49s/it] 21%|        | 1379/6500 [4:10:47<14:52:11, 10.45s/it]                                                         21%|        | 1379/6500 [4:10:47<14:52:11, 10.45s/it] 21%|        | 1380/6500 [4:10:57<14:49:05, 10.42s/it]                                                         21%|        | 1380/6500 [4:10:57<14:49:05, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8077460527420044, 'eval_runtime': 4.4352, 'eval_samples_per_second': 5.186, 'eval_steps_per_second': 1.353, 'epoch': 0.21}
                                                         21%|        | 1380/6500 [4:11:02<14:49:05, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1380
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5673, 'learning_rate': 8.929314363927727e-05, 'epoch': 0.21}
{'loss': 0.5573, 'learning_rate': 8.927819011407937e-05, 'epoch': 0.21}
{'loss': 0.551, 'learning_rate': 8.926322740784332e-05, 'epoch': 0.21}
{'loss': 0.5625, 'learning_rate': 8.92482555240666e-05, 'epoch': 0.21}
{'loss': 0.569, 'learning_rate': 8.923327446624878e-05, 'epoch': 0.21}
 21%|        | 1381/6500 [4:11:13<16:52:15, 11.86s/it]                                                         21%|        | 1381/6500 [4:11:13<16:52:15, 11.86s/it] 21%|       | 1382/6500 [4:11:23<16:13:35, 11.41s/it]                                                         21%|       | 1382/6500 [4:11:23<16:13:35, 11.41s/it] 21%|       | 1383/6500 [4:11:33<15:46:17, 11.10s/it]                                                         21%|       | 1383/6500 [4:11:33<15:46:17, 11.10s/it] 21%|       | 1384/6500 [4:11:44<15:27:16, 10.87s/it]                                                         21%|       | 1384/6500 [4:11:44<15:27:16, 10.87s/it] 21%|       | 1385/6500 [4:11:54<15:13:47, 10.72s/it]                                                         21%|       | 1385/6500 [4:11:54<15:13:47, 10.72s/it] 21%|       | 1386/6500 [4:12:04<15:04:19, 10.61s/it]                                            {'loss': 0.5491, 'learning_rate': 8.921828423789158e-05, 'epoch': 0.21}
{'loss': 0.5955, 'learning_rate': 8.920328484249892e-05, 'epoch': 0.21}
{'loss': 0.5638, 'learning_rate': 8.918827628357677e-05, 'epoch': 0.21}
{'loss': 0.5891, 'learning_rate': 8.917325856463331e-05, 'epoch': 0.21}
{'loss': 0.6007, 'learning_rate': 8.915823168917884e-05, 'epoch': 0.21}
             21%|       | 1386/6500 [4:12:04<15:04:19, 10.61s/it] 21%|       | 1387/6500 [4:12:15<14:57:49, 10.54s/it]                                                         21%|       | 1387/6500 [4:12:15<14:57:49, 10.54s/it] 21%|       | 1388/6500 [4:12:25<14:53:23, 10.49s/it]                                                         21%|       | 1388/6500 [4:12:25<14:53:23, 10.49s/it] 21%|       | 1389/6500 [4:12:35<14:50:03, 10.45s/it]                                                         21%|       | 1389/6500 [4:12:35<14:50:03, 10.45s/it] 21%|       | 1390/6500 [4:12:46<14:57:41, 10.54s/it]                                                         21%|       | 1390/6500 [4:12:46<14:57:41, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8123325705528259, 'eval_runtime': 4.0295, 'eval_samples_per_second': 5.708, 'eval_steps_per_second': 1.489, 'epoch': 0.21}
                                                         21%|       | 1390/6500 [4:12:50<14:57:41, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.579, 'learning_rate': 8.91431956607258e-05, 'epoch': 0.21}
{'loss': 0.5834, 'learning_rate': 8.912815048278876e-05, 'epoch': 0.21}
{'loss': 0.5915, 'learning_rate': 8.911309615888446e-05, 'epoch': 0.21}
{'loss': 0.5699, 'learning_rate': 8.909803269253174e-05, 'epoch': 0.21}
{'loss': 0.5539, 'learning_rate': 8.908296008725161e-05, 'epoch': 0.21}
 21%|       | 1391/6500 [4:13:01<16:48:20, 11.84s/it]                                                         21%|       | 1391/6500 [4:13:01<16:48:20, 11.84s/it] 21%|       | 1392/6500 [4:13:11<16:10:02, 11.39s/it]                                                         21%|       | 1392/6500 [4:13:11<16:10:02, 11.39s/it] 21%|       | 1393/6500 [4:13:22<15:43:07, 11.08s/it]                                                         21%|       | 1393/6500 [4:13:22<15:43:07, 11.08s/it] 21%|       | 1394/6500 [4:13:32<15:24:32, 10.86s/it]                                                         21%|       | 1394/6500 [4:13:32<15:24:32, 10.86s/it] 21%|       | 1395/6500 [4:13:42<15:11:40, 10.72s/it]                                                         21%|       | 1395/6500 [4:13:43<15:11:40, 10.72s/it] 21%|       | 1396/6500 [4:13:53<15:02:42, 10.61s/it]                                        {'loss': 0.5999, 'learning_rate': 8.906787834656717e-05, 'epoch': 0.21}
{'loss': 0.5705, 'learning_rate': 8.905278747400369e-05, 'epoch': 0.21}
{'loss': 0.5721, 'learning_rate': 8.903768747308861e-05, 'epoch': 0.22}
{'loss': 0.5436, 'learning_rate': 8.902257834735144e-05, 'epoch': 0.22}
{'loss': 0.5954, 'learning_rate': 8.900746010032383e-05, 'epoch': 0.22}
                 21%|       | 1396/6500 [4:13:53<15:02:42, 10.61s/it] 21%|       | 1397/6500 [4:14:03<14:56:05, 10.54s/it]                                                         21%|       | 1397/6500 [4:14:03<14:56:05, 10.54s/it] 22%|       | 1398/6500 [4:14:14<14:51:32, 10.48s/it]                                                         22%|       | 1398/6500 [4:14:14<14:51:32, 10.48s/it] 22%|       | 1399/6500 [4:14:24<14:48:05, 10.45s/it]                                                         22%|       | 1399/6500 [4:14:24<14:48:05, 10.45s/it] 22%|       | 1400/6500 [4:14:34<14:46:35, 10.43s/it]                                                         22%|       | 1400/6500 [4:14:34<14:46:35, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8055394291877747, 'eval_runtime': 3.966, 'eval_samples_per_second': 5.799, 'eval_steps_per_second': 1.513, 'epoch': 0.22}
                                                         22%|       | 1400/6500 [4:14:38<14:46:35, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1400/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1400/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6325, 'learning_rate': 8.899233273553958e-05, 'epoch': 0.22}
{'loss': 0.5732, 'learning_rate': 8.897719625653465e-05, 'epoch': 0.22}
{'loss': 0.5525, 'learning_rate': 8.896205066684707e-05, 'epoch': 0.22}
{'loss': 0.576, 'learning_rate': 8.894689597001704e-05, 'epoch': 0.22}
{'loss': 1.1017, 'learning_rate': 8.893173216958687e-05, 'epoch': 0.22}
 22%|       | 1401/6500 [4:14:49<16:38:05, 11.74s/it]                                                         22%|       | 1401/6500 [4:14:49<16:38:05, 11.74s/it] 22%|       | 1402/6500 [4:15:00<16:02:30, 11.33s/it]                                                         22%|       | 1402/6500 [4:15:00<16:02:30, 11.33s/it] 22%|       | 1403/6500 [4:15:10<15:37:42, 11.04s/it]                                                         22%|       | 1403/6500 [4:15:10<15:37:42, 11.04s/it] 22%|       | 1404/6500 [4:15:20<15:19:49, 10.83s/it]                                                         22%|       | 1404/6500 [4:15:20<15:19:49, 10.83s/it] 22%|       | 1405/6500 [4:15:31<15:07:10, 10.68s/it]                                                         22%|       | 1405/6500 [4:15:31<15:07:10, 10.68s/it] 22%|       | 1406/6500 [4:15:42<15:14:10, 10.77s/it]                                        {'loss': 0.5913, 'learning_rate': 8.891655926910103e-05, 'epoch': 0.22}
{'loss': 0.565, 'learning_rate': 8.890137727210607e-05, 'epoch': 0.22}
{'loss': 0.5394, 'learning_rate': 8.88861861821507e-05, 'epoch': 0.22}
{'loss': 0.557, 'learning_rate': 8.887098600278573e-05, 'epoch': 0.22}
{'loss': 0.6072, 'learning_rate': 8.885577673756414e-05, 'epoch': 0.22}
                 22%|       | 1406/6500 [4:15:42<15:14:10, 10.77s/it] 22%|       | 1407/6500 [4:15:52<15:04:34, 10.66s/it]                                                         22%|       | 1407/6500 [4:15:52<15:04:34, 10.66s/it] 22%|       | 1408/6500 [4:16:02<14:56:44, 10.57s/it]                                                         22%|       | 1408/6500 [4:16:02<14:56:44, 10.57s/it] 22%|       | 1409/6500 [4:16:13<14:51:21, 10.51s/it]                                                         22%|       | 1409/6500 [4:16:13<14:51:21, 10.51s/it] 22%|       | 1410/6500 [4:16:23<14:47:36, 10.46s/it]                                                         22%|       | 1410/6500 [4:16:23<14:47:36, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8110669851303101, 'eval_runtime': 4.1493, 'eval_samples_per_second': 5.543, 'eval_steps_per_second': 1.446, 'epoch': 0.22}
                                                         22%|       | 1410/6500 [4:16:27<14:47:36, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1410/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1410

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1410
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.555, 'learning_rate': 8.884055839004098e-05, 'epoch': 0.22}
{'loss': 0.548, 'learning_rate': 8.882533096377344e-05, 'epoch': 0.22}
{'loss': 0.5394, 'learning_rate': 8.881009446232086e-05, 'epoch': 0.22}
{'loss': 0.5617, 'learning_rate': 8.879484888924467e-05, 'epoch': 0.22}
{'loss': 0.5694, 'learning_rate': 8.877959424810843e-05, 'epoch': 0.22}
 22%|       | 1411/6500 [4:16:38<16:42:23, 11.82s/it]                                                         22%|       | 1411/6500 [4:16:38<16:42:23, 11.82s/it] 22%|       | 1412/6500 [4:16:48<16:06:12, 11.39s/it]                                                         22%|       | 1412/6500 [4:16:48<16:06:12, 11.39s/it] 22%|       | 1413/6500 [4:16:59<15:40:54, 11.10s/it]                                                         22%|       | 1413/6500 [4:16:59<15:40:54, 11.10s/it] 22%|       | 1414/6500 [4:17:09<15:22:44, 10.89s/it]                                                         22%|       | 1414/6500 [4:17:09<15:22:44, 10.89s/it] 22%|       | 1415/6500 [4:17:20<15:09:22, 10.73s/it]                                                         22%|       | 1415/6500 [4:17:20<15:09:22, 10.73s/it] 22%|       | 1416/6500 [4:17:30<15:03:27, 10.66s/it]                                        {'loss': 0.5496, 'learning_rate': 8.87643305424778e-05, 'epoch': 0.22}
{'loss': 0.5858, 'learning_rate': 8.87490577759206e-05, 'epoch': 0.22}
{'loss': 0.5543, 'learning_rate': 8.873377595200676e-05, 'epoch': 0.22}
{'loss': 0.5943, 'learning_rate': 8.871848507430829e-05, 'epoch': 0.22}
{'loss': 0.5802, 'learning_rate': 8.870318514639935e-05, 'epoch': 0.22}
                 22%|       | 1416/6500 [4:17:30<15:03:27, 10.66s/it] 22%|       | 1417/6500 [4:17:40<14:56:06, 10.58s/it]                                                         22%|       | 1417/6500 [4:17:40<14:56:06, 10.58s/it] 22%|       | 1418/6500 [4:17:51<14:51:06, 10.52s/it]                                                         22%|       | 1418/6500 [4:17:51<14:51:06, 10.52s/it] 22%|       | 1419/6500 [4:18:01<14:46:43, 10.47s/it]                                                         22%|       | 1419/6500 [4:18:01<14:46:43, 10.47s/it] 22%|       | 1420/6500 [4:18:12<14:44:10, 10.44s/it]                                                         22%|       | 1420/6500 [4:18:12<14:44:10, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8169536590576172, 'eval_runtime': 4.0762, 'eval_samples_per_second': 5.642, 'eval_steps_per_second': 1.472, 'epoch': 0.22}
                                                         22%|       | 1420/6500 [4:18:16<14:44:10, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5618, 'learning_rate': 8.868787617185619e-05, 'epoch': 0.22}
{'loss': 0.5791, 'learning_rate': 8.86725581542572e-05, 'epoch': 0.22}
{'loss': 0.5626, 'learning_rate': 8.865723109718288e-05, 'epoch': 0.22}
{'loss': 0.5551, 'learning_rate': 8.864189500421582e-05, 'epoch': 0.22}
{'loss': 0.5603, 'learning_rate': 8.862654987894076e-05, 'epoch': 0.22}
 22%|       | 1421/6500 [4:18:27<16:41:37, 11.83s/it]                                                         22%|       | 1421/6500 [4:18:27<16:41:37, 11.83s/it] 22%|       | 1422/6500 [4:18:37<16:04:24, 11.40s/it]                                                         22%|       | 1422/6500 [4:18:37<16:04:24, 11.40s/it] 22%|       | 1423/6500 [4:18:48<15:46:10, 11.18s/it]                                                         22%|       | 1423/6500 [4:18:48<15:46:10, 11.18s/it] 22%|       | 1424/6500 [4:18:58<15:25:03, 10.93s/it]                                                         22%|       | 1424/6500 [4:18:58<15:25:03, 10.93s/it] 22%|       | 1425/6500 [4:19:08<15:10:35, 10.77s/it]                                                         22%|       | 1425/6500 [4:19:08<15:10:35, 10.77s/it] 22%|       | 1426/6500 [4:19:19<15:01:04, 10.66s/it]                                        {'loss': 0.5901, 'learning_rate': 8.861119572494453e-05, 'epoch': 0.22}
{'loss': 0.5588, 'learning_rate': 8.859583254581605e-05, 'epoch': 0.22}
{'loss': 0.5674, 'learning_rate': 8.858046034514637e-05, 'epoch': 0.22}
{'loss': 0.5458, 'learning_rate': 8.856507912652867e-05, 'epoch': 0.22}
{'loss': 0.6076, 'learning_rate': 8.85496888935582e-05, 'epoch': 0.22}
                 22%|       | 1426/6500 [4:19:19<15:01:04, 10.66s/it] 22%|       | 1427/6500 [4:19:29<14:54:16, 10.58s/it]                                                         22%|       | 1427/6500 [4:19:29<14:54:16, 10.58s/it] 22%|       | 1428/6500 [4:19:40<14:48:55, 10.52s/it]                                                         22%|       | 1428/6500 [4:19:40<14:48:55, 10.52s/it] 22%|       | 1429/6500 [4:19:50<14:45:51, 10.48s/it]                                                         22%|       | 1429/6500 [4:19:50<14:45:51, 10.48s/it] 22%|       | 1430/6500 [4:20:00<14:42:56, 10.45s/it]                                                         22%|       | 1430/6500 [4:20:00<14:42:56, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8055736422538757, 'eval_runtime': 3.9774, 'eval_samples_per_second': 5.783, 'eval_steps_per_second': 1.509, 'epoch': 0.22}
                                                         22%|       | 1430/6500 [4:20:04<14:42:56, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1430I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1430

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6002, 'learning_rate': 8.853428964983233e-05, 'epoch': 0.22}
{'loss': 0.5394, 'learning_rate': 8.851888139895057e-05, 'epoch': 0.22}
{'loss': 0.5656, 'learning_rate': 8.850346414451445e-05, 'epoch': 0.22}
{'loss': 0.6603, 'learning_rate': 8.84880378901277e-05, 'epoch': 0.22}
{'loss': 0.9852, 'learning_rate': 8.847260263939612e-05, 'epoch': 0.22}
 22%|       | 1431/6500 [4:20:15<16:34:36, 11.77s/it]                                                         22%|       | 1431/6500 [4:20:15<16:34:36, 11.77s/it] 22%|       | 1432/6500 [4:20:26<15:59:08, 11.36s/it]                                                         22%|       | 1432/6500 [4:20:26<15:59:08, 11.36s/it] 22%|       | 1433/6500 [4:20:36<15:33:54, 11.06s/it]                                                         22%|       | 1433/6500 [4:20:36<15:33:54, 11.06s/it] 22%|       | 1434/6500 [4:20:46<15:16:20, 10.85s/it]                                                         22%|       | 1434/6500 [4:20:46<15:16:20, 10.85s/it] 22%|       | 1435/6500 [4:20:57<15:03:26, 10.70s/it]                                                         22%|       | 1435/6500 [4:20:57<15:03:26, 10.70s/it] 22%|       | 1436/6500 [4:21:07<14:54:36, 10.60s/it]                                        {'loss': 0.5882, 'learning_rate': 8.845715839592758e-05, 'epoch': 0.22}
{'loss': 0.5621, 'learning_rate': 8.844170516333208e-05, 'epoch': 0.22}
{'loss': 0.5293, 'learning_rate': 8.842624294522174e-05, 'epoch': 0.22}
{'loss': 0.5744, 'learning_rate': 8.841077174521075e-05, 'epoch': 0.22}
{'loss': 0.5779, 'learning_rate': 8.83952915669154e-05, 'epoch': 0.22}
                 22%|       | 1436/6500 [4:21:07<14:54:36, 10.60s/it] 22%|       | 1437/6500 [4:21:17<14:48:59, 10.54s/it]                                                         22%|       | 1437/6500 [4:21:17<14:48:59, 10.54s/it] 22%|       | 1438/6500 [4:21:28<14:44:06, 10.48s/it]                                                         22%|       | 1438/6500 [4:21:28<14:44:06, 10.48s/it] 22%|       | 1439/6500 [4:21:39<14:59:23, 10.66s/it]                                                         22%|       | 1439/6500 [4:21:39<14:59:23, 10.66s/it] 22%|       | 1440/6500 [4:21:49<14:52:08, 10.58s/it]                                                         22%|       | 1440/6500 [4:21:49<14:52:08, 10.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8107069134712219, 'eval_runtime': 3.9618, 'eval_samples_per_second': 5.805, 'eval_steps_per_second': 1.514, 'epoch': 0.22}
                                                         22%|       | 1440/6500 [4:21:53<14:52:08, 10.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1440
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.525, 'learning_rate': 8.837980241395408e-05, 'epoch': 0.22}
{'loss': 0.552, 'learning_rate': 8.836430428994732e-05, 'epoch': 0.22}
{'loss': 0.5358, 'learning_rate': 8.834879719851768e-05, 'epoch': 0.22}
{'loss': 0.5503, 'learning_rate': 8.833328114328986e-05, 'epoch': 0.22}
{'loss': 0.5453, 'learning_rate': 8.831775612789063e-05, 'epoch': 0.22}
 22%|       | 1441/6500 [4:22:04<16:40:32, 11.87s/it]                                                         22%|       | 1441/6500 [4:22:04<16:40:32, 11.87s/it] 22%|       | 1442/6500 [4:22:14<16:02:04, 11.41s/it]                                                         22%|       | 1442/6500 [4:22:14<16:02:04, 11.41s/it] 22%|       | 1443/6500 [4:22:25<15:35:04, 11.09s/it]                                                         22%|       | 1443/6500 [4:22:25<15:35:04, 11.09s/it] 22%|       | 1444/6500 [4:22:35<15:16:05, 10.87s/it]                                                         22%|       | 1444/6500 [4:22:35<15:16:05, 10.87s/it] 22%|       | 1445/6500 [4:22:46<15:03:00, 10.72s/it]                                                         22%|       | 1445/6500 [4:22:46<15:03:00, 10.72s/it] 22%|       | 1446/6500 [4:22:56<14:54:31, 10.62s/it]                                        {'loss': 0.5634, 'learning_rate': 8.83022221559489e-05, 'epoch': 0.22}
{'loss': 0.56, 'learning_rate': 8.828667923109563e-05, 'epoch': 0.22}
{'loss': 0.5514, 'learning_rate': 8.827112735696385e-05, 'epoch': 0.22}
{'loss': 0.6005, 'learning_rate': 8.825556653718876e-05, 'epoch': 0.22}
{'loss': 0.5693, 'learning_rate': 8.82399967754076e-05, 'epoch': 0.22}
                 22%|       | 1446/6500 [4:22:56<14:54:31, 10.62s/it] 22%|       | 1447/6500 [4:23:06<14:47:43, 10.54s/it]                                                         22%|       | 1447/6500 [4:23:06<14:47:43, 10.54s/it] 22%|       | 1448/6500 [4:23:17<14:43:22, 10.49s/it]                                                         22%|       | 1448/6500 [4:23:17<14:43:22, 10.49s/it] 22%|       | 1449/6500 [4:23:27<14:39:46, 10.45s/it]                                                         22%|       | 1449/6500 [4:23:27<14:39:46, 10.45s/it] 22%|       | 1450/6500 [4:23:38<14:40:27, 10.46s/it]                                                         22%|       | 1450/6500 [4:23:38<14:40:27, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8110804557800293, 'eval_runtime': 3.9617, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.514, 'epoch': 0.22}
                                                         22%|       | 1450/6500 [4:23:41<14:40:27, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1450the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1450

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1450
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5687, 'learning_rate': 8.822441807525967e-05, 'epoch': 0.22}
{'loss': 0.5841, 'learning_rate': 8.820883044038644e-05, 'epoch': 0.22}
{'loss': 0.5374, 'learning_rate': 8.81932338744314e-05, 'epoch': 0.22}
{'loss': 0.5598, 'learning_rate': 8.817762838104016e-05, 'epoch': 0.22}
{'loss': 0.5755, 'learning_rate': 8.816201396386042e-05, 'epoch': 0.22}
 22%|       | 1451/6500 [4:23:52<16:29:21, 11.76s/it]                                                         22%|       | 1451/6500 [4:23:52<16:29:21, 11.76s/it] 22%|       | 1452/6500 [4:24:03<15:54:17, 11.34s/it]                                                         22%|       | 1452/6500 [4:24:03<15:54:17, 11.34s/it] 22%|       | 1453/6500 [4:24:13<15:28:57, 11.04s/it]                                                         22%|       | 1453/6500 [4:24:13<15:28:57, 11.04s/it] 22%|       | 1454/6500 [4:24:23<15:11:33, 10.84s/it]                                                         22%|       | 1454/6500 [4:24:23<15:11:33, 10.84s/it] 22%|       | 1455/6500 [4:24:34<15:05:38, 10.77s/it]                                                         22%|       | 1455/6500 [4:24:34<15:05:38, 10.77s/it] 22%|       | 1456/6500 [4:24:44<14:55:19, 10.65s/it]                                        {'loss': 0.5688, 'learning_rate': 8.814639062654194e-05, 'epoch': 0.22}
{'loss': 0.5627, 'learning_rate': 8.813075837273658e-05, 'epoch': 0.22}
{'loss': 0.5387, 'learning_rate': 8.81151172060983e-05, 'epoch': 0.22}
{'loss': 0.5647, 'learning_rate': 8.80994671302831e-05, 'epoch': 0.22}
{'loss': 0.6264, 'learning_rate': 8.808380814894912e-05, 'epoch': 0.22}
                 22%|       | 1456/6500 [4:24:44<14:55:19, 10.65s/it] 22%|       | 1457/6500 [4:24:55<14:48:12, 10.57s/it]                                                         22%|       | 1457/6500 [4:24:55<14:48:12, 10.57s/it] 22%|       | 1458/6500 [4:25:05<14:42:50, 10.51s/it]                                                         22%|       | 1458/6500 [4:25:05<14:42:50, 10.51s/it] 22%|       | 1459/6500 [4:25:15<14:38:56, 10.46s/it]                                                         22%|       | 1459/6500 [4:25:15<14:38:56, 10.46s/it] 22%|       | 1460/6500 [4:25:26<14:36:24, 10.43s/it]                                                         22%|       | 1460/6500 [4:25:26<14:36:24, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8048121333122253, 'eval_runtime': 3.9453, 'eval_samples_per_second': 5.83, 'eval_steps_per_second': 1.521, 'epoch': 0.22}
                                                         22%|       | 1460/6500 [4:25:30<14:36:24, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5535, 'learning_rate': 8.806814026575654e-05, 'epoch': 0.22}
{'loss': 0.5342, 'learning_rate': 8.805246348436762e-05, 'epoch': 0.22}
{'loss': 0.5653, 'learning_rate': 8.803677780844674e-05, 'epoch': 0.23}
{'loss': 1.0815, 'learning_rate': 8.80210832416603e-05, 'epoch': 0.23}
{'loss': 0.579, 'learning_rate': 8.800537978767682e-05, 'epoch': 0.23}
 22%|       | 1461/6500 [4:25:41<16:26:15, 11.74s/it]                                                         22%|       | 1461/6500 [4:25:41<16:26:15, 11.74s/it] 22%|       | 1462/6500 [4:25:51<15:51:15, 11.33s/it]                                                         22%|       | 1462/6500 [4:25:51<15:51:15, 11.33s/it] 23%|       | 1463/6500 [4:26:01<15:27:08, 11.04s/it]                                                         23%|       | 1463/6500 [4:26:01<15:27:08, 11.04s/it] 23%|       | 1464/6500 [4:26:12<15:09:28, 10.84s/it]                                                         23%|       | 1464/6500 [4:26:12<15:09:28, 10.84s/it] 23%|       | 1465/6500 [4:26:22<14:57:43, 10.70s/it]                                                         23%|       | 1465/6500 [4:26:22<14:57:43, 10.70s/it] 23%|       | 1466/6500 [4:26:32<14:49:27, 10.60s/it]                                        {'loss': 0.5546, 'learning_rate': 8.79896674501669e-05, 'epoch': 0.23}
{'loss': 0.5637, 'learning_rate': 8.797394623280319e-05, 'epoch': 0.23}
{'loss': 0.5283, 'learning_rate': 8.795821613926045e-05, 'epoch': 0.23}
{'loss': 0.589, 'learning_rate': 8.794247717321547e-05, 'epoch': 0.23}
{'loss': 0.5481, 'learning_rate': 8.792672933834713e-05, 'epoch': 0.23}
                 23%|       | 1466/6500 [4:26:32<14:49:27, 10.60s/it] 23%|       | 1467/6500 [4:26:43<14:43:41, 10.53s/it]                                                         23%|       | 1467/6500 [4:26:43<14:43:41, 10.53s/it] 23%|       | 1468/6500 [4:26:53<14:40:23, 10.50s/it]                                                         23%|       | 1468/6500 [4:26:53<14:40:23, 10.50s/it] 23%|       | 1469/6500 [4:27:04<14:37:09, 10.46s/it]                                                         23%|       | 1469/6500 [4:27:04<14:37:09, 10.46s/it] 23%|       | 1470/6500 [4:27:14<14:34:38, 10.43s/it]                                                         23%|       | 1470/6500 [4:27:14<14:34:38, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8092735409736633, 'eval_runtime': 3.9789, 'eval_samples_per_second': 5.781, 'eval_steps_per_second': 1.508, 'epoch': 0.23}
                                                         23%|       | 1470/6500 [4:27:18<14:34:38, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1470
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1470

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1470/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5396, 'learning_rate': 8.791097263833643e-05, 'epoch': 0.23}
{'loss': 0.5337, 'learning_rate': 8.789520707686635e-05, 'epoch': 0.23}
{'loss': 0.5511, 'learning_rate': 8.787943265762204e-05, 'epoch': 0.23}
{'loss': 0.5571, 'learning_rate': 8.786364938429065e-05, 'epoch': 0.23}
{'loss': 0.5472, 'learning_rate': 8.784785726056143e-05, 'epoch': 0.23}
 23%|       | 1471/6500 [4:27:31<17:08:03, 12.27s/it]                                                         23%|       | 1471/6500 [4:27:31<17:08:03, 12.27s/it] 23%|       | 1472/6500 [4:27:41<16:21:42, 11.71s/it]                                                         23%|       | 1472/6500 [4:27:41<16:21:42, 11.71s/it] 23%|       | 1473/6500 [4:27:51<15:49:18, 11.33s/it]                                                         23%|       | 1473/6500 [4:27:51<15:49:18, 11.33s/it] 23%|       | 1474/6500 [4:28:02<15:26:59, 11.07s/it]                                                         23%|       | 1474/6500 [4:28:02<15:26:59, 11.07s/it] 23%|       | 1475/6500 [4:28:12<15:10:56, 10.88s/it]                                                         23%|       | 1475/6500 [4:28:12<15:10:56, 10.88s/it] 23%|       | 1476/6500 [4:28:23<14:59:29, 10.74s/it]                                        {'loss': 0.5816, 'learning_rate': 8.78320562901257e-05, 'epoch': 0.23}
{'loss': 0.537, 'learning_rate': 8.781624647667684e-05, 'epoch': 0.23}
{'loss': 0.5928, 'learning_rate': 8.780042782391028e-05, 'epoch': 0.23}
{'loss': 0.5751, 'learning_rate': 8.778460033552356e-05, 'epoch': 0.23}
{'loss': 0.5413, 'learning_rate': 8.776876401521624e-05, 'epoch': 0.23}
                 23%|       | 1476/6500 [4:28:23<14:59:29, 10.74s/it] 23%|       | 1477/6500 [4:28:33<14:51:49, 10.65s/it]                                                         23%|       | 1477/6500 [4:28:33<14:51:49, 10.65s/it] 23%|       | 1478/6500 [4:28:44<14:46:22, 10.59s/it]                                                         23%|       | 1478/6500 [4:28:44<14:46:22, 10.59s/it] 23%|       | 1479/6500 [4:28:54<14:42:33, 10.55s/it]                                                         23%|       | 1479/6500 [4:28:54<14:42:33, 10.55s/it] 23%|       | 1480/6500 [4:29:04<14:39:53, 10.52s/it]                                                         23%|       | 1480/6500 [4:29:04<14:39:53, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8094837069511414, 'eval_runtime': 3.9784, 'eval_samples_per_second': 5.781, 'eval_steps_per_second': 1.508, 'epoch': 0.23}
                                                         23%|       | 1480/6500 [4:29:08<14:39:53, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1480the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1480

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1480/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5614, 'learning_rate': 8.775291886668995e-05, 'epoch': 0.23}
{'loss': 0.5699, 'learning_rate': 8.773706489364843e-05, 'epoch': 0.23}
{'loss': 0.5398, 'learning_rate': 8.772120209979745e-05, 'epoch': 0.23}
{'loss': 0.5618, 'learning_rate': 8.770533048884482e-05, 'epoch': 0.23}
{'loss': 0.5872, 'learning_rate': 8.768945006450043e-05, 'epoch': 0.23}
 23%|       | 1481/6500 [4:29:19<16:28:56, 11.82s/it]                                                         23%|       | 1481/6500 [4:29:19<16:28:56, 11.82s/it] 23%|       | 1482/6500 [4:29:30<15:53:40, 11.40s/it]                                                         23%|       | 1482/6500 [4:29:30<15:53:40, 11.40s/it] 23%|       | 1483/6500 [4:29:40<15:28:51, 11.11s/it]                                                         23%|       | 1483/6500 [4:29:40<15:28:51, 11.11s/it] 23%|       | 1484/6500 [4:29:51<15:11:05, 10.90s/it]                                                         23%|       | 1484/6500 [4:29:51<15:11:05, 10.90s/it] 23%|       | 1485/6500 [4:30:01<14:58:46, 10.75s/it]                                                         23%|       | 1485/6500 [4:30:01<14:58:46, 10.75s/it] 23%|       | 1486/6500 [4:30:11<14:48:48, 10.64s/it]                                        {'loss': 0.5438, 'learning_rate': 8.767356083047626e-05, 'epoch': 0.23}
{'loss': 0.5533, 'learning_rate': 8.765766279048629e-05, 'epoch': 0.23}
{'loss': 0.5336, 'learning_rate': 8.764175594824662e-05, 'epoch': 0.23}
{'loss': 0.5891, 'learning_rate': 8.762584030747534e-05, 'epoch': 0.23}
{'loss': 0.5991, 'learning_rate': 8.760991587189268e-05, 'epoch': 0.23}
                 23%|       | 1486/6500 [4:30:11<14:48:48, 10.64s/it] 23%|       | 1487/6500 [4:30:22<14:47:47, 10.63s/it]                                                         23%|       | 1487/6500 [4:30:22<14:47:47, 10.63s/it] 23%|       | 1488/6500 [4:30:32<14:41:57, 10.56s/it]                                                         23%|       | 1488/6500 [4:30:32<14:41:57, 10.56s/it] 23%|       | 1489/6500 [4:30:43<14:36:53, 10.50s/it]                                                         23%|       | 1489/6500 [4:30:43<14:36:53, 10.50s/it] 23%|       | 1490/6500 [4:30:53<14:35:02, 10.48s/it]                                                         23%|       | 1490/6500 [4:30:53<14:35:02, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8096781373023987, 'eval_runtime': 3.9814, 'eval_samples_per_second': 5.777, 'eval_steps_per_second': 1.507, 'epoch': 0.23}
                                                         23%|       | 1490/6500 [4:30:57<14:35:02, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1490/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5391, 'learning_rate': 8.759398264522085e-05, 'epoch': 0.23}
{'loss': 0.5625, 'learning_rate': 8.757804063118415e-05, 'epoch': 0.23}
{'loss': 0.5356, 'learning_rate': 8.756208983350893e-05, 'epoch': 0.23}
{'loss': 1.0695, 'learning_rate': 8.754613025592359e-05, 'epoch': 0.23}
{'loss': 0.5742, 'learning_rate': 8.753016190215859e-05, 'epoch': 0.23}
 23%|       | 1491/6500 [4:31:08<16:24:30, 11.79s/it]                                                         23%|       | 1491/6500 [4:31:08<16:24:30, 11.79s/it] 23%|       | 1492/6500 [4:31:18<15:48:31, 11.36s/it]                                                         23%|       | 1492/6500 [4:31:18<15:48:31, 11.36s/it] 23%|       | 1493/6500 [4:31:29<15:23:37, 11.07s/it]                                                         23%|       | 1493/6500 [4:31:29<15:23:37, 11.07s/it] 23%|       | 1494/6500 [4:31:39<15:05:45, 10.86s/it]                                                         23%|       | 1494/6500 [4:31:39<15:05:45, 10.86s/it] 23%|       | 1495/6500 [4:31:50<14:55:43, 10.74s/it]                                                         23%|       | 1495/6500 [4:31:50<14:55:43, 10.74s/it] 23%|       | 1496/6500 [4:32:00<14:46:37, 10.63s/it]                                        {'loss': 0.5626, 'learning_rate': 8.751418477594645e-05, 'epoch': 0.23}
{'loss': 0.5194, 'learning_rate': 8.749819888102166e-05, 'epoch': 0.23}
{'loss': 0.5404, 'learning_rate': 8.748220422112092e-05, 'epoch': 0.23}
{'loss': 0.5849, 'learning_rate': 8.746620079998282e-05, 'epoch': 0.23}
{'loss': 0.5207, 'learning_rate': 8.745018862134808e-05, 'epoch': 0.23}
                 23%|       | 1496/6500 [4:32:00<14:46:37, 10.63s/it] 23%|       | 1497/6500 [4:32:10<14:39:52, 10.55s/it]                                                         23%|       | 1497/6500 [4:32:10<14:39:52, 10.55s/it] 23%|       | 1498/6500 [4:32:21<14:35:36, 10.50s/it]                                                         23%|       | 1498/6500 [4:32:21<14:35:36, 10.50s/it] 23%|       | 1499/6500 [4:32:31<14:32:03, 10.46s/it]                                                         23%|       | 1499/6500 [4:32:31<14:32:03, 10.46s/it] 23%|       | 1500/6500 [4:32:41<14:29:48, 10.44s/it]                                                         23%|       | 1500/6500 [4:32:42<14:29:48, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.81051105260849, 'eval_runtime': 3.9745, 'eval_samples_per_second': 5.787, 'eval_steps_per_second': 1.51, 'epoch': 0.23}
                                                         23%|       | 1500/6500 [4:32:45<14:29:48, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1500I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1500

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5369, 'learning_rate': 8.743416768895947e-05, 'epoch': 0.23}
{'loss': 0.5249, 'learning_rate': 8.741813800656174e-05, 'epoch': 0.23}
{'loss': 0.5423, 'learning_rate': 8.740209957790178e-05, 'epoch': 0.23}
{'loss': 0.5411, 'learning_rate': 8.738605240672843e-05, 'epoch': 0.23}
{'loss': 0.5379, 'learning_rate': 8.736999649679264e-05, 'epoch': 0.23}
 23%|       | 1501/6500 [4:32:56<16:19:36, 11.76s/it]                                                         23%|       | 1501/6500 [4:32:56<16:19:36, 11.76s/it] 23%|       | 1502/6500 [4:33:07<15:44:49, 11.34s/it]                                                         23%|       | 1502/6500 [4:33:07<15:44:49, 11.34s/it] 23%|       | 1503/6500 [4:33:17<15:29:26, 11.16s/it]                                                         23%|       | 1503/6500 [4:33:17<15:29:26, 11.16s/it] 23%|       | 1504/6500 [4:33:28<15:09:31, 10.92s/it]                                                         23%|       | 1504/6500 [4:33:28<15:09:31, 10.92s/it] 23%|       | 1505/6500 [4:33:38<14:55:37, 10.76s/it]                                                         23%|       | 1505/6500 [4:33:38<14:55:37, 10.76s/it] 23%|       | 1506/6500 [4:33:49<14:47:11, 10.66s/it]                                        {'loss': 0.5702, 'learning_rate': 8.735393185184741e-05, 'epoch': 0.23}
{'loss': 0.5378, 'learning_rate': 8.73378584756477e-05, 'epoch': 0.23}
{'loss': 0.608, 'learning_rate': 8.73217763719506e-05, 'epoch': 0.23}
{'loss': 0.5625, 'learning_rate': 8.73056855445152e-05, 'epoch': 0.23}
{'loss': 0.5641, 'learning_rate': 8.728958599710262e-05, 'epoch': 0.23}
                 23%|       | 1506/6500 [4:33:49<14:47:11, 10.66s/it] 23%|       | 1507/6500 [4:33:59<14:40:04, 10.58s/it]                                                         23%|       | 1507/6500 [4:33:59<14:40:04, 10.58s/it] 23%|       | 1508/6500 [4:34:09<14:34:40, 10.51s/it]                                                         23%|       | 1508/6500 [4:34:09<14:34:40, 10.51s/it] 23%|       | 1509/6500 [4:34:20<14:31:01, 10.47s/it]                                                         23%|       | 1509/6500 [4:34:20<14:31:01, 10.47s/it] 23%|       | 1510/6500 [4:34:30<14:28:21, 10.44s/it]                                                         23%|       | 1510/6500 [4:34:30<14:28:21, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8074454665184021, 'eval_runtime': 4.2198, 'eval_samples_per_second': 5.451, 'eval_steps_per_second': 1.422, 'epoch': 0.23}
                                                         23%|       | 1510/6500 [4:34:34<14:28:21, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1510I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1510

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5715, 'learning_rate': 8.727347773347603e-05, 'epoch': 0.23}
{'loss': 0.5377, 'learning_rate': 8.725736075740066e-05, 'epoch': 0.23}
{'loss': 0.5502, 'learning_rate': 8.724123507264372e-05, 'epoch': 0.23}
{'loss': 0.5558, 'learning_rate': 8.722510068297454e-05, 'epoch': 0.23}
{'loss': 0.5644, 'learning_rate': 8.720895759216439e-05, 'epoch': 0.23}
 23%|       | 1511/6500 [4:34:45<16:23:37, 11.83s/it]                                                         23%|       | 1511/6500 [4:34:45<16:23:37, 11.83s/it] 23%|       | 1512/6500 [4:34:56<15:47:14, 11.39s/it]                                                         23%|       | 1512/6500 [4:34:56<15:47:14, 11.39s/it] 23%|       | 1513/6500 [4:35:06<15:21:23, 11.09s/it]                                                         23%|       | 1513/6500 [4:35:06<15:21:23, 11.09s/it] 23%|       | 1514/6500 [4:35:16<15:03:13, 10.87s/it]                                                         23%|       | 1514/6500 [4:35:16<15:03:13, 10.87s/it] 23%|       | 1515/6500 [4:35:28<15:33:59, 11.24s/it]                                                         23%|       | 1515/6500 [4:35:28<15:33:59, 11.24s/it] 23%|       | 1516/6500 [4:35:39<15:13:01, 10.99s/it]                                        {'loss': 0.5386, 'learning_rate': 8.719280580398663e-05, 'epoch': 0.23}
{'loss': 0.5488, 'learning_rate': 8.717664532221668e-05, 'epoch': 0.23}
{'loss': 0.5444, 'learning_rate': 8.71604761506319e-05, 'epoch': 0.23}
{'loss': 0.6356, 'learning_rate': 8.714429829301176e-05, 'epoch': 0.23}
{'loss': 0.5407, 'learning_rate': 8.712811175313773e-05, 'epoch': 0.23}
                 23%|       | 1516/6500 [4:35:39<15:13:01, 10.99s/it] 23%|       | 1517/6500 [4:35:49<14:57:40, 10.81s/it]                                                         23%|       | 1517/6500 [4:35:49<14:57:40, 10.81s/it] 23%|       | 1518/6500 [4:36:00<14:46:23, 10.68s/it]                                                         23%|       | 1518/6500 [4:36:00<14:46:23, 10.68s/it] 23%|       | 1519/6500 [4:36:10<14:43:28, 10.64s/it]                                                         23%|       | 1519/6500 [4:36:10<14:43:28, 10.64s/it] 23%|       | 1520/6500 [4:36:20<14:35:53, 10.55s/it]                                                         23%|       | 1520/6500 [4:36:20<14:35:53, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8060897588729858, 'eval_runtime': 4.1052, 'eval_samples_per_second': 5.603, 'eval_steps_per_second': 1.462, 'epoch': 0.23}
                                                         23%|       | 1520/6500 [4:36:25<14:35:53, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1520
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1520
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1520/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1520/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.521, 'learning_rate': 8.711191653479333e-05, 'epoch': 0.23}
{'loss': 0.5528, 'learning_rate': 8.709571264176409e-05, 'epoch': 0.23}
{'loss': 1.0674, 'learning_rate': 8.707950007783755e-05, 'epoch': 0.23}
{'loss': 0.5545, 'learning_rate': 8.706327884680332e-05, 'epoch': 0.23}
{'loss': 0.5649, 'learning_rate': 8.704704895245301e-05, 'epoch': 0.23}
 23%|       | 1521/6500 [4:36:35<16:27:21, 11.90s/it]                                                         23%|       | 1521/6500 [4:36:35<16:27:21, 11.90s/it] 23%|       | 1522/6500 [4:36:46<15:49:06, 11.44s/it]                                                         23%|       | 1522/6500 [4:36:46<15:49:06, 11.44s/it] 23%|       | 1523/6500 [4:36:56<15:22:43, 11.12s/it]                                                         23%|       | 1523/6500 [4:36:56<15:22:43, 11.12s/it] 23%|       | 1524/6500 [4:37:07<15:03:41, 10.90s/it]                                                         23%|       | 1524/6500 [4:37:07<15:03:41, 10.90s/it] 23%|       | 1525/6500 [4:37:17<14:50:23, 10.74s/it]                                                         23%|       | 1525/6500 [4:37:17<14:50:23, 10.74s/it] 23%|       | 1526/6500 [4:37:27<14:40:58, 10.63s/it]                                        {'loss': 0.5458, 'learning_rate': 8.703081039858026e-05, 'epoch': 0.23}
{'loss': 0.5208, 'learning_rate': 8.701456318898073e-05, 'epoch': 0.23}
{'loss': 0.5741, 'learning_rate': 8.69983073274521e-05, 'epoch': 0.24}
{'loss': 0.5596, 'learning_rate': 8.69820428177941e-05, 'epoch': 0.24}
{'loss': 0.5148, 'learning_rate': 8.696576966380843e-05, 'epoch': 0.24}
                 23%|       | 1526/6500 [4:37:27<14:40:58, 10.63s/it] 23%|       | 1527/6500 [4:37:38<14:34:11, 10.55s/it]                                                         23%|       | 1527/6500 [4:37:38<14:34:11, 10.55s/it] 24%|       | 1528/6500 [4:37:48<14:30:40, 10.51s/it]                                                         24%|       | 1528/6500 [4:37:48<14:30:40, 10.51s/it] 24%|       | 1529/6500 [4:37:58<14:26:52, 10.46s/it]                                                         24%|       | 1529/6500 [4:37:58<14:26:52, 10.46s/it] 24%|       | 1530/6500 [4:38:09<14:24:23, 10.44s/it]                                                         24%|       | 1530/6500 [4:38:09<14:24:23, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8121458888053894, 'eval_runtime': 3.9604, 'eval_samples_per_second': 5.807, 'eval_steps_per_second': 1.515, 'epoch': 0.24}
                                                         24%|       | 1530/6500 [4:38:13<14:24:23, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1530/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1530/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5282, 'learning_rate': 8.694948786929886e-05, 'epoch': 0.24}
{'loss': 0.5196, 'learning_rate': 8.693319743807116e-05, 'epoch': 0.24}
{'loss': 0.5463, 'learning_rate': 8.691689837393311e-05, 'epoch': 0.24}
{'loss': 0.5266, 'learning_rate': 8.690059068069454e-05, 'epoch': 0.24}
{'loss': 0.5541, 'learning_rate': 8.688427436216724e-05, 'epoch': 0.24}
 24%|       | 1531/6500 [4:38:24<16:14:00, 11.76s/it]                                                         24%|       | 1531/6500 [4:38:24<16:14:00, 11.76s/it] 24%|       | 1532/6500 [4:38:34<15:43:28, 11.39s/it]                                                         24%|       | 1532/6500 [4:38:34<15:43:28, 11.39s/it] 24%|       | 1533/6500 [4:38:45<15:18:13, 11.09s/it]                                                         24%|       | 1533/6500 [4:38:45<15:18:13, 11.09s/it] 24%|       | 1534/6500 [4:38:55<15:02:08, 10.90s/it]                                                         24%|       | 1534/6500 [4:38:55<15:02:08, 10.90s/it] 24%|       | 1535/6500 [4:39:05<14:49:24, 10.75s/it]                                                         24%|       | 1535/6500 [4:39:05<14:49:24, 10.75s/it] 24%|       | 1536/6500 [4:39:16<14:51:47, 10.78s/it]                                        {'loss': 0.5354, 'learning_rate': 8.686794942216508e-05, 'epoch': 0.24}
{'loss': 0.5564, 'learning_rate': 8.685161586450388e-05, 'epoch': 0.24}
{'loss': 0.5701, 'learning_rate': 8.683527369300156e-05, 'epoch': 0.24}
{'loss': 0.5485, 'learning_rate': 8.681892291147798e-05, 'epoch': 0.24}
{'loss': 0.5529, 'learning_rate': 8.6802563523755e-05, 'epoch': 0.24}
                 24%|       | 1536/6500 [4:39:16<14:51:47, 10.78s/it] 24%|       | 1537/6500 [4:39:27<14:42:08, 10.66s/it]                                                         24%|       | 1537/6500 [4:39:27<14:42:08, 10.66s/it] 24%|       | 1538/6500 [4:39:37<14:35:11, 10.58s/it]                                                         24%|       | 1538/6500 [4:39:37<14:35:11, 10.58s/it] 24%|       | 1539/6500 [4:39:48<14:31:06, 10.54s/it]                                                         24%|       | 1539/6500 [4:39:48<14:31:06, 10.54s/it] 24%|       | 1540/6500 [4:39:58<14:30:05, 10.53s/it]                                                         24%|       | 1540/6500 [4:39:58<14:30:05, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8087812662124634, 'eval_runtime': 5.241, 'eval_samples_per_second': 4.388, 'eval_steps_per_second': 1.145, 'epoch': 0.24}
                                                         24%|       | 1540/6500 [4:40:03<14:30:05, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5504, 'learning_rate': 8.678619553365659e-05, 'epoch': 0.24}
{'loss': 0.5288, 'learning_rate': 8.676981894500862e-05, 'epoch': 0.24}
{'loss': 0.5277, 'learning_rate': 8.675343376163905e-05, 'epoch': 0.24}
{'loss': 0.566, 'learning_rate': 8.673703998737778e-05, 'epoch': 0.24}
{'loss': 0.5481, 'learning_rate': 8.67206376260568e-05, 'epoch': 0.24}
 24%|       | 1541/6500 [4:40:14<16:49:05, 12.21s/it]                                                         24%|       | 1541/6500 [4:40:14<16:49:05, 12.21s/it] 24%|       | 1542/6500 [4:40:25<16:04:13, 11.67s/it]                                                         24%|       | 1542/6500 [4:40:25<16:04:13, 11.67s/it] 24%|       | 1543/6500 [4:40:35<15:31:56, 11.28s/it]                                                         24%|       | 1543/6500 [4:40:35<15:31:56, 11.28s/it] 24%|       | 1544/6500 [4:40:45<15:09:44, 11.01s/it]                                                         24%|       | 1544/6500 [4:40:45<15:09:44, 11.01s/it] 24%|       | 1545/6500 [4:40:56<14:55:17, 10.84s/it]                                                         24%|       | 1545/6500 [4:40:56<14:55:17, 10.84s/it] 24%|       | 1546/6500 [4:41:06<14:43:53, 10.71s/it]                                        {'loss': 0.5458, 'learning_rate': 8.670422668151003e-05, 'epoch': 0.24}
{'loss': 0.5266, 'learning_rate': 8.668780715757345e-05, 'epoch': 0.24}
{'loss': 0.5577, 'learning_rate': 8.6671379058085e-05, 'epoch': 0.24}
{'loss': 0.5935, 'learning_rate': 8.665494238688467e-05, 'epoch': 0.24}
{'loss': 0.544, 'learning_rate': 8.663849714781442e-05, 'epoch': 0.24}
                 24%|       | 1546/6500 [4:41:06<14:43:53, 10.71s/it] 24%|       | 1547/6500 [4:41:17<14:35:31, 10.61s/it]                                                         24%|       | 1547/6500 [4:41:17<14:35:31, 10.61s/it] 24%|       | 1548/6500 [4:41:27<14:30:24, 10.55s/it]                                                         24%|       | 1548/6500 [4:41:27<14:30:24, 10.55s/it] 24%|       | 1549/6500 [4:41:37<14:26:36, 10.50s/it]                                                         24%|       | 1549/6500 [4:41:37<14:26:36, 10.50s/it] 24%|       | 1550/6500 [4:41:48<14:24:00, 10.47s/it]                                                         24%|       | 1550/6500 [4:41:48<14:24:00, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8074169754981995, 'eval_runtime': 3.9899, 'eval_samples_per_second': 5.765, 'eval_steps_per_second': 1.504, 'epoch': 0.24}
                                                         24%|       | 1550/6500 [4:41:52<14:24:00, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5172, 'learning_rate': 8.662204334471822e-05, 'epoch': 0.24}
{'loss': 0.5473, 'learning_rate': 8.660558098144206e-05, 'epoch': 0.24}
{'loss': 1.0537, 'learning_rate': 8.658911006183391e-05, 'epoch': 0.24}
{'loss': 0.5752, 'learning_rate': 8.657263058974375e-05, 'epoch': 0.24}
{'loss': 0.5426, 'learning_rate': 8.655614256902355e-05, 'epoch': 0.24}
 24%|       | 1551/6500 [4:42:03<16:14:24, 11.81s/it]                                                         24%|       | 1551/6500 [4:42:03<16:14:24, 11.81s/it] 24%|       | 1552/6500 [4:42:13<15:46:02, 11.47s/it]                                                         24%|       | 1552/6500 [4:42:13<15:46:02, 11.47s/it] 24%|       | 1553/6500 [4:42:24<15:19:11, 11.15s/it]                                                         24%|       | 1553/6500 [4:42:24<15:19:11, 11.15s/it] 24%|       | 1554/6500 [4:42:34<15:00:48, 10.93s/it]                                                         24%|       | 1554/6500 [4:42:34<15:00:48, 10.93s/it] 24%|       | 1555/6500 [4:42:45<14:47:49, 10.77s/it]                                                         24%|       | 1555/6500 [4:42:45<14:47:49, 10.77s/it] 24%|       | 1556/6500 [4:42:55<14:38:15, 10.66s/it]                                        {'loss': 0.5202, 'learning_rate': 8.65396460035273e-05, 'epoch': 0.24}
{'loss': 0.5289, 'learning_rate': 8.652314089711095e-05, 'epoch': 0.24}
{'loss': 0.5732, 'learning_rate': 8.650662725363249e-05, 'epoch': 0.24}
{'loss': 0.5304, 'learning_rate': 8.649010507695187e-05, 'epoch': 0.24}
{'loss': 0.534, 'learning_rate': 8.647357437093105e-05, 'epoch': 0.24}
                 24%|       | 1556/6500 [4:42:55<14:38:15, 10.66s/it] 24%|       | 1557/6500 [4:43:05<14:31:45, 10.58s/it]                                                         24%|       | 1557/6500 [4:43:05<14:31:45, 10.58s/it] 24%|       | 1558/6500 [4:43:16<14:27:09, 10.53s/it]                                                         24%|       | 1558/6500 [4:43:16<14:27:09, 10.53s/it] 24%|       | 1559/6500 [4:43:26<14:24:04, 10.49s/it]                                                         24%|       | 1559/6500 [4:43:26<14:24:04, 10.49s/it] 24%|       | 1560/6500 [4:43:37<14:21:39, 10.47s/it]                                                         24%|       | 1560/6500 [4:43:37<14:21:39, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8099765181541443, 'eval_runtime': 3.9853, 'eval_samples_per_second': 5.771, 'eval_steps_per_second': 1.506, 'epoch': 0.24}
                                                         24%|       | 1560/6500 [4:43:41<14:21:39, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5128, 'learning_rate': 8.645703513943397e-05, 'epoch': 0.24}
{'loss': 0.5322, 'learning_rate': 8.64404873863266e-05, 'epoch': 0.24}
{'loss': 0.5192, 'learning_rate': 8.642393111547687e-05, 'epoch': 0.24}
{'loss': 0.526, 'learning_rate': 8.64073663307547e-05, 'epoch': 0.24}
{'loss': 0.5681, 'learning_rate': 8.639079303603202e-05, 'epoch': 0.24}
 24%|       | 1561/6500 [4:43:52<16:11:46, 11.81s/it]                                                         24%|       | 1561/6500 [4:43:52<16:11:46, 11.81s/it] 24%|       | 1562/6500 [4:44:02<15:36:51, 11.38s/it]                                                         24%|       | 1562/6500 [4:44:02<15:36:51, 11.38s/it] 24%|       | 1563/6500 [4:44:12<15:12:21, 11.09s/it]                                                         24%|       | 1563/6500 [4:44:12<15:12:21, 11.09s/it] 24%|       | 1564/6500 [4:44:23<14:55:11, 10.88s/it]                                                         24%|       | 1564/6500 [4:44:23<14:55:11, 10.88s/it] 24%|       | 1565/6500 [4:44:33<14:42:43, 10.73s/it]                                                         24%|       | 1565/6500 [4:44:33<14:42:43, 10.73s/it] 24%|       | 1566/6500 [4:44:44<14:34:06, 10.63s/it]                                        {'loss': 0.5223, 'learning_rate': 8.637421123518272e-05, 'epoch': 0.24}
{'loss': 0.5721, 'learning_rate': 8.635762093208269e-05, 'epoch': 0.24}
{'loss': 0.5612, 'learning_rate': 8.634102213060984e-05, 'epoch': 0.24}
{'loss': 0.537, 'learning_rate': 8.632441483464402e-05, 'epoch': 0.24}
{'loss': 0.5537, 'learning_rate': 8.630779904806709e-05, 'epoch': 0.24}
                 24%|       | 1566/6500 [4:44:44<14:34:06, 10.63s/it] 24%|       | 1567/6500 [4:44:54<14:28:27, 10.56s/it]                                                         24%|       | 1567/6500 [4:44:54<14:28:27, 10.56s/it] 24%|       | 1568/6500 [4:45:05<14:32:57, 10.62s/it]                                                         24%|       | 1568/6500 [4:45:05<14:32:57, 10.62s/it] 24%|       | 1569/6500 [4:45:15<14:27:07, 10.55s/it]                                                         24%|       | 1569/6500 [4:45:15<14:27:07, 10.55s/it] 24%|       | 1570/6500 [4:45:25<14:23:05, 10.50s/it]                                                         24%|       | 1570/6500 [4:45:25<14:23:05, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8074234127998352, 'eval_runtime': 3.9845, 'eval_samples_per_second': 5.772, 'eval_steps_per_second': 1.506, 'epoch': 0.24}
                                                         24%|       | 1570/6500 [4:45:29<14:23:05, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5466, 'learning_rate': 8.629117477476289e-05, 'epoch': 0.24}
{'loss': 0.5309, 'learning_rate': 8.627454201861724e-05, 'epoch': 0.24}
{'loss': 0.5403, 'learning_rate': 8.625790078351794e-05, 'epoch': 0.24}
{'loss': 0.5657, 'learning_rate': 8.624125107335478e-05, 'epoch': 0.24}
{'loss': 0.5281, 'learning_rate': 8.622459289201954e-05, 'epoch': 0.24}
 24%|       | 1571/6500 [4:45:40<16:09:17, 11.80s/it]                                                         24%|       | 1571/6500 [4:45:40<16:09:17, 11.80s/it] 24%|       | 1572/6500 [4:45:51<15:34:02, 11.37s/it]                                                         24%|       | 1572/6500 [4:45:51<15:34:02, 11.37s/it] 24%|       | 1573/6500 [4:46:01<15:09:54, 11.08s/it]                                                         24%|       | 1573/6500 [4:46:01<15:09:54, 11.08s/it] 24%|       | 1574/6500 [4:46:11<14:52:52, 10.88s/it]                                                         24%|       | 1574/6500 [4:46:11<14:52:52, 10.88s/it] 24%|       | 1575/6500 [4:46:22<14:40:40, 10.73s/it]                                                         24%|       | 1575/6500 [4:46:22<14:40:40, 10.73s/it] 24%|       | 1576/6500 [4:46:32<14:32:16, 10.63s/it]                                        {'loss': 0.538, 'learning_rate': 8.620792624340596e-05, 'epoch': 0.24}
{'loss': 0.5173, 'learning_rate': 8.619125113140975e-05, 'epoch': 0.24}
{'loss': 0.5791, 'learning_rate': 8.617456755992867e-05, 'epoch': 0.24}
{'loss': 0.577, 'learning_rate': 8.615787553286234e-05, 'epoch': 0.24}
{'loss': 0.5432, 'learning_rate': 8.614117505411246e-05, 'epoch': 0.24}
                 24%|       | 1576/6500 [4:46:32<14:32:16, 10.63s/it] 24%|       | 1577/6500 [4:46:43<14:26:09, 10.56s/it]                                                         24%|       | 1577/6500 [4:46:43<14:26:09, 10.56s/it] 24%|       | 1578/6500 [4:46:53<14:22:07, 10.51s/it]                                                         24%|       | 1578/6500 [4:46:53<14:22:07, 10.51s/it] 24%|       | 1579/6500 [4:47:03<14:19:14, 10.48s/it]                                                         24%|       | 1579/6500 [4:47:03<14:19:14, 10.48s/it] 24%|       | 1580/6500 [4:47:14<14:17:07, 10.45s/it]                                                         24%|       | 1580/6500 [4:47:14<14:17:07, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8053522706031799, 'eval_runtime': 4.2309, 'eval_samples_per_second': 5.436, 'eval_steps_per_second': 1.418, 'epoch': 0.24}
                                                         24%|       | 1580/6500 [4:47:18<14:17:07, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1580I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1580

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1580/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5369, 'learning_rate': 8.612446612758265e-05, 'epoch': 0.24}
{'loss': 0.5316, 'learning_rate': 8.610774875717851e-05, 'epoch': 0.24}
{'loss': 1.0692, 'learning_rate': 8.609102294680766e-05, 'epoch': 0.24}
{'loss': 0.5455, 'learning_rate': 8.607428870037962e-05, 'epoch': 0.24}
{'loss': 0.5474, 'learning_rate': 8.605754602180594e-05, 'epoch': 0.24}
 24%|       | 1581/6500 [4:47:29<16:11:05, 11.85s/it]                                                         24%|       | 1581/6500 [4:47:29<16:11:05, 11.85s/it] 24%|       | 1582/6500 [4:47:39<15:35:00, 11.41s/it]                                                         24%|       | 1582/6500 [4:47:39<15:35:00, 11.41s/it] 24%|       | 1583/6500 [4:47:50<15:09:46, 11.10s/it]                                                         24%|       | 1583/6500 [4:47:50<15:09:46, 11.10s/it] 24%|       | 1584/6500 [4:48:00<15:00:49, 10.99s/it]                                                         24%|       | 1584/6500 [4:48:00<15:00:49, 10.99s/it] 24%|       | 1585/6500 [4:48:11<14:45:21, 10.81s/it]                                                         24%|       | 1585/6500 [4:48:11<14:45:21, 10.81s/it] 24%|       | 1586/6500 [4:48:21<14:35:08, 10.69s/it]                                        {'loss': 0.5051, 'learning_rate': 8.60407949150001e-05, 'epoch': 0.24}
{'loss': 0.5522, 'learning_rate': 8.602403538387758e-05, 'epoch': 0.24}
{'loss': 0.5434, 'learning_rate': 8.600726743235583e-05, 'epoch': 0.24}
{'loss': 0.5045, 'learning_rate': 8.599049106435424e-05, 'epoch': 0.24}
{'loss': 0.5221, 'learning_rate': 8.59737062837942e-05, 'epoch': 0.24}
                 24%|       | 1586/6500 [4:48:21<14:35:08, 10.69s/it] 24%|       | 1587/6500 [4:48:32<14:28:00, 10.60s/it]                                                         24%|       | 1587/6500 [4:48:32<14:28:00, 10.60s/it] 24%|       | 1588/6500 [4:48:42<14:22:58, 10.54s/it]                                                         24%|       | 1588/6500 [4:48:42<14:22:58, 10.54s/it] 24%|       | 1589/6500 [4:48:52<14:19:11, 10.50s/it]                                                         24%|       | 1589/6500 [4:48:52<14:19:11, 10.50s/it] 24%|       | 1590/6500 [4:49:03<14:16:35, 10.47s/it]                                                         24%|       | 1590/6500 [4:49:03<14:16:35, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8076563477516174, 'eval_runtime': 3.9803, 'eval_samples_per_second': 5.778, 'eval_steps_per_second': 1.507, 'epoch': 0.24}
                                                         24%|       | 1590/6500 [4:49:07<14:16:35, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5083, 'learning_rate': 8.595691309459901e-05, 'epoch': 0.24}
{'loss': 0.5319, 'learning_rate': 8.594011150069403e-05, 'epoch': 0.24}
{'loss': 0.5269, 'learning_rate': 8.59233015060065e-05, 'epoch': 0.25}
{'loss': 0.5301, 'learning_rate': 8.590648311446567e-05, 'epoch': 0.25}
{'loss': 0.5383, 'learning_rate': 8.58896563300027e-05, 'epoch': 0.25}
 24%|       | 1591/6500 [4:49:18<16:03:55, 11.78s/it]                                                         24%|       | 1591/6500 [4:49:18<16:03:55, 11.78s/it] 24%|       | 1592/6500 [4:49:28<15:29:16, 11.36s/it]                                                         24%|       | 1592/6500 [4:49:28<15:29:16, 11.36s/it] 25%|       | 1593/6500 [4:49:38<15:04:36, 11.06s/it]                                                         25%|       | 1593/6500 [4:49:38<15:04:36, 11.06s/it] 25%|       | 1594/6500 [4:49:49<14:47:42, 10.86s/it]                                                         25%|       | 1594/6500 [4:49:49<14:47:42, 10.86s/it] 25%|       | 1595/6500 [4:49:59<14:35:34, 10.71s/it]                                                         25%|       | 1595/6500 [4:49:59<14:35:34, 10.71s/it] 25%|       | 1596/6500 [4:50:10<14:27:36, 10.62s/it]                                        {'loss': 0.5187, 'learning_rate': 8.58728211565508e-05, 'epoch': 0.25}
{'loss': 0.5765, 'learning_rate': 8.585597759804506e-05, 'epoch': 0.25}
{'loss': 0.5332, 'learning_rate': 8.583912565842257e-05, 'epoch': 0.25}
{'loss': 0.5476, 'learning_rate': 8.582226534162235e-05, 'epoch': 0.25}
{'loss': 0.5477, 'learning_rate': 8.580539665158542e-05, 'epoch': 0.25}
                 25%|       | 1596/6500 [4:50:10<14:27:36, 10.62s/it] 25%|       | 1597/6500 [4:50:20<14:21:35, 10.54s/it]                                                         25%|       | 1597/6500 [4:50:20<14:21:35, 10.54s/it] 25%|       | 1598/6500 [4:50:30<14:17:39, 10.50s/it]                                                         25%|       | 1598/6500 [4:50:30<14:17:39, 10.50s/it] 25%|       | 1599/6500 [4:50:41<14:14:43, 10.46s/it]                                                         25%|       | 1599/6500 [4:50:41<14:14:43, 10.46s/it] 25%|       | 1600/6500 [4:50:51<14:19:25, 10.52s/it]                                                         25%|       | 1600/6500 [4:50:51<14:19:25, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8074595332145691, 'eval_runtime': 3.988, 'eval_samples_per_second': 5.767, 'eval_steps_per_second': 1.505, 'epoch': 0.25}
                                                         25%|       | 1600/6500 [4:50:55<14:19:25, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1600
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5107, 'learning_rate': 8.578851959225472e-05, 'epoch': 0.25}
{'loss': 0.5246, 'learning_rate': 8.577163416757517e-05, 'epoch': 0.25}
{'loss': 0.545, 'learning_rate': 8.57547403814936e-05, 'epoch': 0.25}
{'loss': 0.5445, 'learning_rate': 8.573783823795889e-05, 'epoch': 0.25}
{'loss': 0.5275, 'learning_rate': 8.572092774092176e-05, 'epoch': 0.25}
 25%|       | 1601/6500 [4:51:06<16:06:03, 11.83s/it]                                                         25%|       | 1601/6500 [4:51:06<16:06:03, 11.83s/it] 25%|       | 1602/6500 [4:51:17<15:31:29, 11.41s/it]                                                         25%|       | 1602/6500 [4:51:17<15:31:29, 11.41s/it] 25%|       | 1603/6500 [4:51:27<15:07:28, 11.12s/it]                                                         25%|       | 1603/6500 [4:51:27<15:07:28, 11.12s/it] 25%|       | 1604/6500 [4:51:38<14:50:33, 10.91s/it]                                                         25%|       | 1604/6500 [4:51:38<14:50:33, 10.91s/it] 25%|       | 1605/6500 [4:51:48<14:38:26, 10.77s/it]                                                         25%|       | 1605/6500 [4:51:48<14:38:26, 10.77s/it] 25%|       | 1606/6500 [4:51:58<14:30:22, 10.67s/it]                                        {'loss': 0.5314, 'learning_rate': 8.570400889433497e-05, 'epoch': 0.25}
{'loss': 0.5196, 'learning_rate': 8.568708170215319e-05, 'epoch': 0.25}
{'loss': 0.6256, 'learning_rate': 8.567014616833302e-05, 'epoch': 0.25}
{'loss': 0.5131, 'learning_rate': 8.56532022968331e-05, 'epoch': 0.25}
{'loss': 0.5207, 'learning_rate': 8.563625009161387e-05, 'epoch': 0.25}
                 25%|       | 1606/6500 [4:51:58<14:30:22, 10.67s/it] 25%|       | 1607/6500 [4:52:09<14:24:32, 10.60s/it]                                                         25%|       | 1607/6500 [4:52:09<14:24:32, 10.60s/it] 25%|       | 1608/6500 [4:52:19<14:20:41, 10.56s/it]                                                         25%|       | 1608/6500 [4:52:19<14:20:41, 10.56s/it] 25%|       | 1609/6500 [4:52:30<14:17:37, 10.52s/it]                                                         25%|       | 1609/6500 [4:52:30<14:17:37, 10.52s/it] 25%|       | 1610/6500 [4:52:40<14:15:22, 10.50s/it]                                                         25%|       | 1610/6500 [4:52:40<14:15:22, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8095836639404297, 'eval_runtime': 4.213, 'eval_samples_per_second': 5.459, 'eval_steps_per_second': 1.424, 'epoch': 0.25}
                                                         25%|       | 1610/6500 [4:52:44<14:15:22, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5291, 'learning_rate': 8.56192895566379e-05, 'epoch': 0.25}
{'loss': 1.0545, 'learning_rate': 8.560232069586953e-05, 'epoch': 0.25}
{'loss': 0.5409, 'learning_rate': 8.558534351327517e-05, 'epoch': 0.25}
{'loss': 0.5424, 'learning_rate': 8.556835801282314e-05, 'epoch': 0.25}
{'loss': 0.5289, 'learning_rate': 8.555136419848368e-05, 'epoch': 0.25}
 25%|       | 1611/6500 [4:52:55<16:08:11, 11.88s/it]                                                         25%|       | 1611/6500 [4:52:55<16:08:11, 11.88s/it] 25%|       | 1612/6500 [4:53:06<15:32:22, 11.44s/it]                                                         25%|       | 1612/6500 [4:53:06<15:32:22, 11.44s/it] 25%|       | 1613/6500 [4:53:16<15:07:52, 11.15s/it]                                                         25%|       | 1613/6500 [4:53:16<15:07:52, 11.15s/it] 25%|       | 1614/6500 [4:53:27<14:50:57, 10.94s/it]                                                         25%|       | 1614/6500 [4:53:27<14:50:57, 10.94s/it] 25%|       | 1615/6500 [4:53:37<14:38:38, 10.79s/it]                                                         25%|       | 1615/6500 [4:53:37<14:38:38, 10.79s/it] 25%|       | 1616/6500 [4:53:48<14:36:54, 10.77s/it]                                        {'loss': 0.5127, 'learning_rate': 8.553436207422898e-05, 'epoch': 0.25}
{'loss': 0.557, 'learning_rate': 8.55173516440332e-05, 'epoch': 0.25}
{'loss': 0.5211, 'learning_rate': 8.550033291187243e-05, 'epoch': 0.25}
{'loss': 0.5114, 'learning_rate': 8.548330588172469e-05, 'epoch': 0.25}
{'loss': 0.5038, 'learning_rate': 8.546627055756994e-05, 'epoch': 0.25}
                 25%|       | 1616/6500 [4:53:48<14:36:54, 10.77s/it] 25%|       | 1617/6500 [4:53:58<14:28:43, 10.67s/it]                                                         25%|       | 1617/6500 [4:53:58<14:28:43, 10.67s/it] 25%|       | 1618/6500 [4:54:09<14:22:43, 10.60s/it]                                                         25%|       | 1618/6500 [4:54:09<14:22:43, 10.60s/it] 25%|       | 1619/6500 [4:54:19<14:18:37, 10.55s/it]                                                         25%|       | 1619/6500 [4:54:19<14:18:37, 10.55s/it] 25%|       | 1620/6500 [4:54:30<14:15:11, 10.51s/it]                                                         25%|       | 1620/6500 [4:54:30<14:15:11, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8101946711540222, 'eval_runtime': 3.9722, 'eval_samples_per_second': 5.79, 'eval_steps_per_second': 1.51, 'epoch': 0.25}
                                                         25%|       | 1620/6500 [4:54:34<14:15:11, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5228, 'learning_rate': 8.544922694339008e-05, 'epoch': 0.25}
{'loss': 0.5189, 'learning_rate': 8.543217504316896e-05, 'epoch': 0.25}
{'loss': 0.5073, 'learning_rate': 8.541511486089237e-05, 'epoch': 0.25}
{'loss': 0.545, 'learning_rate': 8.539804640054798e-05, 'epoch': 0.25}
{'loss': 0.5181, 'learning_rate': 8.538096966612548e-05, 'epoch': 0.25}
 25%|       | 1621/6500 [4:54:44<16:01:57, 11.83s/it]                                                         25%|       | 1621/6500 [4:54:44<16:01:57, 11.83s/it] 25%|       | 1622/6500 [4:54:55<15:28:01, 11.41s/it]                                                         25%|       | 1622/6500 [4:54:55<15:28:01, 11.41s/it] 25%|       | 1623/6500 [4:55:05<15:04:03, 11.12s/it]                                                         25%|       | 1623/6500 [4:55:05<15:04:03, 11.12s/it] 25%|       | 1624/6500 [4:55:16<14:47:30, 10.92s/it]                                                         25%|       | 1624/6500 [4:55:16<14:47:30, 10.92s/it] 25%|       | 1625/6500 [4:55:26<14:35:48, 10.78s/it]                                                         25%|       | 1625/6500 [4:55:26<14:35:48, 10.78s/it] 25%|       | 1626/6500 [4:55:37<14:27:29, 10.68s/it]                                        {'loss': 0.5458, 'learning_rate': 8.536388466161645e-05, 'epoch': 0.25}
{'loss': 0.5524, 'learning_rate': 8.534679139101439e-05, 'epoch': 0.25}
{'loss': 0.5214, 'learning_rate': 8.532968985831476e-05, 'epoch': 0.25}
{'loss': 0.5392, 'learning_rate': 8.531258006751492e-05, 'epoch': 0.25}
{'loss': 0.5433, 'learning_rate': 8.529546202261421e-05, 'epoch': 0.25}
                 25%|       | 1626/6500 [4:55:37<14:27:29, 10.68s/it] 25%|       | 1627/6500 [4:55:47<14:21:37, 10.61s/it]                                                         25%|       | 1627/6500 [4:55:47<14:21:37, 10.61s/it] 25%|       | 1628/6500 [4:55:58<14:17:32, 10.56s/it]                                                         25%|       | 1628/6500 [4:55:58<14:17:32, 10.56s/it] 25%|       | 1629/6500 [4:56:08<14:14:50, 10.53s/it]                                                         25%|       | 1629/6500 [4:56:08<14:14:50, 10.53s/it] 25%|       | 1630/6500 [4:56:18<14:12:30, 10.50s/it]                                                         25%|       | 1630/6500 [4:56:18<14:12:30, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8053090572357178, 'eval_runtime': 3.9839, 'eval_samples_per_second': 5.773, 'eval_steps_per_second': 1.506, 'epoch': 0.25}
                                                         25%|       | 1630/6500 [4:56:22<14:12:30, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5307, 'learning_rate': 8.527833572761383e-05, 'epoch': 0.25}
{'loss': 0.5022, 'learning_rate': 8.526120118651699e-05, 'epoch': 0.25}
{'loss': 0.5619, 'learning_rate': 8.524405840332875e-05, 'epoch': 0.25}
{'loss': 0.5263, 'learning_rate': 8.522690738205617e-05, 'epoch': 0.25}
{'loss': 0.521, 'learning_rate': 8.520974812670814e-05, 'epoch': 0.25}
 25%|       | 1631/6500 [4:56:33<16:02:03, 11.86s/it]                                                         25%|       | 1631/6500 [4:56:33<16:02:03, 11.86s/it] 25%|       | 1632/6500 [4:56:44<15:27:25, 11.43s/it]                                                         25%|       | 1632/6500 [4:56:44<15:27:25, 11.43s/it] 25%|       | 1633/6500 [4:56:55<15:09:40, 11.21s/it]                                                         25%|       | 1633/6500 [4:56:55<15:09:40, 11.21s/it] 25%|       | 1634/6500 [4:57:05<14:50:58, 10.99s/it]                                                         25%|       | 1634/6500 [4:57:05<14:50:58, 10.99s/it] 25%|       | 1635/6500 [4:57:16<14:37:55, 10.83s/it]                                                         25%|       | 1635/6500 [4:57:16<14:37:55, 10.83s/it] 25%|       | 1636/6500 [4:57:26<14:28:34, 10.71s/it]                                        {'loss': 0.5009, 'learning_rate': 8.519258064129558e-05, 'epoch': 0.25}
{'loss': 0.5495, 'learning_rate': 8.517540492983127e-05, 'epoch': 0.25}
{'loss': 0.5928, 'learning_rate': 8.515822099632993e-05, 'epoch': 0.25}
{'loss': 0.5136, 'learning_rate': 8.514102884480819e-05, 'epoch': 0.25}
{'loss': 0.5178, 'learning_rate': 8.512382847928461e-05, 'epoch': 0.25}
                 25%|       | 1636/6500 [4:57:26<14:28:34, 10.71s/it] 25%|       | 1637/6500 [4:57:36<14:21:42, 10.63s/it]                                                         25%|       | 1637/6500 [4:57:36<14:21:42, 10.63s/it] 25%|       | 1638/6500 [4:57:47<14:17:07, 10.58s/it]                                                         25%|       | 1638/6500 [4:57:47<14:17:07, 10.58s/it] 25%|       | 1639/6500 [4:57:57<14:13:49, 10.54s/it]                                                         25%|       | 1639/6500 [4:57:57<14:13:49, 10.54s/it] 25%|       | 1640/6500 [4:58:08<14:11:22, 10.51s/it]                                                         25%|       | 1640/6500 [4:58:08<14:11:22, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8052141666412354, 'eval_runtime': 4.2095, 'eval_samples_per_second': 5.464, 'eval_steps_per_second': 1.425, 'epoch': 0.25}
                                                         25%|       | 1640/6500 [4:58:12<14:11:22, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1640
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1640
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1640/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1640/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.523, 'learning_rate': 8.510661990377971e-05, 'epoch': 0.25}
{'loss': 1.0516, 'learning_rate': 8.508940312231584e-05, 'epoch': 0.25}
{'loss': 0.5527, 'learning_rate': 8.507217813891733e-05, 'epoch': 0.25}
{'loss': 0.5166, 'learning_rate': 8.505494495761043e-05, 'epoch': 0.25}
{'loss': 0.5043, 'learning_rate': 8.503770358242329e-05, 'epoch': 0.25}
 25%|       | 1641/6500 [4:58:23<16:02:33, 11.89s/it]                                                         25%|       | 1641/6500 [4:58:23<16:02:33, 11.89s/it] 25%|       | 1642/6500 [4:58:33<15:27:23, 11.45s/it]                                                         25%|       | 1642/6500 [4:58:33<15:27:23, 11.45s/it] 25%|       | 1643/6500 [4:58:44<15:02:31, 11.15s/it]                                                         25%|       | 1643/6500 [4:58:44<15:02:31, 11.15s/it] 25%|       | 1644/6500 [4:58:54<14:45:26, 10.94s/it]                                                         25%|       | 1644/6500 [4:58:54<14:45:26, 10.94s/it] 25%|       | 1645/6500 [4:59:05<14:33:08, 10.79s/it]                                                         25%|       | 1645/6500 [4:59:05<14:33:08, 10.79s/it] 25%|       | 1646/6500 [4:59:15<14:24:26, 10.69s/it]                                        {'loss': 0.5153, 'learning_rate': 8.502045401738595e-05, 'epoch': 0.25}
{'loss': 0.5604, 'learning_rate': 8.500319626653042e-05, 'epoch': 0.25}
{'loss': 0.5144, 'learning_rate': 8.49859303338906e-05, 'epoch': 0.25}
{'loss': 0.5164, 'learning_rate': 8.496865622350227e-05, 'epoch': 0.25}
{'loss': 0.5043, 'learning_rate': 8.495137393940317e-05, 'epoch': 0.25}
                 25%|       | 1646/6500 [4:59:15<14:24:26, 10.69s/it] 25%|       | 1647/6500 [4:59:26<14:18:12, 10.61s/it]                                                         25%|       | 1647/6500 [4:59:26<14:18:12, 10.61s/it] 25%|       | 1648/6500 [4:59:36<14:13:50, 10.56s/it]                                                         25%|       | 1648/6500 [4:59:36<14:13:50, 10.56s/it] 25%|       | 1649/6500 [4:59:47<14:19:25, 10.63s/it]                                                         25%|       | 1649/6500 [4:59:47<14:19:25, 10.63s/it] 25%|       | 1650/6500 [4:59:57<14:14:38, 10.57s/it]                                                         25%|       | 1650/6500 [4:59:57<14:14:38, 10.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8068708181381226, 'eval_runtime': 3.9835, 'eval_samples_per_second': 5.774, 'eval_steps_per_second': 1.506, 'epoch': 0.25}
                                                         25%|       | 1650/6500 [5:00:01<14:14:38, 10.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1650/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1650/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5155, 'learning_rate': 8.493408348563291e-05, 'epoch': 0.25}
{'loss': 0.5212, 'learning_rate': 8.491678486623304e-05, 'epoch': 0.25}
{'loss': 0.524, 'learning_rate': 8.489947808524701e-05, 'epoch': 0.25}
{'loss': 0.5486, 'learning_rate': 8.488216314672018e-05, 'epoch': 0.25}
{'loss': 0.5072, 'learning_rate': 8.486484005469977e-05, 'epoch': 0.25}
 25%|       | 1651/6500 [5:00:12<15:59:50, 11.88s/it]                                                         25%|       | 1651/6500 [5:00:12<15:59:50, 11.88s/it] 25%|       | 1652/6500 [5:00:23<15:24:36, 11.44s/it]                                                         25%|       | 1652/6500 [5:00:23<15:24:36, 11.44s/it] 25%|       | 1653/6500 [5:00:33<14:59:57, 11.14s/it]                                                         25%|       | 1653/6500 [5:00:33<14:59:57, 11.14s/it] 25%|       | 1654/6500 [5:00:43<14:42:23, 10.93s/it]                                                         25%|       | 1654/6500 [5:00:43<14:42:23, 10.93s/it] 25%|       | 1655/6500 [5:00:54<14:30:13, 10.78s/it]                                                         25%|       | 1655/6500 [5:00:54<14:30:13, 10.78s/it] 25%|       | 1656/6500 [5:01:04<14:21:24, 10.67s/it]                                        {'loss': 0.5603, 'learning_rate': 8.4847508813235e-05, 'epoch': 0.25}
{'loss': 0.5405, 'learning_rate': 8.483016942637691e-05, 'epoch': 0.25}
{'loss': 0.5207, 'learning_rate': 8.48128218981785e-05, 'epoch': 0.26}
{'loss': 0.5362, 'learning_rate': 8.479546623269463e-05, 'epoch': 0.26}
{'loss': 0.5322, 'learning_rate': 8.47781024339821e-05, 'epoch': 0.26}
                 25%|       | 1656/6500 [5:01:04<14:21:24, 10.67s/it] 25%|       | 1657/6500 [5:01:15<14:15:21, 10.60s/it]                                                         25%|       | 1657/6500 [5:01:15<14:15:21, 10.60s/it] 26%|       | 1658/6500 [5:01:25<14:11:33, 10.55s/it]                                                         26%|       | 1658/6500 [5:01:25<14:11:33, 10.55s/it] 26%|       | 1659/6500 [5:01:36<14:08:20, 10.51s/it]                                                         26%|       | 1659/6500 [5:01:36<14:08:20, 10.51s/it] 26%|       | 1660/6500 [5:01:46<14:06:38, 10.50s/it]                                                         26%|       | 1660/6500 [5:01:46<14:06:38, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8021748661994934, 'eval_runtime': 3.9751, 'eval_samples_per_second': 5.786, 'eval_steps_per_second': 1.509, 'epoch': 0.26}
                                                         26%|       | 1660/6500 [5:01:50<14:06:38, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1660
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5225, 'learning_rate': 8.476073050609958e-05, 'epoch': 0.26}
{'loss': 0.5169, 'learning_rate': 8.474335045310764e-05, 'epoch': 0.26}
{'loss': 0.5459, 'learning_rate': 8.472596227906876e-05, 'epoch': 0.26}
{'loss': 0.5173, 'learning_rate': 8.470856598804737e-05, 'epoch': 0.26}
{'loss': 0.5297, 'learning_rate': 8.469116158410969e-05, 'epoch': 0.26}
 26%|       | 1661/6500 [5:02:01<15:52:54, 11.82s/it]                                                         26%|       | 1661/6500 [5:02:01<15:52:54, 11.82s/it] 26%|       | 1662/6500 [5:02:11<15:19:06, 11.40s/it]                                                         26%|       | 1662/6500 [5:02:11<15:19:06, 11.40s/it] 26%|       | 1663/6500 [5:02:22<14:55:20, 11.11s/it]                                                         26%|       | 1663/6500 [5:02:22<14:55:20, 11.11s/it] 26%|       | 1664/6500 [5:02:32<14:38:51, 10.90s/it]                                                         26%|       | 1664/6500 [5:02:32<14:38:51, 10.90s/it] 26%|       | 1665/6500 [5:02:43<14:32:28, 10.83s/it]                                                         26%|       | 1665/6500 [5:02:43<14:32:28, 10.83s/it] 26%|       | 1666/6500 [5:02:53<14:22:04, 10.70s/it]                                        {'loss': 0.497, 'learning_rate': 8.467374907132392e-05, 'epoch': 0.26}
{'loss': 0.5664, 'learning_rate': 8.465632845376013e-05, 'epoch': 0.26}
{'loss': 0.558, 'learning_rate': 8.463889973549027e-05, 'epoch': 0.26}
{'loss': 0.5069, 'learning_rate': 8.462146292058822e-05, 'epoch': 0.26}
{'loss': 0.5301, 'learning_rate': 8.460401801312969e-05, 'epoch': 0.26}
                 26%|       | 1666/6500 [5:02:53<14:22:04, 10.70s/it] 26%|       | 1667/6500 [5:03:04<14:15:11, 10.62s/it]                                                         26%|       | 1667/6500 [5:03:04<14:15:11, 10.62s/it] 26%|       | 1668/6500 [5:03:14<14:10:41, 10.56s/it]                                                         26%|       | 1668/6500 [5:03:14<14:10:41, 10.56s/it] 26%|       | 1669/6500 [5:03:25<14:07:34, 10.53s/it]                                                         26%|       | 1669/6500 [5:03:25<14:07:34, 10.53s/it] 26%|       | 1670/6500 [5:03:35<14:04:49, 10.49s/it]                                                         26%|       | 1670/6500 [5:03:35<14:04:49, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8082523941993713, 'eval_runtime': 4.2287, 'eval_samples_per_second': 5.439, 'eval_steps_per_second': 1.419, 'epoch': 0.26}
                                                         26%|       | 1670/6500 [5:03:39<14:04:49, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5204, 'learning_rate': 8.458656501719235e-05, 'epoch': 0.26}
{'loss': 1.0617, 'learning_rate': 8.456910393685573e-05, 'epoch': 0.26}
{'loss': 0.538, 'learning_rate': 8.455163477620126e-05, 'epoch': 0.26}
{'loss': 0.5221, 'learning_rate': 8.453415753931222e-05, 'epoch': 0.26}
{'loss': 0.4874, 'learning_rate': 8.451667223027384e-05, 'epoch': 0.26}
 26%|       | 1671/6500 [5:03:50<15:57:11, 11.89s/it]                                                         26%|       | 1671/6500 [5:03:50<15:57:11, 11.89s/it] 26%|       | 1672/6500 [5:04:01<15:21:27, 11.45s/it]                                                         26%|       | 1672/6500 [5:04:01<15:21:27, 11.45s/it] 26%|       | 1673/6500 [5:04:11<14:56:29, 11.14s/it]                                                         26%|       | 1673/6500 [5:04:11<14:56:29, 11.14s/it] 26%|       | 1674/6500 [5:04:21<14:39:29, 10.93s/it]                                                         26%|       | 1674/6500 [5:04:21<14:39:29, 10.93s/it] 26%|       | 1675/6500 [5:04:32<14:27:17, 10.79s/it]                                                         26%|       | 1675/6500 [5:04:32<14:27:17, 10.79s/it] 26%|       | 1676/6500 [5:04:42<14:18:54, 10.68s/it]                                        {'loss': 0.5304, 'learning_rate': 8.44991788531732e-05, 'epoch': 0.26}
{'loss': 0.538, 'learning_rate': 8.448167741209925e-05, 'epoch': 0.26}
{'loss': 0.4749, 'learning_rate': 8.446416791114284e-05, 'epoch': 0.26}
{'loss': 0.516, 'learning_rate': 8.444665035439674e-05, 'epoch': 0.26}
{'loss': 0.4925, 'learning_rate': 8.442912474595558e-05, 'epoch': 0.26}
                 26%|       | 1676/6500 [5:04:42<14:18:54, 10.68s/it] 26%|       | 1677/6500 [5:04:53<14:12:42, 10.61s/it]                                                         26%|       | 1677/6500 [5:04:53<14:12:42, 10.61s/it] 26%|       | 1678/6500 [5:05:03<14:08:20, 10.56s/it]                                                         26%|       | 1678/6500 [5:05:03<14:08:20, 10.56s/it] 26%|       | 1679/6500 [5:05:14<14:04:51, 10.51s/it]                                                         26%|       | 1679/6500 [5:05:14<14:04:51, 10.51s/it] 26%|       | 1680/6500 [5:05:24<14:03:17, 10.50s/it]                                                         26%|       | 1680/6500 [5:05:24<14:03:17, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8087670803070068, 'eval_runtime': 3.9682, 'eval_samples_per_second': 5.796, 'eval_steps_per_second': 1.512, 'epoch': 0.26}
                                                         26%|       | 1680/6500 [5:05:28<14:03:17, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5118, 'learning_rate': 8.441159108991583e-05, 'epoch': 0.26}
{'loss': 0.5108, 'learning_rate': 8.439404939037587e-05, 'epoch': 0.26}
{'loss': 0.5237, 'learning_rate': 8.437649965143601e-05, 'epoch': 0.26}
{'loss': 0.5162, 'learning_rate': 8.435894187719834e-05, 'epoch': 0.26}
{'loss': 0.5227, 'learning_rate': 8.434137607176693e-05, 'epoch': 0.26}
 26%|       | 1681/6500 [5:05:39<15:56:33, 11.91s/it]                                                         26%|       | 1681/6500 [5:05:39<15:56:33, 11.91s/it] 26%|       | 1682/6500 [5:05:50<15:21:36, 11.48s/it]                                                         26%|       | 1682/6500 [5:05:50<15:21:36, 11.48s/it] 26%|       | 1683/6500 [5:06:00<14:56:23, 11.17s/it]                                                         26%|       | 1683/6500 [5:06:00<14:56:23, 11.17s/it] 26%|       | 1684/6500 [5:06:11<14:38:35, 10.95s/it]                                                         26%|       | 1684/6500 [5:06:11<14:38:35, 10.95s/it] 26%|       | 1685/6500 [5:06:21<14:25:35, 10.79s/it]                                                         26%|       | 1685/6500 [5:06:21<14:25:35, 10.79s/it] 26%|       | 1686/6500 [5:06:31<14:16:31, 10.68s/it]                                        {'loss': 0.5658, 'learning_rate': 8.432380223924766e-05, 'epoch': 0.26}
{'loss': 0.5429, 'learning_rate': 8.430622038374831e-05, 'epoch': 0.26}
{'loss': 0.534, 'learning_rate': 8.428863050937853e-05, 'epoch': 0.26}
{'loss': 0.5328, 'learning_rate': 8.427103262024985e-05, 'epoch': 0.26}
{'loss': 0.5052, 'learning_rate': 8.425342672047567e-05, 'epoch': 0.26}
                 26%|       | 1686/6500 [5:06:31<14:16:31, 10.68s/it] 26%|       | 1687/6500 [5:06:42<14:10:40, 10.60s/it]                                                         26%|       | 1687/6500 [5:06:42<14:10:40, 10.60s/it] 26%|       | 1688/6500 [5:06:52<14:06:42, 10.56s/it]                                                         26%|       | 1688/6500 [5:06:52<14:06:42, 10.56s/it] 26%|       | 1689/6500 [5:07:03<14:03:28, 10.52s/it]                                                         26%|       | 1689/6500 [5:07:03<14:03:28, 10.52s/it] 26%|       | 1690/6500 [5:07:13<14:01:17, 10.49s/it]                                                         26%|       | 1690/6500 [5:07:13<14:01:17, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8059988617897034, 'eval_runtime': 3.9842, 'eval_samples_per_second': 5.773, 'eval_steps_per_second': 1.506, 'epoch': 0.26}
                                                         26%|       | 1690/6500 [5:07:17<14:01:17, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1690
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5142, 'learning_rate': 8.423581281417124e-05, 'epoch': 0.26}
{'loss': 0.5379, 'learning_rate': 8.421819090545373e-05, 'epoch': 0.26}
{'loss': 0.529, 'learning_rate': 8.420056099844216e-05, 'epoch': 0.26}
{'loss': 0.5117, 'learning_rate': 8.418292309725738e-05, 'epoch': 0.26}
{'loss': 0.5136, 'learning_rate': 8.416527720602218e-05, 'epoch': 0.26}
 26%|       | 1691/6500 [5:07:28<15:47:57, 11.83s/it]                                                         26%|       | 1691/6500 [5:07:28<15:47:57, 11.83s/it] 26%|       | 1692/6500 [5:07:39<15:14:21, 11.41s/it]                                                         26%|       | 1692/6500 [5:07:39<15:14:21, 11.41s/it] 26%|       | 1693/6500 [5:07:49<14:51:56, 11.13s/it]                                                         26%|       | 1693/6500 [5:07:49<14:51:56, 11.13s/it] 26%|       | 1694/6500 [5:07:59<14:35:21, 10.93s/it]                                                         26%|       | 1694/6500 [5:07:59<14:35:21, 10.93s/it] 26%|       | 1695/6500 [5:08:10<14:23:22, 10.78s/it]                                                         26%|       | 1695/6500 [5:08:10<14:23:22, 10.78s/it] 26%|       | 1696/6500 [5:08:20<14:14:24, 10.67s/it]                                        {'loss': 0.5342, 'learning_rate': 8.414762332886115e-05, 'epoch': 0.26}
{'loss': 0.5886, 'learning_rate': 8.412996146990079e-05, 'epoch': 0.26}
{'loss': 0.5046, 'learning_rate': 8.411229163326944e-05, 'epoch': 0.26}
{'loss': 0.5035, 'learning_rate': 8.409461382309733e-05, 'epoch': 0.26}
{'loss': 0.5216, 'learning_rate': 8.407692804351656e-05, 'epoch': 0.26}
                 26%|       | 1696/6500 [5:08:20<14:14:24, 10.67s/it] 26%|       | 1697/6500 [5:08:31<14:12:21, 10.65s/it]                                                         26%|       | 1697/6500 [5:08:31<14:12:21, 10.65s/it] 26%|       | 1698/6500 [5:08:41<14:05:52, 10.57s/it]                                                         26%|       | 1698/6500 [5:08:41<14:05:52, 10.57s/it] 26%|       | 1699/6500 [5:08:52<14:01:26, 10.52s/it]                                                         26%|       | 1699/6500 [5:08:52<14:01:26, 10.52s/it] 26%|       | 1700/6500 [5:09:02<13:58:15, 10.48s/it]                                                         26%|       | 1700/6500 [5:09:02<13:58:15, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8119842410087585, 'eval_runtime': 3.9764, 'eval_samples_per_second': 5.784, 'eval_steps_per_second': 1.509, 'epoch': 0.26}
                                                         26%|       | 1700/6500 [5:09:06<13:58:15, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1700I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1700

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0337, 'learning_rate': 8.405923429866103e-05, 'epoch': 0.26}
{'loss': 0.5308, 'learning_rate': 8.404153259266657e-05, 'epoch': 0.26}
{'loss': 0.5189, 'learning_rate': 8.402382292967084e-05, 'epoch': 0.26}
{'loss': 0.5185, 'learning_rate': 8.400610531381338e-05, 'epoch': 0.26}
{'loss': 0.4944, 'learning_rate': 8.398837974923555e-05, 'epoch': 0.26}
 26%|       | 1701/6500 [5:09:17<15:46:05, 11.83s/it]                                                         26%|       | 1701/6500 [5:09:17<15:46:05, 11.83s/it] 26%|       | 1702/6500 [5:09:27<15:11:09, 11.39s/it]                                                         26%|       | 1702/6500 [5:09:27<15:11:09, 11.39s/it] 26%|       | 1703/6500 [5:09:38<14:46:54, 11.09s/it]                                                         26%|       | 1703/6500 [5:09:38<14:46:54, 11.09s/it] 26%|       | 1704/6500 [5:09:48<14:29:45, 10.88s/it]                                                         26%|       | 1704/6500 [5:09:48<14:29:45, 10.88s/it] 26%|       | 1705/6500 [5:09:59<14:17:32, 10.73s/it]                                                         26%|       | 1705/6500 [5:09:59<14:17:32, 10.73s/it] 26%|       | 1706/6500 [5:10:09<14:08:59, 10.63s/it]                                        {'loss': 0.5421, 'learning_rate': 8.397064624008062e-05, 'epoch': 0.26}
{'loss': 0.5107, 'learning_rate': 8.395290479049367e-05, 'epoch': 0.26}
{'loss': 0.5021, 'learning_rate': 8.393515540462164e-05, 'epoch': 0.26}
{'loss': 0.4882, 'learning_rate': 8.391739808661339e-05, 'epoch': 0.26}
{'loss': 0.5017, 'learning_rate': 8.389963284061955e-05, 'epoch': 0.26}
                 26%|       | 1706/6500 [5:10:09<14:08:59, 10.63s/it] 26%|       | 1707/6500 [5:10:19<14:02:55, 10.55s/it]                                                         26%|       | 1707/6500 [5:10:19<14:02:55, 10.55s/it] 26%|       | 1708/6500 [5:10:30<13:59:03, 10.51s/it]                                                         26%|       | 1708/6500 [5:10:30<13:59:03, 10.51s/it] 26%|       | 1709/6500 [5:10:40<13:55:51, 10.47s/it]                                                         26%|       | 1709/6500 [5:10:40<13:55:51, 10.47s/it] 26%|       | 1710/6500 [5:10:51<13:53:37, 10.44s/it]                                                         26%|       | 1710/6500 [5:10:51<13:53:37, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.80861496925354, 'eval_runtime': 3.9765, 'eval_samples_per_second': 5.784, 'eval_steps_per_second': 1.509, 'epoch': 0.26}
                                                         26%|       | 1710/6500 [5:10:55<13:53:37, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1710
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5111, 'learning_rate': 8.388185967079265e-05, 'epoch': 0.26}
{'loss': 0.4962, 'learning_rate': 8.386407858128706e-05, 'epoch': 0.26}
{'loss': 0.5384, 'learning_rate': 8.384628957625899e-05, 'epoch': 0.26}
{'loss': 0.494, 'learning_rate': 8.382849265986653e-05, 'epoch': 0.26}
{'loss': 0.5395, 'learning_rate': 8.381068783626959e-05, 'epoch': 0.26}
 26%|       | 1711/6500 [5:11:05<15:39:13, 11.77s/it]                                                         26%|       | 1711/6500 [5:11:05<15:39:13, 11.77s/it] 26%|       | 1712/6500 [5:11:16<15:05:58, 11.35s/it]                                                         26%|       | 1712/6500 [5:11:16<15:05:58, 11.35s/it] 26%|       | 1713/6500 [5:11:26<14:50:54, 11.17s/it]                                                         26%|       | 1713/6500 [5:11:27<14:50:54, 11.17s/it] 26%|       | 1714/6500 [5:11:37<14:31:48, 10.93s/it]                                                         26%|       | 1714/6500 [5:11:37<14:31:48, 10.93s/it] 26%|       | 1715/6500 [5:11:47<14:18:20, 10.76s/it]                                                         26%|       | 1715/6500 [5:11:47<14:18:20, 10.76s/it] 26%|       | 1716/6500 [5:11:58<14:09:01, 10.65s/it]                                        {'loss': 0.5275, 'learning_rate': 8.379287510962993e-05, 'epoch': 0.26}
{'loss': 0.5058, 'learning_rate': 8.377505448411118e-05, 'epoch': 0.26}
{'loss': 0.5255, 'learning_rate': 8.375722596387881e-05, 'epoch': 0.26}
{'loss': 0.518, 'learning_rate': 8.373938955310011e-05, 'epoch': 0.26}
{'loss': 0.4956, 'learning_rate': 8.372154525594424e-05, 'epoch': 0.26}
                 26%|       | 1716/6500 [5:11:58<14:09:01, 10.65s/it] 26%|       | 1717/6500 [5:12:08<14:02:39, 10.57s/it]                                                         26%|       | 1717/6500 [5:12:08<14:02:39, 10.57s/it] 26%|       | 1718/6500 [5:12:18<13:58:05, 10.52s/it]                                                         26%|       | 1718/6500 [5:12:18<13:58:05, 10.52s/it] 26%|       | 1719/6500 [5:12:29<13:55:25, 10.48s/it]                                                         26%|       | 1719/6500 [5:12:29<13:55:25, 10.48s/it] 26%|       | 1720/6500 [5:12:39<13:53:26, 10.46s/it]                                                         26%|       | 1720/6500 [5:12:39<13:53:26, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8064919114112854, 'eval_runtime': 3.9656, 'eval_samples_per_second': 5.8, 'eval_steps_per_second': 1.513, 'epoch': 0.26}
                                                         26%|       | 1720/6500 [5:12:43<13:53:26, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1720I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1720/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5203, 'learning_rate': 8.370369307658219e-05, 'epoch': 0.26}
{'loss': 0.5381, 'learning_rate': 8.368583301918682e-05, 'epoch': 0.26}
{'loss': 0.503, 'learning_rate': 8.36679650879328e-05, 'epoch': 0.27}
{'loss': 0.5137, 'learning_rate': 8.365008928699662e-05, 'epoch': 0.27}
{'loss': 0.4944, 'learning_rate': 8.363220562055669e-05, 'epoch': 0.27}
 26%|       | 1721/6500 [5:12:54<15:37:19, 11.77s/it]                                                         26%|       | 1721/6500 [5:12:54<15:37:19, 11.77s/it] 26%|       | 1722/6500 [5:13:04<15:03:49, 11.35s/it]                                                         26%|       | 1722/6500 [5:13:04<15:03:49, 11.35s/it] 27%|       | 1723/6500 [5:13:15<14:40:16, 11.06s/it]                                                         27%|       | 1723/6500 [5:13:15<14:40:16, 11.06s/it] 27%|       | 1724/6500 [5:13:25<14:24:15, 10.86s/it]                                                         27%|       | 1724/6500 [5:13:25<14:24:15, 10.86s/it] 27%|       | 1725/6500 [5:13:36<14:12:41, 10.71s/it]                                                         27%|       | 1725/6500 [5:13:36<14:12:41, 10.71s/it] 27%|       | 1726/6500 [5:13:46<14:04:20, 10.61s/it]                                        {'loss': 0.5482, 'learning_rate': 8.361431409279317e-05, 'epoch': 0.27}
{'loss': 0.5665, 'learning_rate': 8.359641470788811e-05, 'epoch': 0.27}
{'loss': 0.5085, 'learning_rate': 8.357850747002538e-05, 'epoch': 0.27}
{'loss': 0.503, 'learning_rate': 8.35605923833907e-05, 'epoch': 0.27}
{'loss': 0.5097, 'learning_rate': 8.35426694521716e-05, 'epoch': 0.27}
                 27%|       | 1726/6500 [5:13:46<14:04:20, 10.61s/it] 27%|       | 1727/6500 [5:13:56<13:58:43, 10.54s/it]                                                         27%|       | 1727/6500 [5:13:56<13:58:43, 10.54s/it] 27%|       | 1728/6500 [5:14:07<13:54:39, 10.49s/it]                                                         27%|       | 1728/6500 [5:14:07<13:54:39, 10.49s/it] 27%|       | 1729/6500 [5:14:17<13:51:45, 10.46s/it]                                                         27%|       | 1729/6500 [5:14:17<13:51:45, 10.46s/it] 27%|       | 1730/6500 [5:14:28<13:55:21, 10.51s/it]                                                         27%|       | 1730/6500 [5:14:28<13:55:21, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8122673630714417, 'eval_runtime': 3.9766, 'eval_samples_per_second': 5.784, 'eval_steps_per_second': 1.509, 'epoch': 0.27}
                                                         27%|       | 1730/6500 [5:14:32<13:55:21, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0351, 'learning_rate': 8.352473868055746e-05, 'epoch': 0.27}
{'loss': 0.5313, 'learning_rate': 8.350680007273947e-05, 'epoch': 0.27}
{'loss': 0.5295, 'learning_rate': 8.348885363291071e-05, 'epoch': 0.27}
{'loss': 0.4783, 'learning_rate': 8.3470899365266e-05, 'epoch': 0.27}
{'loss': 0.5083, 'learning_rate': 8.345293727400209e-05, 'epoch': 0.27}
 27%|       | 1731/6500 [5:14:43<15:43:31, 11.87s/it]                                                         27%|       | 1731/6500 [5:14:43<15:43:31, 11.87s/it] 27%|       | 1732/6500 [5:14:53<15:07:40, 11.42s/it]                                                         27%|       | 1732/6500 [5:14:53<15:07:40, 11.42s/it] 27%|       | 1733/6500 [5:15:04<14:42:46, 11.11s/it]                                                         27%|       | 1733/6500 [5:15:04<14:42:46, 11.11s/it] 27%|       | 1734/6500 [5:15:14<14:24:58, 10.89s/it]                                                         27%|       | 1734/6500 [5:15:14<14:24:58, 10.89s/it] 27%|       | 1735/6500 [5:15:24<14:12:43, 10.74s/it]                                                         27%|       | 1735/6500 [5:15:24<14:12:43, 10.74s/it] 27%|       | 1736/6500 [5:15:35<14:04:11, 10.63s/it]                                        {'loss': 0.5438, 'learning_rate': 8.343496736331749e-05, 'epoch': 0.27}
{'loss': 0.4905, 'learning_rate': 8.341698963741256e-05, 'epoch': 0.27}
{'loss': 0.5047, 'learning_rate': 8.339900410048947e-05, 'epoch': 0.27}
{'loss': 0.4937, 'learning_rate': 8.338101075675225e-05, 'epoch': 0.27}
{'loss': 0.4967, 'learning_rate': 8.336300961040673e-05, 'epoch': 0.27}
                 27%|       | 1736/6500 [5:15:35<14:04:11, 10.63s/it] 27%|       | 1737/6500 [5:15:45<13:57:55, 10.56s/it]                                                         27%|       | 1737/6500 [5:15:45<13:57:55, 10.56s/it] 27%|       | 1738/6500 [5:15:55<13:54:06, 10.51s/it]                                                         27%|       | 1738/6500 [5:15:55<13:54:06, 10.51s/it] 27%|       | 1739/6500 [5:16:06<13:51:08, 10.47s/it]                                                         27%|       | 1739/6500 [5:16:06<13:51:08, 10.47s/it] 27%|       | 1740/6500 [5:16:16<13:49:09, 10.45s/it]                                                         27%|       | 1740/6500 [5:16:16<13:49:09, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8091099858283997, 'eval_runtime': 4.1114, 'eval_samples_per_second': 5.594, 'eval_steps_per_second': 1.459, 'epoch': 0.27}
                                                         27%|       | 1740/6500 [5:16:20<13:49:09, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1740
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1740
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4895, 'learning_rate': 8.334500066566054e-05, 'epoch': 0.27}
{'loss': 0.5031, 'learning_rate': 8.33269839267232e-05, 'epoch': 0.27}
{'loss': 0.5233, 'learning_rate': 8.330895939780601e-05, 'epoch': 0.27}
{'loss': 0.4904, 'learning_rate': 8.329092708312207e-05, 'epoch': 0.27}
{'loss': 0.5646, 'learning_rate': 8.327288698688634e-05, 'epoch': 0.27}
 27%|       | 1741/6500 [5:16:31<15:36:10, 11.80s/it]                                                         27%|       | 1741/6500 [5:16:31<15:36:10, 11.80s/it] 27%|       | 1742/6500 [5:16:42<15:02:21, 11.38s/it]                                                         27%|       | 1742/6500 [5:16:42<15:02:21, 11.38s/it] 27%|       | 1743/6500 [5:16:52<14:38:26, 11.08s/it]                                                         27%|       | 1743/6500 [5:16:52<14:38:26, 11.08s/it] 27%|       | 1744/6500 [5:17:02<14:21:36, 10.87s/it]                                                         27%|       | 1744/6500 [5:17:02<14:21:36, 10.87s/it] 27%|       | 1745/6500 [5:17:13<14:09:20, 10.72s/it]                                                         27%|       | 1745/6500 [5:17:13<14:09:20, 10.72s/it] 27%|       | 1746/6500 [5:17:23<14:11:16, 10.74s/it]                                        {'loss': 0.5202, 'learning_rate': 8.325483911331557e-05, 'epoch': 0.27}
{'loss': 0.5098, 'learning_rate': 8.323678346662835e-05, 'epoch': 0.27}
{'loss': 0.5307, 'learning_rate': 8.321872005104509e-05, 'epoch': 0.27}
{'loss': 0.5109, 'learning_rate': 8.3200648870788e-05, 'epoch': 0.27}
{'loss': 0.5217, 'learning_rate': 8.318256993008107e-05, 'epoch': 0.27}
                 27%|       | 1746/6500 [5:17:23<14:11:16, 10.74s/it] 27%|       | 1747/6500 [5:17:34<14:02:47, 10.64s/it]                                                         27%|       | 1747/6500 [5:17:34<14:02:47, 10.64s/it] 27%|       | 1748/6500 [5:17:44<13:56:28, 10.56s/it]                                                         27%|       | 1748/6500 [5:17:44<13:56:28, 10.56s/it] 27%|       | 1749/6500 [5:17:55<13:52:20, 10.51s/it]                                                         27%|       | 1749/6500 [5:17:55<13:52:20, 10.51s/it] 27%|       | 1750/6500 [5:18:05<13:49:24, 10.48s/it]                                                         27%|       | 1750/6500 [5:18:05<13:49:24, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8073069453239441, 'eval_runtime': 3.9711, 'eval_samples_per_second': 5.792, 'eval_steps_per_second': 1.511, 'epoch': 0.27}
                                                         27%|       | 1750/6500 [5:18:09<13:49:24, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1750
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5029, 'learning_rate': 8.316448323315021e-05, 'epoch': 0.27}
{'loss': 0.5252, 'learning_rate': 8.3146388784223e-05, 'epoch': 0.27}
{'loss': 0.5069, 'learning_rate': 8.312828658752896e-05, 'epoch': 0.27}
{'loss': 0.4977, 'learning_rate': 8.311017664729935e-05, 'epoch': 0.27}
{'loss': 0.4926, 'learning_rate': 8.309205896776727e-05, 'epoch': 0.27}
 27%|       | 1751/6500 [5:18:20<15:32:09, 11.78s/it]                                                         27%|       | 1751/6500 [5:18:20<15:32:09, 11.78s/it] 27%|       | 1752/6500 [5:18:30<14:59:01, 11.36s/it]                                                         27%|       | 1752/6500 [5:18:30<14:59:01, 11.36s/it] 27%|       | 1753/6500 [5:18:41<14:36:00, 11.07s/it]                                                         27%|       | 1753/6500 [5:18:41<14:36:00, 11.07s/it] 27%|       | 1754/6500 [5:18:51<14:19:31, 10.87s/it]                                                         27%|       | 1754/6500 [5:18:51<14:19:31, 10.87s/it] 27%|       | 1755/6500 [5:19:01<14:08:00, 10.72s/it]                                                         27%|       | 1755/6500 [5:19:01<14:08:00, 10.72s/it] 27%|       | 1756/6500 [5:19:12<13:59:57, 10.62s/it]                                        {'loss': 0.5909, 'learning_rate': 8.307393355316761e-05, 'epoch': 0.27}
{'loss': 0.5198, 'learning_rate': 8.305580040773706e-05, 'epoch': 0.27}
{'loss': 0.4945, 'learning_rate': 8.303765953571417e-05, 'epoch': 0.27}
{'loss': 0.5158, 'learning_rate': 8.30195109413392e-05, 'epoch': 0.27}
{'loss': 1.0374, 'learning_rate': 8.300135462885435e-05, 'epoch': 0.27}
                 27%|       | 1756/6500 [5:19:12<13:59:57, 10.62s/it] 27%|       | 1757/6500 [5:19:22<13:54:14, 10.55s/it]                                                         27%|       | 1757/6500 [5:19:22<13:54:14, 10.55s/it] 27%|       | 1758/6500 [5:19:33<13:50:08, 10.50s/it]                                                         27%|       | 1758/6500 [5:19:33<13:50:08, 10.50s/it] 27%|       | 1759/6500 [5:19:43<13:47:22, 10.47s/it]                                                         27%|       | 1759/6500 [5:19:43<13:47:22, 10.47s/it] 27%|       | 1760/6500 [5:19:53<13:44:49, 10.44s/it]                                                         27%|       | 1760/6500 [5:19:53<13:44:49, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8115672469139099, 'eval_runtime': 3.974, 'eval_samples_per_second': 5.788, 'eval_steps_per_second': 1.51, 'epoch': 0.27}
                                                         27%|       | 1760/6500 [5:19:57<13:44:49, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1760
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5247, 'learning_rate': 8.298319060250348e-05, 'epoch': 0.27}
{'loss': 0.5226, 'learning_rate': 8.296501886653236e-05, 'epoch': 0.27}
{'loss': 0.51, 'learning_rate': 8.29468394251885e-05, 'epoch': 0.27}
{'loss': 0.4721, 'learning_rate': 8.292865228272126e-05, 'epoch': 0.27}
{'loss': 0.5284, 'learning_rate': 8.291045744338175e-05, 'epoch': 0.27}
 27%|       | 1761/6500 [5:20:08<15:28:45, 11.76s/it]                                                         27%|       | 1761/6500 [5:20:08<15:28:45, 11.76s/it] 27%|       | 1762/6500 [5:20:19<15:16:35, 11.61s/it]                                                         27%|       | 1762/6500 [5:20:19<15:16:35, 11.61s/it] 27%|       | 1763/6500 [5:20:30<14:47:22, 11.24s/it]                                                         27%|       | 1763/6500 [5:20:30<14:47:22, 11.24s/it] 27%|       | 1764/6500 [5:20:40<14:26:26, 10.98s/it]                                                         27%|       | 1764/6500 [5:20:40<14:26:26, 10.98s/it] 27%|       | 1765/6500 [5:20:51<14:11:48, 10.79s/it]                                                         27%|       | 1765/6500 [5:20:51<14:11:48, 10.79s/it] 27%|       | 1766/6500 [5:21:01<14:01:30, 10.67s/it]                                        {'loss': 0.5177, 'learning_rate': 8.289225491142292e-05, 'epoch': 0.27}
{'loss': 0.4765, 'learning_rate': 8.287404469109947e-05, 'epoch': 0.27}
{'loss': 0.4956, 'learning_rate': 8.285582678666797e-05, 'epoch': 0.27}
{'loss': 0.4868, 'learning_rate': 8.283760120238672e-05, 'epoch': 0.27}
{'loss': 0.499, 'learning_rate': 8.281936794251586e-05, 'epoch': 0.27}
                 27%|       | 1766/6500 [5:21:01<14:01:30, 10.67s/it] 27%|       | 1767/6500 [5:21:11<13:54:14, 10.58s/it]                                                         27%|       | 1767/6500 [5:21:11<13:54:14, 10.58s/it] 27%|       | 1768/6500 [5:21:22<13:49:21, 10.52s/it]                                                         27%|       | 1768/6500 [5:21:22<13:49:21, 10.52s/it] 27%|       | 1769/6500 [5:21:32<13:46:02, 10.48s/it]                                                         27%|       | 1769/6500 [5:21:32<13:46:02, 10.48s/it] 27%|       | 1770/6500 [5:21:42<13:43:38, 10.45s/it]                                                         27%|       | 1770/6500 [5:21:42<13:43:38, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8085744380950928, 'eval_runtime': 3.9827, 'eval_samples_per_second': 5.775, 'eval_steps_per_second': 1.507, 'epoch': 0.27}
                                                         27%|       | 1770/6500 [5:21:46<13:43:38, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4852, 'learning_rate': 8.280112701131726e-05, 'epoch': 0.27}
{'loss': 0.5079, 'learning_rate': 8.278287841305468e-05, 'epoch': 0.27}
{'loss': 0.4925, 'learning_rate': 8.276462215199357e-05, 'epoch': 0.27}
{'loss': 0.5064, 'learning_rate': 8.274635823240127e-05, 'epoch': 0.27}
{'loss': 0.5405, 'learning_rate': 8.27280866585468e-05, 'epoch': 0.27}
 27%|       | 1771/6500 [5:21:57<15:27:11, 11.76s/it]                                                         27%|       | 1771/6500 [5:21:57<15:27:11, 11.76s/it] 27%|       | 1772/6500 [5:22:08<14:54:04, 11.35s/it]                                                         27%|       | 1772/6500 [5:22:08<14:54:04, 11.35s/it] 27%|       | 1773/6500 [5:22:18<14:30:46, 11.05s/it]                                                         27%|       | 1773/6500 [5:22:18<14:30:46, 11.05s/it] 27%|       | 1774/6500 [5:22:28<14:14:45, 10.85s/it]                                                         27%|       | 1774/6500 [5:22:28<14:14:45, 10.85s/it] 27%|       | 1775/6500 [5:22:39<14:06:14, 10.75s/it]                                                         27%|       | 1775/6500 [5:22:39<14:06:14, 10.75s/it] 27%|       | 1776/6500 [5:22:49<13:57:42, 10.64s/it]                                        {'loss': 0.5151, 'learning_rate': 8.27098074347011e-05, 'epoch': 0.27}
{'loss': 0.5101, 'learning_rate': 8.269152056513678e-05, 'epoch': 0.27}
{'loss': 0.5073, 'learning_rate': 8.26732260541283e-05, 'epoch': 0.27}
{'loss': 0.4944, 'learning_rate': 8.265492390595186e-05, 'epoch': 0.27}
{'loss': 0.4904, 'learning_rate': 8.263661412488552e-05, 'epoch': 0.27}
                 27%|       | 1776/6500 [5:22:49<13:57:42, 10.64s/it] 27%|       | 1777/6500 [5:23:00<13:51:14, 10.56s/it]                                                         27%|       | 1777/6500 [5:23:00<13:51:14, 10.56s/it] 27%|       | 1778/6500 [5:23:10<13:53:17, 10.59s/it]                                                         27%|       | 1778/6500 [5:23:10<13:53:17, 10.59s/it] 27%|       | 1779/6500 [5:23:21<13:48:14, 10.53s/it]                                                         27%|       | 1779/6500 [5:23:21<13:48:14, 10.53s/it] 27%|       | 1780/6500 [5:23:31<13:44:12, 10.48s/it]                                                         27%|       | 1780/6500 [5:23:31<13:44:12, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8050652146339417, 'eval_runtime': 4.0036, 'eval_samples_per_second': 5.745, 'eval_steps_per_second': 1.499, 'epoch': 0.27}
                                                         27%|       | 1780/6500 [5:23:35<13:44:12, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1780
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5336, 'learning_rate': 8.261829671520907e-05, 'epoch': 0.27}
{'loss': 0.5039, 'learning_rate': 8.259997168120409e-05, 'epoch': 0.27}
{'loss': 0.5027, 'learning_rate': 8.258163902715392e-05, 'epoch': 0.27}
{'loss': 0.4905, 'learning_rate': 8.256329875734375e-05, 'epoch': 0.27}
{'loss': 0.5222, 'learning_rate': 8.254495087606045e-05, 'epoch': 0.27}
 27%|       | 1781/6500 [5:23:46<15:29:06, 11.81s/it]                                                         27%|       | 1781/6500 [5:23:46<15:29:06, 11.81s/it] 27%|       | 1782/6500 [5:23:56<14:55:37, 11.39s/it]                                                         27%|       | 1782/6500 [5:23:56<14:55:37, 11.39s/it] 27%|       | 1783/6500 [5:24:07<14:31:47, 11.09s/it]                                                         27%|       | 1783/6500 [5:24:07<14:31:47, 11.09s/it] 27%|       | 1784/6500 [5:24:17<14:15:03, 10.88s/it]                                                         27%|       | 1784/6500 [5:24:17<14:15:03, 10.88s/it] 27%|       | 1785/6500 [5:24:28<14:03:05, 10.73s/it]                                                         27%|       | 1785/6500 [5:24:28<14:03:05, 10.73s/it] 27%|       | 1786/6500 [5:24:38<13:54:33, 10.62s/it]                                        {'loss': 0.5631, 'learning_rate': 8.252659538759279e-05, 'epoch': 0.27}
{'loss': 0.502, 'learning_rate': 8.250823229623122e-05, 'epoch': 0.27}
{'loss': 0.486, 'learning_rate': 8.2489861606268e-05, 'epoch': 0.28}
{'loss': 0.4992, 'learning_rate': 8.247148332199715e-05, 'epoch': 0.28}
{'loss': 1.0238, 'learning_rate': 8.245309744771452e-05, 'epoch': 0.28}
                 27%|       | 1786/6500 [5:24:38<13:54:33, 10.62s/it] 27%|       | 1787/6500 [5:24:48<13:48:56, 10.55s/it]                                                         27%|       | 1787/6500 [5:24:48<13:48:56, 10.55s/it] 28%|       | 1788/6500 [5:24:59<13:44:54, 10.50s/it]                                                         28%|       | 1788/6500 [5:24:59<13:44:54, 10.50s/it] 28%|       | 1789/6500 [5:25:09<13:41:32, 10.46s/it]                                                         28%|       | 1789/6500 [5:25:09<13:41:32, 10.46s/it] 28%|       | 1790/6500 [5:25:19<13:39:15, 10.44s/it]                                                         28%|       | 1790/6500 [5:25:19<13:39:15, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8110671043395996, 'eval_runtime': 3.9734, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.28}
                                                         28%|       | 1790/6500 [5:25:23<13:39:15, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1790
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1790/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1790/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5227, 'learning_rate': 8.24347039877177e-05, 'epoch': 0.28}
{'loss': 0.5035, 'learning_rate': 8.241630294630599e-05, 'epoch': 0.28}
{'loss': 0.4806, 'learning_rate': 8.239789432778058e-05, 'epoch': 0.28}
{'loss': 0.4968, 'learning_rate': 8.237947813644436e-05, 'epoch': 0.28}
{'loss': 0.5403, 'learning_rate': 8.236105437660198e-05, 'epoch': 0.28}
 28%|       | 1791/6500 [5:25:34<15:21:56, 11.75s/it]                                                         28%|       | 1791/6500 [5:25:34<15:21:56, 11.75s/it] 28%|       | 1792/6500 [5:25:45<14:48:50, 11.33s/it]                                                         28%|       | 1792/6500 [5:25:45<14:48:50, 11.33s/it] 28%|       | 1793/6500 [5:25:55<14:25:35, 11.03s/it]                                                         28%|       | 1793/6500 [5:25:55<14:25:35, 11.03s/it] 28%|       | 1794/6500 [5:26:06<14:24:25, 11.02s/it]                                                         28%|       | 1794/6500 [5:26:06<14:24:25, 11.02s/it] 28%|       | 1795/6500 [5:26:16<14:08:19, 10.82s/it]                                                         28%|       | 1795/6500 [5:26:16<14:08:19, 10.82s/it] 28%|       | 1796/6500 [5:26:27<13:56:55, 10.68s/it]                                        {'loss': 0.4886, 'learning_rate': 8.234262305255991e-05, 'epoch': 0.28}
{'loss': 0.4981, 'learning_rate': 8.232418416862633e-05, 'epoch': 0.28}
{'loss': 0.4702, 'learning_rate': 8.230573772911126e-05, 'epoch': 0.28}
{'loss': 0.4964, 'learning_rate': 8.228728373832642e-05, 'epoch': 0.28}
{'loss': 0.4839, 'learning_rate': 8.226882220058529e-05, 'epoch': 0.28}
                 28%|       | 1796/6500 [5:26:27<13:56:55, 10.68s/it] 28%|       | 1797/6500 [5:26:37<13:50:50, 10.60s/it]                                                         28%|       | 1797/6500 [5:26:37<13:50:50, 10.60s/it] 28%|       | 1798/6500 [5:26:47<13:44:37, 10.52s/it]                                                         28%|       | 1798/6500 [5:26:47<13:44:37, 10.52s/it] 28%|       | 1799/6500 [5:26:58<13:40:25, 10.47s/it]                                                         28%|       | 1799/6500 [5:26:58<13:40:25, 10.47s/it] 28%|       | 1800/6500 [5:27:08<13:37:26, 10.44s/it]                                                         28%|       | 1800/6500 [5:27:08<13:37:26, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8119243383407593, 'eval_runtime': 3.9542, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 1.517, 'epoch': 0.28}
                                                         28%|       | 1800/6500 [5:27:12<13:37:26, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1800
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1800
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4793, 'learning_rate': 8.225035312020318e-05, 'epoch': 0.28}
{'loss': 0.5268, 'learning_rate': 8.223187650149712e-05, 'epoch': 0.28}
{'loss': 0.4882, 'learning_rate': 8.221339234878589e-05, 'epoch': 0.28}
{'loss': 0.52, 'learning_rate': 8.219490066639007e-05, 'epoch': 0.28}
{'loss': 0.5138, 'learning_rate': 8.217640145863197e-05, 'epoch': 0.28}
 28%|       | 1801/6500 [5:27:23<15:19:30, 11.74s/it]                                                         28%|       | 1801/6500 [5:27:23<15:19:30, 11.74s/it] 28%|       | 1802/6500 [5:27:33<14:46:36, 11.32s/it]                                                         28%|       | 1802/6500 [5:27:33<14:46:36, 11.32s/it] 28%|       | 1803/6500 [5:27:44<14:23:35, 11.03s/it]                                                         28%|       | 1803/6500 [5:27:44<14:23:35, 11.03s/it] 28%|       | 1804/6500 [5:27:54<14:08:27, 10.84s/it]                                                         28%|       | 1804/6500 [5:27:54<14:08:27, 10.84s/it] 28%|       | 1805/6500 [5:28:04<13:56:42, 10.69s/it]                                                         28%|       | 1805/6500 [5:28:04<13:56:42, 10.69s/it] 28%|       | 1806/6500 [5:28:15<13:48:42, 10.59s/it]                                        {'loss': 0.4861, 'learning_rate': 8.215789472983565e-05, 'epoch': 0.28}
{'loss': 0.5164, 'learning_rate': 8.213938048432697e-05, 'epoch': 0.28}
{'loss': 0.5063, 'learning_rate': 8.212085872643351e-05, 'epoch': 0.28}
{'loss': 0.4918, 'learning_rate': 8.210232946048462e-05, 'epoch': 0.28}
{'loss': 0.5015, 'learning_rate': 8.208379269081141e-05, 'epoch': 0.28}
                 28%|       | 1806/6500 [5:28:15<13:48:42, 10.59s/it] 28%|       | 1807/6500 [5:28:25<13:43:02, 10.52s/it]                                                         28%|       | 1807/6500 [5:28:25<13:43:02, 10.52s/it] 28%|       | 1808/6500 [5:28:35<13:39:19, 10.48s/it]                                                         28%|       | 1808/6500 [5:28:35<13:39:19, 10.48s/it] 28%|       | 1809/6500 [5:28:46<13:36:08, 10.44s/it]                                                         28%|       | 1809/6500 [5:28:46<13:36:08, 10.44s/it] 28%|       | 1810/6500 [5:28:56<13:39:47, 10.49s/it]                                                         28%|       | 1810/6500 [5:28:56<13:39:47, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8079134225845337, 'eval_runtime': 3.9649, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.28}
                                                         28%|       | 1810/6500 [5:29:00<13:39:47, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5302, 'learning_rate': 8.206524842174672e-05, 'epoch': 0.28}
{'loss': 0.4872, 'learning_rate': 8.204669665762519e-05, 'epoch': 0.28}
{'loss': 0.4954, 'learning_rate': 8.202813740278314e-05, 'epoch': 0.28}
{'loss': 0.4691, 'learning_rate': 8.200957066155872e-05, 'epoch': 0.28}
{'loss': 0.5466, 'learning_rate': 8.199099643829177e-05, 'epoch': 0.28}
 28%|       | 1811/6500 [5:29:11<15:20:49, 11.78s/it]                                                         28%|       | 1811/6500 [5:29:11<15:20:49, 11.78s/it] 28%|       | 1812/6500 [5:29:22<14:47:27, 11.36s/it]                                                         28%|       | 1812/6500 [5:29:22<14:47:27, 11.36s/it] 28%|       | 1813/6500 [5:29:32<14:24:08, 11.06s/it]                                                         28%|       | 1813/6500 [5:29:32<14:24:08, 11.06s/it] 28%|       | 1814/6500 [5:29:42<14:07:42, 10.85s/it]                                                         28%|       | 1814/6500 [5:29:42<14:07:42, 10.85s/it] 28%|       | 1815/6500 [5:29:53<13:56:38, 10.71s/it]                                                         28%|       | 1815/6500 [5:29:53<13:56:38, 10.71s/it] 28%|       | 1816/6500 [5:30:03<13:48:49, 10.62s/it]                                        {'loss': 0.5417, 'learning_rate': 8.197241473732392e-05, 'epoch': 0.28}
{'loss': 0.4877, 'learning_rate': 8.195382556299852e-05, 'epoch': 0.28}
{'loss': 0.504, 'learning_rate': 8.193522891966067e-05, 'epoch': 0.28}
{'loss': 0.4895, 'learning_rate': 8.191662481165724e-05, 'epoch': 0.28}
{'loss': 1.0222, 'learning_rate': 8.189801324333681e-05, 'epoch': 0.28}
                 28%|       | 1816/6500 [5:30:03<13:48:49, 10.62s/it] 28%|       | 1817/6500 [5:30:13<13:42:36, 10.54s/it]                                                         28%|       | 1817/6500 [5:30:13<13:42:36, 10.54s/it] 28%|       | 1818/6500 [5:30:24<13:38:37, 10.49s/it]                                                         28%|       | 1818/6500 [5:30:24<13:38:37, 10.49s/it] 28%|       | 1819/6500 [5:30:34<13:36:47, 10.47s/it]                                                         28%|       | 1819/6500 [5:30:34<13:36:47, 10.47s/it] 28%|       | 1820/6500 [5:30:45<13:34:53, 10.45s/it]                                                         28%|       | 1820/6500 [5:30:45<13:34:53, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8139292001724243, 'eval_runtime': 4.0974, 'eval_samples_per_second': 5.613, 'eval_steps_per_second': 1.464, 'epoch': 0.28}
                                                         28%|       | 1820/6500 [5:30:49<13:34:53, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5172, 'learning_rate': 8.187939421904973e-05, 'epoch': 0.28}
{'loss': 0.5069, 'learning_rate': 8.186076774314809e-05, 'epoch': 0.28}
{'loss': 0.4749, 'learning_rate': 8.184213381998568e-05, 'epoch': 0.28}
{'loss': 0.5161, 'learning_rate': 8.182349245391812e-05, 'epoch': 0.28}
{'loss': 0.5126, 'learning_rate': 8.180484364930267e-05, 'epoch': 0.28}
 28%|       | 1821/6500 [5:31:00<15:21:48, 11.82s/it]                                                         28%|       | 1821/6500 [5:31:00<15:21:48, 11.82s/it] 28%|       | 1822/6500 [5:31:10<14:48:13, 11.39s/it]                                                         28%|       | 1822/6500 [5:31:10<14:48:13, 11.39s/it] 28%|       | 1823/6500 [5:31:20<14:24:31, 11.09s/it]                                                         28%|       | 1823/6500 [5:31:20<14:24:31, 11.09s/it] 28%|       | 1824/6500 [5:31:31<14:08:29, 10.89s/it]                                                         28%|       | 1824/6500 [5:31:31<14:08:29, 10.89s/it] 28%|       | 1825/6500 [5:31:41<13:56:35, 10.74s/it]                                                         28%|       | 1825/6500 [5:31:41<13:56:35, 10.74s/it] 28%|       | 1826/6500 [5:31:52<13:48:57, 10.64s/it]                                        {'loss': 0.4615, 'learning_rate': 8.178618741049842e-05, 'epoch': 0.28}
{'loss': 0.4924, 'learning_rate': 8.17675237418661e-05, 'epoch': 0.28}
{'loss': 0.4801, 'learning_rate': 8.174885264776826e-05, 'epoch': 0.28}
{'loss': 0.504, 'learning_rate': 8.173017413256915e-05, 'epoch': 0.28}
{'loss': 0.4915, 'learning_rate': 8.171148820063476e-05, 'epoch': 0.28}
                 28%|       | 1826/6500 [5:31:52<13:48:57, 10.64s/it] 28%|       | 1827/6500 [5:32:02<13:52:07, 10.68s/it]                                                         28%|       | 1827/6500 [5:32:02<13:52:07, 10.68s/it] 28%|       | 1828/6500 [5:32:13<13:45:19, 10.60s/it]                                                         28%|       | 1828/6500 [5:32:13<13:45:19, 10.60s/it] 28%|       | 1829/6500 [5:32:23<13:40:42, 10.54s/it]                                                         28%|       | 1829/6500 [5:32:23<13:40:42, 10.54s/it] 28%|       | 1830/6500 [5:32:34<13:47:50, 10.64s/it]                                                         28%|       | 1830/6500 [5:32:34<13:47:50, 10.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8096673488616943, 'eval_runtime': 5.2592, 'eval_samples_per_second': 4.373, 'eval_steps_per_second': 1.141, 'epoch': 0.28}
                                                         28%|       | 1830/6500 [5:32:39<13:47:50, 10.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1830I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1830
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1830/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5034, 'learning_rate': 8.169279485633282e-05, 'epoch': 0.28}
{'loss': 0.4965, 'learning_rate': 8.167409410403277e-05, 'epoch': 0.28}
{'loss': 0.4845, 'learning_rate': 8.16553859481058e-05, 'epoch': 0.28}
{'loss': 0.5449, 'learning_rate': 8.163667039292484e-05, 'epoch': 0.28}
{'loss': 0.4962, 'learning_rate': 8.161794744286453e-05, 'epoch': 0.28}
 28%|       | 1831/6500 [5:32:50<16:00:06, 12.34s/it]                                                         28%|       | 1831/6500 [5:32:50<16:00:06, 12.34s/it] 28%|       | 1832/6500 [5:33:01<15:14:21, 11.75s/it]                                                         28%|       | 1832/6500 [5:33:01<15:14:21, 11.75s/it] 28%|       | 1833/6500 [5:33:11<14:42:22, 11.34s/it]                                                         28%|       | 1833/6500 [5:33:11<14:42:22, 11.34s/it] 28%|       | 1834/6500 [5:33:22<14:19:56, 11.06s/it]                                                         28%|       | 1834/6500 [5:33:22<14:19:56, 11.06s/it] 28%|       | 1835/6500 [5:33:32<14:04:09, 10.86s/it]                                                         28%|       | 1835/6500 [5:33:32<14:04:09, 10.86s/it] 28%|       | 1836/6500 [5:33:42<13:52:54, 10.71s/it]                                        {'loss': 0.5133, 'learning_rate': 8.159921710230125e-05, 'epoch': 0.28}
{'loss': 0.5188, 'learning_rate': 8.158047937561309e-05, 'epoch': 0.28}
{'loss': 0.4833, 'learning_rate': 8.156173426717988e-05, 'epoch': 0.28}
{'loss': 0.4907, 'learning_rate': 8.15429817813832e-05, 'epoch': 0.28}
{'loss': 0.5117, 'learning_rate': 8.152422192260631e-05, 'epoch': 0.28}
                 28%|       | 1836/6500 [5:33:42<13:52:54, 10.71s/it] 28%|       | 1837/6500 [5:33:53<13:45:49, 10.63s/it]                                                         28%|       | 1837/6500 [5:33:53<13:45:49, 10.63s/it] 28%|       | 1838/6500 [5:34:03<13:40:13, 10.56s/it]                                                         28%|       | 1838/6500 [5:34:03<13:40:13, 10.56s/it] 28%|       | 1839/6500 [5:34:14<13:36:34, 10.51s/it]                                                         28%|       | 1839/6500 [5:34:14<13:36:34, 10.51s/it] 28%|       | 1840/6500 [5:34:24<13:33:35, 10.48s/it]                                                         28%|       | 1840/6500 [5:34:24<13:33:35, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8073616623878479, 'eval_runtime': 3.9841, 'eval_samples_per_second': 5.773, 'eval_steps_per_second': 1.506, 'epoch': 0.28}
                                                         28%|       | 1840/6500 [5:34:28<13:33:35, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1840/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1840/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5168, 'learning_rate': 8.150545469523421e-05, 'epoch': 0.28}
{'loss': 0.4842, 'learning_rate': 8.148668010365364e-05, 'epoch': 0.28}
{'loss': 0.494, 'learning_rate': 8.146789815225303e-05, 'epoch': 0.28}
{'loss': 0.4716, 'learning_rate': 8.144910884542256e-05, 'epoch': 0.28}
{'loss': 0.5846, 'learning_rate': 8.14303121875541e-05, 'epoch': 0.28}
 28%|       | 1841/6500 [5:34:39<15:15:13, 11.79s/it]                                                         28%|       | 1841/6500 [5:34:39<15:15:13, 11.79s/it] 28%|       | 1842/6500 [5:34:49<14:42:18, 11.37s/it]                                                         28%|       | 1842/6500 [5:34:49<14:42:18, 11.37s/it] 28%|       | 1843/6500 [5:35:00<14:26:14, 11.16s/it]                                                         28%|       | 1843/6500 [5:35:00<14:26:14, 11.16s/it] 28%|       | 1844/6500 [5:35:10<14:08:15, 10.93s/it]                                                         28%|       | 1844/6500 [5:35:10<14:08:15, 10.93s/it] 28%|       | 1845/6500 [5:35:21<13:55:14, 10.77s/it]                                                         28%|       | 1845/6500 [5:35:21<13:55:14, 10.77s/it] 28%|       | 1846/6500 [5:35:31<13:46:00, 10.65s/it]                                        {'loss': 0.4854, 'learning_rate': 8.141150818304129e-05, 'epoch': 0.28}
{'loss': 0.4855, 'learning_rate': 8.139269683627942e-05, 'epoch': 0.28}
{'loss': 0.4978, 'learning_rate': 8.137387815166551e-05, 'epoch': 0.28}
{'loss': 1.0209, 'learning_rate': 8.135505213359835e-05, 'epoch': 0.28}
{'loss': 0.5058, 'learning_rate': 8.133621878647842e-05, 'epoch': 0.28}
                 28%|       | 1846/6500 [5:35:31<13:46:00, 10.65s/it] 28%|       | 1847/6500 [5:35:41<13:39:34, 10.57s/it]                                                         28%|       | 1847/6500 [5:35:41<13:39:34, 10.57s/it] 28%|       | 1848/6500 [5:35:52<13:35:27, 10.52s/it]                                                         28%|       | 1848/6500 [5:35:52<13:35:27, 10.52s/it] 28%|       | 1849/6500 [5:36:02<13:31:54, 10.47s/it]                                                         28%|       | 1849/6500 [5:36:02<13:31:54, 10.47s/it] 28%|       | 1850/6500 [5:36:13<13:29:48, 10.45s/it]                                                         28%|       | 1850/6500 [5:36:13<13:29:48, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8138667941093445, 'eval_runtime': 3.9524, 'eval_samples_per_second': 5.819, 'eval_steps_per_second': 1.518, 'epoch': 0.28}
                                                         28%|       | 1850/6500 [5:36:17<13:29:48, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1850/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1850

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1850/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1850/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5029, 'learning_rate': 8.131737811470786e-05, 'epoch': 0.28}
{'loss': 0.4941, 'learning_rate': 8.129853012269056e-05, 'epoch': 0.28}
{'loss': 0.4636, 'learning_rate': 8.127967481483217e-05, 'epoch': 0.29}
{'loss': 0.5166, 'learning_rate': 8.126081219553995e-05, 'epoch': 0.29}
{'loss': 0.4846, 'learning_rate': 8.124194226922296e-05, 'epoch': 0.29}
 28%|       | 1851/6500 [5:36:27<15:11:28, 11.76s/it]                                                         28%|       | 1851/6500 [5:36:27<15:11:28, 11.76s/it] 28%|       | 1852/6500 [5:36:38<14:38:53, 11.35s/it]                                                         28%|       | 1852/6500 [5:36:38<14:38:53, 11.35s/it] 29%|       | 1853/6500 [5:36:48<14:16:24, 11.06s/it]                                                         29%|       | 1853/6500 [5:36:48<14:16:24, 11.06s/it] 29%|       | 1854/6500 [5:36:59<14:01:38, 10.87s/it]                                                         29%|       | 1854/6500 [5:36:59<14:01:38, 10.87s/it] 29%|       | 1855/6500 [5:37:09<13:49:58, 10.72s/it]                                                         29%|       | 1855/6500 [5:37:09<13:49:58, 10.72s/it] 29%|       | 1856/6500 [5:37:19<13:41:54, 10.62s/it]                                        {'loss': 0.4709, 'learning_rate': 8.122306504029194e-05, 'epoch': 0.29}
{'loss': 0.4673, 'learning_rate': 8.120418051315927e-05, 'epoch': 0.29}
{'loss': 0.4812, 'learning_rate': 8.118528869223914e-05, 'epoch': 0.29}
{'loss': 0.4914, 'learning_rate': 8.11663895819474e-05, 'epoch': 0.29}
{'loss': 0.4717, 'learning_rate': 8.114748318670159e-05, 'epoch': 0.29}
                 29%|       | 1856/6500 [5:37:19<13:41:54, 10.62s/it] 29%|       | 1857/6500 [5:37:30<13:36:16, 10.55s/it]                                                         29%|       | 1857/6500 [5:37:30<13:36:16, 10.55s/it] 29%|       | 1858/6500 [5:37:40<13:32:19, 10.50s/it]                                                         29%|       | 1858/6500 [5:37:40<13:32:19, 10.50s/it] 29%|       | 1859/6500 [5:37:51<13:44:34, 10.66s/it]                                                         29%|       | 1859/6500 [5:37:51<13:44:34, 10.66s/it] 29%|       | 1860/6500 [5:38:02<13:38:02, 10.58s/it]                                                         29%|       | 1860/6500 [5:38:02<13:38:02, 10.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8100119829177856, 'eval_runtime': 3.9686, 'eval_samples_per_second': 5.795, 'eval_steps_per_second': 1.512, 'epoch': 0.29}
                                                         29%|       | 1860/6500 [5:38:05<13:38:02, 10.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1860
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5183, 'learning_rate': 8.112856951092097e-05, 'epoch': 0.29}
{'loss': 0.4688, 'learning_rate': 8.110964855902647e-05, 'epoch': 0.29}
{'loss': 0.5242, 'learning_rate': 8.109072033544079e-05, 'epoch': 0.29}
{'loss': 0.5171, 'learning_rate': 8.107178484458824e-05, 'epoch': 0.29}
{'loss': 0.5131, 'learning_rate': 8.105284209089492e-05, 'epoch': 0.29}
 29%|       | 1861/6500 [5:38:16<15:17:37, 11.87s/it]                                                         29%|       | 1861/6500 [5:38:16<15:17:37, 11.87s/it] 29%|       | 1862/6500 [5:38:27<14:43:16, 11.43s/it]                                                         29%|       | 1862/6500 [5:38:27<14:43:16, 11.43s/it] 29%|       | 1863/6500 [5:38:37<14:19:04, 11.12s/it]                                                         29%|       | 1863/6500 [5:38:37<14:19:04, 11.12s/it] 29%|       | 1864/6500 [5:38:48<14:02:07, 10.90s/it]                                                         29%|       | 1864/6500 [5:38:48<14:02:07, 10.90s/it] 29%|       | 1865/6500 [5:38:58<13:50:13, 10.75s/it]                                                         29%|       | 1865/6500 [5:38:58<13:50:13, 10.75s/it] 29%|       | 1866/6500 [5:39:08<13:41:31, 10.64s/it]                                        {'loss': 0.5054, 'learning_rate': 8.103389207878856e-05, 'epoch': 0.29}
{'loss': 0.5006, 'learning_rate': 8.101493481269862e-05, 'epoch': 0.29}
{'loss': 0.4912, 'learning_rate': 8.099597029705625e-05, 'epoch': 0.29}
{'loss': 0.4769, 'learning_rate': 8.097699853629426e-05, 'epoch': 0.29}
{'loss': 0.5195, 'learning_rate': 8.095801953484723e-05, 'epoch': 0.29}
                 29%|       | 1866/6500 [5:39:08<13:41:31, 10.64s/it] 29%|       | 1867/6500 [5:39:19<13:35:58, 10.57s/it]                                                         29%|       | 1867/6500 [5:39:19<13:35:58, 10.57s/it] 29%|       | 1868/6500 [5:39:29<13:31:49, 10.52s/it]                                                         29%|       | 1868/6500 [5:39:29<13:31:49, 10.52s/it] 29%|       | 1869/6500 [5:39:40<13:28:44, 10.48s/it]                                                         29%|       | 1869/6500 [5:39:40<13:28:44, 10.48s/it] 29%|       | 1870/6500 [5:39:50<13:27:19, 10.46s/it]                                                         29%|       | 1870/6500 [5:39:50<13:27:19, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8055536150932312, 'eval_runtime': 3.9688, 'eval_samples_per_second': 5.795, 'eval_steps_per_second': 1.512, 'epoch': 0.29}
                                                         29%|       | 1870/6500 [5:39:54<13:27:19, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4875, 'learning_rate': 8.093903329715135e-05, 'epoch': 0.29}
{'loss': 0.4961, 'learning_rate': 8.092003982764455e-05, 'epoch': 0.29}
{'loss': 0.4793, 'learning_rate': 8.090103913076642e-05, 'epoch': 0.29}
{'loss': 0.5191, 'learning_rate': 8.08820312109583e-05, 'epoch': 0.29}
{'loss': 0.5586, 'learning_rate': 8.086301607266314e-05, 'epoch': 0.29}
 29%|       | 1871/6500 [5:40:05<15:07:58, 11.77s/it]                                                         29%|       | 1871/6500 [5:40:05<15:07:58, 11.77s/it] 29%|       | 1872/6500 [5:40:15<14:35:31, 11.35s/it]                                                         29%|       | 1872/6500 [5:40:15<14:35:31, 11.35s/it] 29%|       | 1873/6500 [5:40:26<14:13:15, 11.06s/it]                                                         29%|       | 1873/6500 [5:40:26<14:13:15, 11.06s/it] 29%|       | 1874/6500 [5:40:37<14:23:55, 11.21s/it]                                                         29%|       | 1874/6500 [5:40:37<14:23:55, 11.21s/it] 29%|       | 1875/6500 [5:40:48<14:14:43, 11.09s/it]                                                         29%|       | 1875/6500 [5:40:48<14:14:43, 11.09s/it] 29%|       | 1876/6500 [5:40:58<13:58:10, 10.88s/it]                                        {'loss': 0.4843, 'learning_rate': 8.084399372032562e-05, 'epoch': 0.29}
{'loss': 0.4818, 'learning_rate': 8.082496415839212e-05, 'epoch': 0.29}
{'loss': 0.488, 'learning_rate': 8.080592739131063e-05, 'epoch': 0.29}
{'loss': 1.006, 'learning_rate': 8.078688342353095e-05, 'epoch': 0.29}
{'loss': 0.5121, 'learning_rate': 8.076783225950444e-05, 'epoch': 0.29}
                 29%|       | 1876/6500 [5:40:58<13:58:10, 10.88s/it] 29%|       | 1877/6500 [5:41:09<13:46:03, 10.72s/it]                                                         29%|       | 1877/6500 [5:41:09<13:46:03, 10.72s/it] 29%|       | 1878/6500 [5:41:19<13:37:57, 10.62s/it]                                                         29%|       | 1878/6500 [5:41:19<13:37:57, 10.62s/it] 29%|       | 1879/6500 [5:41:29<13:31:27, 10.54s/it]                                                         29%|       | 1879/6500 [5:41:29<13:31:27, 10.54s/it] 29%|       | 1880/6500 [5:41:40<13:27:20, 10.48s/it]                                                         29%|       | 1880/6500 [5:41:40<13:27:20, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8120742440223694, 'eval_runtime': 3.9898, 'eval_samples_per_second': 5.765, 'eval_steps_per_second': 1.504, 'epoch': 0.29}
                                                         29%|       | 1880/6500 [5:41:44<13:27:20, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1880
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1880/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1880/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4923, 'learning_rate': 8.074877390368423e-05, 'epoch': 0.29}
{'loss': 0.4679, 'learning_rate': 8.072970836052504e-05, 'epoch': 0.29}
{'loss': 0.4886, 'learning_rate': 8.07106356344834e-05, 'epoch': 0.29}
{'loss': 0.5128, 'learning_rate': 8.069155573001739e-05, 'epoch': 0.29}
{'loss': 0.4849, 'learning_rate': 8.067246865158682e-05, 'epoch': 0.29}
 29%|       | 1881/6500 [5:41:55<15:10:14, 11.82s/it]                                                         29%|       | 1881/6500 [5:41:55<15:10:14, 11.82s/it] 29%|       | 1882/6500 [5:42:05<14:36:57, 11.39s/it]                                                         29%|       | 1882/6500 [5:42:05<14:36:57, 11.39s/it] 29%|       | 1883/6500 [5:42:15<14:14:06, 11.10s/it]                                                         29%|       | 1883/6500 [5:42:15<14:14:06, 11.10s/it] 29%|       | 1884/6500 [5:42:26<14:00:36, 10.93s/it]                                                         29%|       | 1884/6500 [5:42:26<14:00:36, 10.93s/it] 29%|       | 1885/6500 [5:42:36<13:47:59, 10.76s/it]                                                         29%|       | 1885/6500 [5:42:36<13:47:59, 10.76s/it] 29%|       | 1886/6500 [5:42:47<13:39:14, 10.65s/it]                                        {'loss': 0.4826, 'learning_rate': 8.065337440365321e-05, 'epoch': 0.29}
{'loss': 0.4639, 'learning_rate': 8.06342729906797e-05, 'epoch': 0.29}
{'loss': 0.4783, 'learning_rate': 8.061516441713115e-05, 'epoch': 0.29}
{'loss': 0.4858, 'learning_rate': 8.059604868747405e-05, 'epoch': 0.29}
{'loss': 0.4758, 'learning_rate': 8.057692580617659e-05, 'epoch': 0.29}
                 29%|       | 1886/6500 [5:42:47<13:39:14, 10.65s/it] 29%|       | 1887/6500 [5:42:57<13:32:36, 10.57s/it]                                                         29%|       | 1887/6500 [5:42:57<13:32:36, 10.57s/it] 29%|       | 1888/6500 [5:43:08<13:28:17, 10.52s/it]                                                         29%|       | 1888/6500 [5:43:08<13:28:17, 10.52s/it] 29%|       | 1889/6500 [5:43:18<13:25:09, 10.48s/it]                                                         29%|       | 1889/6500 [5:43:18<13:25:09, 10.48s/it] 29%|       | 1890/6500 [5:43:28<13:22:51, 10.45s/it]                                                         29%|       | 1890/6500 [5:43:28<13:22:51, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8107123970985413, 'eval_runtime': 3.9816, 'eval_samples_per_second': 5.777, 'eval_steps_per_second': 1.507, 'epoch': 0.29}
                                                         29%|       | 1890/6500 [5:43:32<13:22:51, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1890/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1890/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5027, 'learning_rate': 8.055779577770866e-05, 'epoch': 0.29}
{'loss': 0.4746, 'learning_rate': 8.053865860654175e-05, 'epoch': 0.29}
{'loss': 0.5122, 'learning_rate': 8.051951429714906e-05, 'epoch': 0.29}
{'loss': 0.4976, 'learning_rate': 8.050036285400546e-05, 'epoch': 0.29}
{'loss': 0.4737, 'learning_rate': 8.04812042815875e-05, 'epoch': 0.29}
 29%|       | 1891/6500 [5:43:44<15:24:16, 12.03s/it]                                                         29%|       | 1891/6500 [5:43:44<15:24:16, 12.03s/it] 29%|       | 1892/6500 [5:43:54<14:46:23, 11.54s/it]                                                         29%|       | 1892/6500 [5:43:54<14:46:23, 11.54s/it] 29%|       | 1893/6500 [5:44:05<14:19:38, 11.20s/it]                                                         29%|       | 1893/6500 [5:44:05<14:19:38, 11.20s/it] 29%|       | 1894/6500 [5:44:15<14:00:32, 10.95s/it]                                                         29%|       | 1894/6500 [5:44:15<14:00:32, 10.95s/it] 29%|       | 1895/6500 [5:44:26<13:48:17, 10.79s/it]                                                         29%|       | 1895/6500 [5:44:26<13:48:17, 10.79s/it] 29%|       | 1896/6500 [5:44:36<13:39:14, 10.68s/it]                                        {'loss': 0.4984, 'learning_rate': 8.046203858437337e-05, 'epoch': 0.29}
{'loss': 0.4857, 'learning_rate': 8.044286576684293e-05, 'epoch': 0.29}
{'loss': 0.4902, 'learning_rate': 8.04236858334777e-05, 'epoch': 0.29}
{'loss': 0.48, 'learning_rate': 8.04044987887609e-05, 'epoch': 0.29}
{'loss': 0.5141, 'learning_rate': 8.038530463717738e-05, 'epoch': 0.29}
                 29%|       | 1896/6500 [5:44:36<13:39:14, 10.68s/it] 29%|       | 1897/6500 [5:44:46<13:33:09, 10.60s/it]                                                         29%|       | 1897/6500 [5:44:46<13:33:09, 10.60s/it] 29%|       | 1898/6500 [5:44:57<13:30:46, 10.57s/it]                                                         29%|       | 1898/6500 [5:44:57<13:30:46, 10.57s/it] 29%|       | 1899/6500 [5:45:07<13:26:51, 10.52s/it]                                                         29%|       | 1899/6500 [5:45:07<13:26:51, 10.52s/it] 29%|       | 1900/6500 [5:45:20<14:09:31, 11.08s/it]                                                         29%|       | 1900/6500 [5:45:20<14:09:31, 11.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8047990202903748, 'eval_runtime': 4.5444, 'eval_samples_per_second': 5.061, 'eval_steps_per_second': 1.32, 'epoch': 0.29}
                                                         29%|       | 1900/6500 [5:45:24<14:09:31, 11.08s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4855, 'learning_rate': 8.036610338321362e-05, 'epoch': 0.29}
{'loss': 0.4911, 'learning_rate': 8.034689503135783e-05, 'epoch': 0.29}
{'loss': 0.4597, 'learning_rate': 8.032767958609986e-05, 'epoch': 0.29}
{'loss': 0.533, 'learning_rate': 8.030845705193116e-05, 'epoch': 0.29}
{'loss': 0.5271, 'learning_rate': 8.028922743334492e-05, 'epoch': 0.29}
 29%|       | 1901/6500 [5:45:35<15:48:28, 12.37s/it]                                                         29%|       | 1901/6500 [5:45:35<15:48:28, 12.37s/it] 29%|       | 1902/6500 [5:45:46<15:02:37, 11.78s/it]                                                         29%|       | 1902/6500 [5:45:46<15:02:37, 11.78s/it] 29%|       | 1903/6500 [5:45:56<14:31:53, 11.38s/it]                                                         29%|       | 1903/6500 [5:45:56<14:31:53, 11.38s/it] 29%|       | 1904/6500 [5:46:06<14:08:34, 11.08s/it]                                                         29%|       | 1904/6500 [5:46:06<14:08:34, 11.08s/it] 29%|       | 1905/6500 [5:46:17<13:52:29, 10.87s/it]                                                         29%|       | 1905/6500 [5:46:17<13:52:29, 10.87s/it] 29%|       | 1906/6500 [5:46:27<13:41:27, 10.73s/it]                                        {'loss': 0.4675, 'learning_rate': 8.026999073483593e-05, 'epoch': 0.29}
{'loss': 0.4792, 'learning_rate': 8.025074696090063e-05, 'epoch': 0.29}
{'loss': 0.4873, 'learning_rate': 8.023149611603717e-05, 'epoch': 0.29}
{'loss': 1.0174, 'learning_rate': 8.021223820474529e-05, 'epoch': 0.29}
{'loss': 0.5132, 'learning_rate': 8.019297323152642e-05, 'epoch': 0.29}
                 29%|       | 1906/6500 [5:46:27<13:41:27, 10.73s/it] 29%|       | 1907/6500 [5:46:38<13:40:00, 10.71s/it]                                                         29%|       | 1907/6500 [5:46:38<13:40:00, 10.71s/it] 29%|       | 1908/6500 [5:46:48<13:32:49, 10.62s/it]                                                         29%|       | 1908/6500 [5:46:48<13:32:49, 10.62s/it] 29%|       | 1909/6500 [5:46:59<13:28:14, 10.56s/it]                                                         29%|       | 1909/6500 [5:46:59<13:28:14, 10.56s/it] 29%|       | 1910/6500 [5:47:09<13:25:13, 10.53s/it]                                                         29%|       | 1910/6500 [5:47:09<13:25:13, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8100149631500244, 'eval_runtime': 3.9709, 'eval_samples_per_second': 5.792, 'eval_steps_per_second': 1.511, 'epoch': 0.29}
                                                         29%|       | 1910/6500 [5:47:13<13:25:13, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1910/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1910/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4927, 'learning_rate': 8.017370120088365e-05, 'epoch': 0.29}
{'loss': 0.4536, 'learning_rate': 8.015442211732167e-05, 'epoch': 0.29}
{'loss': 0.5034, 'learning_rate': 8.013513598534688e-05, 'epoch': 0.29}
{'loss': 0.5035, 'learning_rate': 8.011584280946726e-05, 'epoch': 0.29}
{'loss': 0.4525, 'learning_rate': 8.00965425941925e-05, 'epoch': 0.29}
 29%|       | 1911/6500 [5:47:24<15:05:31, 11.84s/it]                                                         29%|       | 1911/6500 [5:47:24<15:05:31, 11.84s/it] 29%|       | 1912/6500 [5:47:34<14:32:44, 11.41s/it]                                                         29%|       | 1912/6500 [5:47:34<14:32:44, 11.41s/it] 29%|       | 1913/6500 [5:47:45<14:09:41, 11.11s/it]                                                         29%|       | 1913/6500 [5:47:45<14:09:41, 11.11s/it] 29%|       | 1914/6500 [5:47:55<13:53:28, 10.90s/it]                                                         29%|       | 1914/6500 [5:47:55<13:53:28, 10.90s/it] 29%|       | 1915/6500 [5:48:06<13:42:14, 10.76s/it]                                                         29%|       | 1915/6500 [5:48:06<13:42:14, 10.76s/it] 29%|       | 1916/6500 [5:48:16<13:33:51, 10.65s/it]                                        {'loss': 0.4866, 'learning_rate': 8.007723534403389e-05, 'epoch': 0.29}
{'loss': 0.4671, 'learning_rate': 8.005792106350441e-05, 'epoch': 0.29}
{'loss': 0.475, 'learning_rate': 8.003859975711862e-05, 'epoch': 0.3}
{'loss': 0.4758, 'learning_rate': 8.001927142939278e-05, 'epoch': 0.3}
{'loss': 0.4882, 'learning_rate': 7.999993608484477e-05, 'epoch': 0.3}
                 29%|       | 1916/6500 [5:48:16<13:33:51, 10.65s/it] 29%|       | 1917/6500 [5:48:26<13:28:19, 10.58s/it]                                                         29%|       | 1917/6500 [5:48:26<13:28:19, 10.58s/it] 30%|       | 1918/6500 [5:48:37<13:24:21, 10.53s/it]                                                         30%|       | 1918/6500 [5:48:37<13:24:21, 10.53s/it] 30%|       | 1919/6500 [5:48:47<13:21:41, 10.50s/it]                                                         30%|       | 1919/6500 [5:48:47<13:21:41, 10.50s/it] 30%|       | 1920/6500 [5:48:58<13:19:30, 10.47s/it]                                                         30%|       | 1920/6500 [5:48:58<13:19:30, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.808393657207489, 'eval_runtime': 3.9625, 'eval_samples_per_second': 5.804, 'eval_steps_per_second': 1.514, 'epoch': 0.3}
                                                         30%|       | 1920/6500 [5:49:02<13:19:30, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4733, 'learning_rate': 7.998059372799409e-05, 'epoch': 0.3}
{'loss': 0.4724, 'learning_rate': 7.996124436336192e-05, 'epoch': 0.3}
{'loss': 0.5391, 'learning_rate': 7.994188799547105e-05, 'epoch': 0.3}
{'loss': 0.4824, 'learning_rate': 7.99225246288459e-05, 'epoch': 0.3}
{'loss': 0.5032, 'learning_rate': 7.990315426801255e-05, 'epoch': 0.3}
 30%|       | 1921/6500 [5:49:13<15:00:18, 11.80s/it]                                                         30%|       | 1921/6500 [5:49:13<15:00:18, 11.80s/it] 30%|       | 1922/6500 [5:49:23<14:28:41, 11.39s/it]                                                         30%|       | 1922/6500 [5:49:23<14:28:41, 11.39s/it] 30%|       | 1923/6500 [5:49:33<14:06:21, 11.09s/it]                                                         30%|       | 1923/6500 [5:49:33<14:06:21, 11.09s/it] 30%|       | 1924/6500 [5:49:45<14:09:37, 11.14s/it]                                                         30%|       | 1924/6500 [5:49:45<14:09:37, 11.14s/it] 30%|       | 1925/6500 [5:49:55<13:53:01, 10.92s/it]                                                         30%|       | 1925/6500 [5:49:55<13:53:01, 10.92s/it] 30%|       | 1926/6500 [5:50:06<13:41:37, 10.78s/it]                                        {'loss': 0.5027, 'learning_rate': 7.988377691749871e-05, 'epoch': 0.3}
{'loss': 0.4779, 'learning_rate': 7.986439258183372e-05, 'epoch': 0.3}
{'loss': 0.4871, 'learning_rate': 7.984500126554853e-05, 'epoch': 0.3}
{'loss': 0.4839, 'learning_rate': 7.982560297317575e-05, 'epoch': 0.3}
{'loss': 0.4899, 'learning_rate': 7.980619770924962e-05, 'epoch': 0.3}
                 30%|       | 1926/6500 [5:50:06<13:41:37, 10.78s/it] 30%|       | 1927/6500 [5:50:16<13:33:17, 10.67s/it]                                                         30%|       | 1927/6500 [5:50:16<13:33:17, 10.67s/it] 30%|       | 1928/6500 [5:50:26<13:27:26, 10.60s/it]                                                         30%|       | 1928/6500 [5:50:26<13:27:26, 10.60s/it] 30%|       | 1929/6500 [5:50:37<13:23:33, 10.55s/it]                                                         30%|       | 1929/6500 [5:50:37<13:23:33, 10.55s/it] 30%|       | 1930/6500 [5:50:47<13:20:39, 10.51s/it]                                                         30%|       | 1930/6500 [5:50:47<13:20:39, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8074111938476562, 'eval_runtime': 3.9786, 'eval_samples_per_second': 5.781, 'eval_steps_per_second': 1.508, 'epoch': 0.3}
                                                         30%|       | 1930/6500 [5:50:51<13:20:39, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1930/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.479, 'learning_rate': 7.9786785478306e-05, 'epoch': 0.3}
{'loss': 0.4633, 'learning_rate': 7.97673662848824e-05, 'epoch': 0.3}
{'loss': 0.4911, 'learning_rate': 7.974794013351789e-05, 'epoch': 0.3}
{'loss': 0.5505, 'learning_rate': 7.972850702875327e-05, 'epoch': 0.3}
{'loss': 0.4835, 'learning_rate': 7.970906697513088e-05, 'epoch': 0.3}
 30%|       | 1931/6500 [5:51:02<14:59:58, 11.82s/it]                                                         30%|       | 1931/6500 [5:51:02<14:59:58, 11.82s/it] 30%|       | 1932/6500 [5:51:13<14:28:12, 11.40s/it]                                                         30%|       | 1932/6500 [5:51:13<14:28:12, 11.40s/it] 30%|       | 1933/6500 [5:51:23<14:05:25, 11.11s/it]                                                         30%|       | 1933/6500 [5:51:23<14:05:25, 11.11s/it] 30%|       | 1934/6500 [5:51:33<13:49:54, 10.91s/it]                                                         30%|       | 1934/6500 [5:51:33<13:49:54, 10.91s/it] 30%|       | 1935/6500 [5:51:44<13:38:40, 10.76s/it]                                                         30%|       | 1935/6500 [5:51:44<13:38:40, 10.76s/it] 30%|       | 1936/6500 [5:51:54<13:31:06, 10.66s/it]                                        {'loss': 0.471, 'learning_rate': 7.96896199771947e-05, 'epoch': 0.3}
{'loss': 0.4949, 'learning_rate': 7.96701660394904e-05, 'epoch': 0.3}
{'loss': 1.011, 'learning_rate': 7.965070516656517e-05, 'epoch': 0.3}
{'loss': 0.4982, 'learning_rate': 7.963123736296787e-05, 'epoch': 0.3}
{'loss': 0.4801, 'learning_rate': 7.961176263324901e-05, 'epoch': 0.3}
                 30%|       | 1936/6500 [5:51:54<13:31:06, 10.66s/it] 30%|       | 1937/6500 [5:52:05<13:25:24, 10.59s/it]                                                         30%|       | 1937/6500 [5:52:05<13:25:24, 10.59s/it] 30%|       | 1938/6500 [5:52:15<13:21:19, 10.54s/it]                                                         30%|       | 1938/6500 [5:52:15<13:21:19, 10.54s/it] 30%|       | 1939/6500 [5:52:26<13:18:56, 10.51s/it]                                                         30%|       | 1939/6500 [5:52:26<13:18:56, 10.51s/it] 30%|       | 1940/6500 [5:52:36<13:25:15, 10.60s/it]                                                         30%|       | 1940/6500 [5:52:36<13:25:15, 10.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8095506429672241, 'eval_runtime': 3.9531, 'eval_samples_per_second': 5.818, 'eval_steps_per_second': 1.518, 'epoch': 0.3}
                                                         30%|       | 1940/6500 [5:52:40<13:25:15, 10.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1940
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4769, 'learning_rate': 7.959228098196067e-05, 'epoch': 0.3}
{'loss': 0.4583, 'learning_rate': 7.957279241365659e-05, 'epoch': 0.3}
{'loss': 0.5102, 'learning_rate': 7.955329693289207e-05, 'epoch': 0.3}
{'loss': 0.4722, 'learning_rate': 7.953379454422409e-05, 'epoch': 0.3}
{'loss': 0.4641, 'learning_rate': 7.951428525221121e-05, 'epoch': 0.3}
 30%|       | 1941/6500 [5:52:51<15:02:56, 11.88s/it]                                                         30%|       | 1941/6500 [5:52:51<15:02:56, 11.88s/it] 30%|       | 1942/6500 [5:53:02<14:28:49, 11.44s/it]                                                         30%|       | 1942/6500 [5:53:02<14:28:49, 11.44s/it] 30%|       | 1943/6500 [5:53:12<14:05:21, 11.13s/it]                                                         30%|       | 1943/6500 [5:53:12<14:05:21, 11.13s/it] 30%|       | 1944/6500 [5:53:22<13:49:07, 10.92s/it]                                                         30%|       | 1944/6500 [5:53:22<13:49:07, 10.92s/it] 30%|       | 1945/6500 [5:53:33<13:38:04, 10.78s/it]                                                         30%|       | 1945/6500 [5:53:33<13:38:04, 10.78s/it] 30%|       | 1946/6500 [5:53:43<13:29:22, 10.66s/it]                                        {'loss': 0.4601, 'learning_rate': 7.94947690614136e-05, 'epoch': 0.3}
{'loss': 0.4733, 'learning_rate': 7.947524597639304e-05, 'epoch': 0.3}
{'loss': 0.4723, 'learning_rate': 7.945571600171295e-05, 'epoch': 0.3}
{'loss': 0.461, 'learning_rate': 7.943617914193833e-05, 'epoch': 0.3}
{'loss': 0.5023, 'learning_rate': 7.941663540163582e-05, 'epoch': 0.3}
                 30%|       | 1946/6500 [5:53:43<13:29:22, 10.66s/it] 30%|       | 1947/6500 [5:53:54<13:23:55, 10.59s/it]                                                         30%|       | 1947/6500 [5:53:54<13:23:55, 10.59s/it] 30%|       | 1948/6500 [5:54:04<13:19:42, 10.54s/it]                                                         30%|       | 1948/6500 [5:54:04<13:19:42, 10.54s/it] 30%|       | 1949/6500 [5:54:15<13:16:35, 10.50s/it]                                                         30%|       | 1949/6500 [5:54:15<13:16:35, 10.50s/it] 30%|       | 1950/6500 [5:54:25<13:14:45, 10.48s/it]                                                         30%|       | 1950/6500 [5:54:25<13:14:45, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8083795309066772, 'eval_runtime': 3.9809, 'eval_samples_per_second': 5.778, 'eval_steps_per_second': 1.507, 'epoch': 0.3}
                                                         30%|       | 1950/6500 [5:54:29<13:14:45, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1950/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4659, 'learning_rate': 7.939708478537364e-05, 'epoch': 0.3}
{'loss': 0.5021, 'learning_rate': 7.937752729772164e-05, 'epoch': 0.3}
{'loss': 0.4937, 'learning_rate': 7.935796294325124e-05, 'epoch': 0.3}
{'loss': 0.4782, 'learning_rate': 7.93383917265355e-05, 'epoch': 0.3}
{'loss': 0.4861, 'learning_rate': 7.931881365214908e-05, 'epoch': 0.3}
 30%|       | 1951/6500 [5:54:40<14:54:36, 11.80s/it]                                                         30%|       | 1951/6500 [5:54:40<14:54:36, 11.80s/it] 30%|       | 1952/6500 [5:54:50<14:24:32, 11.41s/it]                                                         30%|       | 1952/6500 [5:54:50<14:24:32, 11.41s/it] 30%|       | 1953/6500 [5:55:01<14:02:25, 11.12s/it]                                                         30%|       | 1953/6500 [5:55:01<14:02:25, 11.12s/it] 30%|       | 1954/6500 [5:55:11<13:46:38, 10.91s/it]                                                         30%|       | 1954/6500 [5:55:11<13:46:38, 10.91s/it] 30%|       | 1955/6500 [5:55:22<13:35:41, 10.77s/it]                                                         30%|       | 1955/6500 [5:55:22<13:35:41, 10.77s/it] 30%|       | 1956/6500 [5:55:32<13:33:23, 10.74s/it]                                        {'loss': 0.4778, 'learning_rate': 7.929922872466823e-05, 'epoch': 0.3}
{'loss': 0.4684, 'learning_rate': 7.92796369486708e-05, 'epoch': 0.3}
{'loss': 0.4824, 'learning_rate': 7.926003832873627e-05, 'epoch': 0.3}
{'loss': 0.5082, 'learning_rate': 7.924043286944569e-05, 'epoch': 0.3}
{'loss': 0.4707, 'learning_rate': 7.92208205753817e-05, 'epoch': 0.3}
                 30%|       | 1956/6500 [5:55:32<13:33:23, 10.74s/it] 30%|       | 1957/6500 [5:55:43<13:25:58, 10.64s/it]                                                         30%|       | 1957/6500 [5:55:43<13:25:58, 10.64s/it] 30%|       | 1958/6500 [5:55:53<13:20:46, 10.58s/it]                                                         30%|       | 1958/6500 [5:55:53<13:20:46, 10.58s/it] 30%|       | 1959/6500 [5:56:04<13:17:46, 10.54s/it]                                                         30%|       | 1959/6500 [5:56:04<13:17:46, 10.54s/it] 30%|       | 1960/6500 [5:56:14<13:15:08, 10.51s/it]                                                         30%|       | 1960/6500 [5:56:14<13:15:08, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8047692775726318, 'eval_runtime': 3.9792, 'eval_samples_per_second': 5.78, 'eval_steps_per_second': 1.508, 'epoch': 0.3}
                                                         30%|       | 1960/6500 [5:56:18<13:15:08, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1960/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1960/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1960/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4819, 'learning_rate': 7.920120145112855e-05, 'epoch': 0.3}
{'loss': 0.4623, 'learning_rate': 7.918157550127213e-05, 'epoch': 0.3}
{'loss': 0.5092, 'learning_rate': 7.916194273039986e-05, 'epoch': 0.3}
{'loss': 0.5388, 'learning_rate': 7.914230314310079e-05, 'epoch': 0.3}
{'loss': 0.4691, 'learning_rate': 7.912265674396553e-05, 'epoch': 0.3}
 30%|       | 1961/6500 [5:56:29<14:54:51, 11.83s/it]                                                         30%|       | 1961/6500 [5:56:29<14:54:51, 11.83s/it] 30%|       | 1962/6500 [5:56:39<14:22:37, 11.41s/it]                                                         30%|       | 1962/6500 [5:56:39<14:22:37, 11.41s/it] 30%|       | 1963/6500 [5:56:50<14:00:14, 11.11s/it]                                                         30%|       | 1963/6500 [5:56:50<14:00:14, 11.11s/it] 30%|       | 1964/6500 [5:57:00<13:44:49, 10.91s/it]                                                         30%|       | 1964/6500 [5:57:00<13:44:49, 10.91s/it] 30%|       | 1965/6500 [5:57:11<13:33:53, 10.77s/it]                                                         30%|       | 1965/6500 [5:57:11<13:33:53, 10.77s/it] 30%|       | 1966/6500 [5:57:21<13:26:05, 10.67s/it]                                        {'loss': 0.4784, 'learning_rate': 7.910300353758633e-05, 'epoch': 0.3}
{'loss': 0.4573, 'learning_rate': 7.9083343528557e-05, 'epoch': 0.3}
{'loss': 1.0037, 'learning_rate': 7.906367672147297e-05, 'epoch': 0.3}
{'loss': 0.4965, 'learning_rate': 7.904400312093119e-05, 'epoch': 0.3}
{'loss': 0.4855, 'learning_rate': 7.902432273153028e-05, 'epoch': 0.3}
                 30%|       | 1966/6500 [5:57:21<13:26:05, 10.67s/it] 30%|       | 1967/6500 [5:57:32<13:20:21, 10.59s/it]                                                         30%|       | 1967/6500 [5:57:32<13:20:21, 10.59s/it] 30%|       | 1968/6500 [5:57:42<13:16:13, 10.54s/it]                                                         30%|       | 1968/6500 [5:57:42<13:16:13, 10.54s/it] 30%|       | 1969/6500 [5:57:52<13:13:52, 10.51s/it]                                                         30%|       | 1969/6500 [5:57:52<13:13:52, 10.51s/it] 30%|       | 1970/6500 [5:58:03<13:12:00, 10.49s/it]                                                         30%|       | 1970/6500 [5:58:03<13:12:00, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8111518621444702, 'eval_runtime': 4.2118, 'eval_samples_per_second': 5.461, 'eval_steps_per_second': 1.425, 'epoch': 0.3}
                                                         30%|       | 1970/6500 [5:58:07<13:12:00, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1970I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4467, 'learning_rate': 7.900463555787042e-05, 'epoch': 0.3}
{'loss': 0.476, 'learning_rate': 7.898494160455334e-05, 'epoch': 0.3}
{'loss': 0.5039, 'learning_rate': 7.896524087618238e-05, 'epoch': 0.3}
{'loss': 0.4614, 'learning_rate': 7.894553337736248e-05, 'epoch': 0.3}
{'loss': 0.463, 'learning_rate': 7.892581911270013e-05, 'epoch': 0.3}
 30%|       | 1971/6500 [5:58:18<14:57:25, 11.89s/it]                                                         30%|       | 1971/6500 [5:58:18<14:57:25, 11.89s/it] 30%|       | 1972/6500 [5:58:29<14:33:08, 11.57s/it]                                                         30%|       | 1972/6500 [5:58:29<14:33:08, 11.57s/it] 30%|       | 1973/6500 [5:58:39<14:06:12, 11.22s/it]                                                         30%|       | 1973/6500 [5:58:39<14:06:12, 11.22s/it] 30%|       | 1974/6500 [5:58:50<13:48:01, 10.98s/it]                                                         30%|       | 1974/6500 [5:58:50<13:48:01, 10.98s/it] 30%|       | 1975/6500 [5:59:00<13:35:40, 10.82s/it]                                                         30%|       | 1975/6500 [5:59:00<13:35:40, 10.82s/it] 30%|       | 1976/6500 [5:59:11<13:26:27, 10.70s/it]                                        {'loss': 0.4508, 'learning_rate': 7.890609808680347e-05, 'epoch': 0.3}
{'loss': 0.4735, 'learning_rate': 7.888637030428211e-05, 'epoch': 0.3}
{'loss': 0.4639, 'learning_rate': 7.886663576974733e-05, 'epoch': 0.3}
{'loss': 0.4623, 'learning_rate': 7.884689448781196e-05, 'epoch': 0.3}
{'loss': 0.491, 'learning_rate': 7.882714646309038e-05, 'epoch': 0.3}
                 30%|       | 1976/6500 [5:59:11<13:26:27, 10.70s/it] 30%|       | 1977/6500 [5:59:21<13:20:10, 10.61s/it]                                                         30%|       | 1977/6500 [5:59:21<13:20:10, 10.61s/it] 30%|       | 1978/6500 [5:59:31<13:15:48, 10.56s/it]                                                         30%|       | 1978/6500 [5:59:31<13:15:48, 10.56s/it] 30%|       | 1979/6500 [5:59:42<13:12:27, 10.52s/it]                                                         30%|       | 1979/6500 [5:59:42<13:12:27, 10.52s/it] 30%|       | 1980/6500 [5:59:52<13:10:19, 10.49s/it]                                                         30%|       | 1980/6500 [5:59:52<13:10:19, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8073629140853882, 'eval_runtime': 3.9828, 'eval_samples_per_second': 5.775, 'eval_steps_per_second': 1.506, 'epoch': 0.3}
                                                         30%|       | 1980/6500 [5:59:56<13:10:19, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1980
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4544, 'learning_rate': 7.88073917001986e-05, 'epoch': 0.3}
{'loss': 0.5167, 'learning_rate': 7.878763020375415e-05, 'epoch': 0.3}
{'loss': 0.476, 'learning_rate': 7.876786197837617e-05, 'epoch': 0.31}
{'loss': 0.4741, 'learning_rate': 7.874808702868536e-05, 'epoch': 0.31}
{'loss': 0.499, 'learning_rate': 7.872830535930401e-05, 'epoch': 0.31}
 30%|       | 1981/6500 [6:00:07<14:50:53, 11.83s/it]                                                         30%|       | 1981/6500 [6:00:07<14:50:53, 11.83s/it] 30%|       | 1982/6500 [6:00:18<14:18:56, 11.41s/it]                                                         30%|       | 1982/6500 [6:00:18<14:18:56, 11.41s/it] 31%|       | 1983/6500 [6:00:28<13:56:19, 11.11s/it]                                                         31%|       | 1983/6500 [6:00:28<13:56:19, 11.11s/it] 31%|       | 1984/6500 [6:00:38<13:40:48, 10.91s/it]                                                         31%|       | 1984/6500 [6:00:38<13:40:48, 10.91s/it] 31%|       | 1985/6500 [6:00:49<13:29:51, 10.76s/it]                                                         31%|       | 1985/6500 [6:00:49<13:29:51, 10.76s/it] 31%|       | 1986/6500 [6:00:59<13:22:12, 10.66s/it]                                        {'loss': 0.4689, 'learning_rate': 7.870851697485596e-05, 'epoch': 0.31}
{'loss': 0.4846, 'learning_rate': 7.868872187996659e-05, 'epoch': 0.31}
{'loss': 0.4673, 'learning_rate': 7.866892007926294e-05, 'epoch': 0.31}
{'loss': 0.4937, 'learning_rate': 7.864911157737352e-05, 'epoch': 0.31}
{'loss': 0.4686, 'learning_rate': 7.862929637892845e-05, 'epoch': 0.31}
                 31%|       | 1986/6500 [6:00:59<13:22:12, 10.66s/it] 31%|       | 1987/6500 [6:01:10<13:16:44, 10.59s/it]                                                         31%|       | 1987/6500 [6:01:10<13:16:44, 10.59s/it] 31%|       | 1988/6500 [6:01:20<13:19:05, 10.63s/it]                                                         31%|       | 1988/6500 [6:01:20<13:19:05, 10.63s/it] 31%|       | 1989/6500 [6:01:31<13:14:44, 10.57s/it]                                                         31%|       | 1989/6500 [6:01:31<13:14:44, 10.57s/it] 31%|       | 1990/6500 [6:01:41<13:11:13, 10.53s/it]                                                         31%|       | 1990/6500 [6:01:41<13:11:13, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8064435720443726, 'eval_runtime': 4.2012, 'eval_samples_per_second': 5.475, 'eval_steps_per_second': 1.428, 'epoch': 0.31}
                                                         31%|       | 1990/6500 [6:01:46<13:11:13, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-1990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1990
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1990/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1990/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-1990/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4703, 'learning_rate': 7.860947448855944e-05, 'epoch': 0.31}
{'loss': 0.4555, 'learning_rate': 7.85896459108997e-05, 'epoch': 0.31}
{'loss': 0.5605, 'learning_rate': 7.856981065058407e-05, 'epoch': 0.31}
{'loss': 0.4773, 'learning_rate': 7.85499687122489e-05, 'epoch': 0.31}
{'loss': 0.4548, 'learning_rate': 7.853012010053212e-05, 'epoch': 0.31}
 31%|       | 1991/6500 [6:01:56<14:55:14, 11.91s/it]                                                         31%|       | 1991/6500 [6:01:56<14:55:14, 11.91s/it] 31%|       | 1992/6500 [6:02:07<14:21:06, 11.46s/it]                                                         31%|       | 1992/6500 [6:02:07<14:21:06, 11.46s/it] 31%|       | 1993/6500 [6:02:17<13:56:55, 11.14s/it]                                                         31%|       | 1993/6500 [6:02:17<13:56:55, 11.14s/it] 31%|       | 1994/6500 [6:02:28<13:40:12, 10.92s/it]                                                         31%|       | 1994/6500 [6:02:28<13:40:12, 10.92s/it] 31%|       | 1995/6500 [6:02:38<13:28:44, 10.77s/it]                                                         31%|       | 1995/6500 [6:02:38<13:28:44, 10.77s/it] 31%|       | 1996/6500 [6:02:48<13:20:11, 10.66s/it]                                        {'loss': 0.4794, 'learning_rate': 7.851026482007324e-05, 'epoch': 0.31}
{'loss': 0.9222, 'learning_rate': 7.849040287551331e-05, 'epoch': 0.31}
{'loss': 0.5641, 'learning_rate': 7.847053427149494e-05, 'epoch': 0.31}
{'loss': 0.4857, 'learning_rate': 7.845065901266227e-05, 'epoch': 0.31}
{'loss': 0.4825, 'learning_rate': 7.843077710366105e-05, 'epoch': 0.31}
                 31%|       | 1996/6500 [6:02:48<13:20:11, 10.66s/it] 31%|       | 1997/6500 [6:02:59<13:13:56, 10.58s/it]                                                         31%|       | 1997/6500 [6:02:59<13:13:56, 10.58s/it] 31%|       | 1998/6500 [6:03:09<13:09:55, 10.53s/it]                                                         31%|       | 1998/6500 [6:03:09<13:09:55, 10.53s/it] 31%|       | 1999/6500 [6:03:20<13:06:39, 10.49s/it]                                                         31%|       | 1999/6500 [6:03:20<13:06:39, 10.49s/it] 31%|       | 2000/6500 [6:03:30<13:04:44, 10.46s/it]                                                         31%|       | 2000/6500 [6:03:30<13:04:44, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8103156089782715, 'eval_runtime': 3.9459, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.31}
                                                         31%|       | 2000/6500 [6:03:34<13:04:44, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2000
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4464, 'learning_rate': 7.841088854913853e-05, 'epoch': 0.31}
{'loss': 0.4922, 'learning_rate': 7.839099335374355e-05, 'epoch': 0.31}
{'loss': 0.4855, 'learning_rate': 7.837109152212652e-05, 'epoch': 0.31}
{'loss': 0.4437, 'learning_rate': 7.835118305893933e-05, 'epoch': 0.31}
{'loss': 0.4734, 'learning_rate': 7.833126796883548e-05, 'epoch': 0.31}
 31%|       | 2001/6500 [6:03:45<14:42:55, 11.78s/it]                                                         31%|       | 2001/6500 [6:03:45<14:42:55, 11.78s/it] 31%|       | 2002/6500 [6:03:55<14:11:09, 11.35s/it]                                                         31%|       | 2002/6500 [6:03:55<14:11:09, 11.35s/it] 31%|       | 2003/6500 [6:04:06<13:48:38, 11.06s/it]                                                         31%|       | 2003/6500 [6:04:06<13:48:38, 11.06s/it] 31%|       | 2004/6500 [6:04:16<13:40:55, 10.96s/it]                                                         31%|       | 2004/6500 [6:04:16<13:40:55, 10.96s/it] 31%|       | 2005/6500 [6:04:27<13:29:08, 10.80s/it]                                                         31%|       | 2005/6500 [6:04:27<13:29:08, 10.80s/it] 31%|       | 2006/6500 [6:04:37<13:19:11, 10.67s/it]                                        {'loss': 0.4553, 'learning_rate': 7.831134625646999e-05, 'epoch': 0.31}
{'loss': 0.4759, 'learning_rate': 7.829141792649946e-05, 'epoch': 0.31}
{'loss': 0.4714, 'learning_rate': 7.8271482983582e-05, 'epoch': 0.31}
{'loss': 0.4778, 'learning_rate': 7.825154143237729e-05, 'epoch': 0.31}
{'loss': 0.4668, 'learning_rate': 7.823159327754655e-05, 'epoch': 0.31}
                 31%|       | 2006/6500 [6:04:37<13:19:11, 10.67s/it] 31%|       | 2007/6500 [6:04:48<13:12:29, 10.58s/it]                                                         31%|       | 2007/6500 [6:04:48<13:12:29, 10.58s/it] 31%|       | 2008/6500 [6:04:58<13:07:14, 10.52s/it]                                                         31%|       | 2008/6500 [6:04:58<13:07:14, 10.52s/it] 31%|       | 2009/6500 [6:05:08<13:03:52, 10.47s/it]                                                         31%|       | 2009/6500 [6:05:08<13:03:52, 10.47s/it] 31%|       | 2010/6500 [6:05:19<13:01:09, 10.44s/it]                                                         31%|       | 2010/6500 [6:05:19<13:01:09, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8057028651237488, 'eval_runtime': 3.9472, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.31}
                                                         31%|       | 2010/6500 [6:05:23<13:01:09, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2010/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2010/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2010/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4677, 'learning_rate': 7.821163852375251e-05, 'epoch': 0.31}
{'loss': 0.5114, 'learning_rate': 7.819167717565951e-05, 'epoch': 0.31}
{'loss': 0.4777, 'learning_rate': 7.817170923793338e-05, 'epoch': 0.31}
{'loss': 0.4868, 'learning_rate': 7.815173471524149e-05, 'epoch': 0.31}
{'loss': 0.4882, 'learning_rate': 7.813175361225278e-05, 'epoch': 0.31}
 31%|       | 2011/6500 [6:05:33<14:38:59, 11.75s/it]                                                         31%|       | 2011/6500 [6:05:33<14:38:59, 11.75s/it] 31%|       | 2012/6500 [6:05:44<14:07:45, 11.33s/it]                                                         31%|       | 2012/6500 [6:05:44<14:07:45, 11.33s/it] 31%|       | 2013/6500 [6:05:54<13:46:11, 11.05s/it]                                                         31%|       | 2013/6500 [6:05:54<13:46:11, 11.05s/it] 31%|       | 2014/6500 [6:06:05<13:30:59, 10.85s/it]                                                         31%|       | 2014/6500 [6:06:05<13:30:59, 10.85s/it] 31%|       | 2015/6500 [6:06:15<13:20:40, 10.71s/it]                                                         31%|       | 2015/6500 [6:06:15<13:20:40, 10.71s/it] 31%|       | 2016/6500 [6:06:25<13:13:04, 10.61s/it]                                        {'loss': 0.4619, 'learning_rate': 7.811176593363772e-05, 'epoch': 0.31}
{'loss': 0.4681, 'learning_rate': 7.809177168406827e-05, 'epoch': 0.31}
{'loss': 0.488, 'learning_rate': 7.807177086821802e-05, 'epoch': 0.31}
{'loss': 0.4773, 'learning_rate': 7.805176349076199e-05, 'epoch': 0.31}
{'loss': 0.4712, 'learning_rate': 7.80317495563768e-05, 'epoch': 0.31}
                 31%|       | 2016/6500 [6:06:25<13:13:04, 10.61s/it] 31%|       | 2017/6500 [6:06:36<13:08:06, 10.55s/it]                                                         31%|       | 2017/6500 [6:06:36<13:08:06, 10.55s/it] 31%|       | 2018/6500 [6:06:46<13:04:27, 10.50s/it]                                                         31%|       | 2018/6500 [6:06:46<13:04:27, 10.50s/it] 31%|       | 2019/6500 [6:06:57<13:01:49, 10.47s/it]                                                         31%|       | 2019/6500 [6:06:57<13:01:49, 10.47s/it] 31%|       | 2020/6500 [6:07:07<13:05:19, 10.52s/it]                                                         31%|       | 2020/6500 [6:07:07<13:05:19, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8033508658409119, 'eval_runtime': 3.9548, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.31}
                                                         31%|       | 2020/6500 [6:07:11<13:05:19, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2020
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4545, 'learning_rate': 7.80117290697406e-05, 'epoch': 0.31}
{'loss': 0.4893, 'learning_rate': 7.799170203553304e-05, 'epoch': 0.31}
{'loss': 0.5367, 'learning_rate': 7.797166845843531e-05, 'epoch': 0.31}
{'loss': 0.464, 'learning_rate': 7.795162834313014e-05, 'epoch': 0.31}
{'loss': 0.4627, 'learning_rate': 7.793158169430181e-05, 'epoch': 0.31}
 31%|       | 2021/6500 [6:07:22<14:43:10, 11.83s/it]                                                         31%|       | 2021/6500 [6:07:22<14:43:10, 11.83s/it] 31%|       | 2022/6500 [6:07:32<14:10:25, 11.39s/it]                                                         31%|       | 2022/6500 [6:07:32<14:10:25, 11.39s/it] 31%|       | 2023/6500 [6:07:43<13:46:53, 11.08s/it]                                                         31%|       | 2023/6500 [6:07:43<13:46:53, 11.08s/it] 31%|       | 2024/6500 [6:07:53<13:30:21, 10.86s/it]                                                         31%|       | 2024/6500 [6:07:53<13:30:21, 10.86s/it] 31%|       | 2025/6500 [6:08:04<13:19:08, 10.71s/it]                                                         31%|       | 2025/6500 [6:08:04<13:19:08, 10.71s/it] 31%|       | 2026/6500 [6:08:14<13:11:02, 10.61s/it]                                        {'loss': 0.4831, 'learning_rate': 7.79115285166361e-05, 'epoch': 0.31}
{'loss': 0.9943, 'learning_rate': 7.789146881482027e-05, 'epoch': 0.31}
{'loss': 0.4918, 'learning_rate': 7.787140259354322e-05, 'epoch': 0.31}
{'loss': 0.468, 'learning_rate': 7.785132985749526e-05, 'epoch': 0.31}
{'loss': 0.4595, 'learning_rate': 7.78312506113683e-05, 'epoch': 0.31}
                 31%|       | 2026/6500 [6:08:14<13:11:02, 10.61s/it] 31%|       | 2027/6500 [6:08:24<13:05:23, 10.54s/it]                                                         31%|       | 2027/6500 [6:08:24<13:05:23, 10.54s/it] 31%|       | 2028/6500 [6:08:35<13:01:35, 10.49s/it]                                                         31%|       | 2028/6500 [6:08:35<13:01:35, 10.49s/it] 31%|       | 2029/6500 [6:08:45<12:58:41, 10.45s/it]                                                         31%|       | 2029/6500 [6:08:45<12:58:41, 10.45s/it] 31%|       | 2030/6500 [6:08:55<12:57:29, 10.44s/it]                                                         31%|       | 2030/6500 [6:08:55<12:57:29, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8098951578140259, 'eval_runtime': 4.1783, 'eval_samples_per_second': 5.505, 'eval_steps_per_second': 1.436, 'epoch': 0.31}
                                                         31%|       | 2030/6500 [6:09:00<12:57:29, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2030
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2030/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.448, 'learning_rate': 7.78111648598557e-05, 'epoch': 0.31}
{'loss': 0.4941, 'learning_rate': 7.779107260765242e-05, 'epoch': 0.31}
{'loss': 0.4559, 'learning_rate': 7.777097385945489e-05, 'epoch': 0.31}
{'loss': 0.4644, 'learning_rate': 7.775086861996108e-05, 'epoch': 0.31}
{'loss': 0.4383, 'learning_rate': 7.773075689387043e-05, 'epoch': 0.31}
 31%|       | 2031/6500 [6:09:11<14:42:18, 11.85s/it]                                                         31%|       | 2031/6500 [6:09:11<14:42:18, 11.85s/it] 31%|      | 2032/6500 [6:09:21<14:08:32, 11.40s/it]                                                         31%|      | 2032/6500 [6:09:21<14:08:32, 11.40s/it] 31%|      | 2033/6500 [6:09:31<13:44:37, 11.08s/it]                                                         31%|      | 2033/6500 [6:09:31<13:44:37, 11.08s/it] 31%|      | 2034/6500 [6:09:42<13:28:14, 10.86s/it]                                                         31%|      | 2034/6500 [6:09:42<13:28:14, 10.86s/it] 31%|      | 2035/6500 [6:09:52<13:17:55, 10.72s/it]                                                         31%|      | 2035/6500 [6:09:52<13:17:55, 10.72s/it] 31%|      | 2036/6500 [6:10:02<13:09:59, 10.62s/it]                      {'loss': 0.4608, 'learning_rate': 7.771063868588399e-05, 'epoch': 0.31}
{'loss': 0.4583, 'learning_rate': 7.769051400070425e-05, 'epoch': 0.31}
{'loss': 0.4475, 'learning_rate': 7.767038284303521e-05, 'epoch': 0.31}
{'loss': 0.499, 'learning_rate': 7.76502452175824e-05, 'epoch': 0.31}
{'loss': 0.4633, 'learning_rate': 7.763010112905291e-05, 'epoch': 0.31}
                                   31%|      | 2036/6500 [6:10:02<13:09:59, 10.62s/it] 31%|      | 2037/6500 [6:10:13<13:11:33, 10.64s/it]                                                         31%|      | 2037/6500 [6:10:13<13:11:33, 10.64s/it] 31%|      | 2038/6500 [6:10:23<13:04:33, 10.55s/it]                                                         31%|      | 2038/6500 [6:10:23<13:04:33, 10.55s/it] 31%|      | 2039/6500 [6:10:34<13:00:05, 10.49s/it]                                                         31%|      | 2039/6500 [6:10:34<13:00:05, 10.49s/it] 31%|      | 2040/6500 [6:10:44<12:56:59, 10.45s/it]                                                         31%|      | 2040/6500 [6:10:44<12:56:59, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8114981651306152, 'eval_runtime': 3.9949, 'eval_samples_per_second': 5.757, 'eval_steps_per_second': 1.502, 'epoch': 0.31}
                                                         31%|      | 2040/6500 [6:10:48<12:56:59, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4973, 'learning_rate': 7.760995058215525e-05, 'epoch': 0.31}
{'loss': 0.4931, 'learning_rate': 7.758979358159954e-05, 'epoch': 0.31}
{'loss': 0.4777, 'learning_rate': 7.756963013209733e-05, 'epoch': 0.31}
{'loss': 0.4868, 'learning_rate': 7.754946023836168e-05, 'epoch': 0.31}
{'loss': 0.4787, 'learning_rate': 7.752928390510722e-05, 'epoch': 0.31}
 31%|      | 2041/6500 [6:10:59<14:34:53, 11.77s/it]                                                         31%|      | 2041/6500 [6:10:59<14:34:53, 11.77s/it] 31%|      | 2042/6500 [6:11:09<14:03:00, 11.35s/it]                                                         31%|      | 2042/6500 [6:11:09<14:03:00, 11.35s/it] 31%|      | 2043/6500 [6:11:20<13:40:32, 11.05s/it]                                                         31%|      | 2043/6500 [6:11:20<13:40:32, 11.05s/it] 31%|      | 2044/6500 [6:11:30<13:25:00, 10.84s/it]                                                         31%|      | 2044/6500 [6:11:30<13:25:00, 10.84s/it] 31%|      | 2045/6500 [6:11:40<13:14:04, 10.69s/it]                                                         31%|      | 2045/6500 [6:11:40<13:14:04, 10.69s/it] 31%|      | 2046/6500 [6:11:51<13:06:49, 10.60s/it]                  {'loss': 0.4587, 'learning_rate': 7.750910113705001e-05, 'epoch': 0.31}
{'loss': 0.4759, 'learning_rate': 7.748891193890768e-05, 'epoch': 0.31}
{'loss': 0.5052, 'learning_rate': 7.746871631539934e-05, 'epoch': 0.32}
{'loss': 0.4414, 'learning_rate': 7.744851427124554e-05, 'epoch': 0.32}
{'loss': 0.4772, 'learning_rate': 7.742830581116843e-05, 'epoch': 0.32}
                                       31%|      | 2046/6500 [6:11:51<13:06:49, 10.60s/it] 31%|      | 2047/6500 [6:12:01<13:01:11, 10.53s/it]                                                         31%|      | 2047/6500 [6:12:01<13:01:11, 10.53s/it] 32%|      | 2048/6500 [6:12:11<12:57:15, 10.48s/it]                                                         32%|      | 2048/6500 [6:12:11<12:57:15, 10.48s/it] 32%|      | 2049/6500 [6:12:22<12:54:41, 10.44s/it]                                                         32%|      | 2049/6500 [6:12:22<12:54:41, 10.44s/it] 32%|      | 2050/6500 [6:12:32<12:53:00, 10.42s/it]                                                         32%|      | 2050/6500 [6:12:32<12:53:00, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8039287328720093, 'eval_runtime': 3.9435, 'eval_samples_per_second': 5.832, 'eval_steps_per_second': 1.521, 'epoch': 0.32}
                                                         32%|      | 2050/6500 [6:12:36<12:53:00, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2050/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4493, 'learning_rate': 7.74080909398916e-05, 'epoch': 0.32}
{'loss': 0.5071, 'learning_rate': 7.738786966214016e-05, 'epoch': 0.32}
{'loss': 0.5202, 'learning_rate': 7.736764198264072e-05, 'epoch': 0.32}
{'loss': 0.4603, 'learning_rate': 7.734740790612136e-05, 'epoch': 0.32}
{'loss': 0.4812, 'learning_rate': 7.732716743731166e-05, 'epoch': 0.32}
 32%|      | 2051/6500 [6:12:47<14:29:22, 11.72s/it]                                                         32%|      | 2051/6500 [6:12:47<14:29:22, 11.72s/it] 32%|      | 2052/6500 [6:12:57<14:00:19, 11.34s/it]                                                         32%|      | 2052/6500 [6:12:57<14:00:19, 11.34s/it] 32%|      | 2053/6500 [6:13:08<13:49:15, 11.19s/it]                                                         32%|      | 2053/6500 [6:13:08<13:49:15, 11.19s/it] 32%|      | 2054/6500 [6:13:19<13:30:53, 10.94s/it]                                                         32%|      | 2054/6500 [6:13:19<13:30:53, 10.94s/it] 32%|      | 2055/6500 [6:13:29<13:17:48, 10.77s/it]                                                         32%|      | 2055/6500 [6:13:29<13:17:48, 10.77s/it] 32%|      | 2056/6500 [6:13:39<13:08:39, 10.65s/it]                  {'loss': 0.4482, 'learning_rate': 7.730692058094273e-05, 'epoch': 0.32}
{'loss': 0.9973, 'learning_rate': 7.728666734174715e-05, 'epoch': 0.32}
{'loss': 0.4783, 'learning_rate': 7.726640772445899e-05, 'epoch': 0.32}
{'loss': 0.483, 'learning_rate': 7.72461417338138e-05, 'epoch': 0.32}
{'loss': 0.4416, 'learning_rate': 7.722586937454864e-05, 'epoch': 0.32}
                                       32%|      | 2056/6500 [6:13:39<13:08:39, 10.65s/it] 32%|      | 2057/6500 [6:13:50<13:02:11, 10.56s/it]                                                         32%|      | 2057/6500 [6:13:50<13:02:11, 10.56s/it] 32%|      | 2058/6500 [6:14:00<12:57:52, 10.51s/it]                                                         32%|      | 2058/6500 [6:14:00<12:57:52, 10.51s/it] 32%|      | 2059/6500 [6:14:10<12:54:48, 10.47s/it]                                                         32%|      | 2059/6500 [6:14:10<12:54:48, 10.47s/it] 32%|      | 2060/6500 [6:14:21<12:52:30, 10.44s/it]                                                         32%|      | 2060/6500 [6:14:21<12:52:30, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8092721104621887, 'eval_runtime': 3.9759, 'eval_samples_per_second': 5.785, 'eval_steps_per_second': 1.509, 'epoch': 0.32}
                                                         32%|      | 2060/6500 [6:14:25<12:52:30, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2060
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4928, 'learning_rate': 7.720559065140203e-05, 'epoch': 0.32}
{'loss': 0.4764, 'learning_rate': 7.718530556911403e-05, 'epoch': 0.32}
{'loss': 0.4419, 'learning_rate': 7.716501413242616e-05, 'epoch': 0.32}
{'loss': 0.4607, 'learning_rate': 7.714471634608138e-05, 'epoch': 0.32}
{'loss': 0.4456, 'learning_rate': 7.712441221482421e-05, 'epoch': 0.32}
 32%|      | 2061/6500 [6:14:36<14:31:09, 11.77s/it]                                                         32%|      | 2061/6500 [6:14:36<14:31:09, 11.77s/it] 32%|      | 2062/6500 [6:14:46<13:59:45, 11.35s/it]                                                         32%|      | 2062/6500 [6:14:46<13:59:45, 11.35s/it] 32%|      | 2063/6500 [6:14:56<13:37:24, 11.05s/it]                                                         32%|      | 2063/6500 [6:14:56<13:37:24, 11.05s/it] 32%|      | 2064/6500 [6:15:07<13:22:07, 10.85s/it]                                                         32%|      | 2064/6500 [6:15:07<13:22:07, 10.85s/it] 32%|      | 2065/6500 [6:15:17<13:11:15, 10.70s/it]                                                         32%|      | 2065/6500 [6:15:17<13:11:15, 10.70s/it] 32%|      | 2066/6500 [6:15:28<13:04:00, 10.61s/it]                  {'loss': 0.4639, 'learning_rate': 7.710410174340061e-05, 'epoch': 0.32}
{'loss': 0.4603, 'learning_rate': 7.7083784936558e-05, 'epoch': 0.32}
{'loss': 0.4649, 'learning_rate': 7.706346179904535e-05, 'epoch': 0.32}
{'loss': 0.4699, 'learning_rate': 7.704313233561305e-05, 'epoch': 0.32}
{'loss': 0.4602, 'learning_rate': 7.7022796551013e-05, 'epoch': 0.32}
                                       32%|      | 2066/6500 [6:15:28<13:04:00, 10.61s/it] 32%|      | 2067/6500 [6:15:38<12:58:50, 10.54s/it]                                                         32%|      | 2067/6500 [6:15:38<12:58:50, 10.54s/it] 32%|      | 2068/6500 [6:15:48<12:54:51, 10.49s/it]                                                         32%|      | 2068/6500 [6:15:48<12:54:51, 10.49s/it] 32%|      | 2069/6500 [6:15:59<13:00:41, 10.57s/it]                                                         32%|      | 2069/6500 [6:15:59<13:00:41, 10.57s/it] 32%|      | 2070/6500 [6:16:09<12:56:25, 10.52s/it]                                                         32%|      | 2070/6500 [6:16:09<12:56:25, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8093778491020203, 'eval_runtime': 3.9733, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.32}
                                                         32%|      | 2070/6500 [6:16:13<12:56:25, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5047, 'learning_rate': 7.700245444999854e-05, 'epoch': 0.32}
{'loss': 0.4537, 'learning_rate': 7.698210603732454e-05, 'epoch': 0.32}
{'loss': 0.4781, 'learning_rate': 7.69617513177473e-05, 'epoch': 0.32}
{'loss': 0.4812, 'learning_rate': 7.694139029602465e-05, 'epoch': 0.32}
{'loss': 0.4447, 'learning_rate': 7.692102297691578e-05, 'epoch': 0.32}
 32%|      | 2071/6500 [6:16:24<14:31:11, 11.80s/it]                                                         32%|      | 2071/6500 [6:16:24<14:31:11, 11.80s/it] 32%|      | 2072/6500 [6:16:35<13:59:14, 11.37s/it]                                                         32%|      | 2072/6500 [6:16:35<13:59:14, 11.37s/it] 32%|      | 2073/6500 [6:16:45<13:36:58, 11.07s/it]                                                         32%|      | 2073/6500 [6:16:45<13:36:58, 11.07s/it] 32%|      | 2074/6500 [6:16:55<13:21:12, 10.86s/it]                                                         32%|      | 2074/6500 [6:16:55<13:21:12, 10.86s/it] 32%|      | 2075/6500 [6:17:06<13:10:26, 10.72s/it]                                                         32%|      | 2075/6500 [6:17:06<13:10:26, 10.72s/it] 32%|      | 2076/6500 [6:17:16<13:03:09, 10.62s/it]                  {'loss': 0.467, 'learning_rate': 7.690064936518151e-05, 'epoch': 0.32}
{'loss': 0.4723, 'learning_rate': 7.688026946558397e-05, 'epoch': 0.32}
{'loss': 0.4749, 'learning_rate': 7.685988328288691e-05, 'epoch': 0.32}
{'loss': 0.4607, 'learning_rate': 7.683949082185544e-05, 'epoch': 0.32}
{'loss': 0.4648, 'learning_rate': 7.681909208725617e-05, 'epoch': 0.32}
                                       32%|      | 2076/6500 [6:17:16<13:03:09, 10.62s/it] 32%|      | 2077/6500 [6:17:27<12:57:38, 10.55s/it]                                                         32%|      | 2077/6500 [6:17:27<12:57:38, 10.55s/it] 32%|      | 2078/6500 [6:17:37<12:53:27, 10.49s/it]                                                         32%|      | 2078/6500 [6:17:37<12:53:27, 10.49s/it] 32%|      | 2079/6500 [6:17:47<12:50:37, 10.46s/it]                                                         32%|      | 2079/6500 [6:17:47<12:50:37, 10.46s/it] 32%|      | 2080/6500 [6:17:58<12:48:36, 10.43s/it]                                                         32%|      | 2080/6500 [6:17:58<12:48:36, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8081077337265015, 'eval_runtime': 3.9641, 'eval_samples_per_second': 5.802, 'eval_steps_per_second': 1.514, 'epoch': 0.32}
                                                         32%|      | 2080/6500 [6:18:02<12:48:36, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4449, 'learning_rate': 7.679868708385718e-05, 'epoch': 0.32}
{'loss': 0.5582, 'learning_rate': 7.677827581642804e-05, 'epoch': 0.32}
{'loss': 0.4558, 'learning_rate': 7.675785828973972e-05, 'epoch': 0.32}
{'loss': 0.4368, 'learning_rate': 7.673743450856473e-05, 'epoch': 0.32}
{'loss': 0.468, 'learning_rate': 7.6717004477677e-05, 'epoch': 0.32}
 32%|      | 2081/6500 [6:18:12<14:25:41, 11.75s/it]                                                         32%|      | 2081/6500 [6:18:12<14:25:41, 11.75s/it] 32%|      | 2082/6500 [6:18:23<13:55:09, 11.34s/it]                                                         32%|      | 2082/6500 [6:18:23<13:55:09, 11.34s/it] 32%|      | 2083/6500 [6:18:33<13:33:48, 11.05s/it]                                                         32%|      | 2083/6500 [6:18:33<13:33:48, 11.05s/it] 32%|      | 2084/6500 [6:18:44<13:18:41, 10.85s/it]                                                         32%|      | 2084/6500 [6:18:44<13:18:41, 10.85s/it] 32%|      | 2085/6500 [6:18:54<13:13:22, 10.78s/it]                                                         32%|      | 2085/6500 [6:18:54<13:13:22, 10.78s/it] 32%|      | 2086/6500 [6:19:05<13:04:21, 10.66s/it]                  {'loss': 0.9924, 'learning_rate': 7.669656820185189e-05, 'epoch': 0.32}
{'loss': 0.4789, 'learning_rate': 7.66761256858663e-05, 'epoch': 0.32}
{'loss': 0.4805, 'learning_rate': 7.66556769344985e-05, 'epoch': 0.32}
{'loss': 0.4662, 'learning_rate': 7.663522195252832e-05, 'epoch': 0.32}
{'loss': 0.4352, 'learning_rate': 7.661476074473695e-05, 'epoch': 0.32}
                                       32%|      | 2086/6500 [6:19:05<13:04:21, 10.66s/it] 32%|      | 2087/6500 [6:19:15<12:58:27, 10.58s/it]                                                         32%|      | 2087/6500 [6:19:15<12:58:27, 10.58s/it] 32%|      | 2088/6500 [6:19:25<12:53:26, 10.52s/it]                                                         32%|      | 2088/6500 [6:19:25<12:53:26, 10.52s/it] 32%|      | 2089/6500 [6:19:36<12:50:26, 10.48s/it]                                                         32%|      | 2089/6500 [6:19:36<12:50:26, 10.48s/it] 32%|      | 2090/6500 [6:19:46<12:48:09, 10.45s/it]                                                         32%|      | 2090/6500 [6:19:46<12:48:09, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8100438117980957, 'eval_runtime': 4.1953, 'eval_samples_per_second': 5.482, 'eval_steps_per_second': 1.43, 'epoch': 0.32}
                                                         32%|      | 2090/6500 [6:19:50<12:48:09, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4916, 'learning_rate': 7.659429331590706e-05, 'epoch': 0.32}
{'loss': 0.4621, 'learning_rate': 7.65738196708228e-05, 'epoch': 0.32}
{'loss': 0.4456, 'learning_rate': 7.655333981426978e-05, 'epoch': 0.32}
{'loss': 0.4481, 'learning_rate': 7.6532853751035e-05, 'epoch': 0.32}
{'loss': 0.4587, 'learning_rate': 7.651236148590699e-05, 'epoch': 0.32}
 32%|      | 2091/6500 [6:20:01<14:30:03, 11.84s/it]                                                         32%|      | 2091/6500 [6:20:01<14:30:03, 11.84s/it] 32%|      | 2092/6500 [6:20:12<13:57:46, 11.40s/it]                                                         32%|      | 2092/6500 [6:20:12<13:57:46, 11.40s/it] 32%|      | 2093/6500 [6:20:22<13:35:01, 11.10s/it]                                                         32%|      | 2093/6500 [6:20:22<13:35:01, 11.10s/it] 32%|      | 2094/6500 [6:20:32<13:18:45, 10.88s/it]                                                         32%|      | 2094/6500 [6:20:32<13:18:45, 10.88s/it] 32%|      | 2095/6500 [6:20:43<13:07:48, 10.73s/it]                                                         32%|      | 2095/6500 [6:20:43<13:07:48, 10.73s/it] 32%|      | 2096/6500 [6:20:53<12:59:56, 10.63s/it]                  {'loss': 0.4554, 'learning_rate': 7.64918630236757e-05, 'epoch': 0.32}
{'loss': 0.4429, 'learning_rate': 7.647135836913249e-05, 'epoch': 0.32}
{'loss': 0.4757, 'learning_rate': 7.645084752707019e-05, 'epoch': 0.32}
{'loss': 0.4402, 'learning_rate': 7.643033050228312e-05, 'epoch': 0.32}
{'loss': 0.4824, 'learning_rate': 7.640980729956699e-05, 'epoch': 0.32}
                                       32%|      | 2096/6500 [6:20:53<12:59:56, 10.63s/it] 32%|      | 2097/6500 [6:21:03<12:54:14, 10.55s/it]                                                         32%|      | 2097/6500 [6:21:04<12:54:14, 10.55s/it] 32%|      | 2098/6500 [6:21:14<12:50:22, 10.50s/it]                                                         32%|      | 2098/6500 [6:21:14<12:50:22, 10.50s/it] 32%|      | 2099/6500 [6:21:24<12:47:52, 10.47s/it]                                                         32%|      | 2099/6500 [6:21:24<12:47:52, 10.47s/it] 32%|      | 2100/6500 [6:21:35<12:45:36, 10.44s/it]                                                         32%|      | 2100/6500 [6:21:35<12:45:36, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8129218816757202, 'eval_runtime': 3.9617, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.515, 'epoch': 0.32}
                                                         32%|      | 2100/6500 [6:21:39<12:45:36, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4941, 'learning_rate': 7.6389277923719e-05, 'epoch': 0.32}
{'loss': 0.4755, 'learning_rate': 7.636874237953771e-05, 'epoch': 0.32}
{'loss': 0.4704, 'learning_rate': 7.634820067182323e-05, 'epoch': 0.32}
{'loss': 0.4795, 'learning_rate': 7.632765280537703e-05, 'epoch': 0.32}
{'loss': 0.4729, 'learning_rate': 7.630709878500208e-05, 'epoch': 0.32}
 32%|      | 2101/6500 [6:21:50<14:31:09, 11.88s/it]                                                         32%|      | 2101/6500 [6:21:50<14:31:09, 11.88s/it] 32%|      | 2102/6500 [6:22:00<13:57:51, 11.43s/it]                                                         32%|      | 2102/6500 [6:22:00<13:57:51, 11.43s/it] 32%|      | 2103/6500 [6:22:11<13:34:54, 11.12s/it]                                                         32%|      | 2103/6500 [6:22:11<13:34:54, 11.12s/it] 32%|      | 2104/6500 [6:22:21<13:20:56, 10.93s/it]                                                         32%|      | 2104/6500 [6:22:21<13:20:56, 10.93s/it] 32%|      | 2105/6500 [6:22:32<13:09:07, 10.77s/it]                                                         32%|      | 2105/6500 [6:22:32<13:09:07, 10.77s/it] 32%|      | 2106/6500 [6:22:42<13:00:25, 10.66s/it]                  {'loss': 0.4449, 'learning_rate': 7.628653861550275e-05, 'epoch': 0.32}
{'loss': 0.483, 'learning_rate': 7.626597230168482e-05, 'epoch': 0.32}
{'loss': 0.4578, 'learning_rate': 7.624539984835557e-05, 'epoch': 0.32}
{'loss': 0.4636, 'learning_rate': 7.622482126032368e-05, 'epoch': 0.32}
{'loss': 0.4502, 'learning_rate': 7.620423654239928e-05, 'epoch': 0.32}
                                       32%|      | 2106/6500 [6:22:42<13:00:25, 10.66s/it] 32%|      | 2107/6500 [6:22:52<12:54:13, 10.57s/it]                                                         32%|      | 2107/6500 [6:22:52<12:54:13, 10.57s/it] 32%|      | 2108/6500 [6:23:03<12:49:42, 10.52s/it]                                                         32%|      | 2108/6500 [6:23:03<12:49:42, 10.52s/it] 32%|      | 2109/6500 [6:23:13<12:46:44, 10.48s/it]                                                         32%|      | 2109/6500 [6:23:13<12:46:44, 10.48s/it] 32%|      | 2110/6500 [6:23:23<12:44:25, 10.45s/it]                                                         32%|      | 2110/6500 [6:23:23<12:44:25, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8084378242492676, 'eval_runtime': 3.9701, 'eval_samples_per_second': 5.793, 'eval_steps_per_second': 1.511, 'epoch': 0.32}
                                                         32%|      | 2110/6500 [6:23:27<12:44:25, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4722, 'learning_rate': 7.618364569939391e-05, 'epoch': 0.32}
{'loss': 0.5297, 'learning_rate': 7.616304873612055e-05, 'epoch': 0.32}
{'loss': 0.4686, 'learning_rate': 7.614244565739361e-05, 'epoch': 0.33}
{'loss': 0.4517, 'learning_rate': 7.612183646802895e-05, 'epoch': 0.33}
{'loss': 0.4681, 'learning_rate': 7.610122117284386e-05, 'epoch': 0.33}
 32%|      | 2111/6500 [6:23:38<14:19:58, 11.76s/it]                                                         32%|      | 2111/6500 [6:23:38<14:19:58, 11.76s/it] 32%|      | 2112/6500 [6:23:49<13:49:45, 11.35s/it]                                                         32%|      | 2112/6500 [6:23:49<13:49:45, 11.35s/it] 33%|      | 2113/6500 [6:23:59<13:28:13, 11.05s/it]                                                         33%|      | 2113/6500 [6:23:59<13:28:13, 11.05s/it] 33%|      | 2114/6500 [6:24:09<13:13:16, 10.85s/it]                                                         33%|      | 2114/6500 [6:24:09<13:13:16, 10.85s/it] 33%|      | 2115/6500 [6:24:20<13:02:41, 10.71s/it]                                                         33%|      | 2115/6500 [6:24:20<13:02:41, 10.71s/it] 33%|      | 2116/6500 [6:24:30<12:54:40, 10.60s/it]                  {'loss': 0.9939, 'learning_rate': 7.6080599776657e-05, 'epoch': 0.33}
{'loss': 0.4842, 'learning_rate': 7.605997228428853e-05, 'epoch': 0.33}
{'loss': 0.4583, 'learning_rate': 7.603933870055997e-05, 'epoch': 0.33}
{'loss': 0.4426, 'learning_rate': 7.601869903029432e-05, 'epoch': 0.33}
{'loss': 0.4536, 'learning_rate': 7.599805327831596e-05, 'epoch': 0.33}
                                       33%|      | 2116/6500 [6:24:30<12:54:40, 10.60s/it] 33%|      | 2117/6500 [6:24:41<13:06:22, 10.76s/it]                                                         33%|      | 2117/6500 [6:24:41<13:06:22, 10.76s/it] 33%|      | 2118/6500 [6:24:52<12:57:56, 10.65s/it]                                                         33%|      | 2118/6500 [6:24:52<12:57:56, 10.65s/it] 33%|      | 2119/6500 [6:25:02<12:51:58, 10.57s/it]                                                         33%|      | 2119/6500 [6:25:02<12:51:58, 10.57s/it] 33%|      | 2120/6500 [6:25:12<12:47:45, 10.52s/it]                                                         33%|      | 2120/6500 [6:25:12<12:47:45, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8143872618675232, 'eval_runtime': 4.3478, 'eval_samples_per_second': 5.29, 'eval_steps_per_second': 1.38, 'epoch': 0.33}
                                                         33%|      | 2120/6500 [6:25:17<12:47:45, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2120/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2120/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4791, 'learning_rate': 7.597740144945073e-05, 'epoch': 0.33}
{'loss': 0.4575, 'learning_rate': 7.595674354852584e-05, 'epoch': 0.33}
{'loss': 0.4462, 'learning_rate': 7.593607958036998e-05, 'epoch': 0.33}
{'loss': 0.4341, 'learning_rate': 7.59154095498132e-05, 'epoch': 0.33}
{'loss': 0.453, 'learning_rate': 7.589473346168702e-05, 'epoch': 0.33}
 33%|      | 2121/6500 [6:25:28<14:31:13, 11.94s/it]                                                         33%|      | 2121/6500 [6:25:28<14:31:13, 11.94s/it] 33%|      | 2122/6500 [6:25:38<13:56:10, 11.46s/it]                                                         33%|      | 2122/6500 [6:25:38<13:56:10, 11.46s/it] 33%|      | 2123/6500 [6:25:48<13:32:30, 11.14s/it]                                                         33%|      | 2123/6500 [6:25:48<13:32:30, 11.14s/it] 33%|      | 2124/6500 [6:25:59<13:15:07, 10.90s/it]                                                         33%|      | 2124/6500 [6:25:59<13:15:07, 10.90s/it] 33%|      | 2125/6500 [6:26:09<13:02:51, 10.74s/it]                                                         33%|      | 2125/6500 [6:26:09<13:02:51, 10.74s/it] 33%|      | 2126/6500 [6:26:19<12:54:15, 10.62s/it]                  {'loss': 0.4528, 'learning_rate': 7.587405132082433e-05, 'epoch': 0.33}
{'loss': 0.4401, 'learning_rate': 7.585336313205944e-05, 'epoch': 0.33}
{'loss': 0.4754, 'learning_rate': 7.583266890022814e-05, 'epoch': 0.33}
{'loss': 0.4451, 'learning_rate': 7.581196863016754e-05, 'epoch': 0.33}
{'loss': 0.4838, 'learning_rate': 7.579126232671621e-05, 'epoch': 0.33}
                                       33%|      | 2126/6500 [6:26:20<12:54:15, 10.62s/it] 33%|      | 2127/6500 [6:26:30<12:48:02, 10.54s/it]                                                         33%|      | 2127/6500 [6:26:30<12:48:02, 10.54s/it] 33%|      | 2128/6500 [6:26:40<12:43:46, 10.48s/it]                                                         33%|      | 2128/6500 [6:26:40<12:43:46, 10.48s/it] 33%|      | 2129/6500 [6:26:51<12:40:59, 10.45s/it]                                                         33%|      | 2129/6500 [6:26:51<12:40:59, 10.45s/it] 33%|      | 2130/6500 [6:27:01<12:38:49, 10.42s/it]                                                         33%|      | 2130/6500 [6:27:01<12:38:49, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8159165382385254, 'eval_runtime': 3.9518, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.33}
                                                         33%|      | 2130/6500 [6:27:05<12:38:49, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2130
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4719, 'learning_rate': 7.577054999471411e-05, 'epoch': 0.33}
{'loss': 0.4505, 'learning_rate': 7.574983163900265e-05, 'epoch': 0.33}
{'loss': 0.464, 'learning_rate': 7.572910726442462e-05, 'epoch': 0.33}
{'loss': 0.4611, 'learning_rate': 7.57083768758242e-05, 'epoch': 0.33}
{'loss': 0.4546, 'learning_rate': 7.568764047804698e-05, 'epoch': 0.33}
 33%|      | 2131/6500 [6:27:16<14:16:15, 11.76s/it]                                                         33%|      | 2131/6500 [6:27:16<14:16:15, 11.76s/it] 33%|      | 2132/6500 [6:27:26<13:45:12, 11.34s/it]                                                         33%|      | 2132/6500 [6:27:26<13:45:12, 11.34s/it] 33%|      | 2133/6500 [6:27:37<13:23:56, 11.05s/it]                                                         33%|      | 2133/6500 [6:27:37<13:23:56, 11.05s/it] 33%|      | 2134/6500 [6:27:47<13:14:40, 10.92s/it]                                                         33%|      | 2134/6500 [6:27:47<13:14:40, 10.92s/it] 33%|      | 2135/6500 [6:27:58<13:02:27, 10.76s/it]                                                         33%|      | 2135/6500 [6:27:58<13:02:27, 10.76s/it] 33%|      | 2136/6500 [6:28:08<12:53:43, 10.64s/it]                  {'loss': 0.4554, 'learning_rate': 7.566689807593998e-05, 'epoch': 0.33}
{'loss': 0.4828, 'learning_rate': 7.56461496743516e-05, 'epoch': 0.33}
{'loss': 0.4441, 'learning_rate': 7.562539527813169e-05, 'epoch': 0.33}
{'loss': 0.4627, 'learning_rate': 7.560463489213143e-05, 'epoch': 0.33}
{'loss': 0.4313, 'learning_rate': 7.558386852120344e-05, 'epoch': 0.33}
                                       33%|      | 2136/6500 [6:28:08<12:53:43, 10.64s/it] 33%|      | 2137/6500 [6:28:18<12:47:58, 10.56s/it]                                                         33%|      | 2137/6500 [6:28:18<12:47:58, 10.56s/it] 33%|      | 2138/6500 [6:28:29<12:43:39, 10.50s/it]                                                         33%|      | 2138/6500 [6:28:29<12:43:39, 10.50s/it] 33%|      | 2139/6500 [6:28:39<12:40:31, 10.46s/it]                                                         33%|      | 2139/6500 [6:28:39<12:40:31, 10.46s/it] 33%|      | 2140/6500 [6:28:49<12:39:34, 10.45s/it]                                                         33%|      | 2140/6500 [6:28:49<12:39:34, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8104758262634277, 'eval_runtime': 4.0198, 'eval_samples_per_second': 5.722, 'eval_steps_per_second': 1.493, 'epoch': 0.33}
                                                         33%|      | 2140/6500 [6:28:53<12:39:34, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2140
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5026, 'learning_rate': 7.556309617020175e-05, 'epoch': 0.33}
{'loss': 0.5037, 'learning_rate': 7.554231784398173e-05, 'epoch': 0.33}
{'loss': 0.4486, 'learning_rate': 7.552153354740023e-05, 'epoch': 0.33}
{'loss': 0.4538, 'learning_rate': 7.550074328531545e-05, 'epoch': 0.33}
{'loss': 0.4531, 'learning_rate': 7.547994706258694e-05, 'epoch': 0.33}
 33%|      | 2141/6500 [6:29:04<14:17:27, 11.80s/it]                                                         33%|      | 2141/6500 [6:29:04<14:17:27, 11.80s/it] 33%|      | 2142/6500 [6:29:15<13:47:00, 11.39s/it]                                                         33%|      | 2142/6500 [6:29:15<13:47:00, 11.39s/it] 33%|      | 2143/6500 [6:29:25<13:25:42, 11.10s/it]                                                         33%|      | 2143/6500 [6:29:25<13:25:42, 11.10s/it] 33%|      | 2144/6500 [6:29:36<13:10:54, 10.89s/it]                                                         33%|      | 2144/6500 [6:29:36<13:10:54, 10.89s/it] 33%|      | 2145/6500 [6:29:46<13:00:09, 10.75s/it]                                                         33%|      | 2145/6500 [6:29:46<13:00:09, 10.75s/it] 33%|      | 2146/6500 [6:29:56<12:52:28, 10.65s/it]                  {'loss': 0.9892, 'learning_rate': 7.545914488407575e-05, 'epoch': 0.33}
{'loss': 0.4708, 'learning_rate': 7.543833675464422e-05, 'epoch': 0.33}
{'loss': 0.4589, 'learning_rate': 7.541752267915615e-05, 'epoch': 0.33}
{'loss': 0.4304, 'learning_rate': 7.539670266247671e-05, 'epoch': 0.33}
{'loss': 0.4643, 'learning_rate': 7.537587670947244e-05, 'epoch': 0.33}
                                       33%|      | 2146/6500 [6:29:56<12:52:28, 10.65s/it] 33%|      | 2147/6500 [6:30:07<12:47:14, 10.58s/it]                                                         33%|      | 2147/6500 [6:30:07<12:47:14, 10.58s/it] 33%|      | 2148/6500 [6:30:17<12:43:40, 10.53s/it]                                                         33%|      | 2148/6500 [6:30:17<12:43:40, 10.53s/it] 33%|      | 2149/6500 [6:30:28<12:40:55, 10.49s/it]                                                         33%|      | 2149/6500 [6:30:28<12:40:55, 10.49s/it] 33%|      | 2150/6500 [6:30:38<12:46:58, 10.58s/it]                                                         33%|      | 2150/6500 [6:30:38<12:46:58, 10.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8139358758926392, 'eval_runtime': 4.4183, 'eval_samples_per_second': 5.206, 'eval_steps_per_second': 1.358, 'epoch': 0.33}
                                                         33%|      | 2150/6500 [6:30:43<12:46:58, 10.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2150/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4726, 'learning_rate': 7.535504482501126e-05, 'epoch': 0.33}
{'loss': 0.4249, 'learning_rate': 7.53342070139625e-05, 'epoch': 0.33}
{'loss': 0.4501, 'learning_rate': 7.53133632811969e-05, 'epoch': 0.33}
{'loss': 0.4308, 'learning_rate': 7.529251363158653e-05, 'epoch': 0.33}
{'loss': 0.4473, 'learning_rate': 7.527165807000492e-05, 'epoch': 0.33}
 33%|      | 2151/6500 [6:30:54<14:31:43, 12.03s/it]                                                         33%|      | 2151/6500 [6:30:54<14:31:43, 12.03s/it] 33%|      | 2152/6500 [6:31:04<13:56:42, 11.55s/it]                                                         33%|      | 2152/6500 [6:31:04<13:56:42, 11.55s/it] 33%|      | 2153/6500 [6:31:15<13:31:34, 11.20s/it]                                                         33%|      | 2153/6500 [6:31:15<13:31:34, 11.20s/it] 33%|      | 2154/6500 [6:31:25<13:14:06, 10.96s/it]                                                         33%|      | 2154/6500 [6:31:25<13:14:06, 10.96s/it] 33%|      | 2155/6500 [6:31:36<13:01:57, 10.80s/it]                                                         33%|      | 2155/6500 [6:31:36<13:01:57, 10.80s/it] 33%|      | 2156/6500 [6:31:46<12:53:10, 10.68s/it]                  {'loss': 0.4428, 'learning_rate': 7.525079660132685e-05, 'epoch': 0.33}
{'loss': 0.4577, 'learning_rate': 7.522992923042861e-05, 'epoch': 0.33}
{'loss': 0.4501, 'learning_rate': 7.520905596218781e-05, 'epoch': 0.33}
{'loss': 0.432, 'learning_rate': 7.518817680148347e-05, 'epoch': 0.33}
{'loss': 0.497, 'learning_rate': 7.516729175319592e-05, 'epoch': 0.33}
                                       33%|      | 2156/6500 [6:31:46<12:53:10, 10.68s/it] 33%|      | 2157/6500 [6:31:56<12:48:29, 10.62s/it]                                                         33%|      | 2157/6500 [6:31:56<12:48:29, 10.62s/it] 33%|      | 2158/6500 [6:32:07<12:43:52, 10.56s/it]                                                         33%|      | 2158/6500 [6:32:07<12:43:52, 10.56s/it] 33%|      | 2159/6500 [6:32:17<12:40:33, 10.51s/it]                                                         33%|      | 2159/6500 [6:32:17<12:40:33, 10.51s/it] 33%|      | 2160/6500 [6:32:28<12:38:03, 10.48s/it]                                                         33%|      | 2160/6500 [6:32:28<12:38:03, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8137548565864563, 'eval_runtime': 3.9688, 'eval_samples_per_second': 5.795, 'eval_steps_per_second': 1.512, 'epoch': 0.33}
                                                         33%|      | 2160/6500 [6:32:32<12:38:03, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2160/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4484, 'learning_rate': 7.514640082220697e-05, 'epoch': 0.33}
{'loss': 0.4569, 'learning_rate': 7.512550401339971e-05, 'epoch': 0.33}
{'loss': 0.489, 'learning_rate': 7.510460133165867e-05, 'epoch': 0.33}
{'loss': 0.4387, 'learning_rate': 7.508369278186967e-05, 'epoch': 0.33}
{'loss': 0.4504, 'learning_rate': 7.506277836892001e-05, 'epoch': 0.33}
 33%|      | 2161/6500 [6:32:42<14:13:05, 11.80s/it]                                                         33%|      | 2161/6500 [6:32:42<14:13:05, 11.80s/it] 33%|      | 2162/6500 [6:32:53<13:43:26, 11.39s/it]                                                         33%|      | 2162/6500 [6:32:53<13:43:26, 11.39s/it] 33%|      | 2163/6500 [6:33:03<13:22:14, 11.10s/it]                                                         33%|      | 2163/6500 [6:33:03<13:22:14, 11.10s/it] 33%|      | 2164/6500 [6:33:14<13:06:55, 10.89s/it]                                                         33%|      | 2164/6500 [6:33:14<13:06:55, 10.89s/it] 33%|      | 2165/6500 [6:33:24<12:56:20, 10.75s/it]                                                         33%|      | 2165/6500 [6:33:24<12:56:20, 10.75s/it] 33%|      | 2166/6500 [6:33:35<12:54:21, 10.72s/it]                  {'loss': 0.4692, 'learning_rate': 7.50418580976983e-05, 'epoch': 0.33}
{'loss': 0.4644, 'learning_rate': 7.502093197309452e-05, 'epoch': 0.33}
{'loss': 0.444, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.33}
{'loss': 0.4307, 'learning_rate': 7.49790621833075e-05, 'epoch': 0.33}
{'loss': 0.4708, 'learning_rate': 7.495811852791107e-05, 'epoch': 0.33}
                                       33%|      | 2166/6500 [6:33:35<12:54:21, 10.72s/it] 33%|      | 2167/6500 [6:33:45<12:47:04, 10.62s/it]                                                         33%|      | 2167/6500 [6:33:45<12:47:04, 10.62s/it] 33%|      | 2168/6500 [6:33:56<12:43:12, 10.57s/it]                                                         33%|      | 2168/6500 [6:33:56<12:43:12, 10.57s/it] 33%|      | 2169/6500 [6:34:06<12:39:27, 10.52s/it]                                                         33%|      | 2169/6500 [6:34:06<12:39:27, 10.52s/it] 33%|      | 2170/6500 [6:34:16<12:36:32, 10.48s/it]                                                         33%|      | 2170/6500 [6:34:16<12:36:32, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8092931509017944, 'eval_runtime': 3.9682, 'eval_samples_per_second': 5.796, 'eval_steps_per_second': 1.512, 'epoch': 0.33}
                                                         33%|      | 2170/6500 [6:34:20<12:36:32, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5192, 'learning_rate': 7.493716903870621e-05, 'epoch': 0.33}
{'loss': 0.4454, 'learning_rate': 7.49162137205897e-05, 'epoch': 0.33}
{'loss': 0.4356, 'learning_rate': 7.489525257845971e-05, 'epoch': 0.33}
{'loss': 0.4657, 'learning_rate': 7.48742856172158e-05, 'epoch': 0.33}
{'loss': 0.9815, 'learning_rate': 7.485331284175887e-05, 'epoch': 0.33}
 33%|      | 2171/6500 [6:34:31<14:11:16, 11.80s/it]                                                         33%|      | 2171/6500 [6:34:31<14:11:16, 11.80s/it] 33%|      | 2172/6500 [6:34:42<13:40:49, 11.38s/it]                                                         33%|      | 2172/6500 [6:34:42<13:40:49, 11.38s/it] 33%|      | 2173/6500 [6:34:52<13:19:15, 11.08s/it]                                                         33%|      | 2173/6500 [6:34:52<13:19:15, 11.08s/it] 33%|      | 2174/6500 [6:35:03<13:04:13, 10.88s/it]                                                         33%|      | 2174/6500 [6:35:03<13:04:13, 10.88s/it] 33%|      | 2175/6500 [6:35:13<12:53:16, 10.73s/it]                                                         33%|      | 2175/6500 [6:35:13<12:53:16, 10.73s/it] 33%|      | 2176/6500 [6:35:23<12:46:13, 10.63s/it]                  {'loss': 0.4635, 'learning_rate': 7.483233425699119e-05, 'epoch': 0.33}
{'loss': 0.4603, 'learning_rate': 7.481134986781634e-05, 'epoch': 0.33}
{'loss': 0.4531, 'learning_rate': 7.47903596791393e-05, 'epoch': 0.34}
{'loss': 0.433, 'learning_rate': 7.476936369586645e-05, 'epoch': 0.34}
{'loss': 0.4725, 'learning_rate': 7.47483619229054e-05, 'epoch': 0.34}
                                       33%|      | 2176/6500 [6:35:23<12:46:13, 10.63s/it] 33%|      | 2177/6500 [6:35:34<12:41:01, 10.56s/it]                                                         33%|      | 2177/6500 [6:35:34<12:41:01, 10.56s/it] 34%|      | 2178/6500 [6:35:44<12:36:52, 10.51s/it]                                                         34%|      | 2178/6500 [6:35:44<12:36:52, 10.51s/it] 34%|      | 2179/6500 [6:35:54<12:34:22, 10.48s/it]                                                         34%|      | 2179/6500 [6:35:54<12:34:22, 10.48s/it] 34%|      | 2180/6500 [6:36:05<12:32:35, 10.45s/it]                                                         34%|      | 2180/6500 [6:36:05<12:32:35, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8117444515228271, 'eval_runtime': 3.9786, 'eval_samples_per_second': 5.781, 'eval_steps_per_second': 1.508, 'epoch': 0.34}
                                                         34%|      | 2180/6500 [6:36:09<12:32:35, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4451, 'learning_rate': 7.472735436516524e-05, 'epoch': 0.34}
{'loss': 0.4365, 'learning_rate': 7.470634102755636e-05, 'epoch': 0.34}
{'loss': 0.442, 'learning_rate': 7.468532191499045e-05, 'epoch': 0.34}
{'loss': 0.4459, 'learning_rate': 7.466429703238065e-05, 'epoch': 0.34}
{'loss': 0.4482, 'learning_rate': 7.464326638464138e-05, 'epoch': 0.34}
 34%|      | 2181/6500 [6:36:20<14:06:41, 11.76s/it]                                                         34%|      | 2181/6500 [6:36:20<14:06:41, 11.76s/it] 34%|      | 2182/6500 [6:36:31<13:50:38, 11.54s/it]                                                         34%|      | 2182/6500 [6:36:31<13:50:38, 11.54s/it] 34%|      | 2183/6500 [6:36:41<13:25:46, 11.20s/it]                                                         34%|      | 2183/6500 [6:36:41<13:25:46, 11.20s/it] 34%|      | 2184/6500 [6:36:52<13:08:39, 10.96s/it]                                                         34%|      | 2184/6500 [6:36:52<13:08:39, 10.96s/it] 34%|      | 2185/6500 [6:37:02<12:56:45, 10.80s/it]                                                         34%|      | 2185/6500 [6:37:02<12:56:45, 10.80s/it] 34%|      | 2186/6500 [6:37:12<12:48:01, 10.68s/it]                  {'loss': 0.4453, 'learning_rate': 7.462222997668841e-05, 'epoch': 0.34}
{'loss': 0.4717, 'learning_rate': 7.460118781343893e-05, 'epoch': 0.34}
{'loss': 0.4376, 'learning_rate': 7.458013989981133e-05, 'epoch': 0.34}
{'loss': 0.4747, 'learning_rate': 7.45590862407255e-05, 'epoch': 0.34}
{'loss': 0.4741, 'learning_rate': 7.453802684110257e-05, 'epoch': 0.34}
                                       34%|      | 2186/6500 [6:37:12<12:48:01, 10.68s/it] 34%|      | 2187/6500 [6:37:23<12:41:58, 10.60s/it]                                                         34%|      | 2187/6500 [6:37:23<12:41:58, 10.60s/it] 34%|      | 2188/6500 [6:37:33<12:37:12, 10.54s/it]                                                         34%|      | 2188/6500 [6:37:33<12:37:12, 10.54s/it] 34%|      | 2189/6500 [6:37:44<12:38:25, 10.56s/it]                                                         34%|      | 2189/6500 [6:37:44<12:38:25, 10.56s/it] 34%|      | 2190/6500 [6:37:54<12:37:58, 10.55s/it]                                                         34%|      | 2190/6500 [6:37:54<12:37:58, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8093695640563965, 'eval_runtime': 4.1203, 'eval_samples_per_second': 5.582, 'eval_steps_per_second': 1.456, 'epoch': 0.34}
                                                         34%|      | 2190/6500 [6:37:58<12:37:58, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2190
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4516, 'learning_rate': 7.451696170586507e-05, 'epoch': 0.34}
{'loss': 0.4637, 'learning_rate': 7.449589083993684e-05, 'epoch': 0.34}
{'loss': 0.4659, 'learning_rate': 7.447481424824305e-05, 'epoch': 0.34}
{'loss': 0.445, 'learning_rate': 7.445373193571026e-05, 'epoch': 0.34}
{'loss': 0.456, 'learning_rate': 7.443264390726629e-05, 'epoch': 0.34}
 34%|      | 2191/6500 [6:38:09<14:13:55, 11.89s/it]                                                         34%|      | 2191/6500 [6:38:09<14:13:55, 11.89s/it] 34%|      | 2192/6500 [6:38:20<13:45:55, 11.50s/it]                                                         34%|      | 2192/6500 [6:38:20<13:45:55, 11.50s/it] 34%|      | 2193/6500 [6:38:30<13:22:31, 11.18s/it]                                                         34%|      | 2193/6500 [6:38:30<13:22:31, 11.18s/it] 34%|      | 2194/6500 [6:38:41<13:05:15, 10.94s/it]                                                         34%|      | 2194/6500 [6:38:41<13:05:15, 10.94s/it] 34%|      | 2195/6500 [6:38:51<12:53:30, 10.78s/it]                                                         34%|      | 2195/6500 [6:38:51<12:53:30, 10.78s/it] 34%|      | 2196/6500 [6:39:02<12:44:55, 10.66s/it]                  {'loss': 0.4747, 'learning_rate': 7.441155016784037e-05, 'epoch': 0.34}
{'loss': 0.4396, 'learning_rate': 7.439045072236301e-05, 'epoch': 0.34}
{'loss': 0.4539, 'learning_rate': 7.436934557576612e-05, 'epoch': 0.34}
{'loss': 0.4286, 'learning_rate': 7.434823473298283e-05, 'epoch': 0.34}
{'loss': 0.4764, 'learning_rate': 7.432711819894775e-05, 'epoch': 0.34}
                                       34%|      | 2196/6500 [6:39:02<12:44:55, 10.66s/it] 34%|      | 2197/6500 [6:39:12<12:39:05, 10.58s/it]                                                         34%|      | 2197/6500 [6:39:12<12:39:05, 10.58s/it] 34%|      | 2198/6500 [6:39:23<12:41:56, 10.63s/it]                                                         34%|      | 2198/6500 [6:39:23<12:41:56, 10.63s/it] 34%|      | 2199/6500 [6:39:33<12:37:11, 10.56s/it]                                                         34%|      | 2199/6500 [6:39:33<12:37:11, 10.56s/it] 34%|      | 2200/6500 [6:39:43<12:33:18, 10.51s/it]                                                         34%|      | 2200/6500 [6:39:43<12:33:18, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.808219313621521, 'eval_runtime': 4.018, 'eval_samples_per_second': 5.724, 'eval_steps_per_second': 1.493, 'epoch': 0.34}
                                                         34%|      | 2200/6500 [6:39:47<12:33:18, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2200I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2200/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5201, 'learning_rate': 7.430599597859666e-05, 'epoch': 0.34}
{'loss': 0.4389, 'learning_rate': 7.428486807686683e-05, 'epoch': 0.34}
{'loss': 0.4434, 'learning_rate': 7.426373449869673e-05, 'epoch': 0.34}
{'loss': 0.4539, 'learning_rate': 7.424259524902622e-05, 'epoch': 0.34}
{'loss': 0.9803, 'learning_rate': 7.422145033279646e-05, 'epoch': 0.34}
 34%|      | 2201/6500 [6:39:58<14:10:04, 11.86s/it]                                                         34%|      | 2201/6500 [6:39:58<14:10:04, 11.86s/it] 34%|      | 2202/6500 [6:40:09<13:37:56, 11.42s/it]                                                         34%|      | 2202/6500 [6:40:09<13:37:56, 11.42s/it] 34%|      | 2203/6500 [6:40:19<13:15:40, 11.11s/it]                                                         34%|      | 2203/6500 [6:40:19<13:15:40, 11.11s/it] 34%|      | 2204/6500 [6:40:30<13:00:00, 10.89s/it]                                                         34%|      | 2204/6500 [6:40:30<13:00:00, 10.89s/it] 34%|      | 2205/6500 [6:40:40<12:48:41, 10.74s/it]                                                         34%|      | 2205/6500 [6:40:40<12:48:41, 10.74s/it] 34%|      | 2206/6500 [6:40:50<12:41:14, 10.64s/it]                  {'loss': 0.47, 'learning_rate': 7.420029975494995e-05, 'epoch': 0.34}
{'loss': 0.4589, 'learning_rate': 7.417914352043052e-05, 'epoch': 0.34}
{'loss': 0.4126, 'learning_rate': 7.41579816341833e-05, 'epoch': 0.34}
{'loss': 0.4375, 'learning_rate': 7.413681410115474e-05, 'epoch': 0.34}
{'loss': 0.4821, 'learning_rate': 7.411564092629267e-05, 'epoch': 0.34}
                                       34%|      | 2206/6500 [6:40:50<12:41:14, 10.64s/it] 34%|      | 2207/6500 [6:41:01<12:35:26, 10.56s/it]                                                         34%|      | 2207/6500 [6:41:01<12:35:26, 10.56s/it] 34%|      | 2208/6500 [6:41:11<12:31:05, 10.50s/it]                                                         34%|      | 2208/6500 [6:41:11<12:31:05, 10.50s/it] 34%|      | 2209/6500 [6:41:22<12:28:17, 10.46s/it]                                                         34%|      | 2209/6500 [6:41:22<12:28:17, 10.46s/it] 34%|      | 2210/6500 [6:41:32<12:26:56, 10.45s/it]                                                         34%|      | 2210/6500 [6:41:32<12:26:56, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8140377402305603, 'eval_runtime': 3.9479, 'eval_samples_per_second': 5.826, 'eval_steps_per_second': 1.52, 'epoch': 0.34}
                                                         34%|      | 2210/6500 [6:41:36<12:26:56, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2210I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2210

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2210/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2210/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4271, 'learning_rate': 7.409446211454615e-05, 'epoch': 0.34}
{'loss': 0.4359, 'learning_rate': 7.407327767086561e-05, 'epoch': 0.34}
{'loss': 0.4214, 'learning_rate': 7.405208760020276e-05, 'epoch': 0.34}
{'loss': 0.4428, 'learning_rate': 7.403089190751069e-05, 'epoch': 0.34}
{'loss': 0.4312, 'learning_rate': 7.400969059774375e-05, 'epoch': 0.34}
 34%|      | 2211/6500 [6:41:47<14:06:22, 11.84s/it]                                                         34%|      | 2211/6500 [6:41:47<14:06:22, 11.84s/it] 34%|      | 2212/6500 [6:41:57<13:35:24, 11.41s/it]                                                         34%|      | 2212/6500 [6:41:57<13:35:24, 11.41s/it] 34%|      | 2213/6500 [6:42:08<13:13:07, 11.10s/it]                                                         34%|      | 2213/6500 [6:42:08<13:13:07, 11.10s/it] 34%|      | 2214/6500 [6:42:19<13:05:39, 11.00s/it]                                                         34%|      | 2214/6500 [6:42:19<13:05:39, 11.00s/it] 34%|      | 2215/6500 [6:42:29<12:52:52, 10.82s/it]                                                         34%|      | 2215/6500 [6:42:29<12:52:52, 10.82s/it] 34%|      | 2216/6500 [6:42:39<12:43:36, 10.69s/it]                  {'loss': 0.4408, 'learning_rate': 7.39884836758576e-05, 'epoch': 0.34}
{'loss': 0.4708, 'learning_rate': 7.396727114680925e-05, 'epoch': 0.34}
{'loss': 0.4372, 'learning_rate': 7.394605301555699e-05, 'epoch': 0.34}
{'loss': 0.4974, 'learning_rate': 7.392482928706044e-05, 'epoch': 0.34}
{'loss': 0.4659, 'learning_rate': 7.390359996628051e-05, 'epoch': 0.34}
                                       34%|      | 2216/6500 [6:42:39<12:43:36, 10.69s/it] 34%|      | 2217/6500 [6:42:50<12:37:28, 10.61s/it]                                                         34%|      | 2217/6500 [6:42:50<12:37:28, 10.61s/it] 34%|      | 2218/6500 [6:43:00<12:32:30, 10.54s/it]                                                         34%|      | 2218/6500 [6:43:00<12:32:30, 10.54s/it] 34%|      | 2219/6500 [6:43:11<12:28:59, 10.50s/it]                                                         34%|      | 2219/6500 [6:43:11<12:28:59, 10.50s/it] 34%|      | 2220/6500 [6:43:21<12:26:43, 10.47s/it]                                                         34%|      | 2220/6500 [6:43:21<12:26:43, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.808986485004425, 'eval_runtime': 4.9888, 'eval_samples_per_second': 4.61, 'eval_steps_per_second': 1.203, 'epoch': 0.34}
                                                         34%|      | 2220/6500 [6:43:26<12:26:43, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2220/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2220/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4539, 'learning_rate': 7.388236505817943e-05, 'epoch': 0.34}
{'loss': 0.469, 'learning_rate': 7.386112456772071e-05, 'epoch': 0.34}
{'loss': 0.4456, 'learning_rate': 7.38398784998692e-05, 'epoch': 0.34}
{'loss': 0.4532, 'learning_rate': 7.381862685959104e-05, 'epoch': 0.34}
{'loss': 0.4451, 'learning_rate': 7.379736965185368e-05, 'epoch': 0.34}
 34%|      | 2221/6500 [6:43:37<14:23:00, 12.10s/it]                                                         34%|      | 2221/6500 [6:43:37<14:23:00, 12.10s/it] 34%|      | 2222/6500 [6:43:47<13:47:08, 11.60s/it]                                                         34%|      | 2222/6500 [6:43:47<13:47:08, 11.60s/it] 34%|      | 2223/6500 [6:43:58<13:22:06, 11.25s/it]                                                         34%|      | 2223/6500 [6:43:58<13:22:06, 11.25s/it] 34%|      | 2224/6500 [6:44:08<13:04:06, 11.00s/it]                                                         34%|      | 2224/6500 [6:44:08<13:04:06, 11.00s/it] 34%|      | 2225/6500 [6:44:19<12:51:19, 10.83s/it]                                                         34%|      | 2225/6500 [6:44:19<12:51:19, 10.83s/it] 34%|      | 2226/6500 [6:44:29<12:42:28, 10.70s/it]                  {'loss': 0.4711, 'learning_rate': 7.377610688162586e-05, 'epoch': 0.34}
{'loss': 0.4316, 'learning_rate': 7.375483855387761e-05, 'epoch': 0.34}
{'loss': 0.4466, 'learning_rate': 7.373356467358027e-05, 'epoch': 0.34}
{'loss': 0.4485, 'learning_rate': 7.371228524570649e-05, 'epoch': 0.34}
{'loss': 0.5297, 'learning_rate': 7.369100027523022e-05, 'epoch': 0.34}
                                       34%|      | 2226/6500 [6:44:29<12:42:28, 10.70s/it] 34%|      | 2227/6500 [6:44:39<12:36:17, 10.62s/it]                                                         34%|      | 2227/6500 [6:44:39<12:36:17, 10.62s/it] 34%|      | 2228/6500 [6:44:50<12:32:28, 10.57s/it]                                                         34%|      | 2228/6500 [6:44:50<12:32:28, 10.57s/it] 34%|      | 2229/6500 [6:45:00<12:28:59, 10.52s/it]                                                         34%|      | 2229/6500 [6:45:00<12:28:59, 10.52s/it] 34%|      | 2230/6500 [6:45:11<12:26:44, 10.49s/it]                                                         34%|      | 2230/6500 [6:45:11<12:26:44, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8034725189208984, 'eval_runtime': 3.967, 'eval_samples_per_second': 5.798, 'eval_steps_per_second': 1.512, 'epoch': 0.34}
                                                         34%|      | 2230/6500 [6:45:15<12:26:44, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2230/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4518, 'learning_rate': 7.366970976712668e-05, 'epoch': 0.34}
{'loss': 0.4313, 'learning_rate': 7.364841372637239e-05, 'epoch': 0.34}
{'loss': 0.4513, 'learning_rate': 7.362711215794517e-05, 'epoch': 0.34}
{'loss': 0.785, 'learning_rate': 7.360580506682414e-05, 'epoch': 0.34}
{'loss': 0.6283, 'learning_rate': 7.35844924579897e-05, 'epoch': 0.34}
 34%|      | 2231/6500 [6:45:26<14:04:24, 11.87s/it]                                                         34%|      | 2231/6500 [6:45:26<14:04:24, 11.87s/it] 34%|      | 2232/6500 [6:45:36<13:33:04, 11.43s/it]                                                         34%|      | 2232/6500 [6:45:36<13:33:04, 11.43s/it] 34%|      | 2233/6500 [6:45:47<13:11:06, 11.12s/it]                                                         34%|      | 2233/6500 [6:45:47<13:11:06, 11.12s/it] 34%|      | 2234/6500 [6:45:57<12:55:31, 10.91s/it]                                                         34%|      | 2234/6500 [6:45:57<12:55:31, 10.91s/it] 34%|      | 2235/6500 [6:46:07<12:44:37, 10.76s/it]                                                         34%|      | 2235/6500 [6:46:07<12:44:37, 10.76s/it] 34%|      | 2236/6500 [6:46:18<12:37:00, 10.65s/it]                  {'loss': 0.4663, 'learning_rate': 7.356317433642357e-05, 'epoch': 0.34}
{'loss': 0.4503, 'learning_rate': 7.354185070710869e-05, 'epoch': 0.34}
{'loss': 0.4195, 'learning_rate': 7.352052157502932e-05, 'epoch': 0.34}
{'loss': 0.4657, 'learning_rate': 7.349918694517106e-05, 'epoch': 0.34}
{'loss': 0.4572, 'learning_rate': 7.347784682252072e-05, 'epoch': 0.34}
                                       34%|      | 2236/6500 [6:46:18<12:37:00, 10.65s/it] 34%|      | 2237/6500 [6:46:28<12:31:30, 10.58s/it]                                                         34%|      | 2237/6500 [6:46:28<12:31:30, 10.58s/it] 34%|      | 2238/6500 [6:46:39<12:27:51, 10.53s/it]                                                         34%|      | 2238/6500 [6:46:39<12:27:51, 10.53s/it] 34%|      | 2239/6500 [6:46:49<12:25:27, 10.50s/it]                                                         34%|      | 2239/6500 [6:46:49<12:25:27, 10.50s/it] 34%|      | 2240/6500 [6:47:00<12:36:42, 10.66s/it]                                                         34%|      | 2240/6500 [6:47:00<12:36:42, 10.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.808982253074646, 'eval_runtime': 3.9896, 'eval_samples_per_second': 5.765, 'eval_steps_per_second': 1.504, 'epoch': 0.34}
                                                         34%|      | 2240/6500 [6:47:04<12:36:42, 10.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.417, 'learning_rate': 7.345650121206645e-05, 'epoch': 0.34}
{'loss': 0.4443, 'learning_rate': 7.343515011879763e-05, 'epoch': 0.34}
{'loss': 0.4183, 'learning_rate': 7.341379354770496e-05, 'epoch': 0.35}
{'loss': 0.4481, 'learning_rate': 7.33924315037804e-05, 'epoch': 0.35}
{'loss': 0.4276, 'learning_rate': 7.337106399201721e-05, 'epoch': 0.35}
 34%|      | 2241/6500 [6:47:15<14:06:45, 11.93s/it]                                                         34%|      | 2241/6500 [6:47:15<14:06:45, 11.93s/it] 34%|      | 2242/6500 [6:47:25<13:34:42, 11.48s/it]                                                         34%|      | 2242/6500 [6:47:25<13:34:42, 11.48s/it] 35%|      | 2243/6500 [6:47:36<13:11:29, 11.16s/it]                                                         35%|      | 2243/6500 [6:47:36<13:11:29, 11.16s/it] 35%|      | 2244/6500 [6:47:46<12:55:09, 10.93s/it]                                                         35%|      | 2244/6500 [6:47:46<12:55:09, 10.93s/it] 35%|      | 2245/6500 [6:47:57<12:43:56, 10.77s/it]                                                         35%|      | 2245/6500 [6:47:57<12:43:56, 10.77s/it] 35%|      | 2246/6500 [6:48:07<12:36:20, 10.67s/it]                  {'loss': 0.4493, 'learning_rate': 7.334969101740991e-05, 'epoch': 0.35}
{'loss': 0.4312, 'learning_rate': 7.33283125849543e-05, 'epoch': 0.35}
{'loss': 0.4435, 'learning_rate': 7.330692869964746e-05, 'epoch': 0.35}
{'loss': 0.479, 'learning_rate': 7.328553936648774e-05, 'epoch': 0.35}
{'loss': 0.4413, 'learning_rate': 7.326414459047477e-05, 'epoch': 0.35}
                                       35%|      | 2246/6500 [6:48:07<12:36:20, 10.67s/it] 35%|      | 2247/6500 [6:48:18<12:39:53, 10.72s/it]                                                         35%|      | 2247/6500 [6:48:18<12:39:53, 10.72s/it] 35%|      | 2248/6500 [6:48:28<12:33:06, 10.63s/it]                                                         35%|      | 2248/6500 [6:48:28<12:33:06, 10.63s/it] 35%|      | 2249/6500 [6:48:39<12:31:21, 10.60s/it]                                                         35%|      | 2249/6500 [6:48:39<12:31:21, 10.60s/it] 35%|      | 2250/6500 [6:48:49<12:27:36, 10.55s/it]                                                         35%|      | 2250/6500 [6:48:49<12:27:36, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8096996545791626, 'eval_runtime': 4.4347, 'eval_samples_per_second': 5.186, 'eval_steps_per_second': 1.353, 'epoch': 0.35}
                                                         35%|      | 2250/6500 [6:48:54<12:27:36, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2250/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4612, 'learning_rate': 7.324274437660947e-05, 'epoch': 0.35}
{'loss': 0.4507, 'learning_rate': 7.322133872989398e-05, 'epoch': 0.35}
{'loss': 0.4325, 'learning_rate': 7.319992765533174e-05, 'epoch': 0.35}
{'loss': 0.4398, 'learning_rate': 7.317851115792749e-05, 'epoch': 0.35}
{'loss': 0.4641, 'learning_rate': 7.315708924268716e-05, 'epoch': 0.35}
 35%|      | 2251/6500 [6:49:05<14:09:00, 11.99s/it]                                                         35%|      | 2251/6500 [6:49:05<14:09:00, 11.99s/it] 35%|      | 2252/6500 [6:49:15<13:35:34, 11.52s/it]                                                         35%|      | 2252/6500 [6:49:15<13:35:34, 11.52s/it] 35%|      | 2253/6500 [6:49:25<13:11:55, 11.19s/it]                                                         35%|      | 2253/6500 [6:49:25<13:11:55, 11.19s/it] 35%|      | 2254/6500 [6:49:36<12:55:09, 10.95s/it]                                                         35%|      | 2254/6500 [6:49:36<12:55:09, 10.95s/it] 35%|      | 2255/6500 [6:49:46<12:43:37, 10.79s/it]                                                         35%|      | 2255/6500 [6:49:46<12:43:37, 10.79s/it] 35%|      | 2256/6500 [6:49:57<12:35:37, 10.68s/it]                  {'loss': 0.4516, 'learning_rate': 7.313566191461804e-05, 'epoch': 0.35}
{'loss': 0.447, 'learning_rate': 7.311422917872861e-05, 'epoch': 0.35}
{'loss': 0.4288, 'learning_rate': 7.309279104002864e-05, 'epoch': 0.35}
{'loss': 0.4547, 'learning_rate': 7.307134750352916e-05, 'epoch': 0.35}
{'loss': 0.5109, 'learning_rate': 7.304989857424249e-05, 'epoch': 0.35}
                                       35%|      | 2256/6500 [6:49:57<12:35:37, 10.68s/it] 35%|      | 2257/6500 [6:50:07<12:29:46, 10.60s/it]                                                         35%|      | 2257/6500 [6:50:07<12:29:46, 10.60s/it] 35%|      | 2258/6500 [6:50:18<12:25:42, 10.55s/it]                                                         35%|      | 2258/6500 [6:50:18<12:25:42, 10.55s/it] 35%|      | 2259/6500 [6:50:28<12:23:00, 10.51s/it]                                                         35%|      | 2259/6500 [6:50:28<12:23:00, 10.51s/it] 35%|      | 2260/6500 [6:50:38<12:20:39, 10.48s/it]                                                         35%|      | 2260/6500 [6:50:38<12:20:39, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8095405697822571, 'eval_runtime': 3.9636, 'eval_samples_per_second': 5.803, 'eval_steps_per_second': 1.514, 'epoch': 0.35}
                                                         35%|      | 2260/6500 [6:50:42<12:20:39, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2260/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4439, 'learning_rate': 7.302844425718218e-05, 'epoch': 0.35}
{'loss': 0.4216, 'learning_rate': 7.300698455736302e-05, 'epoch': 0.35}
{'loss': 0.4544, 'learning_rate': 7.298551947980113e-05, 'epoch': 0.35}
{'loss': 0.964, 'learning_rate': 7.296404902951381e-05, 'epoch': 0.35}
{'loss': 0.468, 'learning_rate': 7.294257321151964e-05, 'epoch': 0.35}
 35%|      | 2261/6500 [6:50:53<13:53:16, 11.79s/it]                                                         35%|      | 2261/6500 [6:50:53<13:53:16, 11.79s/it] 35%|      | 2262/6500 [6:51:04<13:23:59, 11.38s/it]                                                         35%|      | 2262/6500 [6:51:04<13:23:59, 11.38s/it] 35%|      | 2263/6500 [6:51:14<13:08:51, 11.17s/it]                                                         35%|      | 2263/6500 [6:51:14<13:08:51, 11.17s/it] 35%|      | 2264/6500 [6:51:25<12:51:41, 10.93s/it]                                                         35%|      | 2264/6500 [6:51:25<12:51:41, 10.93s/it] 35%|      | 2265/6500 [6:51:35<12:40:11, 10.77s/it]                                                         35%|      | 2265/6500 [6:51:35<12:40:11, 10.77s/it] 35%|      | 2266/6500 [6:51:46<12:32:25, 10.66s/it]                  {'loss': 0.4518, 'learning_rate': 7.292109203083848e-05, 'epoch': 0.35}
{'loss': 0.4396, 'learning_rate': 7.289960549249141e-05, 'epoch': 0.35}
{'loss': 0.4182, 'learning_rate': 7.287811360150081e-05, 'epoch': 0.35}
{'loss': 0.4756, 'learning_rate': 7.285661636289025e-05, 'epoch': 0.35}
{'loss': 0.4379, 'learning_rate': 7.283511378168458e-05, 'epoch': 0.35}
                                       35%|      | 2266/6500 [6:51:46<12:32:25, 10.66s/it] 35%|      | 2267/6500 [6:51:56<12:26:27, 10.58s/it]                                                         35%|      | 2267/6500 [6:51:56<12:26:27, 10.58s/it] 35%|      | 2268/6500 [6:52:06<12:22:23, 10.53s/it]                                                         35%|      | 2268/6500 [6:52:06<12:22:23, 10.53s/it] 35%|      | 2269/6500 [6:52:17<12:19:53, 10.49s/it]                                                         35%|      | 2269/6500 [6:52:17<12:19:53, 10.49s/it] 35%|      | 2270/6500 [6:52:27<12:17:24, 10.46s/it]                                                         35%|      | 2270/6500 [6:52:27<12:17:24, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8115273118019104, 'eval_runtime': 3.9588, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.35}
                                                         35%|      | 2270/6500 [6:52:31<12:17:24, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4333, 'learning_rate': 7.28136058629099e-05, 'epoch': 0.35}
{'loss': 0.4263, 'learning_rate': 7.279209261159357e-05, 'epoch': 0.35}
{'loss': 0.4299, 'learning_rate': 7.277057403276416e-05, 'epoch': 0.35}
{'loss': 0.4336, 'learning_rate': 7.274905013145153e-05, 'epoch': 0.35}
{'loss': 0.4281, 'learning_rate': 7.272752091268673e-05, 'epoch': 0.35}
 35%|      | 2271/6500 [6:52:42<13:51:06, 11.79s/it]                                                         35%|      | 2271/6500 [6:52:42<13:51:06, 11.79s/it] 35%|      | 2272/6500 [6:52:52<13:21:45, 11.38s/it]                                                         35%|      | 2272/6500 [6:52:52<13:21:45, 11.38s/it] 35%|      | 2273/6500 [6:53:03<13:00:52, 11.08s/it]                                                         35%|      | 2273/6500 [6:53:03<13:00:52, 11.08s/it] 35%|      | 2274/6500 [6:53:13<12:45:56, 10.87s/it]                                                         35%|      | 2274/6500 [6:53:13<12:45:56, 10.87s/it] 35%|      | 2275/6500 [6:53:24<12:35:34, 10.73s/it]                                                         35%|      | 2275/6500 [6:53:24<12:35:34, 10.73s/it] 35%|      | 2276/6500 [6:53:34<12:28:31, 10.63s/it]                  {'loss': 0.4648, 'learning_rate': 7.270598638150211e-05, 'epoch': 0.35}
{'loss': 0.4287, 'learning_rate': 7.268444654293122e-05, 'epoch': 0.35}
{'loss': 0.4696, 'learning_rate': 7.266290140200889e-05, 'epoch': 0.35}
{'loss': 0.4649, 'learning_rate': 7.264135096377115e-05, 'epoch': 0.35}
{'loss': 0.4381, 'learning_rate': 7.261979523325528e-05, 'epoch': 0.35}
                                       35%|      | 2276/6500 [6:53:34<12:28:31, 10.63s/it] 35%|      | 2277/6500 [6:53:44<12:23:18, 10.56s/it]                                                         35%|      | 2277/6500 [6:53:44<12:23:18, 10.56s/it] 35%|      | 2278/6500 [6:53:55<12:19:39, 10.51s/it]                                                         35%|      | 2278/6500 [6:53:55<12:19:39, 10.51s/it] 35%|      | 2279/6500 [6:54:06<12:26:02, 10.60s/it]                                                         35%|      | 2279/6500 [6:54:06<12:26:02, 10.60s/it] 35%|      | 2280/6500 [6:54:16<12:21:54, 10.55s/it]                                                         35%|      | 2280/6500 [6:54:16<12:21:54, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8085378408432007, 'eval_runtime': 4.1865, 'eval_samples_per_second': 5.494, 'eval_steps_per_second': 1.433, 'epoch': 0.35}
                                                         35%|      | 2280/6500 [6:54:20<12:21:54, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2280I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2280/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2280/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2280/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4568, 'learning_rate': 7.25982342154998e-05, 'epoch': 0.35}
{'loss': 0.4498, 'learning_rate': 7.257666791554448e-05, 'epoch': 0.35}
{'loss': 0.4375, 'learning_rate': 7.25550963384303e-05, 'epoch': 0.35}
{'loss': 0.4394, 'learning_rate': 7.253351948919948e-05, 'epoch': 0.35}
{'loss': 0.4606, 'learning_rate': 7.25119373728955e-05, 'epoch': 0.35}
 35%|      | 2281/6500 [6:54:31<13:57:36, 11.91s/it]                                                         35%|      | 2281/6500 [6:54:31<13:57:36, 11.91s/it] 35%|      | 2282/6500 [6:54:42<13:44:28, 11.73s/it]                                                         35%|      | 2282/6500 [6:54:42<13:44:28, 11.73s/it] 35%|      | 2283/6500 [6:54:53<13:18:16, 11.36s/it]                                                         35%|      | 2283/6500 [6:54:53<13:18:16, 11.36s/it] 35%|      | 2284/6500 [6:55:03<12:57:40, 11.07s/it]                                                         35%|      | 2284/6500 [6:55:03<12:57:40, 11.07s/it] 35%|      | 2285/6500 [6:55:14<12:43:27, 10.87s/it]                                                         35%|      | 2285/6500 [6:55:14<12:43:27, 10.87s/it] 35%|      | 2286/6500 [6:55:24<12:33:41, 10.73s/it]                  {'loss': 0.4299, 'learning_rate': 7.249034999456301e-05, 'epoch': 0.35}
{'loss': 0.4412, 'learning_rate': 7.246875735924797e-05, 'epoch': 0.35}
{'loss': 0.4214, 'learning_rate': 7.244715947199749e-05, 'epoch': 0.35}
{'loss': 0.4712, 'learning_rate': 7.242555633785999e-05, 'epoch': 0.35}
{'loss': 0.4949, 'learning_rate': 7.240394796188505e-05, 'epoch': 0.35}
                                       35%|      | 2286/6500 [6:55:24<12:33:41, 10.73s/it] 35%|      | 2287/6500 [6:55:35<12:26:20, 10.63s/it]                                                         35%|      | 2287/6500 [6:55:35<12:26:20, 10.63s/it] 35%|      | 2288/6500 [6:55:45<12:21:32, 10.56s/it]                                                         35%|      | 2288/6500 [6:55:45<12:21:32, 10.56s/it] 35%|      | 2289/6500 [6:55:55<12:18:07, 10.52s/it]                                                         35%|      | 2289/6500 [6:55:55<12:18:07, 10.52s/it] 35%|      | 2290/6500 [6:56:06<12:15:31, 10.48s/it]                                                         35%|      | 2290/6500 [6:56:06<12:15:31, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8063738346099854, 'eval_runtime': 4.6335, 'eval_samples_per_second': 4.964, 'eval_steps_per_second': 1.295, 'epoch': 0.35}
                                                         35%|      | 2290/6500 [6:56:10<12:15:31, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4519, 'learning_rate': 7.238233434912347e-05, 'epoch': 0.35}
{'loss': 0.4468, 'learning_rate': 7.236071550462733e-05, 'epoch': 0.35}
{'loss': 0.4324, 'learning_rate': 7.23390914334499e-05, 'epoch': 0.35}
{'loss': 0.9748, 'learning_rate': 7.231746214064566e-05, 'epoch': 0.35}
{'loss': 0.4544, 'learning_rate': 7.229582763127036e-05, 'epoch': 0.35}
 35%|      | 2291/6500 [6:56:21<14:02:19, 12.01s/it]                                                         35%|      | 2291/6500 [6:56:21<14:02:19, 12.01s/it] 35%|      | 2292/6500 [6:56:32<13:28:20, 11.53s/it]                                                         35%|      | 2292/6500 [6:56:32<13:28:20, 11.53s/it] 35%|      | 2293/6500 [6:56:42<13:04:18, 11.19s/it]                                                         35%|      | 2293/6500 [6:56:42<13:04:18, 11.19s/it] 35%|      | 2294/6500 [6:56:53<12:47:23, 10.95s/it]                                                         35%|      | 2294/6500 [6:56:53<12:47:23, 10.95s/it] 35%|      | 2295/6500 [6:57:03<12:43:39, 10.90s/it]                                                         35%|      | 2295/6500 [6:57:03<12:43:39, 10.90s/it] 35%|      | 2296/6500 [6:57:14<12:32:47, 10.74s/it]                  {'loss': 0.4514, 'learning_rate': 7.227418791038089e-05, 'epoch': 0.35}
{'loss': 0.4116, 'learning_rate': 7.225254298303543e-05, 'epoch': 0.35}
{'loss': 0.4617, 'learning_rate': 7.223089285429335e-05, 'epoch': 0.35}
{'loss': 0.4441, 'learning_rate': 7.220923752921524e-05, 'epoch': 0.35}
{'loss': 0.4192, 'learning_rate': 7.218757701286287e-05, 'epoch': 0.35}
                                       35%|      | 2296/6500 [6:57:14<12:32:47, 10.74s/it] 35%|      | 2297/6500 [6:57:24<12:25:29, 10.64s/it]                                                         35%|      | 2297/6500 [6:57:24<12:25:29, 10.64s/it] 35%|      | 2298/6500 [6:57:34<12:19:18, 10.56s/it]                                                         35%|      | 2298/6500 [6:57:34<12:19:18, 10.56s/it] 35%|      | 2299/6500 [6:57:45<12:15:00, 10.50s/it]                                                         35%|      | 2299/6500 [6:57:45<12:15:00, 10.50s/it] 35%|      | 2300/6500 [6:57:55<12:11:35, 10.45s/it]                                                         35%|      | 2300/6500 [6:57:55<12:11:35, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8117519617080688, 'eval_runtime': 3.9427, 'eval_samples_per_second': 5.834, 'eval_steps_per_second': 1.522, 'epoch': 0.35}
                                                         35%|      | 2300/6500 [6:57:59<12:11:35, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2300/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2300/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4358, 'learning_rate': 7.21659113102993e-05, 'epoch': 0.35}
{'loss': 0.4175, 'learning_rate': 7.214424042658872e-05, 'epoch': 0.35}
{'loss': 0.431, 'learning_rate': 7.212256436679658e-05, 'epoch': 0.35}
{'loss': 0.4289, 'learning_rate': 7.210088313598953e-05, 'epoch': 0.35}
{'loss': 0.4329, 'learning_rate': 7.207919673923542e-05, 'epoch': 0.35}
 35%|      | 2301/6500 [6:58:10<13:41:51, 11.74s/it]                                                         35%|      | 2301/6500 [6:58:10<13:41:51, 11.74s/it] 35%|      | 2302/6500 [6:58:20<13:11:57, 11.32s/it]                                                         35%|      | 2302/6500 [6:58:20<13:11:57, 11.32s/it] 35%|      | 2303/6500 [6:58:31<12:51:01, 11.02s/it]                                                         35%|      | 2303/6500 [6:58:31<12:51:01, 11.02s/it] 35%|      | 2304/6500 [6:58:41<12:36:14, 10.81s/it]                                                         35%|      | 2304/6500 [6:58:41<12:36:14, 10.81s/it] 35%|      | 2305/6500 [6:58:51<12:26:07, 10.67s/it]                                                         35%|      | 2305/6500 [6:58:51<12:26:07, 10.67s/it] 35%|      | 2306/6500 [6:59:02<12:18:47, 10.57s/it]                  {'loss': 0.4388, 'learning_rate': 7.20575051816033e-05, 'epoch': 0.35}
{'loss': 0.4284, 'learning_rate': 7.203580846816348e-05, 'epoch': 0.35}
{'loss': 0.4711, 'learning_rate': 7.20141066039874e-05, 'epoch': 0.36}
{'loss': 0.434, 'learning_rate': 7.199239959414775e-05, 'epoch': 0.36}
{'loss': 0.4496, 'learning_rate': 7.197068744371841e-05, 'epoch': 0.36}
                                       35%|      | 2306/6500 [6:59:02<12:18:47, 10.57s/it] 35%|      | 2307/6500 [6:59:12<12:13:37, 10.50s/it]                                                         35%|      | 2307/6500 [6:59:12<12:13:37, 10.50s/it] 36%|      | 2308/6500 [6:59:22<12:10:07, 10.45s/it]                                                         36%|      | 2308/6500 [6:59:22<12:10:07, 10.45s/it] 36%|      | 2309/6500 [6:59:33<12:07:39, 10.42s/it]                                                         36%|      | 2309/6500 [6:59:33<12:07:39, 10.42s/it] 36%|      | 2310/6500 [6:59:43<12:05:33, 10.39s/it]                                                         36%|      | 2310/6500 [6:59:43<12:05:33, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8109696507453918, 'eval_runtime': 3.9463, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.36}
                                                         36%|      | 2310/6500 [6:59:47<12:05:33, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4502, 'learning_rate': 7.194897015777447e-05, 'epoch': 0.36}
{'loss': 0.4285, 'learning_rate': 7.19272477413922e-05, 'epoch': 0.36}
{'loss': 0.4386, 'learning_rate': 7.190552019964909e-05, 'epoch': 0.36}
{'loss': 0.4485, 'learning_rate': 7.188378753762382e-05, 'epoch': 0.36}
{'loss': 0.4489, 'learning_rate': 7.186204976039628e-05, 'epoch': 0.36}
 36%|      | 2311/6500 [6:59:58<13:50:44, 11.90s/it]                                                         36%|      | 2311/6500 [6:59:58<13:50:44, 11.90s/it] 36%|      | 2312/6500 [7:00:09<13:17:59, 11.43s/it]                                                         36%|      | 2312/6500 [7:00:09<13:17:59, 11.43s/it] 36%|      | 2313/6500 [7:00:19<12:55:17, 11.11s/it]                                                         36%|      | 2313/6500 [7:00:19<12:55:17, 11.11s/it] 36%|      | 2314/6500 [7:00:29<12:39:07, 10.88s/it]                                                         36%|      | 2314/6500 [7:00:29<12:39:07, 10.88s/it] 36%|      | 2315/6500 [7:00:40<12:27:49, 10.72s/it]                                                         36%|      | 2315/6500 [7:00:40<12:27:49, 10.72s/it] 36%|      | 2316/6500 [7:00:50<12:19:43, 10.61s/it]                  {'loss': 0.4311, 'learning_rate': 7.184030687304752e-05, 'epoch': 0.36}
{'loss': 0.4336, 'learning_rate': 7.181855888065982e-05, 'epoch': 0.36}
{'loss': 0.4193, 'learning_rate': 7.179680578831666e-05, 'epoch': 0.36}
{'loss': 0.5358, 'learning_rate': 7.177504760110265e-05, 'epoch': 0.36}
{'loss': 0.4293, 'learning_rate': 7.175328432410366e-05, 'epoch': 0.36}
                                       36%|      | 2316/6500 [7:00:50<12:19:43, 10.61s/it] 36%|      | 2317/6500 [7:01:00<12:13:56, 10.53s/it]                                                         36%|      | 2317/6500 [7:01:00<12:13:56, 10.53s/it] 36%|      | 2318/6500 [7:01:11<12:10:07, 10.48s/it]                                                         36%|      | 2318/6500 [7:01:11<12:10:07, 10.48s/it] 36%|      | 2319/6500 [7:01:21<12:07:23, 10.44s/it]                                                         36%|      | 2319/6500 [7:01:21<12:07:23, 10.44s/it] 36%|      | 2320/6500 [7:01:31<12:05:36, 10.42s/it]                                                         36%|      | 2320/6500 [7:01:31<12:05:36, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8084098100662231, 'eval_runtime': 3.9501, 'eval_samples_per_second': 5.823, 'eval_steps_per_second': 1.519, 'epoch': 0.36}
                                                         36%|      | 2320/6500 [7:01:35<12:05:36, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.422, 'learning_rate': 7.173151596240673e-05, 'epoch': 0.36}
{'loss': 0.4401, 'learning_rate': 7.170974252110008e-05, 'epoch': 0.36}
{'loss': 0.9547, 'learning_rate': 7.168796400527312e-05, 'epoch': 0.36}
{'loss': 0.4431, 'learning_rate': 7.16661804200164e-05, 'epoch': 0.36}
{'loss': 0.449, 'learning_rate': 7.164439177042178e-05, 'epoch': 0.36}
 36%|      | 2321/6500 [7:01:46<13:37:37, 11.74s/it]                                                         36%|      | 2321/6500 [7:01:46<13:37:37, 11.74s/it] 36%|      | 2322/6500 [7:01:57<13:08:08, 11.32s/it]                                                         36%|      | 2322/6500 [7:01:57<13:08:08, 11.32s/it] 36%|      | 2323/6500 [7:02:07<12:47:14, 11.02s/it]                                                         36%|      | 2323/6500 [7:02:07<12:47:14, 11.02s/it] 36%|      | 2324/6500 [7:02:17<12:32:54, 10.82s/it]                                                         36%|      | 2324/6500 [7:02:17<12:32:54, 10.82s/it] 36%|      | 2325/6500 [7:02:28<12:22:50, 10.68s/it]                                                         36%|      | 2325/6500 [7:02:28<12:22:50, 10.68s/it] 36%|      | 2326/6500 [7:02:38<12:15:53, 10.58s/it]                  {'loss': 0.4393, 'learning_rate': 7.162259806158215e-05, 'epoch': 0.36}
{'loss': 0.4264, 'learning_rate': 7.16007992985917e-05, 'epoch': 0.36}
{'loss': 0.4579, 'learning_rate': 7.157899548654576e-05, 'epoch': 0.36}
{'loss': 0.4383, 'learning_rate': 7.155718663054083e-05, 'epoch': 0.36}
{'loss': 0.4126, 'learning_rate': 7.153537273567459e-05, 'epoch': 0.36}
                                       36%|      | 2326/6500 [7:02:38<12:15:53, 10.58s/it] 36%|      | 2327/6500 [7:02:48<12:10:39, 10.51s/it]                                                         36%|      | 2327/6500 [7:02:48<12:10:39, 10.51s/it] 36%|      | 2328/6500 [7:02:59<12:14:41, 10.57s/it]                                                         36%|      | 2328/6500 [7:02:59<12:14:41, 10.57s/it] 36%|      | 2329/6500 [7:03:09<12:09:39, 10.50s/it]                                                         36%|      | 2329/6500 [7:03:09<12:09:39, 10.50s/it] 36%|      | 2330/6500 [7:03:20<12:05:59, 10.45s/it]                                                         36%|      | 2330/6500 [7:03:20<12:05:59, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8100558519363403, 'eval_runtime': 3.9462, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.36}
                                                         36%|      | 2330/6500 [7:03:24<12:05:59, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2330
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4299, 'learning_rate': 7.15135538070459e-05, 'epoch': 0.36}
{'loss': 0.4224, 'learning_rate': 7.149172984975482e-05, 'epoch': 0.36}
{'loss': 0.4348, 'learning_rate': 7.146990086890258e-05, 'epoch': 0.36}
{'loss': 0.4156, 'learning_rate': 7.144806686959151e-05, 'epoch': 0.36}
{'loss': 0.4444, 'learning_rate': 7.142622785692524e-05, 'epoch': 0.36}
 36%|      | 2331/6500 [7:03:34<13:36:03, 11.74s/it]                                                         36%|      | 2331/6500 [7:03:34<13:36:03, 11.74s/it] 36%|      | 2332/6500 [7:03:45<13:06:16, 11.32s/it]                                                         36%|      | 2332/6500 [7:03:45<13:06:16, 11.32s/it] 36%|      | 2333/6500 [7:03:55<12:46:29, 11.04s/it]                                                         36%|      | 2333/6500 [7:03:55<12:46:29, 11.04s/it] 36%|      | 2334/6500 [7:04:06<12:31:45, 10.83s/it]                                                         36%|      | 2334/6500 [7:04:06<12:31:45, 10.83s/it] 36%|      | 2335/6500 [7:04:16<12:21:23, 10.68s/it]                                                         36%|      | 2335/6500 [7:04:16<12:21:23, 10.68s/it] 36%|      | 2336/6500 [7:04:26<12:14:05, 10.58s/it]                  {'loss': 0.4288, 'learning_rate': 7.140438383600848e-05, 'epoch': 0.36}
{'loss': 0.4462, 'learning_rate': 7.138253481194714e-05, 'epoch': 0.36}
{'loss': 0.456, 'learning_rate': 7.136068078984829e-05, 'epoch': 0.36}
{'loss': 0.4368, 'learning_rate': 7.133882177482019e-05, 'epoch': 0.36}
{'loss': 0.4508, 'learning_rate': 7.131695777197224e-05, 'epoch': 0.36}
                                       36%|      | 2336/6500 [7:04:26<12:14:05, 10.58s/it] 36%|      | 2337/6500 [7:04:37<12:08:54, 10.51s/it]                                                         36%|      | 2337/6500 [7:04:37<12:08:54, 10.51s/it] 36%|      | 2338/6500 [7:04:47<12:05:00, 10.45s/it]                                                         36%|      | 2338/6500 [7:04:47<12:05:00, 10.45s/it] 36%|      | 2339/6500 [7:04:57<12:04:23, 10.45s/it]                                                         36%|      | 2339/6500 [7:04:57<12:04:23, 10.45s/it] 36%|      | 2340/6500 [7:05:08<12:02:05, 10.41s/it]                                                         36%|      | 2340/6500 [7:05:08<12:02:05, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.80495685338974, 'eval_runtime': 3.9405, 'eval_samples_per_second': 5.837, 'eval_steps_per_second': 1.523, 'epoch': 0.36}
                                                         36%|      | 2340/6500 [7:05:12<12:02:05, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4473, 'learning_rate': 7.129508878641502e-05, 'epoch': 0.36}
{'loss': 0.4381, 'learning_rate': 7.127321482326026e-05, 'epoch': 0.36}
{'loss': 0.4241, 'learning_rate': 7.125133588762088e-05, 'epoch': 0.36}
{'loss': 0.4617, 'learning_rate': 7.122945198461097e-05, 'epoch': 0.36}
{'loss': 0.4345, 'learning_rate': 7.120756311934571e-05, 'epoch': 0.36}
 36%|      | 2341/6500 [7:05:23<13:35:20, 11.76s/it]                                                         36%|      | 2341/6500 [7:05:23<13:35:20, 11.76s/it] 36%|      | 2342/6500 [7:05:33<13:05:51, 11.34s/it]                                                         36%|      | 2342/6500 [7:05:33<13:05:51, 11.34s/it] 36%|      | 2343/6500 [7:05:43<12:44:54, 11.04s/it]                                                         36%|      | 2343/6500 [7:05:43<12:44:54, 11.04s/it] 36%|      | 2344/6500 [7:05:54<12:35:44, 10.91s/it]                                                         36%|      | 2344/6500 [7:05:54<12:35:44, 10.91s/it] 36%|      | 2345/6500 [7:06:04<12:24:04, 10.74s/it]                                                         36%|      | 2345/6500 [7:06:04<12:24:04, 10.74s/it] 36%|      | 2346/6500 [7:06:15<12:15:48, 10.63s/it]                  {'loss': 0.4287, 'learning_rate': 7.118566929694152e-05, 'epoch': 0.36}
{'loss': 0.4225, 'learning_rate': 7.116377052251595e-05, 'epoch': 0.36}
{'loss': 0.4451, 'learning_rate': 7.11418668011877e-05, 'epoch': 0.36}
{'loss': 0.5029, 'learning_rate': 7.111995813807662e-05, 'epoch': 0.36}
{'loss': 0.4297, 'learning_rate': 7.109804453830375e-05, 'epoch': 0.36}
                                       36%|      | 2346/6500 [7:06:15<12:15:48, 10.63s/it] 36%|      | 2347/6500 [7:06:25<12:09:35, 10.54s/it]                                                         36%|      | 2347/6500 [7:06:25<12:09:35, 10.54s/it] 36%|      | 2348/6500 [7:06:35<12:05:01, 10.48s/it]                                                         36%|      | 2348/6500 [7:06:35<12:05:01, 10.48s/it] 36%|      | 2349/6500 [7:06:46<12:06:30, 10.50s/it]                                                         36%|      | 2349/6500 [7:06:46<12:06:30, 10.50s/it] 36%|      | 2350/6500 [7:06:56<12:03:16, 10.46s/it]                                                         36%|      | 2350/6500 [7:06:56<12:03:16, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8079744577407837, 'eval_runtime': 3.9649, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.36}
                                                         36%|      | 2350/6500 [7:07:00<12:03:16, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4202, 'learning_rate': 7.107612600699124e-05, 'epoch': 0.36}
{'loss': 0.4428, 'learning_rate': 7.105420254926241e-05, 'epoch': 0.36}
{'loss': 0.9641, 'learning_rate': 7.103227417024176e-05, 'epoch': 0.36}
{'loss': 0.4474, 'learning_rate': 7.10103408750549e-05, 'epoch': 0.36}
{'loss': 0.4355, 'learning_rate': 7.09884026688286e-05, 'epoch': 0.36}
 36%|      | 2351/6500 [7:07:11<13:32:51, 11.76s/it]                                                         36%|      | 2351/6500 [7:07:11<13:32:51, 11.76s/it] 36%|      | 2352/6500 [7:07:21<13:03:26, 11.33s/it]                                                         36%|      | 2352/6500 [7:07:21<13:03:26, 11.33s/it] 36%|      | 2353/6500 [7:07:32<12:42:37, 11.03s/it]                                                         36%|      | 2353/6500 [7:07:32<12:42:37, 11.03s/it] 36%|      | 2354/6500 [7:07:42<12:28:06, 10.83s/it]                                                         36%|      | 2354/6500 [7:07:42<12:28:06, 10.83s/it] 36%|      | 2355/6500 [7:07:52<12:18:13, 10.69s/it]                                                         36%|      | 2355/6500 [7:07:52<12:18:13, 10.69s/it] 36%|      | 2356/6500 [7:08:03<12:10:38, 10.58s/it]                  {'loss': 0.4223, 'learning_rate': 7.09664595566908e-05, 'epoch': 0.36}
{'loss': 0.4289, 'learning_rate': 7.094451154377054e-05, 'epoch': 0.36}
{'loss': 0.4592, 'learning_rate': 7.092255863519806e-05, 'epoch': 0.36}
{'loss': 0.4231, 'learning_rate': 7.090060083610471e-05, 'epoch': 0.36}
{'loss': 0.4288, 'learning_rate': 7.087863815162298e-05, 'epoch': 0.36}
                                       36%|      | 2356/6500 [7:08:03<12:10:38, 10.58s/it] 36%|      | 2357/6500 [7:08:13<12:05:06, 10.50s/it]                                                         36%|      | 2357/6500 [7:08:13<12:05:06, 10.50s/it] 36%|      | 2358/6500 [7:08:23<12:01:20, 10.45s/it]                                                         36%|      | 2358/6500 [7:08:23<12:01:20, 10.45s/it] 36%|      | 2359/6500 [7:08:34<11:58:37, 10.41s/it]                                                         36%|      | 2359/6500 [7:08:34<11:58:37, 10.41s/it] 36%|      | 2360/6500 [7:08:44<12:04:44, 10.50s/it]                                                         36%|      | 2360/6500 [7:08:44<12:04:44, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8131215572357178, 'eval_runtime': 4.0276, 'eval_samples_per_second': 5.711, 'eval_steps_per_second': 1.49, 'epoch': 0.36}
                                                         36%|      | 2360/6500 [7:08:48<12:04:44, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4189, 'learning_rate': 7.085667058688656e-05, 'epoch': 0.36}
{'loss': 0.4425, 'learning_rate': 7.083469814703017e-05, 'epoch': 0.36}
{'loss': 0.4304, 'learning_rate': 7.081272083718977e-05, 'epoch': 0.36}
{'loss': 0.4292, 'learning_rate': 7.079073866250241e-05, 'epoch': 0.36}
{'loss': 0.4523, 'learning_rate': 7.07687516281063e-05, 'epoch': 0.36}
 36%|      | 2361/6500 [7:08:59<13:36:13, 11.83s/it]                                                         36%|      | 2361/6500 [7:08:59<13:36:13, 11.83s/it] 36%|      | 2362/6500 [7:09:10<13:04:45, 11.38s/it]                                                         36%|      | 2362/6500 [7:09:10<13:04:45, 11.38s/it] 36%|      | 2363/6500 [7:09:20<12:43:08, 11.07s/it]                                                         36%|      | 2363/6500 [7:09:20<12:43:08, 11.07s/it] 36%|      | 2364/6500 [7:09:30<12:27:35, 10.85s/it]                                                         36%|      | 2364/6500 [7:09:30<12:27:35, 10.85s/it] 36%|      | 2365/6500 [7:09:41<12:17:32, 10.70s/it]                                                         36%|      | 2365/6500 [7:09:41<12:17:32, 10.70s/it] 36%|      | 2366/6500 [7:09:51<12:10:19, 10.60s/it]                  {'loss': 0.4225, 'learning_rate': 7.07467597391408e-05, 'epoch': 0.36}
{'loss': 0.4621, 'learning_rate': 7.072476300074633e-05, 'epoch': 0.36}
{'loss': 0.4483, 'learning_rate': 7.070276141806452e-05, 'epoch': 0.36}
{'loss': 0.428, 'learning_rate': 7.06807549962381e-05, 'epoch': 0.36}
{'loss': 0.4413, 'learning_rate': 7.065874374041095e-05, 'epoch': 0.36}
                                       36%|      | 2366/6500 [7:09:51<12:10:19, 10.60s/it] 36%|      | 2367/6500 [7:10:01<12:05:02, 10.53s/it]                                                         36%|      | 2367/6500 [7:10:01<12:05:02, 10.53s/it] 36%|      | 2368/6500 [7:10:12<12:01:30, 10.48s/it]                                                         36%|      | 2368/6500 [7:10:12<12:01:30, 10.48s/it] 36%|      | 2369/6500 [7:10:22<11:58:48, 10.44s/it]                                                         36%|      | 2369/6500 [7:10:22<11:58:48, 10.44s/it] 36%|      | 2370/6500 [7:10:32<11:57:02, 10.42s/it]                                                         36%|      | 2370/6500 [7:10:32<11:57:02, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.806215763092041, 'eval_runtime': 3.9517, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.36}
                                                         36%|      | 2370/6500 [7:10:36<11:57:02, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2370
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2370/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2370/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4518, 'learning_rate': 7.063672765572806e-05, 'epoch': 0.36}
{'loss': 0.4317, 'learning_rate': 7.061470674733554e-05, 'epoch': 0.36}
{'loss': 0.433, 'learning_rate': 7.059268102038066e-05, 'epoch': 0.37}
{'loss': 0.458, 'learning_rate': 7.057065048001181e-05, 'epoch': 0.37}
{'loss': 0.4194, 'learning_rate': 7.054861513137847e-05, 'epoch': 0.37}
 36%|      | 2371/6500 [7:10:47<13:26:58, 11.73s/it]                                                         36%|      | 2371/6500 [7:10:47<13:26:58, 11.73s/it] 36%|      | 2372/6500 [7:10:58<12:58:10, 11.31s/it]                                                         36%|      | 2372/6500 [7:10:58<12:58:10, 11.31s/it] 37%|      | 2373/6500 [7:11:08<12:38:10, 11.02s/it]                                                         37%|      | 2373/6500 [7:11:08<12:38:10, 11.02s/it] 37%|      | 2374/6500 [7:11:18<12:24:11, 10.82s/it]                                                         37%|      | 2374/6500 [7:11:18<12:24:11, 10.82s/it] 37%|      | 2375/6500 [7:11:29<12:14:19, 10.68s/it]                                                         37%|      | 2375/6500 [7:11:29<12:14:19, 10.68s/it] 37%|      | 2376/6500 [7:11:39<12:18:07, 10.74s/it]                  {'loss': 0.4363, 'learning_rate': 7.052657497963129e-05, 'epoch': 0.37}
{'loss': 0.4119, 'learning_rate': 7.050453002992201e-05, 'epoch': 0.37}
{'loss': 0.4705, 'learning_rate': 7.04824802874035e-05, 'epoch': 0.37}
{'loss': 0.4732, 'learning_rate': 7.046042575722976e-05, 'epoch': 0.37}
{'loss': 0.4273, 'learning_rate': 7.04383664445559e-05, 'epoch': 0.37}
                                       37%|      | 2376/6500 [7:11:39<12:18:07, 10.74s/it] 37%|      | 2377/6500 [7:11:50<12:10:25, 10.63s/it]                                                         37%|      | 2377/6500 [7:11:50<12:10:25, 10.63s/it] 37%|      | 2378/6500 [7:12:00<12:04:46, 10.55s/it]                                                         37%|      | 2378/6500 [7:12:00<12:04:46, 10.55s/it] 37%|      | 2379/6500 [7:12:11<12:00:00, 10.48s/it]                                                         37%|      | 2379/6500 [7:12:11<12:00:00, 10.48s/it] 37%|      | 2380/6500 [7:12:21<11:56:46, 10.44s/it]                                                         37%|      | 2380/6500 [7:12:21<11:56:46, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8077322840690613, 'eval_runtime': 3.945, 'eval_samples_per_second': 5.83, 'eval_steps_per_second': 1.521, 'epoch': 0.37}
                                                         37%|      | 2380/6500 [7:12:25<11:56:46, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2380/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.439, 'learning_rate': 7.041630235453816e-05, 'epoch': 0.37}
{'loss': 0.4341, 'learning_rate': 7.039423349233387e-05, 'epoch': 0.37}
{'loss': 0.9648, 'learning_rate': 7.03721598631015e-05, 'epoch': 0.37}
{'loss': 0.4354, 'learning_rate': 7.035008147200062e-05, 'epoch': 0.37}
{'loss': 0.4435, 'learning_rate': 7.032799832419193e-05, 'epoch': 0.37}
 37%|      | 2381/6500 [7:12:36<13:25:51, 11.74s/it]                                                         37%|      | 2381/6500 [7:12:36<13:25:51, 11.74s/it] 37%|      | 2382/6500 [7:12:46<12:56:47, 11.32s/it]                                                         37%|      | 2382/6500 [7:12:46<12:56:47, 11.32s/it] 37%|      | 2383/6500 [7:12:56<12:36:12, 11.02s/it]                                                         37%|      | 2383/6500 [7:12:56<12:36:12, 11.02s/it] 37%|      | 2384/6500 [7:13:07<12:21:55, 10.82s/it]                                                         37%|      | 2384/6500 [7:13:07<12:21:55, 10.82s/it] 37%|      | 2385/6500 [7:13:17<12:12:01, 10.67s/it]                                                         37%|      | 2385/6500 [7:13:17<12:12:01, 10.67s/it] 37%|      | 2386/6500 [7:13:27<12:04:39, 10.57s/it]                  {'loss': 0.402, 'learning_rate': 7.030591042483723e-05, 'epoch': 0.37}
{'loss': 0.4414, 'learning_rate': 7.028381777909943e-05, 'epoch': 0.37}
{'loss': 0.4383, 'learning_rate': 7.026172039214256e-05, 'epoch': 0.37}
{'loss': 0.3953, 'learning_rate': 7.023961826913174e-05, 'epoch': 0.37}
{'loss': 0.4284, 'learning_rate': 7.02175114152332e-05, 'epoch': 0.37}
                                       37%|      | 2386/6500 [7:13:27<12:04:39, 10.57s/it] 37%|      | 2387/6500 [7:13:38<11:59:29, 10.50s/it]                                                         37%|      | 2387/6500 [7:13:38<11:59:29, 10.50s/it] 37%|      | 2388/6500 [7:13:48<11:55:35, 10.44s/it]                                                         37%|      | 2388/6500 [7:13:48<11:55:35, 10.44s/it] 37%|      | 2389/6500 [7:13:58<11:52:53, 10.40s/it]                                                         37%|      | 2389/6500 [7:13:58<11:52:53, 10.40s/it] 37%|      | 2390/6500 [7:14:09<11:51:07, 10.38s/it]                                                         37%|      | 2390/6500 [7:14:09<11:51:07, 10.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8134846687316895, 'eval_runtime': 3.9421, 'eval_samples_per_second': 5.834, 'eval_steps_per_second': 1.522, 'epoch': 0.37}
                                                         37%|      | 2390/6500 [7:14:13<11:51:07, 10.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2390
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2390/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2390/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4091, 'learning_rate': 7.01953998356143e-05, 'epoch': 0.37}
{'loss': 0.4339, 'learning_rate': 7.017328353544346e-05, 'epoch': 0.37}
{'loss': 0.4281, 'learning_rate': 7.015116251989027e-05, 'epoch': 0.37}
{'loss': 0.4378, 'learning_rate': 7.012903679412536e-05, 'epoch': 0.37}
{'loss': 0.4294, 'learning_rate': 7.010690636332047e-05, 'epoch': 0.37}
 37%|      | 2391/6500 [7:14:23<13:23:09, 11.73s/it]                                                         37%|      | 2391/6500 [7:14:23<13:23:09, 11.73s/it] 37%|      | 2392/6500 [7:14:34<13:01:32, 11.42s/it]                                                         37%|      | 2392/6500 [7:14:34<13:01:32, 11.42s/it] 37%|      | 2393/6500 [7:14:44<12:38:46, 11.09s/it]                                                         37%|      | 2393/6500 [7:14:44<12:38:46, 11.09s/it] 37%|      | 2394/6500 [7:14:55<12:23:03, 10.86s/it]                                                         37%|      | 2394/6500 [7:14:55<12:23:03, 10.86s/it] 37%|      | 2395/6500 [7:15:05<12:12:01, 10.70s/it]                                                         37%|      | 2395/6500 [7:15:05<12:12:01, 10.70s/it] 37%|      | 2396/6500 [7:15:15<12:04:19, 10.59s/it]                  {'loss': 0.4236, 'learning_rate': 7.008477123264848e-05, 'epoch': 0.37}
{'loss': 0.4849, 'learning_rate': 7.006263140728333e-05, 'epoch': 0.37}
{'loss': 0.4489, 'learning_rate': 7.004048689240007e-05, 'epoch': 0.37}
{'loss': 0.4449, 'learning_rate': 7.001833769317486e-05, 'epoch': 0.37}
{'loss': 0.4591, 'learning_rate': 6.999618381478492e-05, 'epoch': 0.37}
                                       37%|      | 2396/6500 [7:15:15<12:04:19, 10.59s/it] 37%|      | 2397/6500 [7:15:26<11:58:52, 10.51s/it]                                                         37%|      | 2397/6500 [7:15:26<11:58:52, 10.51s/it] 37%|      | 2398/6500 [7:15:36<11:55:06, 10.46s/it]                                                         37%|      | 2398/6500 [7:15:36<11:55:06, 10.46s/it] 37%|      | 2399/6500 [7:15:46<11:52:29, 10.42s/it]                                                         37%|      | 2399/6500 [7:15:46<11:52:29, 10.42s/it] 37%|      | 2400/6500 [7:15:57<11:50:30, 10.40s/it]                                                         37%|      | 2400/6500 [7:15:57<11:50:30, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8086533546447754, 'eval_runtime': 3.94, 'eval_samples_per_second': 5.838, 'eval_steps_per_second': 1.523, 'epoch': 0.37}
                                                         37%|      | 2400/6500 [7:16:01<11:50:30, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4201, 'learning_rate': 6.997402526240857e-05, 'epoch': 0.37}
{'loss': 0.4276, 'learning_rate': 6.995186204122528e-05, 'epoch': 0.37}
{'loss': 0.4552, 'learning_rate': 6.992969415641555e-05, 'epoch': 0.37}
{'loss': 0.4405, 'learning_rate': 6.990752161316098e-05, 'epoch': 0.37}
{'loss': 0.4181, 'learning_rate': 6.988534441664427e-05, 'epoch': 0.37}
 37%|      | 2401/6500 [7:16:12<13:19:10, 11.70s/it]                                                         37%|      | 2401/6500 [7:16:12<13:19:10, 11.70s/it] 37%|      | 2402/6500 [7:16:22<12:50:38, 11.28s/it]                                                         37%|      | 2402/6500 [7:16:22<12:50:38, 11.28s/it] 37%|      | 2403/6500 [7:16:32<12:30:50, 11.00s/it]                                                         37%|      | 2403/6500 [7:16:32<12:30:50, 11.00s/it] 37%|      | 2404/6500 [7:16:42<12:17:04, 10.80s/it]                                                         37%|      | 2404/6500 [7:16:42<12:17:04, 10.80s/it] 37%|      | 2405/6500 [7:16:53<12:07:32, 10.66s/it]                                                         37%|      | 2405/6500 [7:16:53<12:07:32, 10.66s/it] 37%|      | 2406/6500 [7:17:03<12:00:35, 10.56s/it]                  {'loss': 0.4298, 'learning_rate': 6.986316257204921e-05, 'epoch': 0.37}
{'loss': 0.4437, 'learning_rate': 6.984097608456067e-05, 'epoch': 0.37}
{'loss': 0.5097, 'learning_rate': 6.98187849593646e-05, 'epoch': 0.37}
{'loss': 0.415, 'learning_rate': 6.979658920164806e-05, 'epoch': 0.37}
{'loss': 0.4209, 'learning_rate': 6.977438881659916e-05, 'epoch': 0.37}
                                       37%|      | 2406/6500 [7:17:03<12:00:35, 10.56s/it] 37%|      | 2407/6500 [7:17:13<11:55:39, 10.49s/it]                                                         37%|      | 2407/6500 [7:17:13<11:55:39, 10.49s/it] 37%|      | 2408/6500 [7:17:24<11:56:26, 10.50s/it]                                                         37%|      | 2408/6500 [7:17:24<11:56:26, 10.50s/it] 37%|      | 2409/6500 [7:17:34<11:52:45, 10.45s/it]                                                         37%|      | 2409/6500 [7:17:34<11:52:45, 10.45s/it] 37%|      | 2410/6500 [7:17:45<11:49:38, 10.41s/it]                                                         37%|      | 2410/6500 [7:17:45<11:49:38, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8112075924873352, 'eval_runtime': 4.1652, 'eval_samples_per_second': 5.522, 'eval_steps_per_second': 1.441, 'epoch': 0.37}
                                                         37%|      | 2410/6500 [7:17:49<11:49:38, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2410/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4402, 'learning_rate': 6.975218380940709e-05, 'epoch': 0.37}
{'loss': 0.9483, 'learning_rate': 6.972997418526215e-05, 'epoch': 0.37}
{'loss': 0.4377, 'learning_rate': 6.970775994935571e-05, 'epoch': 0.37}
{'loss': 0.436, 'learning_rate': 6.96855411068802e-05, 'epoch': 0.37}
{'loss': 0.4267, 'learning_rate': 6.966331766302916e-05, 'epoch': 0.37}
 37%|      | 2411/6500 [7:18:00<13:23:48, 11.79s/it]                                                         37%|      | 2411/6500 [7:18:00<13:23:48, 11.79s/it] 37%|      | 2412/6500 [7:18:10<12:53:28, 11.35s/it]                                                         37%|      | 2412/6500 [7:18:10<12:53:28, 11.35s/it] 37%|      | 2413/6500 [7:18:20<12:32:25, 11.05s/it]                                                         37%|      | 2413/6500 [7:18:20<12:32:25, 11.05s/it] 37%|      | 2414/6500 [7:18:31<12:17:22, 10.83s/it]                                                         37%|      | 2414/6500 [7:18:31<12:17:22, 10.83s/it] 37%|      | 2415/6500 [7:18:41<12:06:59, 10.68s/it]                                                         37%|      | 2415/6500 [7:18:41<12:06:59, 10.68s/it] 37%|      | 2416/6500 [7:18:51<12:00:08, 10.58s/it]                  {'loss': 0.4162, 'learning_rate': 6.964108962299717e-05, 'epoch': 0.37}
{'loss': 0.4535, 'learning_rate': 6.961885699197989e-05, 'epoch': 0.37}
{'loss': 0.423, 'learning_rate': 6.959661977517408e-05, 'epoch': 0.37}
{'loss': 0.4201, 'learning_rate': 6.957437797777754e-05, 'epoch': 0.37}
{'loss': 0.4138, 'learning_rate': 6.955213160498917e-05, 'epoch': 0.37}
                                       37%|      | 2416/6500 [7:18:51<12:00:08, 10.58s/it] 37%|      | 2417/6500 [7:19:02<11:54:36, 10.50s/it]                                                         37%|      | 2417/6500 [7:19:02<11:54:36, 10.50s/it] 37%|      | 2418/6500 [7:19:12<11:50:27, 10.44s/it]                                                         37%|      | 2418/6500 [7:19:12<11:50:27, 10.44s/it] 37%|      | 2419/6500 [7:19:22<11:48:05, 10.41s/it]                                                         37%|      | 2419/6500 [7:19:22<11:48:05, 10.41s/it] 37%|      | 2420/6500 [7:19:33<11:46:18, 10.39s/it]                                                         37%|      | 2420/6500 [7:19:33<11:46:18, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8145220279693604, 'eval_runtime': 3.9443, 'eval_samples_per_second': 5.831, 'eval_steps_per_second': 1.521, 'epoch': 0.37}
                                                         37%|      | 2420/6500 [7:19:37<11:46:18, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2420the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2420

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4109, 'learning_rate': 6.952988066200891e-05, 'epoch': 0.37}
{'loss': 0.4347, 'learning_rate': 6.950762515403779e-05, 'epoch': 0.37}
{'loss': 0.4139, 'learning_rate': 6.94853650862779e-05, 'epoch': 0.37}
{'loss': 0.4428, 'learning_rate': 6.946310046393239e-05, 'epoch': 0.37}
{'loss': 0.4105, 'learning_rate': 6.944083129220548e-05, 'epoch': 0.37}
 37%|      | 2421/6500 [7:19:47<13:17:21, 11.73s/it]                                                         37%|      | 2421/6500 [7:19:47<13:17:21, 11.73s/it] 37%|      | 2422/6500 [7:19:58<12:50:39, 11.34s/it]                                                         37%|      | 2422/6500 [7:19:58<12:50:39, 11.34s/it] 37%|      | 2423/6500 [7:20:08<12:29:56, 11.04s/it]                                                         37%|      | 2423/6500 [7:20:08<12:29:56, 11.04s/it] 37%|      | 2424/6500 [7:20:19<12:15:29, 10.83s/it]                                                         37%|      | 2424/6500 [7:20:19<12:15:29, 10.83s/it] 37%|      | 2425/6500 [7:20:29<12:12:58, 10.79s/it]                                                         37%|      | 2425/6500 [7:20:29<12:12:58, 10.79s/it] 37%|      | 2426/6500 [7:20:40<12:03:23, 10.65s/it]                  {'loss': 0.4476, 'learning_rate': 6.941855757630248e-05, 'epoch': 0.37}
{'loss': 0.43, 'learning_rate': 6.939627932142969e-05, 'epoch': 0.37}
{'loss': 0.4225, 'learning_rate': 6.937399653279454e-05, 'epoch': 0.37}
{'loss': 0.4362, 'learning_rate': 6.935170921560552e-05, 'epoch': 0.37}
{'loss': 0.4275, 'learning_rate': 6.932941737507211e-05, 'epoch': 0.37}
                                       37%|      | 2426/6500 [7:20:40<12:03:23, 10.65s/it] 37%|      | 2427/6500 [7:20:50<11:56:53, 10.56s/it]                                                         37%|      | 2427/6500 [7:20:50<11:56:53, 10.56s/it] 37%|      | 2428/6500 [7:21:00<11:51:52, 10.49s/it]                                                         37%|      | 2428/6500 [7:21:00<11:51:52, 10.49s/it] 37%|      | 2429/6500 [7:21:11<11:48:28, 10.44s/it]                                                         37%|      | 2429/6500 [7:21:11<11:48:28, 10.44s/it] 37%|      | 2430/6500 [7:21:21<11:46:10, 10.41s/it]                                                         37%|      | 2430/6500 [7:21:21<11:46:10, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8109102845191956, 'eval_runtime': 3.9531, 'eval_samples_per_second': 5.818, 'eval_steps_per_second': 1.518, 'epoch': 0.37}
                                                         37%|      | 2430/6500 [7:21:25<11:46:10, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2430
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4252, 'learning_rate': 6.930712101640492e-05, 'epoch': 0.37}
{'loss': 0.4208, 'learning_rate': 6.928482014481558e-05, 'epoch': 0.37}
{'loss': 0.4496, 'learning_rate': 6.92625147655168e-05, 'epoch': 0.37}
{'loss': 0.4248, 'learning_rate': 6.924020488372229e-05, 'epoch': 0.37}
{'loss': 0.4199, 'learning_rate': 6.921789050464688e-05, 'epoch': 0.37}
 37%|      | 2431/6500 [7:21:36<13:14:46, 11.72s/it]                                                         37%|      | 2431/6500 [7:21:36<13:14:46, 11.72s/it] 37%|      | 2432/6500 [7:21:46<12:46:37, 11.31s/it]                                                         37%|      | 2432/6500 [7:21:46<12:46:37, 11.31s/it] 37%|      | 2433/6500 [7:21:56<12:26:44, 11.02s/it]                                                         37%|      | 2433/6500 [7:21:56<12:26:44, 11.02s/it] 37%|      | 2434/6500 [7:22:07<12:12:33, 10.81s/it]                                                         37%|      | 2434/6500 [7:22:07<12:12:33, 10.81s/it] 37%|      | 2435/6500 [7:22:17<12:02:53, 10.67s/it]                                                         37%|      | 2435/6500 [7:22:17<12:02:53, 10.67s/it] 37%|      | 2436/6500 [7:22:27<11:55:50, 10.57s/it]                  {'loss': 0.4081, 'learning_rate': 6.91955716335064e-05, 'epoch': 0.37}
{'loss': 0.4486, 'learning_rate': 6.917324827551778e-05, 'epoch': 0.37}
{'loss': 0.4916, 'learning_rate': 6.915092043589895e-05, 'epoch': 0.38}
{'loss': 0.4182, 'learning_rate': 6.912858811986888e-05, 'epoch': 0.38}
{'loss': 0.4053, 'learning_rate': 6.910625133264766e-05, 'epoch': 0.38}
                                       37%|      | 2436/6500 [7:22:27<11:55:50, 10.57s/it] 37%|      | 2437/6500 [7:22:38<11:50:58, 10.50s/it]                                                         37%|      | 2437/6500 [7:22:38<11:50:58, 10.50s/it] 38%|      | 2438/6500 [7:22:48<11:47:32, 10.45s/it]                                                         38%|      | 2438/6500 [7:22:48<11:47:32, 10.45s/it] 38%|      | 2439/6500 [7:22:58<11:45:46, 10.43s/it]                                                         38%|      | 2439/6500 [7:22:58<11:45:46, 10.43s/it] 38%|      | 2440/6500 [7:23:09<11:43:55, 10.40s/it]                                                         38%|      | 2440/6500 [7:23:09<11:43:55, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8180760741233826, 'eval_runtime': 3.9498, 'eval_samples_per_second': 5.823, 'eval_steps_per_second': 1.519, 'epoch': 0.38}
                                                         38%|      | 2440/6500 [7:23:13<11:43:55, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2440/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.435, 'learning_rate': 6.908391007945636e-05, 'epoch': 0.38}
{'loss': 0.9491, 'learning_rate': 6.906156436551712e-05, 'epoch': 0.38}
{'loss': 0.4483, 'learning_rate': 6.903921419605308e-05, 'epoch': 0.38}
{'loss': 0.4387, 'learning_rate': 6.90168595762885e-05, 'epoch': 0.38}
{'loss': 0.3945, 'learning_rate': 6.899450051144862e-05, 'epoch': 0.38}
 38%|      | 2441/6500 [7:23:24<13:18:13, 11.80s/it]                                                         38%|      | 2441/6500 [7:23:24<13:18:13, 11.80s/it] 38%|      | 2442/6500 [7:23:34<12:48:19, 11.36s/it]                                                         38%|      | 2442/6500 [7:23:34<12:48:19, 11.36s/it] 38%|      | 2443/6500 [7:23:45<12:27:28, 11.05s/it]                                                         38%|      | 2443/6500 [7:23:45<12:27:28, 11.05s/it] 38%|      | 2444/6500 [7:23:55<12:13:25, 10.85s/it]                                                         38%|      | 2444/6500 [7:23:55<12:13:25, 10.85s/it] 38%|      | 2445/6500 [7:24:05<12:03:03, 10.70s/it]                                                         38%|      | 2445/6500 [7:24:05<12:03:03, 10.70s/it] 38%|      | 2446/6500 [7:24:16<11:55:41, 10.59s/it]                  {'loss': 0.4225, 'learning_rate': 6.897213700675973e-05, 'epoch': 0.38}
{'loss': 0.4551, 'learning_rate': 6.894976906744916e-05, 'epoch': 0.38}
{'loss': 0.413, 'learning_rate': 6.89273966987453e-05, 'epoch': 0.38}
{'loss': 0.4189, 'learning_rate': 6.890501990587754e-05, 'epoch': 0.38}
{'loss': 0.4096, 'learning_rate': 6.888263869407631e-05, 'epoch': 0.38}
                                       38%|      | 2446/6500 [7:24:16<11:55:41, 10.59s/it] 38%|      | 2447/6500 [7:24:26<11:50:23, 10.52s/it]                                                         38%|      | 2447/6500 [7:24:26<11:50:23, 10.52s/it] 38%|      | 2448/6500 [7:24:36<11:46:13, 10.46s/it]                                                         38%|      | 2448/6500 [7:24:36<11:46:13, 10.46s/it] 38%|      | 2449/6500 [7:24:47<11:43:53, 10.43s/it]                                                         38%|      | 2449/6500 [7:24:47<11:43:53, 10.43s/it] 38%|      | 2450/6500 [7:24:57<11:43:54, 10.43s/it]                                                         38%|      | 2450/6500 [7:24:57<11:43:54, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8172264695167542, 'eval_runtime': 4.1793, 'eval_samples_per_second': 5.503, 'eval_steps_per_second': 1.436, 'epoch': 0.38}
                                                         38%|      | 2450/6500 [7:25:01<11:43:54, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4262, 'learning_rate': 6.886025306857311e-05, 'epoch': 0.38}
{'loss': 0.4115, 'learning_rate': 6.883786303460041e-05, 'epoch': 0.38}
{'loss': 0.4152, 'learning_rate': 6.881546859739179e-05, 'epoch': 0.38}
{'loss': 0.4365, 'learning_rate': 6.879306976218177e-05, 'epoch': 0.38}
{'loss': 0.413, 'learning_rate': 6.877066653420594e-05, 'epoch': 0.38}
 38%|      | 2451/6500 [7:25:12<13:16:37, 11.80s/it]                                                         38%|      | 2451/6500 [7:25:12<13:16:37, 11.80s/it] 38%|      | 2452/6500 [7:25:22<12:47:02, 11.37s/it]                                                         38%|      | 2452/6500 [7:25:22<12:47:02, 11.37s/it] 38%|      | 2453/6500 [7:25:33<12:26:00, 11.06s/it]                                                         38%|      | 2453/6500 [7:25:33<12:26:00, 11.06s/it] 38%|      | 2454/6500 [7:25:43<12:11:29, 10.85s/it]                                                         38%|      | 2454/6500 [7:25:43<12:11:29, 10.85s/it] 38%|      | 2455/6500 [7:25:53<12:01:49, 10.71s/it]                                                         38%|      | 2455/6500 [7:25:53<12:01:49, 10.71s/it] 38%|      | 2456/6500 [7:26:04<11:54:42, 10.60s/it]                  {'loss': 0.4551, 'learning_rate': 6.874825891870093e-05, 'epoch': 0.38}
{'loss': 0.4402, 'learning_rate': 6.872584692090442e-05, 'epoch': 0.38}
{'loss': 0.4266, 'learning_rate': 6.870343054605503e-05, 'epoch': 0.38}
{'loss': 0.4467, 'learning_rate': 6.868100979939249e-05, 'epoch': 0.38}
{'loss': 0.4226, 'learning_rate': 6.865858468615747e-05, 'epoch': 0.38}
                                       38%|      | 2456/6500 [7:26:04<11:54:42, 10.60s/it] 38%|      | 2457/6500 [7:26:15<11:57:48, 10.65s/it]                                                         38%|      | 2457/6500 [7:26:15<11:57:48, 10.65s/it] 38%|      | 2458/6500 [7:26:25<11:51:26, 10.56s/it]                                                         38%|      | 2458/6500 [7:26:25<11:51:26, 10.56s/it] 38%|      | 2459/6500 [7:26:35<11:47:40, 10.51s/it]                                                         38%|      | 2459/6500 [7:26:35<11:47:40, 10.51s/it] 38%|      | 2460/6500 [7:26:46<11:45:01, 10.47s/it]                                                         38%|      | 2460/6500 [7:26:46<11:45:01, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8117114901542664, 'eval_runtime': 4.0161, 'eval_samples_per_second': 5.727, 'eval_steps_per_second': 1.494, 'epoch': 0.38}
                                                         38%|      | 2460/6500 [7:26:50<11:45:01, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4376, 'learning_rate': 6.863615521159172e-05, 'epoch': 0.38}
{'loss': 0.4196, 'learning_rate': 6.8613721380938e-05, 'epoch': 0.38}
{'loss': 0.4438, 'learning_rate': 6.85912831994401e-05, 'epoch': 0.38}
{'loss': 0.4169, 'learning_rate': 6.856884067234279e-05, 'epoch': 0.38}
{'loss': 0.413, 'learning_rate': 6.854639380489184e-05, 'epoch': 0.38}
 38%|      | 2461/6500 [7:27:01<13:13:16, 11.78s/it]                                                         38%|      | 2461/6500 [7:27:01<13:13:16, 11.78s/it] 38%|      | 2462/6500 [7:27:11<12:44:09, 11.35s/it]                                                         38%|      | 2462/6500 [7:27:11<12:44:09, 11.35s/it] 38%|      | 2463/6500 [7:27:21<12:24:17, 11.06s/it]                                                         38%|      | 2463/6500 [7:27:21<12:24:17, 11.06s/it] 38%|      | 2464/6500 [7:27:32<12:09:52, 10.85s/it]                                                         38%|      | 2464/6500 [7:27:32<12:09:52, 10.85s/it] 38%|      | 2465/6500 [7:27:42<11:59:54, 10.70s/it]                                                         38%|      | 2465/6500 [7:27:42<11:59:54, 10.70s/it] 38%|      | 2466/6500 [7:27:52<11:52:56, 10.60s/it]                  {'loss': 0.4071, 'learning_rate': 6.852394260233414e-05, 'epoch': 0.38}
{'loss': 0.4622, 'learning_rate': 6.850148706991745e-05, 'epoch': 0.38}
{'loss': 0.4741, 'learning_rate': 6.847902721289068e-05, 'epoch': 0.38}
{'loss': 0.4132, 'learning_rate': 6.845656303650365e-05, 'epoch': 0.38}
{'loss': 0.4322, 'learning_rate': 6.843409454600722e-05, 'epoch': 0.38}
                                       38%|      | 2466/6500 [7:27:52<11:52:56, 10.60s/it] 38%|      | 2467/6500 [7:28:03<11:47:58, 10.53s/it]                                                         38%|      | 2467/6500 [7:28:03<11:47:58, 10.53s/it] 38%|      | 2468/6500 [7:28:13<11:44:32, 10.48s/it]                                                         38%|      | 2468/6500 [7:28:13<11:44:32, 10.48s/it] 38%|      | 2469/6500 [7:28:24<11:42:10, 10.45s/it]                                                         38%|      | 2469/6500 [7:28:24<11:42:10, 10.45s/it] 38%|      | 2470/6500 [7:28:34<11:40:41, 10.43s/it]                                                         38%|      | 2470/6500 [7:28:34<11:40:41, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8150389194488525, 'eval_runtime': 3.9571, 'eval_samples_per_second': 5.812, 'eval_steps_per_second': 1.516, 'epoch': 0.38}
                                                         38%|      | 2470/6500 [7:28:38<11:40:41, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6419, 'learning_rate': 6.841162174665326e-05, 'epoch': 0.38}
{'loss': 0.7473, 'learning_rate': 6.838914464369467e-05, 'epoch': 0.38}
{'loss': 0.4417, 'learning_rate': 6.836666324238532e-05, 'epoch': 0.38}
{'loss': 0.4239, 'learning_rate': 6.834417754798011e-05, 'epoch': 0.38}
{'loss': 0.3933, 'learning_rate': 6.832168756573496e-05, 'epoch': 0.38}
 38%|      | 2471/6500 [7:28:49<13:10:03, 11.77s/it]                                                         38%|      | 2471/6500 [7:28:49<13:10:03, 11.77s/it] 38%|      | 2472/6500 [7:28:59<12:41:29, 11.34s/it]                                                         38%|      | 2472/6500 [7:28:59<12:41:29, 11.34s/it] 38%|      | 2473/6500 [7:29:10<12:33:46, 11.23s/it]                                                         38%|      | 2473/6500 [7:29:10<12:33:46, 11.23s/it] 38%|      | 2474/6500 [7:29:20<12:15:58, 10.97s/it]                                                         38%|      | 2474/6500 [7:29:20<12:15:58, 10.97s/it] 38%|      | 2475/6500 [7:29:31<12:02:47, 10.77s/it]                                                         38%|      | 2475/6500 [7:29:31<12:02:47, 10.77s/it] 38%|      | 2476/6500 [7:29:41<11:56:10, 10.68s/it]                  {'loss': 0.4345, 'learning_rate': 6.82991933009067e-05, 'epoch': 0.38}
{'loss': 0.4254, 'learning_rate': 6.827669475875328e-05, 'epoch': 0.38}
{'loss': 0.39, 'learning_rate': 6.825419194453359e-05, 'epoch': 0.38}
{'loss': 0.4267, 'learning_rate': 6.823168486350753e-05, 'epoch': 0.38}
{'loss': 0.3984, 'learning_rate': 6.820917352093597e-05, 'epoch': 0.38}
                                       38%|      | 2476/6500 [7:29:41<11:56:10, 10.68s/it] 38%|      | 2477/6500 [7:29:52<11:50:08, 10.59s/it]                                                         38%|      | 2477/6500 [7:29:52<11:50:08, 10.59s/it] 38%|      | 2478/6500 [7:30:02<11:45:15, 10.52s/it]                                                         38%|      | 2478/6500 [7:30:02<11:45:15, 10.52s/it] 38%|      | 2479/6500 [7:30:12<11:41:30, 10.47s/it]                                                         38%|      | 2479/6500 [7:30:12<11:41:30, 10.47s/it] 38%|      | 2480/6500 [7:30:24<12:02:29, 10.78s/it]                                                         38%|      | 2480/6500 [7:30:24<12:02:29, 10.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8195083737373352, 'eval_runtime': 4.2898, 'eval_samples_per_second': 5.361, 'eval_steps_per_second': 1.399, 'epoch': 0.38}
                                                         38%|      | 2480/6500 [7:30:28<12:02:29, 10.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2480I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2480/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4159, 'learning_rate': 6.818665792208082e-05, 'epoch': 0.38}
{'loss': 0.4131, 'learning_rate': 6.816413807220495e-05, 'epoch': 0.38}
{'loss': 0.4228, 'learning_rate': 6.814161397657224e-05, 'epoch': 0.38}
{'loss': 0.4104, 'learning_rate': 6.811908564044756e-05, 'epoch': 0.38}
{'loss': 0.4223, 'learning_rate': 6.809655306909681e-05, 'epoch': 0.38}
 38%|      | 2481/6500 [7:30:39<13:34:51, 12.17s/it]                                                         38%|      | 2481/6500 [7:30:39<13:34:51, 12.17s/it] 38%|      | 2482/6500 [7:30:50<12:58:58, 11.63s/it]                                                         38%|      | 2482/6500 [7:30:50<12:58:58, 11.63s/it] 38%|      | 2483/6500 [7:31:00<12:33:26, 11.25s/it]                                                         38%|      | 2483/6500 [7:31:00<12:33:26, 11.25s/it] 38%|      | 2484/6500 [7:31:10<12:15:31, 10.99s/it]                                                         38%|      | 2484/6500 [7:31:10<12:15:31, 10.99s/it] 38%|      | 2485/6500 [7:31:21<12:03:00, 10.80s/it]                                                         38%|      | 2485/6500 [7:31:21<12:03:00, 10.80s/it] 38%|      | 2486/6500 [7:31:31<11:54:06, 10.67s/it]                  {'loss': 0.4443, 'learning_rate': 6.807401626778679e-05, 'epoch': 0.38}
{'loss': 0.4256, 'learning_rate': 6.805147524178535e-05, 'epoch': 0.38}
{'loss': 0.433, 'learning_rate': 6.802892999636134e-05, 'epoch': 0.38}
{'loss': 0.426, 'learning_rate': 6.800638053678455e-05, 'epoch': 0.38}
{'loss': 0.4114, 'learning_rate': 6.79838268683258e-05, 'epoch': 0.38}
                                       38%|      | 2486/6500 [7:31:31<11:54:06, 10.67s/it] 38%|      | 2487/6500 [7:31:42<11:48:08, 10.59s/it]                                                         38%|      | 2487/6500 [7:31:42<11:48:08, 10.59s/it] 38%|      | 2488/6500 [7:31:52<11:43:35, 10.52s/it]                                                         38%|      | 2488/6500 [7:31:52<11:43:35, 10.52s/it] 38%|      | 2489/6500 [7:32:03<11:45:59, 10.56s/it]                                                         38%|      | 2489/6500 [7:32:03<11:45:59, 10.56s/it] 38%|      | 2490/6500 [7:32:13<11:42:27, 10.51s/it]                                                         38%|      | 2490/6500 [7:32:13<11:42:27, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8133412003517151, 'eval_runtime': 3.9614, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.515, 'epoch': 0.38}
                                                         38%|      | 2490/6500 [7:32:17<11:42:27, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2490/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2490/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4148, 'learning_rate': 6.796126899625688e-05, 'epoch': 0.38}
{'loss': 0.4434, 'learning_rate': 6.793870692585051e-05, 'epoch': 0.38}
{'loss': 0.4246, 'learning_rate': 6.791614066238047e-05, 'epoch': 0.38}
{'loss': 0.4162, 'learning_rate': 6.789357021112148e-05, 'epoch': 0.38}
{'loss': 0.409, 'learning_rate': 6.787099557734927e-05, 'epoch': 0.38}
 38%|      | 2491/6500 [7:32:28<13:08:51, 11.81s/it]                                                         38%|      | 2491/6500 [7:32:28<13:08:51, 11.81s/it] 38%|      | 2492/6500 [7:32:38<12:40:36, 11.39s/it]                                                         38%|      | 2492/6500 [7:32:38<12:40:36, 11.39s/it] 38%|      | 2493/6500 [7:32:49<12:20:54, 11.09s/it]                                                         38%|      | 2493/6500 [7:32:49<12:20:54, 11.09s/it] 38%|      | 2494/6500 [7:32:59<12:06:56, 10.89s/it]                                                         38%|      | 2494/6500 [7:32:59<12:06:56, 10.89s/it] 38%|      | 2495/6500 [7:33:09<11:57:17, 10.75s/it]                                                         38%|      | 2495/6500 [7:33:09<11:57:17, 10.75s/it] 38%|      | 2496/6500 [7:33:20<11:50:19, 10.64s/it]                  {'loss': 0.4331, 'learning_rate': 6.784841676634048e-05, 'epoch': 0.38}
{'loss': 0.4844, 'learning_rate': 6.78258337833728e-05, 'epoch': 0.38}
{'loss': 0.4133, 'learning_rate': 6.780324663372485e-05, 'epoch': 0.38}
{'loss': 0.4021, 'learning_rate': 6.778065532267624e-05, 'epoch': 0.38}
{'loss': 0.4287, 'learning_rate': 6.775805985550756e-05, 'epoch': 0.38}
                                       38%|      | 2496/6500 [7:33:20<11:50:19, 10.64s/it] 38%|      | 2497/6500 [7:33:30<11:45:26, 10.57s/it]                                                         38%|      | 2497/6500 [7:33:30<11:45:26, 10.57s/it] 38%|      | 2498/6500 [7:33:41<11:42:02, 10.53s/it]                                                         38%|      | 2498/6500 [7:33:41<11:42:02, 10.53s/it] 38%|      | 2499/6500 [7:33:51<11:39:45, 10.49s/it]                                                         38%|      | 2499/6500 [7:33:51<11:39:45, 10.49s/it] 38%|      | 2500/6500 [7:34:01<11:37:50, 10.47s/it]                                                         38%|      | 2500/6500 [7:34:01<11:37:50, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8184682726860046, 'eval_runtime': 3.9467, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.38}
                                                         38%|      | 2500/6500 [7:34:05<11:37:50, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9381, 'learning_rate': 6.773546023750037e-05, 'epoch': 0.38}
{'loss': 0.4414, 'learning_rate': 6.771285647393719e-05, 'epoch': 0.38}
{'loss': 0.4198, 'learning_rate': 6.769024857010148e-05, 'epoch': 0.39}
{'loss': 0.4161, 'learning_rate': 6.766763653127773e-05, 'epoch': 0.39}
{'loss': 0.3999, 'learning_rate': 6.764502036275138e-05, 'epoch': 0.39}
 38%|      | 2501/6500 [7:34:16<13:04:54, 11.78s/it]                                                         38%|      | 2501/6500 [7:34:16<13:04:54, 11.78s/it] 38%|      | 2502/6500 [7:34:27<12:37:18, 11.37s/it]                                                         38%|      | 2502/6500 [7:34:27<12:37:18, 11.37s/it] 39%|      | 2503/6500 [7:34:37<12:17:42, 11.07s/it]                                                         39%|      | 2503/6500 [7:34:37<12:17:42, 11.07s/it] 39%|      | 2504/6500 [7:34:47<12:04:16, 10.88s/it]                                                         39%|      | 2504/6500 [7:34:47<12:04:16, 10.88s/it] 39%|      | 2505/6500 [7:34:58<12:00:20, 10.82s/it]                                                         39%|      | 2505/6500 [7:34:58<12:00:20, 10.82s/it] 39%|      | 2506/6500 [7:35:09<11:51:50, 10.69s/it]                  {'loss': 0.4466, 'learning_rate': 6.762240006980878e-05, 'epoch': 0.39}
{'loss': 0.4066, 'learning_rate': 6.759977565773734e-05, 'epoch': 0.39}
{'loss': 0.4074, 'learning_rate': 6.757714713182533e-05, 'epoch': 0.39}
{'loss': 0.4019, 'learning_rate': 6.755451449736204e-05, 'epoch': 0.39}
{'loss': 0.414, 'learning_rate': 6.753187775963773e-05, 'epoch': 0.39}
                                       39%|      | 2506/6500 [7:35:09<11:51:50, 10.69s/it] 39%|      | 2507/6500 [7:35:19<11:45:38, 10.60s/it]                                                         39%|      | 2507/6500 [7:35:19<11:45:38, 10.60s/it] 39%|      | 2508/6500 [7:35:29<11:41:40, 10.55s/it]                                                         39%|      | 2508/6500 [7:35:29<11:41:40, 10.55s/it] 39%|      | 2509/6500 [7:35:40<11:38:43, 10.50s/it]                                                         39%|      | 2509/6500 [7:35:40<11:38:43, 10.50s/it] 39%|      | 2510/6500 [7:35:50<11:42:13, 10.56s/it]                                                         39%|      | 2510/6500 [7:35:50<11:42:13, 10.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8196359872817993, 'eval_runtime': 4.399, 'eval_samples_per_second': 5.228, 'eval_steps_per_second': 1.364, 'epoch': 0.39}
                                                         39%|      | 2510/6500 [7:35:55<11:42:13, 10.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4128, 'learning_rate': 6.750923692394359e-05, 'epoch': 0.39}
{'loss': 0.3972, 'learning_rate': 6.748659199557177e-05, 'epoch': 0.39}
{'loss': 0.4337, 'learning_rate': 6.74639429798154e-05, 'epoch': 0.39}
{'loss': 0.4099, 'learning_rate': 6.744128988196853e-05, 'epoch': 0.39}
{'loss': 0.4399, 'learning_rate': 6.741863270732619e-05, 'epoch': 0.39}
 39%|      | 2511/6500 [7:36:06<13:20:09, 12.04s/it]                                                         39%|      | 2511/6500 [7:36:06<13:20:09, 12.04s/it] 39%|      | 2512/6500 [7:36:16<12:48:07, 11.56s/it]                                                         39%|      | 2512/6500 [7:36:16<12:48:07, 11.56s/it] 39%|      | 2513/6500 [7:36:27<12:25:10, 11.21s/it]                                                         39%|      | 2513/6500 [7:36:27<12:25:10, 11.21s/it] 39%|      | 2514/6500 [7:36:37<12:08:38, 10.97s/it]                                                         39%|      | 2514/6500 [7:36:37<12:08:38, 10.97s/it] 39%|      | 2515/6500 [7:36:48<11:57:00, 10.80s/it]                                                         39%|      | 2515/6500 [7:36:48<11:57:00, 10.80s/it] 39%|      | 2516/6500 [7:36:58<11:49:04, 10.68s/it]                  {'loss': 0.4281, 'learning_rate': 6.739597146118436e-05, 'epoch': 0.39}
{'loss': 0.411, 'learning_rate': 6.737330614884001e-05, 'epoch': 0.39}
{'loss': 0.4263, 'learning_rate': 6.735063677559095e-05, 'epoch': 0.39}
{'loss': 0.4255, 'learning_rate': 6.732796334673603e-05, 'epoch': 0.39}
{'loss': 0.417, 'learning_rate': 6.730528586757505e-05, 'epoch': 0.39}
                                       39%|      | 2516/6500 [7:36:58<11:49:04, 10.68s/it] 39%|      | 2517/6500 [7:37:08<11:43:23, 10.60s/it]                                                         39%|      | 2517/6500 [7:37:08<11:43:23, 10.60s/it] 39%|      | 2518/6500 [7:37:19<11:39:14, 10.54s/it]                                                         39%|      | 2518/6500 [7:37:19<11:39:14, 10.54s/it] 39%|      | 2519/6500 [7:37:29<11:36:55, 10.50s/it]                                                         39%|      | 2519/6500 [7:37:29<11:36:55, 10.50s/it] 39%|      | 2520/6500 [7:37:40<11:34:57, 10.48s/it]                                                         39%|      | 2520/6500 [7:37:40<11:34:57, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8127627968788147, 'eval_runtime': 3.962, 'eval_samples_per_second': 5.805, 'eval_steps_per_second': 1.514, 'epoch': 0.39}
                                                         39%|      | 2520/6500 [7:37:44<11:34:57, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.417, 'learning_rate': 6.72826043434087e-05, 'epoch': 0.39}
{'loss': 0.4425, 'learning_rate': 6.725991877953868e-05, 'epoch': 0.39}
{'loss': 0.4067, 'learning_rate': 6.723722918126758e-05, 'epoch': 0.39}
{'loss': 0.4118, 'learning_rate': 6.721453555389897e-05, 'epoch': 0.39}
{'loss': 0.3984, 'learning_rate': 6.719183790273733e-05, 'epoch': 0.39}
 39%|      | 2521/6500 [7:37:55<13:07:57, 11.88s/it]                                                         39%|      | 2521/6500 [7:37:55<13:07:57, 11.88s/it] 39%|      | 2522/6500 [7:38:05<12:38:36, 11.44s/it]                                                         39%|      | 2522/6500 [7:38:05<12:38:36, 11.44s/it] 39%|      | 2523/6500 [7:38:16<12:17:59, 11.13s/it]                                                         39%|      | 2523/6500 [7:38:16<12:17:59, 11.13s/it] 39%|      | 2524/6500 [7:38:26<12:03:21, 10.92s/it]                                                         39%|      | 2524/6500 [7:38:26<12:03:21, 10.92s/it] 39%|      | 2525/6500 [7:38:36<11:53:05, 10.76s/it]                                                         39%|      | 2525/6500 [7:38:36<11:53:05, 10.76s/it] 39%|      | 2526/6500 [7:38:47<11:45:54, 10.66s/it]                  {'loss': 0.4455, 'learning_rate': 6.716913623308812e-05, 'epoch': 0.39}
{'loss': 0.468, 'learning_rate': 6.714643055025769e-05, 'epoch': 0.39}
{'loss': 0.4049, 'learning_rate': 6.712372085955339e-05, 'epoch': 0.39}
{'loss': 0.427, 'learning_rate': 6.710100716628344e-05, 'epoch': 0.39}
{'loss': 0.4058, 'learning_rate': 6.707828947575706e-05, 'epoch': 0.39}
                                       39%|      | 2526/6500 [7:38:47<11:45:54, 10.66s/it] 39%|      | 2527/6500 [7:38:57<11:41:19, 10.59s/it]                                                         39%|      | 2527/6500 [7:38:57<11:41:19, 10.59s/it] 39%|      | 2528/6500 [7:39:08<11:37:26, 10.54s/it]                                                         39%|      | 2528/6500 [7:39:08<11:37:26, 10.54s/it] 39%|      | 2529/6500 [7:39:18<11:34:49, 10.50s/it]                                                         39%|      | 2529/6500 [7:39:18<11:34:49, 10.50s/it] 39%|      | 2530/6500 [7:39:28<11:32:10, 10.46s/it]                                                         39%|      | 2530/6500 [7:39:28<11:32:10, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8230094909667969, 'eval_runtime': 3.9471, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.39}
                                                         39%|      | 2530/6500 [7:39:32<11:32:10, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2530
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2530/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2530/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2530/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9444, 'learning_rate': 6.705556779328433e-05, 'epoch': 0.39}
{'loss': 0.4303, 'learning_rate': 6.703284212417632e-05, 'epoch': 0.39}
{'loss': 0.4267, 'learning_rate': 6.701011247374505e-05, 'epoch': 0.39}
{'loss': 0.3953, 'learning_rate': 6.69873788473034e-05, 'epoch': 0.39}
{'loss': 0.4148, 'learning_rate': 6.696464125016522e-05, 'epoch': 0.39}
 39%|      | 2531/6500 [7:39:43<12:58:12, 11.76s/it]                                                         39%|      | 2531/6500 [7:39:43<12:58:12, 11.76s/it] 39%|      | 2532/6500 [7:39:54<12:30:10, 11.34s/it]                                                         39%|      | 2532/6500 [7:39:54<12:30:10, 11.34s/it] 39%|      | 2533/6500 [7:40:04<12:10:31, 11.05s/it]                                                         39%|      | 2533/6500 [7:40:04<12:10:31, 11.05s/it] 39%|      | 2534/6500 [7:40:14<11:56:29, 10.84s/it]                                                         39%|      | 2534/6500 [7:40:14<11:56:29, 10.84s/it] 39%|      | 2535/6500 [7:40:25<11:46:48, 10.70s/it]                                                         39%|      | 2535/6500 [7:40:25<11:46:48, 10.70s/it] 39%|      | 2536/6500 [7:40:35<11:40:20, 10.60s/it]                  {'loss': 0.4425, 'learning_rate': 6.694189968764532e-05, 'epoch': 0.39}
{'loss': 0.3888, 'learning_rate': 6.691915416505935e-05, 'epoch': 0.39}
{'loss': 0.4138, 'learning_rate': 6.689640468772398e-05, 'epoch': 0.39}
{'loss': 0.4084, 'learning_rate': 6.687365126095674e-05, 'epoch': 0.39}
{'loss': 0.4187, 'learning_rate': 6.685089389007612e-05, 'epoch': 0.39}
                                       39%|      | 2536/6500 [7:40:35<11:40:20, 10.60s/it] 39%|      | 2537/6500 [7:40:45<11:35:15, 10.53s/it]                                                         39%|      | 2537/6500 [7:40:45<11:35:15, 10.53s/it] 39%|      | 2538/6500 [7:40:56<11:39:56, 10.60s/it]                                                         39%|      | 2538/6500 [7:40:56<11:39:56, 10.60s/it] 39%|      | 2539/6500 [7:41:07<11:35:07, 10.53s/it]                                                         39%|      | 2539/6500 [7:41:07<11:35:07, 10.53s/it] 39%|      | 2540/6500 [7:41:17<11:32:09, 10.49s/it]                                                         39%|      | 2540/6500 [7:41:17<11:32:09, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8228659629821777, 'eval_runtime': 3.9597, 'eval_samples_per_second': 5.808, 'eval_steps_per_second': 1.515, 'epoch': 0.39}
                                                         39%|      | 2540/6500 [7:41:21<11:32:09, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2540
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2540/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4226, 'learning_rate': 6.682813258040151e-05, 'epoch': 0.39}
{'loss': 0.4159, 'learning_rate': 6.68053673372532e-05, 'epoch': 0.39}
{'loss': 0.4206, 'learning_rate': 6.678259816595246e-05, 'epoch': 0.39}
{'loss': 0.4047, 'learning_rate': 6.675982507182145e-05, 'epoch': 0.39}
{'loss': 0.4544, 'learning_rate': 6.673704806018326e-05, 'epoch': 0.39}
 39%|      | 2541/6500 [7:41:32<12:59:42, 11.82s/it]                                                         39%|      | 2541/6500 [7:41:32<12:59:42, 11.82s/it] 39%|      | 2542/6500 [7:41:42<12:30:37, 11.38s/it]                                                         39%|      | 2542/6500 [7:41:42<12:30:37, 11.38s/it] 39%|      | 2543/6500 [7:41:53<12:11:23, 11.09s/it]                                                         39%|      | 2543/6500 [7:41:53<12:11:23, 11.09s/it] 39%|      | 2544/6500 [7:42:03<11:56:55, 10.87s/it]                                                         39%|      | 2544/6500 [7:42:03<11:56:55, 10.87s/it] 39%|      | 2545/6500 [7:42:13<11:46:29, 10.72s/it]                                                         39%|      | 2545/6500 [7:42:13<11:46:29, 10.72s/it] 39%|      | 2546/6500 [7:42:24<11:39:31, 10.61s/it]                  {'loss': 0.4107, 'learning_rate': 6.67142671363618e-05, 'epoch': 0.39}
{'loss': 0.4261, 'learning_rate': 6.669148230568205e-05, 'epoch': 0.39}
{'loss': 0.4385, 'learning_rate': 6.666869357346978e-05, 'epoch': 0.39}
{'loss': 0.4126, 'learning_rate': 6.664590094505174e-05, 'epoch': 0.39}
{'loss': 0.4281, 'learning_rate': 6.662310442575556e-05, 'epoch': 0.39}
                                       39%|      | 2546/6500 [7:42:24<11:39:31, 10.61s/it] 39%|      | 2547/6500 [7:42:34<11:34:41, 10.54s/it]                                                         39%|      | 2547/6500 [7:42:34<11:34:41, 10.54s/it] 39%|      | 2548/6500 [7:42:45<11:31:29, 10.50s/it]                                                         39%|      | 2548/6500 [7:42:45<11:31:29, 10.50s/it] 39%|      | 2549/6500 [7:42:55<11:32:54, 10.52s/it]                                                         39%|      | 2549/6500 [7:42:55<11:32:54, 10.52s/it] 39%|      | 2550/6500 [7:43:05<11:29:51, 10.48s/it]                                                         39%|      | 2550/6500 [7:43:05<11:29:51, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8123570084571838, 'eval_runtime': 4.3102, 'eval_samples_per_second': 5.336, 'eval_steps_per_second': 1.392, 'epoch': 0.39}
                                                         39%|      | 2550/6500 [7:43:10<11:29:51, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2550
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4179, 'learning_rate': 6.660030402090981e-05, 'epoch': 0.39}
{'loss': 0.4341, 'learning_rate': 6.65774997358439e-05, 'epoch': 0.39}
{'loss': 0.4092, 'learning_rate': 6.655469157588823e-05, 'epoch': 0.39}
{'loss': 0.4164, 'learning_rate': 6.653187954637405e-05, 'epoch': 0.39}
{'loss': 0.3967, 'learning_rate': 6.650906365263356e-05, 'epoch': 0.39}
 39%|      | 2551/6500 [7:43:21<13:02:40, 11.89s/it]                                                         39%|      | 2551/6500 [7:43:21<13:02:40, 11.89s/it] 39%|      | 2552/6500 [7:43:31<12:32:12, 11.43s/it]                                                         39%|      | 2552/6500 [7:43:31<12:32:12, 11.43s/it] 39%|      | 2553/6500 [7:43:41<12:11:00, 11.11s/it]                                                         39%|      | 2553/6500 [7:43:41<12:11:00, 11.11s/it] 39%|      | 2554/6500 [7:43:52<12:01:13, 10.97s/it]                                                         39%|      | 2554/6500 [7:43:52<12:01:13, 10.97s/it] 39%|      | 2555/6500 [7:44:02<11:48:44, 10.78s/it]                                                         39%|      | 2555/6500 [7:44:02<11:48:44, 10.78s/it] 39%|      | 2556/6500 [7:44:13<11:40:01, 10.65s/it]                  {'loss': 0.4983, 'learning_rate': 6.64862438999998e-05, 'epoch': 0.39}
{'loss': 0.4123, 'learning_rate': 6.646342029380679e-05, 'epoch': 0.39}
{'loss': 0.4057, 'learning_rate': 6.644059283938938e-05, 'epoch': 0.39}
{'loss': 0.42, 'learning_rate': 6.641776154208334e-05, 'epoch': 0.39}
{'loss': 0.9412, 'learning_rate': 6.639492640722536e-05, 'epoch': 0.39}
                                       39%|      | 2556/6500 [7:44:13<11:40:01, 10.65s/it] 39%|      | 2557/6500 [7:44:23<11:33:55, 10.56s/it]                                                         39%|      | 2557/6500 [7:44:23<11:33:55, 10.56s/it] 39%|      | 2558/6500 [7:44:33<11:29:48, 10.50s/it]                                                         39%|      | 2558/6500 [7:44:33<11:29:48, 10.50s/it] 39%|      | 2559/6500 [7:44:44<11:27:07, 10.46s/it]                                                         39%|      | 2559/6500 [7:44:44<11:27:07, 10.46s/it] 39%|      | 2560/6500 [7:44:54<11:24:45, 10.43s/it]                                                         39%|      | 2560/6500 [7:44:54<11:24:45, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8196150660514832, 'eval_runtime': 3.9536, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 1.518, 'epoch': 0.39}
                                                         39%|      | 2560/6500 [7:44:58<11:24:45, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2560I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2560

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4276, 'learning_rate': 6.637208744015303e-05, 'epoch': 0.39}
{'loss': 0.4294, 'learning_rate': 6.63492446462048e-05, 'epoch': 0.39}
{'loss': 0.4148, 'learning_rate': 6.632639803072003e-05, 'epoch': 0.39}
{'loss': 0.3865, 'learning_rate': 6.630354759903898e-05, 'epoch': 0.39}
{'loss': 0.4339, 'learning_rate': 6.628069335650282e-05, 'epoch': 0.39}
 39%|      | 2561/6500 [7:45:09<12:51:29, 11.75s/it]                                                         39%|      | 2561/6500 [7:45:09<12:51:29, 11.75s/it] 39%|      | 2562/6500 [7:45:19<12:24:07, 11.34s/it]                                                         39%|      | 2562/6500 [7:45:19<12:24:07, 11.34s/it] 39%|      | 2563/6500 [7:45:30<12:04:43, 11.04s/it]                                                         39%|      | 2563/6500 [7:45:30<12:04:43, 11.04s/it] 39%|      | 2564/6500 [7:45:40<11:50:59, 10.84s/it]                                                         39%|      | 2564/6500 [7:45:40<11:50:59, 10.84s/it] 39%|      | 2565/6500 [7:45:50<11:41:49, 10.70s/it]                                                         39%|      | 2565/6500 [7:45:50<11:41:49, 10.70s/it] 39%|      | 2566/6500 [7:46:01<11:35:00, 10.60s/it]                  {'loss': 0.4144, 'learning_rate': 6.625783530845359e-05, 'epoch': 0.39}
{'loss': 0.382, 'learning_rate': 6.623497346023418e-05, 'epoch': 0.39}
{'loss': 0.4129, 'learning_rate': 6.621210781718844e-05, 'epoch': 0.4}
{'loss': 0.393, 'learning_rate': 6.618923838466108e-05, 'epoch': 0.4}
{'loss': 0.4221, 'learning_rate': 6.616636516799766e-05, 'epoch': 0.4}
                                       39%|      | 2566/6500 [7:46:01<11:35:00, 10.60s/it] 39%|      | 2567/6500 [7:46:11<11:30:15, 10.53s/it]                                                         39%|      | 2567/6500 [7:46:11<11:30:15, 10.53s/it] 40%|      | 2568/6500 [7:46:22<11:27:04, 10.48s/it]                                                         40%|      | 2568/6500 [7:46:22<11:27:04, 10.48s/it] 40%|      | 2569/6500 [7:46:32<11:24:10, 10.44s/it]                                                         40%|      | 2569/6500 [7:46:32<11:24:10, 10.44s/it] 40%|      | 2570/6500 [7:46:43<11:29:56, 10.53s/it]                                                         40%|      | 2570/6500 [7:46:43<11:29:56, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8250889182090759, 'eval_runtime': 3.9685, 'eval_samples_per_second': 5.796, 'eval_steps_per_second': 1.512, 'epoch': 0.4}
                                                         40%|      | 2570/6500 [7:46:47<11:29:56, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2570
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2570/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3928, 'learning_rate': 6.61434881725447e-05, 'epoch': 0.4}
{'loss': 0.4342, 'learning_rate': 6.612060740364952e-05, 'epoch': 0.4}
{'loss': 0.3976, 'learning_rate': 6.609772286666037e-05, 'epoch': 0.4}
{'loss': 0.4369, 'learning_rate': 6.607483456692638e-05, 'epoch': 0.4}
{'loss': 0.4324, 'learning_rate': 6.605194250979755e-05, 'epoch': 0.4}
 40%|      | 2571/6500 [7:46:57<12:53:40, 11.81s/it]                                                         40%|      | 2571/6500 [7:46:57<12:53:40, 11.81s/it] 40%|      | 2572/6500 [7:47:08<12:25:22, 11.39s/it]                                                         40%|      | 2572/6500 [7:47:08<12:25:22, 11.39s/it] 40%|      | 2573/6500 [7:47:18<12:04:57, 11.08s/it]                                                         40%|      | 2573/6500 [7:47:18<12:04:57, 11.08s/it] 40%|      | 2574/6500 [7:47:29<11:50:58, 10.87s/it]                                                         40%|      | 2574/6500 [7:47:29<11:50:58, 10.87s/it] 40%|      | 2575/6500 [7:47:39<11:41:04, 10.72s/it]                                                         40%|      | 2575/6500 [7:47:39<11:41:04, 10.72s/it] 40%|      | 2576/6500 [7:47:49<11:34:30, 10.62s/it]                  {'loss': 0.4352, 'learning_rate': 6.602904670062476e-05, 'epoch': 0.4}
{'loss': 0.427, 'learning_rate': 6.600614714475975e-05, 'epoch': 0.4}
{'loss': 0.4254, 'learning_rate': 6.598324384755518e-05, 'epoch': 0.4}
{'loss': 0.4124, 'learning_rate': 6.596033681436452e-05, 'epoch': 0.4}
{'loss': 0.4086, 'learning_rate': 6.593742605054218e-05, 'epoch': 0.4}
                                       40%|      | 2576/6500 [7:47:49<11:34:30, 10.62s/it] 40%|      | 2577/6500 [7:48:00<11:29:27, 10.54s/it]                                                         40%|      | 2577/6500 [7:48:00<11:29:27, 10.54s/it] 40%|      | 2578/6500 [7:48:10<11:26:18, 10.50s/it]                                                         40%|      | 2578/6500 [7:48:10<11:26:18, 10.50s/it] 40%|      | 2579/6500 [7:48:20<11:23:35, 10.46s/it]                                                         40%|      | 2579/6500 [7:48:20<11:23:35, 10.46s/it] 40%|      | 2580/6500 [7:48:31<11:24:32, 10.48s/it]                                                         40%|      | 2580/6500 [7:48:31<11:24:32, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8142964839935303, 'eval_runtime': 4.2638, 'eval_samples_per_second': 5.394, 'eval_steps_per_second': 1.407, 'epoch': 0.4}
                                                         40%|      | 2580/6500 [7:48:35<11:24:32, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2580
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2580/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4455, 'learning_rate': 6.59145115614434e-05, 'epoch': 0.4}
{'loss': 0.4142, 'learning_rate': 6.58915933524243e-05, 'epoch': 0.4}
{'loss': 0.4121, 'learning_rate': 6.58686714288419e-05, 'epoch': 0.4}
{'loss': 0.4009, 'learning_rate': 6.584574579605401e-05, 'epoch': 0.4}
{'loss': 0.4332, 'learning_rate': 6.58228164594194e-05, 'epoch': 0.4}
 40%|      | 2581/6500 [7:48:46<12:55:18, 11.87s/it]                                                         40%|      | 2581/6500 [7:48:46<12:55:18, 11.87s/it] 40%|      | 2582/6500 [7:48:56<12:25:53, 11.42s/it]                                                         40%|      | 2582/6500 [7:48:56<12:25:53, 11.42s/it] 40%|      | 2583/6500 [7:49:07<12:04:52, 11.10s/it]                                                         40%|      | 2583/6500 [7:49:07<12:04:52, 11.10s/it] 40%|      | 2584/6500 [7:49:17<11:50:18, 10.88s/it]                                                         40%|      | 2584/6500 [7:49:17<11:50:18, 10.88s/it] 40%|      | 2585/6500 [7:49:28<11:39:59, 10.73s/it]                                                         40%|      | 2585/6500 [7:49:28<11:39:59, 10.73s/it] 40%|      | 2586/6500 [7:49:39<11:56:18, 10.98s/it]                  {'loss': 0.4779, 'learning_rate': 6.579988342429763e-05, 'epoch': 0.4}
{'loss': 0.407, 'learning_rate': 6.577694669604919e-05, 'epoch': 0.4}
{'loss': 0.4029, 'learning_rate': 6.575400628003538e-05, 'epoch': 0.4}
{'loss': 0.4151, 'learning_rate': 6.57310621816184e-05, 'epoch': 0.4}
{'loss': 0.9289, 'learning_rate': 6.570811440616125e-05, 'epoch': 0.4}
                                       40%|      | 2586/6500 [7:49:39<11:56:18, 10.98s/it] 40%|      | 2587/6500 [7:49:50<11:44:35, 10.80s/it]                                                         40%|      | 2587/6500 [7:49:50<11:44:35, 10.80s/it] 40%|      | 2588/6500 [7:50:00<11:37:12, 10.69s/it]                                                         40%|      | 2588/6500 [7:50:00<11:37:12, 10.69s/it] 40%|      | 2589/6500 [7:50:10<11:30:15, 10.59s/it]                                                         40%|      | 2589/6500 [7:50:10<11:30:15, 10.59s/it] 40%|      | 2590/6500 [7:50:21<11:24:54, 10.51s/it]                                                         40%|      | 2590/6500 [7:50:21<11:24:54, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8191676139831543, 'eval_runtime': 3.9423, 'eval_samples_per_second': 5.834, 'eval_steps_per_second': 1.522, 'epoch': 0.4}
                                                         40%|      | 2590/6500 [7:50:25<11:24:54, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2590
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4287, 'learning_rate': 6.568516295902788e-05, 'epoch': 0.4}
{'loss': 0.4216, 'learning_rate': 6.566220784558304e-05, 'epoch': 0.4}
{'loss': 0.3946, 'learning_rate': 6.563924907119234e-05, 'epoch': 0.4}
{'loss': 0.4073, 'learning_rate': 6.561628664122226e-05, 'epoch': 0.4}
{'loss': 0.435, 'learning_rate': 6.559332056104012e-05, 'epoch': 0.4}
 40%|      | 2591/6500 [7:50:35<12:48:30, 11.80s/it]                                                         40%|      | 2591/6500 [7:50:35<12:48:30, 11.80s/it] 40%|      | 2592/6500 [7:50:46<12:19:55, 11.36s/it]                                                         40%|      | 2592/6500 [7:50:46<12:19:55, 11.36s/it] 40%|      | 2593/6500 [7:50:56<12:00:06, 11.06s/it]                                                         40%|      | 2593/6500 [7:50:56<12:00:06, 11.06s/it] 40%|      | 2594/6500 [7:51:06<11:45:46, 10.84s/it]                                                         40%|      | 2594/6500 [7:51:06<11:45:46, 10.84s/it] 40%|      | 2595/6500 [7:51:17<11:36:07, 10.70s/it]                                                         40%|      | 2595/6500 [7:51:17<11:36:07, 10.70s/it] 40%|      | 2596/6500 [7:51:27<11:28:48, 10.59s/it]                  {'loss': 0.4052, 'learning_rate': 6.557035083601413e-05, 'epoch': 0.4}
{'loss': 0.4109, 'learning_rate': 6.554737747151328e-05, 'epoch': 0.4}
{'loss': 0.3915, 'learning_rate': 6.552440047290747e-05, 'epoch': 0.4}
{'loss': 0.4072, 'learning_rate': 6.550141984556747e-05, 'epoch': 0.4}
{'loss': 0.4058, 'learning_rate': 6.547843559486481e-05, 'epoch': 0.4}
                                       40%|      | 2596/6500 [7:51:27<11:28:48, 10.59s/it] 40%|      | 2597/6500 [7:51:38<11:24:08, 10.52s/it]                                                         40%|      | 2597/6500 [7:51:38<11:24:08, 10.52s/it] 40%|      | 2598/6500 [7:51:48<11:20:30, 10.46s/it]                                                         40%|      | 2598/6500 [7:51:48<11:20:30, 10.46s/it] 40%|      | 2599/6500 [7:51:58<11:18:19, 10.43s/it]                                                         40%|      | 2599/6500 [7:51:58<11:18:19, 10.43s/it] 40%|      | 2600/6500 [7:52:09<11:16:41, 10.41s/it]                                                         40%|      | 2600/6500 [7:52:09<11:16:41, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8222211599349976, 'eval_runtime': 3.9455, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.4}
                                                         40%|      | 2600/6500 [7:52:13<11:16:41, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2600
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3992, 'learning_rate': 6.545544772617194e-05, 'epoch': 0.4}
{'loss': 0.4315, 'learning_rate': 6.543245624486214e-05, 'epoch': 0.4}
{'loss': 0.4022, 'learning_rate': 6.540946115630952e-05, 'epoch': 0.4}
{'loss': 0.4336, 'learning_rate': 6.538646246588906e-05, 'epoch': 0.4}
{'loss': 0.4125, 'learning_rate': 6.536346017897653e-05, 'epoch': 0.4}
 40%|      | 2601/6500 [7:52:23<12:42:45, 11.74s/it]                                                         40%|      | 2601/6500 [7:52:23<12:42:45, 11.74s/it] 40%|      | 2602/6500 [7:52:34<12:23:21, 11.44s/it]                                                         40%|      | 2602/6500 [7:52:34<12:23:21, 11.44s/it] 40%|      | 2603/6500 [7:52:45<12:01:42, 11.11s/it]                                                         40%|      | 2603/6500 [7:52:45<12:01:42, 11.11s/it] 40%|      | 2604/6500 [7:52:55<11:46:07, 10.87s/it]                                                         40%|      | 2604/6500 [7:52:55<11:46:07, 10.87s/it] 40%|      | 2605/6500 [7:53:05<11:35:33, 10.71s/it]                                                         40%|      | 2605/6500 [7:53:05<11:35:33, 10.71s/it] 40%|      | 2606/6500 [7:53:16<11:28:11, 10.60s/it]                  {'loss': 0.3999, 'learning_rate': 6.53404543009486e-05, 'epoch': 0.4}
{'loss': 0.4181, 'learning_rate': 6.531744483718274e-05, 'epoch': 0.4}
{'loss': 0.4089, 'learning_rate': 6.529443179305728e-05, 'epoch': 0.4}
{'loss': 0.4057, 'learning_rate': 6.52714151739514e-05, 'epoch': 0.4}
{'loss': 0.4086, 'learning_rate': 6.524839498524508e-05, 'epoch': 0.4}
                                       40%|      | 2606/6500 [7:53:16<11:28:11, 10.60s/it] 40%|      | 2607/6500 [7:53:26<11:23:17, 10.53s/it]                                                         40%|      | 2607/6500 [7:53:26<11:23:17, 10.53s/it] 40%|      | 2608/6500 [7:53:36<11:19:40, 10.48s/it]                                                         40%|      | 2608/6500 [7:53:36<11:19:40, 10.48s/it] 40%|      | 2609/6500 [7:53:47<11:17:02, 10.44s/it]                                                         40%|      | 2609/6500 [7:53:47<11:17:02, 10.44s/it] 40%|      | 2610/6500 [7:53:58<11:37:50, 10.76s/it]                                                         40%|      | 2610/6500 [7:53:58<11:37:50, 10.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8137469291687012, 'eval_runtime': 4.2013, 'eval_samples_per_second': 5.475, 'eval_steps_per_second': 1.428, 'epoch': 0.4}
                                                         40%|      | 2610/6500 [7:54:02<11:37:50, 10.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2610/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2610/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4363, 'learning_rate': 6.522537123231912e-05, 'epoch': 0.4}
{'loss': 0.398, 'learning_rate': 6.520234392055522e-05, 'epoch': 0.4}
{'loss': 0.4141, 'learning_rate': 6.517931305533584e-05, 'epoch': 0.4}
{'loss': 0.3895, 'learning_rate': 6.515627864204434e-05, 'epoch': 0.4}
{'loss': 0.4438, 'learning_rate': 6.513324068606488e-05, 'epoch': 0.4}
 40%|      | 2611/6500 [7:54:13<13:01:46, 12.06s/it]                                                         40%|      | 2611/6500 [7:54:13<13:01:46, 12.06s/it] 40%|      | 2612/6500 [7:54:24<12:28:14, 11.55s/it]                                                         40%|      | 2612/6500 [7:54:24<12:28:14, 11.55s/it] 40%|      | 2613/6500 [7:54:34<12:04:29, 11.18s/it]                                                         40%|      | 2613/6500 [7:54:34<12:04:29, 11.18s/it] 40%|      | 2614/6500 [7:54:44<11:48:07, 10.93s/it]                                                         40%|      | 2614/6500 [7:54:44<11:48:07, 10.93s/it] 40%|      | 2615/6500 [7:54:55<11:37:10, 10.77s/it]                                                         40%|      | 2615/6500 [7:54:55<11:37:10, 10.77s/it] 40%|      | 2616/6500 [7:55:05<11:28:39, 10.64s/it]                  {'loss': 0.4568, 'learning_rate': 6.511019919278239e-05, 'epoch': 0.4}
{'loss': 0.4002, 'learning_rate': 6.508715416758273e-05, 'epoch': 0.4}
{'loss': 0.4065, 'learning_rate': 6.50641056158525e-05, 'epoch': 0.4}
{'loss': 0.4181, 'learning_rate': 6.504105354297918e-05, 'epoch': 0.4}
{'loss': 0.9414, 'learning_rate': 6.501799795435104e-05, 'epoch': 0.4}
                                       40%|      | 2616/6500 [7:55:05<11:28:39, 10.64s/it] 40%|      | 2617/6500 [7:55:15<11:22:48, 10.55s/it]                                                         40%|      | 2617/6500 [7:55:15<11:22:48, 10.55s/it] 40%|      | 2618/6500 [7:55:26<11:32:21, 10.70s/it]                                                         40%|      | 2618/6500 [7:55:26<11:32:21, 10.70s/it] 40%|      | 2619/6500 [7:55:37<11:25:29, 10.60s/it]                                                         40%|      | 2619/6500 [7:55:37<11:25:29, 10.60s/it] 40%|      | 2620/6500 [7:55:47<11:20:27, 10.52s/it]                                                         40%|      | 2620/6500 [7:55:47<11:20:27, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8218508362770081, 'eval_runtime': 3.9425, 'eval_samples_per_second': 5.834, 'eval_steps_per_second': 1.522, 'epoch': 0.4}
                                                         40%|      | 2620/6500 [7:55:51<11:20:27, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4214, 'learning_rate': 6.499493885535721e-05, 'epoch': 0.4}
{'loss': 0.4271, 'learning_rate': 6.497187625138757e-05, 'epoch': 0.4}
{'loss': 0.387, 'learning_rate': 6.49488101478329e-05, 'epoch': 0.4}
{'loss': 0.4263, 'learning_rate': 6.492574055008473e-05, 'epoch': 0.4}
{'loss': 0.4148, 'learning_rate': 6.490266746353547e-05, 'epoch': 0.4}
 40%|      | 2621/6500 [7:56:02<12:43:02, 11.80s/it]                                                         40%|      | 2621/6500 [7:56:02<12:43:02, 11.80s/it] 40%|      | 2622/6500 [7:56:12<12:14:36, 11.37s/it]                                                         40%|      | 2622/6500 [7:56:12<12:14:36, 11.37s/it] 40%|      | 2623/6500 [7:56:23<11:54:51, 11.06s/it]                                                         40%|      | 2623/6500 [7:56:23<11:54:51, 11.06s/it] 40%|      | 2624/6500 [7:56:33<11:40:47, 10.85s/it]                                                         40%|      | 2624/6500 [7:56:33<11:40:47, 10.85s/it] 40%|      | 2625/6500 [7:56:43<11:31:06, 10.70s/it]                                                         40%|      | 2625/6500 [7:56:43<11:31:06, 10.70s/it] 40%|      | 2626/6500 [7:56:54<11:23:58, 10.59s/it]                  {'loss': 0.3873, 'learning_rate': 6.48795908935783e-05, 'epoch': 0.4}
{'loss': 0.4124, 'learning_rate': 6.485651084560723e-05, 'epoch': 0.4}
{'loss': 0.3996, 'learning_rate': 6.483342732501707e-05, 'epoch': 0.4}
{'loss': 0.4091, 'learning_rate': 6.481034033720347e-05, 'epoch': 0.4}
{'loss': 0.4013, 'learning_rate': 6.478724988756285e-05, 'epoch': 0.4}
                                       40%|      | 2626/6500 [7:56:54<11:23:58, 10.59s/it] 40%|      | 2627/6500 [7:57:04<11:19:48, 10.53s/it]                                                         40%|      | 2627/6500 [7:57:04<11:19:48, 10.53s/it] 40%|      | 2628/6500 [7:57:14<11:16:18, 10.48s/it]                                                         40%|      | 2628/6500 [7:57:14<11:16:18, 10.48s/it] 40%|      | 2629/6500 [7:57:25<11:13:39, 10.44s/it]                                                         40%|      | 2629/6500 [7:57:25<11:13:39, 10.44s/it] 40%|      | 2630/6500 [7:57:35<11:11:32, 10.41s/it]                                                         40%|      | 2630/6500 [7:57:35<11:11:32, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8241983652114868, 'eval_runtime': 3.9464, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.4}
                                                         40%|      | 2630/6500 [7:57:39<11:11:32, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2630/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.412, 'learning_rate': 6.47641559814925e-05, 'epoch': 0.4}
{'loss': 0.4028, 'learning_rate': 6.474105862439045e-05, 'epoch': 0.4}
{'loss': 0.3981, 'learning_rate': 6.471795782165556e-05, 'epoch': 0.41}
{'loss': 0.4483, 'learning_rate': 6.469485357868754e-05, 'epoch': 0.41}
{'loss': 0.4101, 'learning_rate': 6.467174590088681e-05, 'epoch': 0.41}
 40%|      | 2631/6500 [7:57:50<12:37:57, 11.75s/it]                                                         40%|      | 2631/6500 [7:57:50<12:37:57, 11.75s/it] 40%|      | 2632/6500 [7:58:00<12:12:03, 11.36s/it]                                                         40%|      | 2632/6500 [7:58:00<12:12:03, 11.36s/it] 41%|      | 2633/6500 [7:58:11<11:53:25, 11.07s/it]                                                         41%|      | 2633/6500 [7:58:11<11:53:25, 11.07s/it] 41%|      | 2634/6500 [7:58:21<11:39:52, 10.86s/it]                                                         41%|      | 2634/6500 [7:58:21<11:39:52, 10.86s/it] 41%|      | 2635/6500 [7:58:32<11:39:33, 10.86s/it]                                                         41%|      | 2635/6500 [7:58:32<11:39:33, 10.86s/it] 41%|      | 2636/6500 [7:58:42<11:30:28, 10.72s/it]                  {'loss': 0.4174, 'learning_rate': 6.46486347936547e-05, 'epoch': 0.41}
{'loss': 0.4393, 'learning_rate': 6.462552026239328e-05, 'epoch': 0.41}
{'loss': 0.4017, 'learning_rate': 6.46024023125054e-05, 'epoch': 0.41}
{'loss': 0.4131, 'learning_rate': 6.457928094939478e-05, 'epoch': 0.41}
{'loss': 0.4137, 'learning_rate': 6.455615617846588e-05, 'epoch': 0.41}
                                       41%|      | 2636/6500 [7:58:42<11:30:28, 10.72s/it] 41%|      | 2637/6500 [7:58:53<11:24:08, 10.63s/it]                                                         41%|      | 2637/6500 [7:58:53<11:24:08, 10.63s/it] 41%|      | 2638/6500 [7:59:03<11:19:16, 10.55s/it]                                                         41%|      | 2638/6500 [7:59:03<11:19:16, 10.55s/it] 41%|      | 2639/6500 [7:59:14<11:15:50, 10.50s/it]                                                         41%|      | 2639/6500 [7:59:14<11:15:50, 10.50s/it] 41%|      | 2640/6500 [7:59:24<11:13:27, 10.47s/it]                                                         41%|      | 2640/6500 [7:59:24<11:13:27, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8168781995773315, 'eval_runtime': 3.9552, 'eval_samples_per_second': 5.815, 'eval_steps_per_second': 1.517, 'epoch': 0.41}
                                                         41%|      | 2640/6500 [7:59:28<11:13:27, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2640/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4172, 'learning_rate': 6.453302800512398e-05, 'epoch': 0.41}
{'loss': 0.4032, 'learning_rate': 6.450989643477515e-05, 'epoch': 0.41}
{'loss': 0.392, 'learning_rate': 6.448676147282625e-05, 'epoch': 0.41}
{'loss': 0.3931, 'learning_rate': 6.446362312468492e-05, 'epoch': 0.41}
{'loss': 0.4974, 'learning_rate': 6.444048139575963e-05, 'epoch': 0.41}
 41%|      | 2641/6500 [7:59:39<12:38:28, 11.79s/it]                                                         41%|      | 2641/6500 [7:59:39<12:38:28, 11.79s/it] 41%|      | 2642/6500 [7:59:49<12:11:07, 11.37s/it]                                                         41%|      | 2642/6500 [7:59:49<12:11:07, 11.37s/it] 41%|      | 2643/6500 [8:00:00<11:51:56, 11.08s/it]                                                         41%|      | 2643/6500 [8:00:00<11:51:56, 11.08s/it] 41%|      | 2644/6500 [8:00:10<11:38:43, 10.87s/it]                                                         41%|      | 2644/6500 [8:00:10<11:38:43, 10.87s/it] 41%|      | 2645/6500 [8:00:20<11:29:16, 10.73s/it]                                                         41%|      | 2645/6500 [8:00:20<11:29:16, 10.73s/it] 41%|      | 2646/6500 [8:00:31<11:22:36, 10.63s/it]                  {'loss': 0.4053, 'learning_rate': 6.441733629145961e-05, 'epoch': 0.41}
{'loss': 0.4059, 'learning_rate': 6.43941878171949e-05, 'epoch': 0.41}
{'loss': 0.4155, 'learning_rate': 6.437103597837631e-05, 'epoch': 0.41}
{'loss': 0.9401, 'learning_rate': 6.434788078041543e-05, 'epoch': 0.41}
{'loss': 0.4243, 'learning_rate': 6.432472222872465e-05, 'epoch': 0.41}
                                       41%|      | 2646/6500 [8:00:31<11:22:36, 10.63s/it] 41%|      | 2647/6500 [8:00:41<11:18:05, 10.56s/it]                                                         41%|      | 2647/6500 [8:00:41<11:18:05, 10.56s/it] 41%|      | 2648/6500 [8:00:52<11:14:38, 10.51s/it]                                                         41%|      | 2648/6500 [8:00:52<11:14:38, 10.51s/it] 41%|      | 2649/6500 [8:01:02<11:12:00, 10.47s/it]                                                         41%|      | 2649/6500 [8:01:02<11:12:00, 10.47s/it] 41%|      | 2650/6500 [8:01:12<11:10:32, 10.45s/it]                                                         41%|      | 2650/6500 [8:01:12<11:10:32, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8230348825454712, 'eval_runtime': 4.1852, 'eval_samples_per_second': 5.496, 'eval_steps_per_second': 1.434, 'epoch': 0.41}
                                                         41%|      | 2650/6500 [8:01:17<11:10:32, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4125, 'learning_rate': 6.430156032871715e-05, 'epoch': 0.41}
{'loss': 0.4109, 'learning_rate': 6.427839508580687e-05, 'epoch': 0.41}
{'loss': 0.3843, 'learning_rate': 6.425522650540857e-05, 'epoch': 0.41}
{'loss': 0.4268, 'learning_rate': 6.423205459293773e-05, 'epoch': 0.41}
{'loss': 0.3981, 'learning_rate': 6.420887935381067e-05, 'epoch': 0.41}
 41%|      | 2651/6500 [8:01:28<12:46:34, 11.95s/it]                                                         41%|      | 2651/6500 [8:01:28<12:46:34, 11.95s/it] 41%|      | 2652/6500 [8:01:38<12:16:32, 11.48s/it]                                                         41%|      | 2652/6500 [8:01:38<12:16:32, 11.48s/it] 41%|      | 2653/6500 [8:01:49<11:55:07, 11.15s/it]                                                         41%|      | 2653/6500 [8:01:49<11:55:07, 11.15s/it] 41%|      | 2654/6500 [8:01:59<11:40:22, 10.93s/it]                                                         41%|      | 2654/6500 [8:01:59<11:40:22, 10.93s/it] 41%|      | 2655/6500 [8:02:11<11:52:14, 11.11s/it]                                                         41%|      | 2655/6500 [8:02:11<11:52:14, 11.11s/it] 41%|      | 2656/6500 [8:02:21<11:40:24, 10.93s/it]                  {'loss': 0.383, 'learning_rate': 6.418570079344444e-05, 'epoch': 0.41}
{'loss': 0.3952, 'learning_rate': 6.416251891725692e-05, 'epoch': 0.41}
{'loss': 0.3983, 'learning_rate': 6.413933373066671e-05, 'epoch': 0.41}
{'loss': 0.4006, 'learning_rate': 6.411614523909321e-05, 'epoch': 0.41}
{'loss': 0.3904, 'learning_rate': 6.409295344795657e-05, 'epoch': 0.41}
                                       41%|      | 2656/6500 [8:02:21<11:40:24, 10.93s/it] 41%|      | 2657/6500 [8:02:31<11:29:07, 10.76s/it]                                                         41%|      | 2657/6500 [8:02:31<11:29:07, 10.76s/it] 41%|      | 2658/6500 [8:02:42<11:20:47, 10.63s/it]                                                         41%|      | 2658/6500 [8:02:42<11:20:47, 10.63s/it] 41%|      | 2659/6500 [8:02:52<11:15:06, 10.55s/it]                                                         41%|      | 2659/6500 [8:02:52<11:15:06, 10.55s/it] 41%|      | 2660/6500 [8:03:02<11:11:07, 10.49s/it]                                                         41%|      | 2660/6500 [8:03:02<11:11:07, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8281197547912598, 'eval_runtime': 4.7446, 'eval_samples_per_second': 4.848, 'eval_steps_per_second': 1.265, 'epoch': 0.41}
                                                         41%|      | 2660/6500 [8:03:07<11:11:07, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2660/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4218, 'learning_rate': 6.406975836267776e-05, 'epoch': 0.41}
{'loss': 0.3949, 'learning_rate': 6.404655998867848e-05, 'epoch': 0.41}
{'loss': 0.4171, 'learning_rate': 6.40233583313812e-05, 'epoch': 0.41}
{'loss': 0.4117, 'learning_rate': 6.400015339620917e-05, 'epoch': 0.41}
{'loss': 0.4027, 'learning_rate': 6.397694518858643e-05, 'epoch': 0.41}
 41%|      | 2661/6500 [8:03:18<12:49:13, 12.02s/it]                                                         41%|      | 2661/6500 [8:03:18<12:49:13, 12.02s/it] 41%|      | 2662/6500 [8:03:28<12:16:20, 11.51s/it]                                                         41%|      | 2662/6500 [8:03:28<12:16:20, 11.51s/it] 41%|      | 2663/6500 [8:03:39<11:53:42, 11.16s/it]                                                         41%|      | 2663/6500 [8:03:39<11:53:42, 11.16s/it] 41%|      | 2664/6500 [8:03:49<11:38:07, 10.92s/it]                                                         41%|      | 2664/6500 [8:03:49<11:38:07, 10.92s/it] 41%|      | 2665/6500 [8:03:59<11:26:57, 10.75s/it]                                                         41%|      | 2665/6500 [8:03:59<11:26:57, 10.75s/it] 41%|      | 2666/6500 [8:04:10<11:19:03, 10.63s/it]                  {'loss': 0.4146, 'learning_rate': 6.39537337139377e-05, 'epoch': 0.41}
{'loss': 0.4046, 'learning_rate': 6.393051897768858e-05, 'epoch': 0.41}
{'loss': 0.4104, 'learning_rate': 6.390730098526533e-05, 'epoch': 0.41}
{'loss': 0.4023, 'learning_rate': 6.388407974209505e-05, 'epoch': 0.41}
{'loss': 0.4325, 'learning_rate': 6.386085525360553e-05, 'epoch': 0.41}
                                       41%|      | 2666/6500 [8:04:10<11:19:03, 10.63s/it] 41%|      | 2667/6500 [8:04:20<11:18:14, 10.62s/it]                                                         41%|      | 2667/6500 [8:04:20<11:18:14, 10.62s/it] 41%|      | 2668/6500 [8:04:31<11:13:05, 10.54s/it]                                                         41%|      | 2668/6500 [8:04:31<11:13:05, 10.54s/it] 41%|      | 2669/6500 [8:04:41<11:09:13, 10.48s/it]                                                         41%|      | 2669/6500 [8:04:41<11:09:13, 10.48s/it] 41%|      | 2670/6500 [8:04:51<11:08:09, 10.47s/it]                                                         41%|      | 2670/6500 [8:04:51<11:08:09, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8193048238754272, 'eval_runtime': 3.9539, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 1.518, 'epoch': 0.41}
                                                         41%|      | 2670/6500 [8:04:55<11:08:09, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2670/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2670/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2670/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4003, 'learning_rate': 6.383762752522539e-05, 'epoch': 0.41}
{'loss': 0.4051, 'learning_rate': 6.381439656238393e-05, 'epoch': 0.41}
{'loss': 0.3944, 'learning_rate': 6.379116237051127e-05, 'epoch': 0.41}
{'loss': 0.4197, 'learning_rate': 6.376792495503828e-05, 'epoch': 0.41}
{'loss': 0.4743, 'learning_rate': 6.374468432139652e-05, 'epoch': 0.41}
 41%|      | 2671/6500 [8:05:06<12:30:39, 11.76s/it]                                                         41%|      | 2671/6500 [8:05:06<12:30:39, 11.76s/it] 41%|      | 2672/6500 [8:05:17<12:03:20, 11.34s/it]                                                         41%|      | 2672/6500 [8:05:17<12:03:20, 11.34s/it] 41%|      | 2673/6500 [8:05:27<11:44:07, 11.04s/it]                                                         41%|      | 2673/6500 [8:05:27<11:44:07, 11.04s/it] 41%|      | 2674/6500 [8:05:37<11:30:45, 10.83s/it]                                                         41%|      | 2674/6500 [8:05:37<11:30:45, 10.83s/it] 41%|      | 2675/6500 [8:05:48<11:21:35, 10.69s/it]                                                         41%|      | 2675/6500 [8:05:48<11:21:35, 10.69s/it] 41%|      | 2676/6500 [8:05:58<11:14:46, 10.59s/it]                  {'loss': 0.3954, 'learning_rate': 6.372144047501837e-05, 'epoch': 0.41}
{'loss': 0.3936, 'learning_rate': 6.369819342133694e-05, 'epoch': 0.41}
{'loss': 0.4066, 'learning_rate': 6.367494316578609e-05, 'epoch': 0.41}
{'loss': 0.9288, 'learning_rate': 6.36516897138004e-05, 'epoch': 0.41}
{'loss': 0.4257, 'learning_rate': 6.362843307081527e-05, 'epoch': 0.41}
                                       41%|      | 2676/6500 [8:05:58<11:14:46, 10.59s/it] 41%|      | 2677/6500 [8:06:08<11:10:07, 10.52s/it]                                                         41%|      | 2677/6500 [8:06:08<11:10:07, 10.52s/it] 41%|      | 2678/6500 [8:06:19<11:06:45, 10.47s/it]                                                         41%|      | 2678/6500 [8:06:19<11:06:45, 10.47s/it] 41%|      | 2679/6500 [8:06:29<11:04:17, 10.43s/it]                                                         41%|      | 2679/6500 [8:06:29<11:04:17, 10.43s/it] 41%|      | 2680/6500 [8:06:39<11:03:18, 10.42s/it]                                                         41%|      | 2680/6500 [8:06:39<11:03:18, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.827335774898529, 'eval_runtime': 4.3204, 'eval_samples_per_second': 5.324, 'eval_steps_per_second': 1.389, 'epoch': 0.41}
                                                         41%|      | 2680/6500 [8:06:44<11:03:18, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2680/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2680/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4051, 'learning_rate': 6.360517324226676e-05, 'epoch': 0.41}
{'loss': 0.385, 'learning_rate': 6.358191023359172e-05, 'epoch': 0.41}
{'loss': 0.4016, 'learning_rate': 6.355864405022774e-05, 'epoch': 0.41}
{'loss': 0.4294, 'learning_rate': 6.353537469761315e-05, 'epoch': 0.41}
{'loss': 0.3959, 'learning_rate': 6.351210218118704e-05, 'epoch': 0.41}
 41%|      | 2681/6500 [8:06:55<12:34:18, 11.85s/it]                                                         41%|      | 2681/6500 [8:06:55<12:34:18, 11.85s/it] 41%|     | 2682/6500 [8:07:05<12:05:19, 11.40s/it]                                                         41%|     | 2682/6500 [8:07:05<12:05:19, 11.40s/it] 41%|     | 2683/6500 [8:07:16<11:52:28, 11.20s/it]                                                         41%|     | 2683/6500 [8:07:16<11:52:28, 11.20s/it] 41%|     | 2684/6500 [8:07:26<11:35:55, 10.94s/it]                                                         41%|     | 2684/6500 [8:07:26<11:35:55, 10.94s/it] 41%|     | 2685/6500 [8:07:36<11:24:54, 10.77s/it]                                                         41%|     | 2685/6500 [8:07:36<11:24:54, 10.77s/it] 41%|     | 2686/6500 [8:07:47<11:16:58, 10.65s/it]{'loss': 0.3955, 'learning_rate': 6.34888265063892e-05, 'epoch': 0.41}
{'loss': 0.383, 'learning_rate': 6.346554767866017e-05, 'epoch': 0.41}
{'loss': 0.4005, 'learning_rate': 6.344226570344123e-05, 'epoch': 0.41}
{'loss': 0.4021, 'learning_rate': 6.341898058617442e-05, 'epoch': 0.41}
{'loss': 0.3892, 'learning_rate': 6.339569233230249e-05, 'epoch': 0.41}
                                                         41%|     | 2686/6500 [8:07:47<11:16:58, 10.65s/it] 41%|     | 2687/6500 [8:07:57<11:10:33, 10.55s/it]                                                         41%|     | 2687/6500 [8:07:57<11:10:33, 10.55s/it] 41%|     | 2688/6500 [8:08:07<11:06:22, 10.49s/it]                                                         41%|     | 2688/6500 [8:08:07<11:06:22, 10.49s/it] 41%|     | 2689/6500 [8:08:18<11:03:45, 10.45s/it]                                                         41%|     | 2689/6500 [8:08:18<11:03:45, 10.45s/it] 41%|     | 2690/6500 [8:08:28<11:01:11, 10.41s/it]                                                         41%|     | 2690/6500 [8:08:28<11:01:11, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8307484984397888, 'eval_runtime': 3.9602, 'eval_samples_per_second': 5.808, 'eval_steps_per_second': 1.515, 'epoch': 0.41}
                                                         41%|     | 2690/6500 [8:08:32<11:01:11, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2690
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4147, 'learning_rate': 6.337240094726893e-05, 'epoch': 0.41}
{'loss': 0.3936, 'learning_rate': 6.334910643651794e-05, 'epoch': 0.41}
{'loss': 0.4249, 'learning_rate': 6.33258088054945e-05, 'epoch': 0.41}
{'loss': 0.4072, 'learning_rate': 6.330250805964425e-05, 'epoch': 0.41}
{'loss': 0.3997, 'learning_rate': 6.327920420441365e-05, 'epoch': 0.41}
 41%|     | 2691/6500 [8:08:43<12:25:14, 11.74s/it]                                                         41%|     | 2691/6500 [8:08:43<12:25:14, 11.74s/it] 41%|     | 2692/6500 [8:08:53<11:59:16, 11.33s/it]                                                         41%|     | 2692/6500 [8:08:53<11:59:16, 11.33s/it] 41%|     | 2693/6500 [8:09:04<11:40:01, 11.03s/it]                                                         41%|     | 2693/6500 [8:09:04<11:40:01, 11.03s/it] 41%|     | 2694/6500 [8:09:14<11:26:28, 10.82s/it]                                                         41%|     | 2694/6500 [8:09:14<11:26:28, 10.82s/it] 41%|     | 2695/6500 [8:09:24<11:17:21, 10.68s/it]                                                         41%|     | 2695/6500 [8:09:24<11:17:21, 10.68s/it] 41%|     | 2696/6500 [8:09:35<11:11:01, 10.58s/{'loss': 0.4249, 'learning_rate': 6.325589724524978e-05, 'epoch': 0.41}
{'loss': 0.4044, 'learning_rate': 6.323258718760055e-05, 'epoch': 0.41}
{'loss': 0.4091, 'learning_rate': 6.32092740369145e-05, 'epoch': 0.42}
{'loss': 0.396, 'learning_rate': 6.318595779864098e-05, 'epoch': 0.42}
{'loss': 0.4208, 'learning_rate': 6.316263847822997e-05, 'epoch': 0.42}
it]                                                         41%|     | 2696/6500 [8:09:35<11:11:01, 10.58s/it] 41%|     | 2697/6500 [8:09:45<11:06:18, 10.51s/it]                                                         41%|     | 2697/6500 [8:09:45<11:06:18, 10.51s/it] 42%|     | 2698/6500 [8:09:56<11:05:45, 10.51s/it]                                                         42%|     | 2698/6500 [8:09:56<11:05:45, 10.51s/it] 42%|     | 2699/6500 [8:10:06<11:07:14, 10.53s/it]                                                         42%|     | 2699/6500 [8:10:06<11:07:14, 10.53s/it] 42%|     | 2700/6500 [8:10:17<11:03:46, 10.48s/it]                                                         42%|     | 2700/6500 [8:10:17<11:03:46, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8174105286598206, 'eval_runtime': 3.9687, 'eval_samples_per_second': 5.795, 'eval_steps_per_second': 1.512, 'epoch': 0.42}
                                                         42%|     | 2700/6500 [8:10:20<11:03:46, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2700
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2700

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2700
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3921, 'learning_rate': 6.313931608113226e-05, 'epoch': 0.42}
{'loss': 0.393, 'learning_rate': 6.311599061279932e-05, 'epoch': 0.42}
{'loss': 0.3791, 'learning_rate': 6.30926620786833e-05, 'epoch': 0.42}
{'loss': 0.4387, 'learning_rate': 6.30693304842371e-05, 'epoch': 0.42}
{'loss': 0.4468, 'learning_rate': 6.30459958349144e-05, 'epoch': 0.42}
 42%|     | 2701/6500 [8:10:31<12:26:10, 11.78s/it]                                                         42%|     | 2701/6500 [8:10:31<12:26:10, 11.78s/it] 42%|     | 2702/6500 [8:10:42<11:58:35, 11.35s/it]                                                         42%|     | 2702/6500 [8:10:42<11:58:35, 11.35s/it] 42%|     | 2703/6500 [8:10:52<11:39:55, 11.06s/it]                                                         42%|     | 2703/6500 [8:10:52<11:39:55, 11.06s/it] 42%|     | 2704/6500 [8:11:02<11:26:04, 10.84s/it]                                                         42%|     | 2704/6500 [8:11:02<11:26:04, 10.84s/it] 42%|     | 2705/6500 [8:11:13<11:16:41, 10.70s/it]                                                         42%|     | 2705/6500 [8:11:13<11:16:41, 10.70s/it] 42%|     | 2706/6500 [8:11:23<11:10:00, 10.60s/{'loss': 0.3843, 'learning_rate': 6.302265813616947e-05, 'epoch': 0.42}
{'loss': 0.4134, 'learning_rate': 6.299931739345741e-05, 'epoch': 0.42}
{'loss': 0.5153, 'learning_rate': 6.297597361223392e-05, 'epoch': 0.42}
{'loss': 0.8273, 'learning_rate': 6.29526267979555e-05, 'epoch': 0.42}
{'loss': 0.4143, 'learning_rate': 6.292927695607933e-05, 'epoch': 0.42}
it]                                                         42%|     | 2706/6500 [8:11:23<11:10:00, 10.60s/it] 42%|     | 2707/6500 [8:11:33<11:05:07, 10.52s/it]                                                         42%|     | 2707/6500 [8:11:33<11:05:07, 10.52s/it] 42%|     | 2708/6500 [8:11:44<11:01:47, 10.47s/it]                                                         42%|     | 2708/6500 [8:11:44<11:01:47, 10.47s/it] 42%|     | 2709/6500 [8:11:54<11:00:00, 10.45s/it]                                                         42%|     | 2709/6500 [8:11:54<11:00:00, 10.45s/it] 42%|     | 2710/6500 [8:12:05<10:58:03, 10.42s/it]                                                         42%|     | 2710/6500 [8:12:05<10:58:03, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8254388570785522, 'eval_runtime': 4.1818, 'eval_samples_per_second': 5.5, 'eval_steps_per_second': 1.435, 'epoch': 0.42}
                                                         42%|     | 2710/6500 [8:12:09<10:58:03, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2710
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4126, 'learning_rate': 6.290592409206327e-05, 'epoch': 0.42}
{'loss': 0.3841, 'learning_rate': 6.288256821136594e-05, 'epoch': 0.42}
{'loss': 0.4089, 'learning_rate': 6.285920931944661e-05, 'epoch': 0.42}
{'loss': 0.4124, 'learning_rate': 6.283584742176528e-05, 'epoch': 0.42}
{'loss': 0.369, 'learning_rate': 6.281248252378267e-05, 'epoch': 0.42}
 42%|     | 2711/6500 [8:12:20<12:26:02, 11.81s/it]                                                         42%|     | 2711/6500 [8:12:20<12:26:02, 11.81s/it] 42%|     | 2712/6500 [8:12:30<11:58:33, 11.38s/it]                                                         42%|     | 2712/6500 [8:12:30<11:58:33, 11.38s/it] 42%|     | 2713/6500 [8:12:40<11:39:00, 11.07s/it]                                                         42%|     | 2713/6500 [8:12:40<11:39:00, 11.07s/it] 42%|     | 2714/6500 [8:12:51<11:25:26, 10.86s/it]                                                         42%|     | 2714/6500 [8:12:51<11:25:26, 10.86s/it] 42%|     | 2715/6500 [8:13:01<11:22:54, 10.83s/it]                                                         42%|     | 2715/6500 [8:13:01<11:22:54, 10.83s/it] 42%|     | 2716/6500 [8:13:12<11:14:01, 10.69s/{'loss': 0.409, 'learning_rate': 6.278911463096016e-05, 'epoch': 0.42}
{'loss': 0.3914, 'learning_rate': 6.276574374875986e-05, 'epoch': 0.42}
{'loss': 0.4052, 'learning_rate': 6.274236988264459e-05, 'epoch': 0.42}
{'loss': 0.4094, 'learning_rate': 6.271899303807783e-05, 'epoch': 0.42}
{'loss': 0.4117, 'learning_rate': 6.269561322052378e-05, 'epoch': 0.42}
it]                                                         42%|     | 2716/6500 [8:13:12<11:14:01, 10.69s/it] 42%|     | 2717/6500 [8:13:22<11:07:39, 10.59s/it]                                                         42%|     | 2717/6500 [8:13:22<11:07:39, 10.59s/it] 42%|     | 2718/6500 [8:13:33<11:03:10, 10.52s/it]                                                         42%|     | 2718/6500 [8:13:33<11:03:10, 10.52s/it] 42%|     | 2719/6500 [8:13:43<10:59:47, 10.47s/it]                                                         42%|     | 2719/6500 [8:13:43<10:59:47, 10.47s/it] 42%|     | 2720/6500 [8:13:53<10:57:35, 10.44s/it]                                                         42%|     | 2720/6500 [8:13:53<10:57:35, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8274580836296082, 'eval_runtime': 3.9625, 'eval_samples_per_second': 5.804, 'eval_steps_per_second': 1.514, 'epoch': 0.42}
                                                         42%|     | 2720/6500 [8:13:57<10:57:35, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2720
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2720/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4022, 'learning_rate': 6.267223043544732e-05, 'epoch': 0.42}
{'loss': 0.401, 'learning_rate': 6.264884468831405e-05, 'epoch': 0.42}
{'loss': 0.4314, 'learning_rate': 6.262545598459025e-05, 'epoch': 0.42}
{'loss': 0.4071, 'learning_rate': 6.260206432974288e-05, 'epoch': 0.42}
{'loss': 0.4142, 'learning_rate': 6.257866972923956e-05, 'epoch': 0.42}
 42%|     | 2721/6500 [8:14:08<12:20:03, 11.75s/it]                                                         42%|     | 2721/6500 [8:14:08<12:20:03, 11.75s/it] 42%|     | 2722/6500 [8:14:18<11:53:31, 11.33s/it]                                                         42%|     | 2722/6500 [8:14:18<11:53:31, 11.33s/it] 42%|     | 2723/6500 [8:14:29<11:34:46, 11.04s/it]                                                         42%|     | 2723/6500 [8:14:29<11:34:46, 11.04s/it] 42%|     | 2724/6500 [8:14:39<11:21:47, 10.83s/it]                                                         42%|     | 2724/6500 [8:14:39<11:21:47, 10.83s/it] 42%|     | 2725/6500 [8:14:50<11:12:27, 10.69s/it]                                                         42%|     | 2725/6500 [8:14:50<11:12:27, 10.69s/it] 42%|     | 2726/6500 [8:15:00<11:05:54, 10.59s/{'loss': 0.4201, 'learning_rate': 6.25552721885487e-05, 'epoch': 0.42}
{'loss': 0.3967, 'learning_rate': 6.25318717131393e-05, 'epoch': 0.42}
{'loss': 0.4063, 'learning_rate': 6.250846830848108e-05, 'epoch': 0.42}
{'loss': 0.4165, 'learning_rate': 6.248506198004445e-05, 'epoch': 0.42}
{'loss': 0.4055, 'learning_rate': 6.246165273330049e-05, 'epoch': 0.42}
it]                                                         42%|     | 2726/6500 [8:15:00<11:05:54, 10.59s/it] 42%|     | 2727/6500 [8:15:10<11:01:31, 10.52s/it]                                                         42%|     | 2727/6500 [8:15:10<11:01:31, 10.52s/it] 42%|     | 2728/6500 [8:15:21<10:58:07, 10.47s/it]                                                         42%|     | 2728/6500 [8:15:21<10:58:07, 10.47s/it] 42%|     | 2729/6500 [8:15:31<10:55:43, 10.43s/it]                                                         42%|     | 2729/6500 [8:15:31<10:55:43, 10.43s/it] 42%|     | 2730/6500 [8:15:41<10:54:33, 10.42s/it]                                                         42%|     | 2730/6500 [8:15:41<10:54:33, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.820294201374054, 'eval_runtime': 3.9512, 'eval_samples_per_second': 5.821, 'eval_steps_per_second': 1.519, 'epoch': 0.42}
                                                         42%|     | 2730/6500 [8:15:45<10:54:33, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2730/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4009, 'learning_rate': 6.243824057372098e-05, 'epoch': 0.42}
{'loss': 0.3906, 'learning_rate': 6.241482550677833e-05, 'epoch': 0.42}
{'loss': 0.4027, 'learning_rate': 6.239140753794574e-05, 'epoch': 0.42}
{'loss': 0.4741, 'learning_rate': 6.236798667269695e-05, 'epoch': 0.42}
{'loss': 0.394, 'learning_rate': 6.23445629165065e-05, 'epoch': 0.42}
 42%|     | 2731/6500 [8:15:56<12:16:48, 11.73s/it]                                                         42%|     | 2731/6500 [8:15:56<12:16:48, 11.73s/it] 42%|     | 2732/6500 [8:16:07<11:55:48, 11.40s/it]                                                         42%|     | 2732/6500 [8:16:07<11:55:48, 11.40s/it] 42%|     | 2733/6500 [8:16:17<11:36:13, 11.09s/it]                                                         42%|     | 2733/6500 [8:16:17<11:36:13, 11.09s/it] 42%|     | 2734/6500 [8:16:27<11:21:55, 10.86s/it]                                                         42%|     | 2734/6500 [8:16:27<11:21:55, 10.86s/it] 42%|     | 2735/6500 [8:16:38<11:11:55, 10.71s/it]                                                         42%|     | 2735/6500 [8:16:38<11:11:55, 10.71s/it] 42%|     | 2736/6500 [8:16:48<11:05:23, 10.61s/{'loss': 0.383, 'learning_rate': 6.23211362748495e-05, 'epoch': 0.42}
{'loss': 0.4128, 'learning_rate': 6.229770675320184e-05, 'epoch': 0.42}
{'loss': 0.929, 'learning_rate': 6.227427435703997e-05, 'epoch': 0.42}
{'loss': 0.4179, 'learning_rate': 6.225083909184109e-05, 'epoch': 0.42}
{'loss': 0.4003, 'learning_rate': 6.222740096308309e-05, 'epoch': 0.42}
it]                                                         42%|     | 2736/6500 [8:16:48<11:05:23, 10.61s/it] 42%|     | 2737/6500 [8:16:58<11:00:24, 10.53s/it]                                                         42%|     | 2737/6500 [8:16:58<11:00:24, 10.53s/it] 42%|     | 2738/6500 [8:17:09<10:56:56, 10.48s/it]                                                         42%|     | 2738/6500 [8:17:09<10:56:56, 10.48s/it] 42%|     | 2739/6500 [8:17:19<10:54:49, 10.45s/it]                                                         42%|     | 2739/6500 [8:17:19<10:54:49, 10.45s/it] 42%|     | 2740/6500 [8:17:30<10:53:04, 10.42s/it]                                                         42%|     | 2740/6500 [8:17:30<10:53:04, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8280592560768127, 'eval_runtime': 3.9587, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.42}
                                                         42%|     | 2740/6500 [8:17:34<10:53:04, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2740
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4107, 'learning_rate': 6.220395997624443e-05, 'epoch': 0.42}
{'loss': 0.3705, 'learning_rate': 6.218051613680435e-05, 'epoch': 0.42}
{'loss': 0.421, 'learning_rate': 6.215706945024267e-05, 'epoch': 0.42}
{'loss': 0.3844, 'learning_rate': 6.213361992203991e-05, 'epoch': 0.42}
{'loss': 0.3868, 'learning_rate': 6.211016755767729e-05, 'epoch': 0.42}
 42%|     | 2741/6500 [8:17:44<12:15:51, 11.75s/it]                                                         42%|     | 2741/6500 [8:17:44<12:15:51, 11.75s/it] 42%|     | 2742/6500 [8:17:55<11:49:28, 11.33s/it]                                                         42%|     | 2742/6500 [8:17:55<11:49:28, 11.33s/it] 42%|     | 2743/6500 [8:18:05<11:30:46, 11.03s/it]                                                         42%|     | 2743/6500 [8:18:05<11:30:46, 11.03s/it] 42%|     | 2744/6500 [8:18:15<11:17:23, 10.82s/it]                                                         42%|     | 2744/6500 [8:18:15<11:17:23, 10.82s/it] 42%|     | 2745/6500 [8:18:26<11:08:41, 10.68s/it]                                                         42%|     | 2745/6500 [8:18:26<11:08:41, 10.68s/it] 42%|     | 2746/6500 [8:18:36<11:02:01, 10.58s/{'loss': 0.3843, 'learning_rate': 6.208671236263663e-05, 'epoch': 0.42}
{'loss': 0.3947, 'learning_rate': 6.206325434240043e-05, 'epoch': 0.42}
{'loss': 0.4008, 'learning_rate': 6.203979350245188e-05, 'epoch': 0.42}
{'loss': 0.3781, 'learning_rate': 6.20163298482748e-05, 'epoch': 0.42}
{'loss': 0.4263, 'learning_rate': 6.199286338535369e-05, 'epoch': 0.42}
it]                                                         42%|     | 2746/6500 [8:18:36<11:02:01, 10.58s/it] 42%|     | 2747/6500 [8:18:46<10:57:33, 10.51s/it]                                                         42%|     | 2747/6500 [8:18:46<10:57:33, 10.51s/it] 42%|     | 2748/6500 [8:18:57<11:05:28, 10.64s/it]                                                         42%|     | 2748/6500 [8:18:57<11:05:28, 10.64s/it] 42%|     | 2749/6500 [8:19:08<10:59:41, 10.55s/it]                                                         42%|     | 2749/6500 [8:19:08<10:59:41, 10.55s/it] 42%|     | 2750/6500 [8:19:18<10:55:43, 10.49s/it]                                                         42%|     | 2750/6500 [8:19:18<10:55:43, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.830708384513855, 'eval_runtime': 3.9296, 'eval_samples_per_second': 5.853, 'eval_steps_per_second': 1.527, 'epoch': 0.42}
                                                         42%|     | 2750/6500 [8:19:22<10:55:43, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2750
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3934, 'learning_rate': 6.196939411917369e-05, 'epoch': 0.42}
{'loss': 0.4344, 'learning_rate': 6.194592205522056e-05, 'epoch': 0.42}
{'loss': 0.4221, 'learning_rate': 6.192244719898079e-05, 'epoch': 0.42}
{'loss': 0.4069, 'learning_rate': 6.189896955594148e-05, 'epoch': 0.42}
{'loss': 0.4108, 'learning_rate': 6.187548913159039e-05, 'epoch': 0.42}
 42%|     | 2751/6500 [8:19:33<12:16:06, 11.78s/it]                                                         42%|     | 2751/6500 [8:19:33<12:16:06, 11.78s/it] 42%|     | 2752/6500 [8:19:43<11:49:03, 11.35s/it]                                                         42%|     | 2752/6500 [8:19:43<11:49:03, 11.35s/it] 42%|     | 2753/6500 [8:19:54<11:29:53, 11.05s/it]                                                         42%|     | 2753/6500 [8:19:54<11:29:53, 11.05s/it] 42%|     | 2754/6500 [8:20:04<11:16:16, 10.83s/it]                                                         42%|     | 2754/6500 [8:20:04<11:16:16, 10.83s/it] 42%|     | 2755/6500 [8:20:14<11:06:54, 10.68s/it]                                                         42%|     | 2755/6500 [8:20:14<11:06:54, 10.68s/it] 42%|     | 2756/6500 [8:20:25<11:00:30, 10.59s/{'loss': 0.4098, 'learning_rate': 6.185200593141593e-05, 'epoch': 0.42}
{'loss': 0.3957, 'learning_rate': 6.182851996090713e-05, 'epoch': 0.42}
{'loss': 0.4058, 'learning_rate': 6.18050312255537e-05, 'epoch': 0.42}
{'loss': 0.4277, 'learning_rate': 6.1781539730846e-05, 'epoch': 0.42}
{'loss': 0.3836, 'learning_rate': 6.175804548227502e-05, 'epoch': 0.42}
it]                                                         42%|     | 2756/6500 [8:20:25<11:00:30, 10.59s/it] 42%|     | 2757/6500 [8:20:35<10:55:40, 10.51s/it]                                                         42%|     | 2757/6500 [8:20:35<10:55:40, 10.51s/it] 42%|     | 2758/6500 [8:20:45<10:52:22, 10.46s/it]                                                         42%|     | 2758/6500 [8:20:45<10:52:22, 10.46s/it] 42%|     | 2759/6500 [8:20:56<10:50:13, 10.43s/it]                                                         42%|     | 2759/6500 [8:20:56<10:50:13, 10.43s/it] 42%|     | 2760/6500 [8:21:06<10:48:36, 10.41s/it]                                                         42%|     | 2760/6500 [8:21:06<10:48:36, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8182523846626282, 'eval_runtime': 3.9342, 'eval_samples_per_second': 5.846, 'eval_steps_per_second': 1.525, 'epoch': 0.42}
                                                         42%|     | 2760/6500 [8:21:10<10:48:36, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2760
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2760/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2760/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2760/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4037, 'learning_rate': 6.173454848533242e-05, 'epoch': 0.42}
{'loss': 0.3815, 'learning_rate': 6.171104874551044e-05, 'epoch': 0.42}
{'loss': 0.4347, 'learning_rate': 6.168754626830202e-05, 'epoch': 0.43}
{'loss': 0.4546, 'learning_rate': 6.166404105920073e-05, 'epoch': 0.43}
{'loss': 0.3964, 'learning_rate': 6.164053312370074e-05, 'epoch': 0.43}
 42%|     | 2761/6500 [8:21:21<12:10:06, 11.72s/it]                                                         42%|     | 2761/6500 [8:21:21<12:10:06, 11.72s/it] 42%|     | 2762/6500 [8:21:31<11:44:04, 11.30s/it]                                                         42%|     | 2762/6500 [8:21:31<11:44:04, 11.30s/it] 43%|     | 2763/6500 [8:21:41<11:25:45, 11.01s/it]                                                         43%|     | 2763/6500 [8:21:41<11:25:45, 11.01s/it] 43%|     | 2764/6500 [8:21:52<11:17:29, 10.88s/it]                                                         43%|     | 2764/6500 [8:21:52<11:17:29, 10.88s/it] 43%|     | 2765/6500 [8:22:02<11:07:18, 10.72s/it]                                                         43%|     | 2765/6500 [8:22:02<11:07:18, 10.72s/it] 43%|     | 2766/6500 [8:22:13<10:59:56, 10.60s/{'loss': 0.4041, 'learning_rate': 6.161702246729692e-05, 'epoch': 0.43}
{'loss': 0.385, 'learning_rate': 6.159350909548475e-05, 'epoch': 0.43}
{'loss': 0.9235, 'learning_rate': 6.156999301376031e-05, 'epoch': 0.43}
{'loss': 0.4108, 'learning_rate': 6.154647422762033e-05, 'epoch': 0.43}
{'loss': 0.4095, 'learning_rate': 6.152295274256222e-05, 'epoch': 0.43}
it]                                                         43%|     | 2766/6500 [8:22:13<10:59:56, 10.60s/it] 43%|     | 2767/6500 [8:22:23<10:54:57, 10.53s/it]                                                         43%|     | 2767/6500 [8:22:23<10:54:57, 10.53s/it] 43%|     | 2768/6500 [8:22:33<10:51:16, 10.47s/it]                                                         43%|     | 2768/6500 [8:22:33<10:51:16, 10.47s/it] 43%|     | 2769/6500 [8:22:44<10:48:42, 10.43s/it]                                                         43%|     | 2769/6500 [8:22:44<10:48:42, 10.43s/it] 43%|     | 2770/6500 [8:22:54<10:48:15, 10.43s/it]                                                         43%|     | 2770/6500 [8:22:54<10:48:15, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8278416991233826, 'eval_runtime': 3.9355, 'eval_samples_per_second': 5.844, 'eval_steps_per_second': 1.525, 'epoch': 0.43}
                                                         43%|     | 2770/6500 [8:22:58<10:48:15, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3761, 'learning_rate': 6.149942856408396e-05, 'epoch': 0.43}
{'loss': 0.3951, 'learning_rate': 6.147590169768419e-05, 'epoch': 0.43}
{'loss': 0.4191, 'learning_rate': 6.145237214886219e-05, 'epoch': 0.43}
{'loss': 0.378, 'learning_rate': 6.142883992311781e-05, 'epoch': 0.43}
{'loss': 0.3943, 'learning_rate': 6.14053050259516e-05, 'epoch': 0.43}
 43%|     | 2771/6500 [8:23:09<12:10:25, 11.75s/it]                                                         43%|     | 2771/6500 [8:23:09<12:10:25, 11.75s/it] 43%|     | 2772/6500 [8:23:19<11:44:17, 11.34s/it]                                                         43%|     | 2772/6500 [8:23:19<11:44:17, 11.34s/it] 43%|     | 2773/6500 [8:23:30<11:25:45, 11.04s/it]                                                         43%|     | 2773/6500 [8:23:30<11:25:45, 11.04s/it] 43%|     | 2774/6500 [8:23:40<11:12:51, 10.84s/it]                                                         43%|     | 2774/6500 [8:23:40<11:12:51, 10.84s/it] 43%|     | 2775/6500 [8:23:50<11:04:15, 10.70s/it]                                                         43%|     | 2775/6500 [8:23:50<11:04:15, 10.70s/it] 43%|     | 2776/6500 [8:24:01<10:57:43, 10.60s/{'loss': 0.3935, 'learning_rate': 6.138176746286468e-05, 'epoch': 0.43}
{'loss': 0.3886, 'learning_rate': 6.135822723935882e-05, 'epoch': 0.43}
{'loss': 0.3978, 'learning_rate': 6.13346843609364e-05, 'epoch': 0.43}
{'loss': 0.3885, 'learning_rate': 6.131113883310041e-05, 'epoch': 0.43}
{'loss': 0.4021, 'learning_rate': 6.128759066135451e-05, 'epoch': 0.43}
it]                                                         43%|     | 2776/6500 [8:24:01<10:57:43, 10.60s/it] 43%|     | 2777/6500 [8:24:11<10:53:31, 10.53s/it]                                                         43%|     | 2777/6500 [8:24:11<10:53:31, 10.53s/it] 43%|     | 2778/6500 [8:24:22<10:50:18, 10.48s/it]                                                         43%|     | 2778/6500 [8:24:22<10:50:18, 10.48s/it] 43%|     | 2779/6500 [8:24:32<10:48:23, 10.46s/it]                                                         43%|     | 2779/6500 [8:24:32<10:48:23, 10.46s/it] 43%|     | 2780/6500 [8:24:43<10:57:34, 10.61s/it]                                                         43%|     | 2780/6500 [8:24:43<10:57:34, 10.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8335220217704773, 'eval_runtime': 3.9569, 'eval_samples_per_second': 5.813, 'eval_steps_per_second': 1.516, 'epoch': 0.43}
                                                         43%|     | 2780/6500 [8:24:47<10:57:34, 10.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2780/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3909, 'learning_rate': 6.126403985120292e-05, 'epoch': 0.43}
{'loss': 0.4313, 'learning_rate': 6.12404864081505e-05, 'epoch': 0.43}
{'loss': 0.3889, 'learning_rate': 6.121693033770274e-05, 'epoch': 0.43}
{'loss': 0.4011, 'learning_rate': 6.11933716453657e-05, 'epoch': 0.43}
{'loss': 0.4128, 'learning_rate': 6.116981033664609e-05, 'epoch': 0.43}
 43%|     | 2781/6500 [8:24:58<12:18:58, 11.92s/it]                                                         43%|     | 2781/6500 [8:24:58<12:18:58, 11.92s/it] 43%|     | 2782/6500 [8:25:08<11:49:50, 11.46s/it]                                                         43%|     | 2782/6500 [8:25:08<11:49:50, 11.46s/it] 43%|     | 2783/6500 [8:25:19<11:29:42, 11.13s/it]                                                         43%|     | 2783/6500 [8:25:19<11:29:42, 11.13s/it] 43%|     | 2784/6500 [8:25:29<11:15:17, 10.90s/it]                                                         43%|     | 2784/6500 [8:25:29<11:15:17, 10.90s/it] 43%|     | 2785/6500 [8:25:39<11:05:42, 10.75s/it]                                                         43%|     | 2785/6500 [8:25:39<11:05:42, 10.75s/it] 43%|     | 2786/6500 [8:25:50<10:58:39, 10.64s/{'loss': 0.386, 'learning_rate': 6.114624641705122e-05, 'epoch': 0.43}
{'loss': 0.4045, 'learning_rate': 6.112267989208904e-05, 'epoch': 0.43}
{'loss': 0.3925, 'learning_rate': 6.109911076726806e-05, 'epoch': 0.43}
{'loss': 0.4083, 'learning_rate': 6.107553904809741e-05, 'epoch': 0.43}
{'loss': 0.3947, 'learning_rate': 6.105196474008686e-05, 'epoch': 0.43}
it]                                                         43%|     | 2786/6500 [8:25:50<10:58:39, 10.64s/it] 43%|     | 2787/6500 [8:26:00<10:53:48, 10.57s/it]                                                         43%|     | 2787/6500 [8:26:00<10:53:48, 10.57s/it] 43%|     | 2788/6500 [8:26:11<10:50:07, 10.51s/it]                                                         43%|     | 2788/6500 [8:26:11<10:50:07, 10.51s/it] 43%|     | 2789/6500 [8:26:21<10:47:55, 10.48s/it]                                                         43%|     | 2789/6500 [8:26:21<10:47:55, 10.48s/it] 43%|     | 2790/6500 [8:26:31<10:46:15, 10.45s/it]                                                         43%|     | 2790/6500 [8:26:31<10:46:15, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8177189230918884, 'eval_runtime': 3.9578, 'eval_samples_per_second': 5.811, 'eval_steps_per_second': 1.516, 'epoch': 0.43}
                                                         43%|     | 2790/6500 [8:26:35<10:46:15, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2790
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2790/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3933, 'learning_rate': 6.1028387848746737e-05, 'epoch': 0.43}
{'loss': 0.3786, 'learning_rate': 6.100480837958802e-05, 'epoch': 0.43}
{'loss': 0.4777, 'learning_rate': 6.098122633812225e-05, 'epoch': 0.43}
{'loss': 0.3964, 'learning_rate': 6.095764172986159e-05, 'epoch': 0.43}
{'loss': 0.3763, 'learning_rate': 6.09340545603188e-05, 'epoch': 0.43}
 43%|     | 2791/6500 [8:26:46<12:07:41, 11.77s/it]                                                         43%|     | 2791/6500 [8:26:46<12:07:41, 11.77s/it] 43%|     | 2792/6500 [8:26:57<11:41:53, 11.36s/it]                                                         43%|     | 2792/6500 [8:26:57<11:41:53, 11.36s/it] 43%|     | 2793/6500 [8:27:07<11:23:19, 11.06s/it]                                                         43%|     | 2793/6500 [8:27:07<11:23:19, 11.06s/it] 43%|     | 2794/6500 [8:27:17<11:10:40, 10.86s/it]                                                         43%|     | 2794/6500 [8:27:17<11:10:40, 10.86s/it] 43%|     | 2795/6500 [8:27:28<11:01:41, 10.72s/it]                                                         43%|     | 2795/6500 [8:27:28<11:01:41, 10.72s/it] 43%|     | 2796/6500 [8:27:38<10:58:52, 10.67s/{'loss': 0.4018, 'learning_rate': 6.091046483500723e-05, 'epoch': 0.43}
{'loss': 0.9314, 'learning_rate': 6.0886872559440845e-05, 'epoch': 0.43}
{'loss': 0.4015, 'learning_rate': 6.086327773913419e-05, 'epoch': 0.43}
{'loss': 0.4124, 'learning_rate': 6.083968037960243e-05, 'epoch': 0.43}
{'loss': 0.4057, 'learning_rate': 6.081608048636127e-05, 'epoch': 0.43}
it]                                                         43%|     | 2796/6500 [8:27:38<10:58:52, 10.67s/it] 43%|     | 2797/6500 [8:27:49<10:53:07, 10.58s/it]                                                         43%|     | 2797/6500 [8:27:49<10:53:07, 10.58s/it] 43%|     | 2798/6500 [8:27:59<10:49:09, 10.52s/it]                                                         43%|     | 2798/6500 [8:27:59<10:49:09, 10.52s/it] 43%|     | 2799/6500 [8:28:09<10:46:18, 10.48s/it]                                                         43%|     | 2799/6500 [8:28:09<10:46:18, 10.48s/it] 43%|     | 2800/6500 [8:28:20<10:44:44, 10.46s/it]                                                         43%|     | 2800/6500 [8:28:20<10:44:44, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8319783210754395, 'eval_runtime': 3.9458, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.43}
                                                         43%|     | 2800/6500 [8:28:24<10:44:44, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2800
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2800/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3742, 'learning_rate': 6.079247806492707e-05, 'epoch': 0.43}
{'loss': 0.4178, 'learning_rate': 6.076887312081673e-05, 'epoch': 0.43}
{'loss': 0.402, 'learning_rate': 6.074526565954778e-05, 'epoch': 0.43}
{'loss': 0.3721, 'learning_rate': 6.072165568663831e-05, 'epoch': 0.43}
{'loss': 0.393, 'learning_rate': 6.069804320760703e-05, 'epoch': 0.43}
 43%|     | 2801/6500 [8:28:35<12:08:43, 11.82s/it]                                                         43%|     | 2801/6500 [8:28:35<12:08:43, 11.82s/it] 43%|     | 2802/6500 [8:28:45<11:41:51, 11.39s/it]                                                         43%|     | 2802/6500 [8:28:45<11:41:51, 11.39s/it] 43%|     | 2803/6500 [8:28:56<11:23:02, 11.09s/it]                                                         43%|     | 2803/6500 [8:28:56<11:23:02, 11.09s/it] 43%|     | 2804/6500 [8:29:06<11:09:50, 10.87s/it]                                                         43%|     | 2804/6500 [8:29:06<11:09:50, 10.87s/it] 43%|     | 2805/6500 [8:29:16<11:00:24, 10.72s/it]                                                         43%|     | 2805/6500 [8:29:16<11:00:24, 10.72s/it] 43%|     | 2806/6500 [8:29:27<10:53:46, 10.62s/{'loss': 0.3883, 'learning_rate': 6.067442822797318e-05, 'epoch': 0.43}
{'loss': 0.3975, 'learning_rate': 6.065081075325663e-05, 'epoch': 0.43}
{'loss': 0.3852, 'learning_rate': 6.0627190788977825e-05, 'epoch': 0.43}
{'loss': 0.403, 'learning_rate': 6.060356834065779e-05, 'epoch': 0.43}
{'loss': 0.3779, 'learning_rate': 6.057994341381813e-05, 'epoch': 0.43}
it]                                                         43%|     | 2806/6500 [8:29:27<10:53:46, 10.62s/it] 43%|     | 2807/6500 [8:29:37<10:49:27, 10.55s/it]                                                         43%|     | 2807/6500 [8:29:37<10:49:27, 10.55s/it] 43%|     | 2808/6500 [8:29:48<10:46:03, 10.50s/it]                                                         43%|     | 2808/6500 [8:29:48<10:46:03, 10.50s/it] 43%|     | 2809/6500 [8:29:58<10:43:37, 10.46s/it]                                                         43%|     | 2809/6500 [8:29:58<10:43:37, 10.46s/it] 43%|     | 2810/6500 [8:30:08<10:41:59, 10.44s/it]                                                         43%|     | 2810/6500 [8:30:08<10:41:59, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8363208174705505, 'eval_runtime': 4.1763, 'eval_samples_per_second': 5.507, 'eval_steps_per_second': 1.437, 'epoch': 0.43}
                                                         43%|     | 2810/6500 [8:30:12<10:41:59, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2810
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2810/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4155, 'learning_rate': 6.055631601398103e-05, 'epoch': 0.43}
{'loss': 0.4147, 'learning_rate': 6.0532686146669236e-05, 'epoch': 0.43}
{'loss': 0.4031, 'learning_rate': 6.050905381740609e-05, 'epoch': 0.43}
{'loss': 0.4083, 'learning_rate': 6.0485419031715516e-05, 'epoch': 0.43}
{'loss': 0.4071, 'learning_rate': 6.0461781795122e-05, 'epoch': 0.43}
 43%|     | 2811/6500 [8:30:23<12:06:31, 11.82s/it]                                                         43%|     | 2811/6500 [8:30:23<12:06:31, 11.82s/it] 43%|     | 2812/6500 [8:30:34<11:46:00, 11.49s/it]                                                         43%|     | 2812/6500 [8:30:34<11:46:00, 11.49s/it] 43%|     | 2813/6500 [8:30:44<11:25:01, 11.15s/it]                                                         43%|     | 2813/6500 [8:30:44<11:25:01, 11.15s/it] 43%|     | 2814/6500 [8:30:55<11:10:12, 10.91s/it]                                                         43%|     | 2814/6500 [8:30:55<11:10:12, 10.91s/it] 43%|     | 2815/6500 [8:31:05<11:00:00, 10.75s/it]                                                         43%|     | 2815/6500 [8:31:05<11:00:00, 10.75s/it] 43%|     | 2816/6500 [8:31:15<10:52:59, 10.64s/{'loss': 0.3962, 'learning_rate': 6.04381421131506e-05, 'epoch': 0.43}
{'loss': 0.3979, 'learning_rate': 6.0414499991326934e-05, 'epoch': 0.43}
{'loss': 0.4088, 'learning_rate': 6.039085543517722e-05, 'epoch': 0.43}
{'loss': 0.4015, 'learning_rate': 6.036720845022823e-05, 'epoch': 0.43}
{'loss': 0.3955, 'learning_rate': 6.034355904200729e-05, 'epoch': 0.43}
it]                                                         43%|     | 2816/6500 [8:31:15<10:52:59, 10.64s/it] 43%|     | 2817/6500 [8:31:26<10:47:44, 10.55s/it]                                                         43%|     | 2817/6500 [8:31:26<10:47:44, 10.55s/it] 43%|     | 2818/6500 [8:31:36<10:44:16, 10.50s/it]                                                         43%|     | 2818/6500 [8:31:36<10:44:16, 10.50s/it] 43%|     | 2819/6500 [8:31:47<10:42:04, 10.47s/it]                                                         43%|     | 2819/6500 [8:31:47<10:42:04, 10.47s/it] 43%|     | 2820/6500 [8:31:57<10:39:57, 10.43s/it]                                                         43%|     | 2820/6500 [8:31:57<10:39:57, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8206308484077454, 'eval_runtime': 3.9434, 'eval_samples_per_second': 5.832, 'eval_steps_per_second': 1.522, 'epoch': 0.43}
                                                         43%|     | 2820/6500 [8:32:01<10:39:57, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2820/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3778, 'learning_rate': 6.0319907216042324e-05, 'epoch': 0.43}
{'loss': 0.4028, 'learning_rate': 6.029625297786179e-05, 'epoch': 0.43}
{'loss': 0.4557, 'learning_rate': 6.0272596332994725e-05, 'epoch': 0.43}
{'loss': 0.4078, 'learning_rate': 6.024893728697072e-05, 'epoch': 0.43}
{'loss': 0.387, 'learning_rate': 6.022527584531994e-05, 'epoch': 0.43}
 43%|     | 2821/6500 [8:32:12<11:59:43, 11.74s/it]                                                         43%|     | 2821/6500 [8:32:12<11:59:43, 11.74s/it] 43%|     | 2822/6500 [8:32:22<11:34:49, 11.33s/it]                                                         43%|     | 2822/6500 [8:32:22<11:34:49, 11.33s/it] 43%|     | 2823/6500 [8:32:33<11:17:09, 11.05s/it]                                                         43%|     | 2823/6500 [8:32:33<11:17:09, 11.05s/it] 43%|     | 2824/6500 [8:32:43<11:05:02, 10.85s/it]                                                         43%|     | 2824/6500 [8:32:43<11:05:02, 10.85s/it] 43%|     | 2825/6500 [8:32:53<10:56:40, 10.72s/it]                                                         43%|     | 2825/6500 [8:32:53<10:56:40, 10.72s/it] 43%|     | 2826/6500 [8:33:04<10:50:20, 10.62s/{'loss': 0.404, 'learning_rate': 6.0201612013573116e-05, 'epoch': 0.43}
{'loss': 0.9282, 'learning_rate': 6.017794579726149e-05, 'epoch': 0.43}
{'loss': 0.4158, 'learning_rate': 6.015427720191693e-05, 'epoch': 0.44}
{'loss': 0.394, 'learning_rate': 6.013060623307181e-05, 'epoch': 0.44}
{'loss': 0.3814, 'learning_rate': 6.010693289625907e-05, 'epoch': 0.44}
it]                                                         43%|     | 2826/6500 [8:33:04<10:50:20, 10.62s/it] 43%|     | 2827/6500 [8:33:14<10:45:36, 10.55s/it]                                                         43%|     | 2827/6500 [8:33:14<10:45:36, 10.55s/it] 44%|     | 2828/6500 [8:33:24<10:42:55, 10.51s/it]                                                         44%|     | 2828/6500 [8:33:24<10:42:55, 10.51s/it] 44%|     | 2829/6500 [8:33:35<10:47:40, 10.59s/it]                                                         44%|     | 2829/6500 [8:33:35<10:47:40, 10.59s/it] 44%|     | 2830/6500 [8:33:46<10:43:44, 10.52s/it]                                                         44%|     | 2830/6500 [8:33:46<10:43:44, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8299442529678345, 'eval_runtime': 4.1841, 'eval_samples_per_second': 5.497, 'eval_steps_per_second': 1.434, 'epoch': 0.44}
                                                         44%|     | 2830/6500 [8:33:50<10:43:44, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2830
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2830/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2830/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3797, 'learning_rate': 6.0083257197012224e-05, 'epoch': 0.44}
{'loss': 0.4142, 'learning_rate': 6.005957914086533e-05, 'epoch': 0.44}
{'loss': 0.379, 'learning_rate': 6.003589873335296e-05, 'epoch': 0.44}
{'loss': 0.3856, 'learning_rate': 6.001221598001028e-05, 'epoch': 0.44}
{'loss': 0.3721, 'learning_rate': 5.9988530886372985e-05, 'epoch': 0.44}
 44%|     | 2831/6500 [8:34:01<12:09:24, 11.93s/it]                                                         44%|     | 2831/6500 [8:34:01<12:09:24, 11.93s/it] 44%|     | 2832/6500 [8:34:11<11:40:48, 11.46s/it]                                                         44%|     | 2832/6500 [8:34:11<11:40:48, 11.46s/it] 44%|     | 2833/6500 [8:34:22<11:21:03, 11.14s/it]                                                         44%|     | 2833/6500 [8:34:22<11:21:03, 11.14s/it] 44%|     | 2834/6500 [8:34:32<11:07:11, 10.92s/it]                                                         44%|     | 2834/6500 [8:34:32<11:07:11, 10.92s/it] 44%|     | 2835/6500 [8:34:42<10:57:30, 10.76s/it]                                                         44%|     | 2835/6500 [8:34:42<10:57:30, 10.76s/it] 44%|     | 2836/6500 [8:34:53<10:50:24, 10.65s/{'loss': 0.3957, 'learning_rate': 5.996484345797733e-05, 'epoch': 0.44}
{'loss': 0.3843, 'learning_rate': 5.994115370036011e-05, 'epoch': 0.44}
{'loss': 0.377, 'learning_rate': 5.991746161905865e-05, 'epoch': 0.44}
{'loss': 0.4162, 'learning_rate': 5.9893767219610844e-05, 'epoch': 0.44}
{'loss': 0.3898, 'learning_rate': 5.9870070507555084e-05, 'epoch': 0.44}
it]                                                         44%|     | 2836/6500 [8:34:53<10:50:24, 10.65s/it] 44%|     | 2837/6500 [8:35:03<10:45:44, 10.58s/it]                                                         44%|     | 2837/6500 [8:35:03<10:45:44, 10.58s/it] 44%|     | 2838/6500 [8:35:14<10:42:15, 10.52s/it]                                                         44%|     | 2838/6500 [8:35:14<10:42:15, 10.52s/it] 44%|     | 2839/6500 [8:35:24<10:39:49, 10.49s/it]                                                         44%|     | 2839/6500 [8:35:24<10:39:49, 10.49s/it] 44%|     | 2840/6500 [8:35:34<10:38:19, 10.46s/it]                                                         44%|     | 2840/6500 [8:35:34<10:38:19, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8400508165359497, 'eval_runtime': 3.9447, 'eval_samples_per_second': 5.831, 'eval_steps_per_second': 1.521, 'epoch': 0.44}
                                                         44%|     | 2840/6500 [8:35:38<10:38:19, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2840/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2840/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4087, 'learning_rate': 5.984637148843037e-05, 'epoch': 0.44}
{'loss': 0.4002, 'learning_rate': 5.9822670167776183e-05, 'epoch': 0.44}
{'loss': 0.3853, 'learning_rate': 5.979896655113259e-05, 'epoch': 0.44}
{'loss': 0.3971, 'learning_rate': 5.977526064404012e-05, 'epoch': 0.44}
{'loss': 0.3949, 'learning_rate': 5.9751552452039916e-05, 'epoch': 0.44}
 44%|     | 2841/6500 [8:35:49<11:58:50, 11.79s/it]                                                         44%|     | 2841/6500 [8:35:49<11:58:50, 11.79s/it] 44%|     | 2842/6500 [8:36:00<11:33:04, 11.37s/it]                                                         44%|     | 2842/6500 [8:36:00<11:33:04, 11.37s/it] 44%|     | 2843/6500 [8:36:10<11:14:37, 11.07s/it]                                                         44%|     | 2843/6500 [8:36:10<11:14:37, 11.07s/it] 44%|     | 2844/6500 [8:36:20<11:01:50, 10.86s/it]                                                         44%|     | 2844/6500 [8:36:20<11:01:50, 10.86s/it] 44%|     | 2845/6500 [8:36:31<10:57:40, 10.80s/it]                                                         44%|     | 2845/6500 [8:36:31<10:57:40, 10.80s/it] 44%|     | 2846/6500 [8:36:41<10:49:46, 10.67s/{'loss': 0.3908, 'learning_rate': 5.9727841980673604e-05, 'epoch': 0.44}
{'loss': 0.3954, 'learning_rate': 5.970412923548339e-05, 'epoch': 0.44}
{'loss': 0.4215, 'learning_rate': 5.9680414222011974e-05, 'epoch': 0.44}
{'loss': 0.3758, 'learning_rate': 5.965669694580258e-05, 'epoch': 0.44}
{'loss': 0.3942, 'learning_rate': 5.9632977412399e-05, 'epoch': 0.44}
it]                                                         44%|     | 2846/6500 [8:36:41<10:49:46, 10.67s/it] 44%|     | 2847/6500 [8:36:52<10:44:05, 10.58s/it]                                                         44%|     | 2847/6500 [8:36:52<10:44:05, 10.58s/it] 44%|     | 2848/6500 [8:37:02<10:39:57, 10.51s/it]                                                         44%|     | 2848/6500 [8:37:02<10:39:57, 10.51s/it] 44%|     | 2849/6500 [8:37:14<11:07:06, 10.96s/it]                                                         44%|     | 2849/6500 [8:37:14<11:07:06, 10.96s/it] 44%|     | 2850/6500 [8:37:25<10:57:05, 10.80s/it]                                                         44%|     | 2850/6500 [8:37:25<10:57:05, 10.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8204439282417297, 'eval_runtime': 3.9955, 'eval_samples_per_second': 5.756, 'eval_steps_per_second': 1.502, 'epoch': 0.44}
                                                         44%|     | 2850/6500 [8:37:29<10:57:05, 10.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2850I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2850

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2850
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2850/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2850/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2850/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3726, 'learning_rate': 5.9609255627345495e-05, 'epoch': 0.44}
{'loss': 0.4205, 'learning_rate': 5.958553159618693e-05, 'epoch': 0.44}
{'loss': 0.4438, 'learning_rate': 5.956180532446863e-05, 'epoch': 0.44}
{'loss': 0.382, 'learning_rate': 5.953807681773649e-05, 'epoch': 0.44}
{'loss': 0.3981, 'learning_rate': 5.9514346081536855e-05, 'epoch': 0.44}
 44%|     | 2851/6500 [8:37:39<12:09:56, 12.00s/it]                                                         44%|     | 2851/6500 [8:37:39<12:09:56, 12.00s/it] 44%|     | 2852/6500 [8:37:50<11:39:23, 11.50s/it]                                                         44%|     | 2852/6500 [8:37:50<11:39:23, 11.50s/it] 44%|     | 2853/6500 [8:38:00<11:17:42, 11.15s/it]                                                         44%|     | 2853/6500 [8:38:00<11:17:42, 11.15s/it] 44%|     | 2854/6500 [8:38:10<11:02:50, 10.91s/it]                                                         44%|     | 2854/6500 [8:38:10<11:02:50, 10.91s/it] 44%|     | 2855/6500 [8:38:21<10:52:39, 10.74s/it]                                                         44%|     | 2855/6500 [8:38:21<10:52:39, 10.74s/it] 44%|     | 2856/6500 [8:38:31<10:45:31, 10.63s/{'loss': 0.3896, 'learning_rate': 5.949061312141668e-05, 'epoch': 0.44}
{'loss': 0.92, 'learning_rate': 5.946687794292341e-05, 'epoch': 0.44}
{'loss': 0.3982, 'learning_rate': 5.944314055160497e-05, 'epoch': 0.44}
{'loss': 0.3986, 'learning_rate': 5.941940095300984e-05, 'epoch': 0.44}
{'loss': 0.378, 'learning_rate': 5.939565915268701e-05, 'epoch': 0.44}
it]                                                         44%|     | 2856/6500 [8:38:31<10:45:31, 10.63s/it] 44%|     | 2857/6500 [8:38:42<10:40:35, 10.55s/it]                                                         44%|     | 2857/6500 [8:38:42<10:40:35, 10.55s/it] 44%|     | 2858/6500 [8:38:52<10:37:14, 10.50s/it]                                                         44%|     | 2858/6500 [8:38:52<10:37:14, 10.50s/it] 44%|     | 2859/6500 [8:39:02<10:34:38, 10.46s/it]                                                         44%|     | 2859/6500 [8:39:02<10:34:38, 10.46s/it] 44%|     | 2860/6500 [8:39:13<10:32:33, 10.43s/it]                                                         44%|     | 2860/6500 [8:39:13<10:32:33, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8289837837219238, 'eval_runtime': 3.9624, 'eval_samples_per_second': 5.805, 'eval_steps_per_second': 1.514, 'epoch': 0.44}
                                                         44%|     | 2860/6500 [8:39:17<10:32:33, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4037, 'learning_rate': 5.937191515618598e-05, 'epoch': 0.44}
{'loss': 0.3983, 'learning_rate': 5.934816896905676e-05, 'epoch': 0.44}
{'loss': 0.3695, 'learning_rate': 5.9324420596849886e-05, 'epoch': 0.44}
{'loss': 0.3829, 'learning_rate': 5.93006700451164e-05, 'epoch': 0.44}
{'loss': 0.3716, 'learning_rate': 5.927691731940783e-05, 'epoch': 0.44}
 44%|     | 2861/6500 [8:39:28<12:05:44, 11.97s/it]                                                         44%|     | 2861/6500 [8:39:28<12:05:44, 11.97s/it] 44%|     | 2862/6500 [8:39:39<11:36:31, 11.49s/it]                                                         44%|     | 2862/6500 [8:39:39<11:36:31, 11.49s/it] 44%|     | 2863/6500 [8:39:49<11:16:23, 11.16s/it]                                                         44%|     | 2863/6500 [8:39:49<11:16:23, 11.16s/it] 44%|     | 2864/6500 [8:39:59<11:02:15, 10.93s/it]                                                         44%|     | 2864/6500 [8:39:59<11:02:15, 10.93s/it] 44%|     | 2865/6500 [8:40:10<10:52:17, 10.77s/it]                                                         44%|     | 2865/6500 [8:40:10<10:52:17, 10.77s/it] 44%|     | 2866/6500 [8:40:20<10:45:24, 10.66s/{'loss': 0.3898, 'learning_rate': 5.925316242527623e-05, 'epoch': 0.44}
{'loss': 0.3813, 'learning_rate': 5.922940536827419e-05, 'epoch': 0.44}
{'loss': 0.3961, 'learning_rate': 5.920564615395475e-05, 'epoch': 0.44}
{'loss': 0.3875, 'learning_rate': 5.91818847878715e-05, 'epoch': 0.44}
{'loss': 0.3777, 'learning_rate': 5.915812127557851e-05, 'epoch': 0.44}
it]                                                         44%|     | 2866/6500 [8:40:20<10:45:24, 10.66s/it] 44%|     | 2867/6500 [8:40:31<10:40:49, 10.58s/it]                                                         44%|     | 2867/6500 [8:40:31<10:40:49, 10.58s/it] 44%|     | 2868/6500 [8:40:41<10:37:19, 10.53s/it]                                                         44%|     | 2868/6500 [8:40:41<10:37:19, 10.53s/it] 44%|     | 2869/6500 [8:40:51<10:37:06, 10.53s/it]                                                         44%|     | 2869/6500 [8:40:51<10:37:06, 10.53s/it] 44%|     | 2870/6500 [8:41:02<10:35:38, 10.51s/it]                                                         44%|     | 2870/6500 [8:41:02<10:35:38, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8421454429626465, 'eval_runtime': 4.1647, 'eval_samples_per_second': 5.523, 'eval_steps_per_second': 1.441, 'epoch': 0.44}
                                                         44%|     | 2870/6500 [8:41:06<10:35:38, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2870
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2870/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2870/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4242, 'learning_rate': 5.9134355622630356e-05, 'epoch': 0.44}
{'loss': 0.3804, 'learning_rate': 5.91105878345821e-05, 'epoch': 0.44}
{'loss': 0.4013, 'learning_rate': 5.9086817916989335e-05, 'epoch': 0.44}
{'loss': 0.4179, 'learning_rate': 5.906304587540813e-05, 'epoch': 0.44}
{'loss': 0.3753, 'learning_rate': 5.903927171539507e-05, 'epoch': 0.44}
 44%|     | 2871/6500 [8:41:17<11:59:39, 11.90s/it]                                                         44%|     | 2871/6500 [8:41:17<11:59:39, 11.90s/it] 44%|     | 2872/6500 [8:41:27<11:32:39, 11.46s/it]                                                         44%|     | 2872/6500 [8:41:27<11:32:39, 11.46s/it] 44%|     | 2873/6500 [8:41:38<11:13:34, 11.14s/it]                                                         44%|     | 2873/6500 [8:41:38<11:13:34, 11.14s/it] 44%|     | 2874/6500 [8:41:48<11:00:32, 10.93s/it]                                                         44%|     | 2874/6500 [8:41:48<11:00:32, 10.93s/it] 44%|     | 2875/6500 [8:41:59<10:50:58, 10.77s/it]                                                         44%|     | 2875/6500 [8:41:59<10:50:58, 10.77s/it] 44%|     | 2876/6500 [8:42:09<10:44:12, 10.67s/{'loss': 0.3973, 'learning_rate': 5.9015495442507194e-05, 'epoch': 0.44}
{'loss': 0.4086, 'learning_rate': 5.899171706230208e-05, 'epoch': 0.44}
{'loss': 0.3966, 'learning_rate': 5.896793658033776e-05, 'epoch': 0.44}
{'loss': 0.3809, 'learning_rate': 5.89441540021728e-05, 'epoch': 0.44}
{'loss': 0.3746, 'learning_rate': 5.892036933336622e-05, 'epoch': 0.44}
it]                                                         44%|     | 2876/6500 [8:42:09<10:44:12, 10.67s/it] 44%|     | 2877/6500 [8:42:20<10:44:19, 10.67s/it]                                                         44%|     | 2877/6500 [8:42:20<10:44:19, 10.67s/it] 44%|     | 2878/6500 [8:42:30<10:39:52, 10.60s/it]                                                         44%|     | 2878/6500 [8:42:30<10:39:52, 10.60s/it] 44%|     | 2879/6500 [8:42:41<10:36:23, 10.54s/it]                                                         44%|     | 2879/6500 [8:42:41<10:36:23, 10.54s/it] 44%|     | 2880/6500 [8:42:51<10:33:50, 10.51s/it]                                                         44%|     | 2880/6500 [8:42:51<10:33:50, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8233110308647156, 'eval_runtime': 3.9651, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.44}
                                                         44%|     | 2880/6500 [8:42:55<10:33:50, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.369, 'learning_rate': 5.889658257947755e-05, 'epoch': 0.44}
{'loss': 0.4769, 'learning_rate': 5.887279374606679e-05, 'epoch': 0.44}
{'loss': 0.3795, 'learning_rate': 5.884900283869445e-05, 'epoch': 0.44}
{'loss': 0.3802, 'learning_rate': 5.882520986292148e-05, 'epoch': 0.44}
{'loss': 0.3966, 'learning_rate': 5.8801414824309365e-05, 'epoch': 0.44}
 44%|     | 2881/6500 [8:43:06<11:53:11, 11.82s/it]                                                         44%|     | 2881/6500 [8:43:06<11:53:11, 11.82s/it] 44%|     | 2882/6500 [8:43:16<11:27:10, 11.40s/it]                                                         44%|     | 2882/6500 [8:43:16<11:27:10, 11.40s/it] 44%|     | 2883/6500 [8:43:27<11:08:58, 11.10s/it]                                                         44%|     | 2883/6500 [8:43:27<11:08:58, 11.10s/it] 44%|     | 2884/6500 [8:43:37<10:55:57, 10.88s/it]                                                         44%|     | 2884/6500 [8:43:37<10:55:57, 10.88s/it] 44%|     | 2885/6500 [8:43:48<10:46:56, 10.74s/it]                                                         44%|     | 2885/6500 [8:43:48<10:46:56, 10.74s/it] 44%|     | 2886/6500 [8:43:58<10:40:07, 10.63s/{'loss': 0.9193, 'learning_rate': 5.8777617728420075e-05, 'epoch': 0.44}
{'loss': 0.3947, 'learning_rate': 5.875381858081599e-05, 'epoch': 0.44}
{'loss': 0.3925, 'learning_rate': 5.8730017387060035e-05, 'epoch': 0.44}
{'loss': 0.3994, 'learning_rate': 5.870621415271559e-05, 'epoch': 0.44}
{'loss': 0.372, 'learning_rate': 5.868240888334653e-05, 'epoch': 0.44}
it]                                                         44%|     | 2886/6500 [8:43:58<10:40:07, 10.63s/it] 44%|     | 2887/6500 [8:44:08<10:35:54, 10.56s/it]                                                         44%|     | 2887/6500 [8:44:08<10:35:54, 10.56s/it] 44%|     | 2888/6500 [8:44:19<10:33:04, 10.52s/it]                                                         44%|     | 2888/6500 [8:44:19<10:33:04, 10.52s/it] 44%|     | 2889/6500 [8:44:29<10:31:01, 10.49s/it]                                                         44%|     | 2889/6500 [8:44:29<10:31:01, 10.49s/it] 44%|     | 2890/6500 [8:44:40<10:29:21, 10.46s/it]                                                         44%|     | 2890/6500 [8:44:40<10:29:21, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8302696347236633, 'eval_runtime': 3.959, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.44}
                                                         44%|     | 2890/6500 [8:44:44<10:29:21, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2890/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4132, 'learning_rate': 5.865860158451717e-05, 'epoch': 0.44}
{'loss': 0.3792, 'learning_rate': 5.863479226179236e-05, 'epoch': 0.44}
{'loss': 0.3741, 'learning_rate': 5.861098092073733e-05, 'epoch': 0.45}
{'loss': 0.3855, 'learning_rate': 5.8587167566917874e-05, 'epoch': 0.45}
{'loss': 0.3941, 'learning_rate': 5.856335220590022e-05, 'epoch': 0.45}
 44%|     | 2891/6500 [8:44:54<11:49:35, 11.80s/it]                                                         44%|     | 2891/6500 [8:44:54<11:49:35, 11.80s/it] 44%|     | 2892/6500 [8:45:05<11:23:51, 11.37s/it]                                                         44%|     | 2892/6500 [8:45:05<11:23:51, 11.37s/it] 45%|     | 2893/6500 [8:45:16<11:18:55, 11.29s/it]                                                         45%|     | 2893/6500 [8:45:16<11:18:55, 11.29s/it] 45%|     | 2894/6500 [8:45:26<11:02:01, 11.02s/it]                                                         45%|     | 2894/6500 [8:45:26<11:02:01, 11.02s/it] 45%|     | 2895/6500 [8:45:37<10:50:18, 10.82s/it]                                                         45%|     | 2895/6500 [8:45:37<10:50:18, 10.82s/it] 45%|     | 2896/6500 [8:45:47<10:41:57, 10.69s/{'loss': 0.3955, 'learning_rate': 5.8539534843251064e-05, 'epoch': 0.45}
{'loss': 0.3898, 'learning_rate': 5.8515715484537534e-05, 'epoch': 0.45}
{'loss': 0.4128, 'learning_rate': 5.849189413532731e-05, 'epoch': 0.45}
{'loss': 0.3775, 'learning_rate': 5.846807080118845e-05, 'epoch': 0.45}
{'loss': 0.4083, 'learning_rate': 5.844424548768952e-05, 'epoch': 0.45}
it]                                                         45%|     | 2896/6500 [8:45:47<10:41:57, 10.69s/it] 45%|     | 2897/6500 [8:45:57<10:36:09, 10.59s/it]                                                         45%|     | 2897/6500 [8:45:57<10:36:09, 10.59s/it] 45%|     | 2898/6500 [8:46:08<10:32:35, 10.54s/it]                                                         45%|     | 2898/6500 [8:46:08<10:32:35, 10.54s/it] 45%|     | 2899/6500 [8:46:18<10:30:01, 10.50s/it]                                                         45%|     | 2899/6500 [8:46:18<10:30:01, 10.50s/it] 45%|     | 2900/6500 [8:46:29<10:28:14, 10.47s/it]                                                         45%|     | 2900/6500 [8:46:29<10:28:14, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8381133675575256, 'eval_runtime': 3.9728, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.45}
                                                         45%|     | 2900/6500 [8:46:33<10:28:14, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2900I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2900

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2900
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2900/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2900/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4044, 'learning_rate': 5.842041820039956e-05, 'epoch': 0.45}
{'loss': 0.3835, 'learning_rate': 5.8396588944888044e-05, 'epoch': 0.45}
{'loss': 0.4005, 'learning_rate': 5.8372757726724914e-05, 'epoch': 0.45}
{'loss': 0.4045, 'learning_rate': 5.8348924551480565e-05, 'epoch': 0.45}
{'loss': 0.394, 'learning_rate': 5.8325089424725865e-05, 'epoch': 0.45}
 45%|     | 2901/6500 [8:46:44<11:46:40, 11.78s/it]                                                         45%|     | 2901/6500 [8:46:44<11:46:40, 11.78s/it] 45%|     | 2902/6500 [8:46:54<11:21:22, 11.36s/it]                                                         45%|     | 2902/6500 [8:46:54<11:21:22, 11.36s/it] 45%|     | 2903/6500 [8:47:04<11:03:53, 11.07s/it]                                                         45%|     | 2903/6500 [8:47:04<11:03:53, 11.07s/it] 45%|     | 2904/6500 [8:47:15<10:51:17, 10.87s/it]                                                         45%|     | 2904/6500 [8:47:15<10:51:17, 10.87s/it] 45%|     | 2905/6500 [8:47:25<10:42:37, 10.73s/it]                                                         45%|     | 2905/6500 [8:47:25<10:42:37, 10.73s/it] 45%|     | 2906/6500 [8:47:35<10:36:38, 10.63s/{'loss': 0.378, 'learning_rate': 5.830125235203213e-05, 'epoch': 0.45}
{'loss': 0.4193, 'learning_rate': 5.8277413338971135e-05, 'epoch': 0.45}
{'loss': 0.3859, 'learning_rate': 5.825357239111511e-05, 'epoch': 0.45}
{'loss': 0.3821, 'learning_rate': 5.8229729514036705e-05, 'epoch': 0.45}
{'loss': 0.3719, 'learning_rate': 5.820588471330906e-05, 'epoch': 0.45}
it]                                                         45%|     | 2906/6500 [8:47:35<10:36:38, 10.63s/it] 45%|     | 2907/6500 [8:47:46<10:32:17, 10.56s/it]                                                         45%|     | 2907/6500 [8:47:46<10:32:17, 10.56s/it] 45%|     | 2908/6500 [8:47:56<10:29:03, 10.51s/it]                                                         45%|     | 2908/6500 [8:47:56<10:29:03, 10.51s/it] 45%|     | 2909/6500 [8:48:07<10:30:08, 10.53s/it]                                                         45%|     | 2909/6500 [8:48:07<10:30:08, 10.53s/it] 45%|     | 2910/6500 [8:48:17<10:30:08, 10.53s/it]                                                         45%|     | 2910/6500 [8:48:17<10:30:08, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8235613703727722, 'eval_runtime': 3.9487, 'eval_samples_per_second': 5.825, 'eval_steps_per_second': 1.52, 'epoch': 0.45}
                                                         45%|     | 2910/6500 [8:48:21<10:30:08, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2910I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2910
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4052, 'learning_rate': 5.818203799450577e-05, 'epoch': 0.45}
{'loss': 0.4573, 'learning_rate': 5.8158189363200834e-05, 'epoch': 0.45}
{'loss': 0.373, 'learning_rate': 5.813433882496875e-05, 'epoch': 0.45}
{'loss': 0.3846, 'learning_rate': 5.811048638538441e-05, 'epoch': 0.45}
{'loss': 0.3949, 'learning_rate': 5.808663205002319e-05, 'epoch': 0.45}
 45%|     | 2911/6500 [8:48:32<11:47:17, 11.82s/it]                                                         45%|     | 2911/6500 [8:48:32<11:47:17, 11.82s/it] 45%|     | 2912/6500 [8:48:43<11:21:33, 11.40s/it]                                                         45%|     | 2912/6500 [8:48:43<11:21:33, 11.40s/it] 45%|     | 2913/6500 [8:48:53<11:03:31, 11.10s/it]                                                         45%|     | 2913/6500 [8:48:53<11:03:31, 11.10s/it] 45%|     | 2914/6500 [8:49:03<10:50:53, 10.89s/it]                                                         45%|     | 2914/6500 [8:49:03<10:50:53, 10.89s/it] 45%|     | 2915/6500 [8:49:14<10:41:50, 10.74s/it]                                                         45%|     | 2915/6500 [8:49:14<10:41:50, 10.74s/it] 45%|     | 2916/6500 [8:49:24<10:35:21, 10.64s/{'loss': 0.9127, 'learning_rate': 5.80627758244609e-05, 'epoch': 0.45}
{'loss': 0.4111, 'learning_rate': 5.803891771427379e-05, 'epoch': 0.45}
{'loss': 0.3813, 'learning_rate': 5.8015057725038534e-05, 'epoch': 0.45}
{'loss': 0.3693, 'learning_rate': 5.799119586233228e-05, 'epoch': 0.45}
{'loss': 0.3723, 'learning_rate': 5.796733213173257e-05, 'epoch': 0.45}
it]                                                         45%|     | 2916/6500 [8:49:24<10:35:21, 10.64s/it] 45%|     | 2917/6500 [8:49:35<10:35:16, 10.64s/it]                                                         45%|     | 2917/6500 [8:49:35<10:35:16, 10.64s/it] 45%|     | 2918/6500 [8:49:45<10:31:09, 10.57s/it]                                                         45%|     | 2918/6500 [8:49:45<10:31:09, 10.57s/it] 45%|     | 2919/6500 [8:49:56<10:29:12, 10.54s/it]                                                         45%|     | 2919/6500 [8:49:56<10:29:12, 10.54s/it] 45%|     | 2920/6500 [8:50:06<10:26:47, 10.50s/it]                                                         45%|     | 2920/6500 [8:50:06<10:26:47, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8305054306983948, 'eval_runtime': 3.9561, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.45}
                                                         45%|     | 2920/6500 [8:50:10<10:26:47, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2920I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2920
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4127, 'learning_rate': 5.7943466538817416e-05, 'epoch': 0.45}
{'loss': 0.3736, 'learning_rate': 5.791959908916526e-05, 'epoch': 0.45}
{'loss': 0.3797, 'learning_rate': 5.789572978835496e-05, 'epoch': 0.45}
{'loss': 0.3664, 'learning_rate': 5.787185864196584e-05, 'epoch': 0.45}
{'loss': 0.3852, 'learning_rate': 5.784798565557762e-05, 'epoch': 0.45}
 45%|     | 2921/6500 [8:50:21<11:45:00, 11.82s/it]                                                         45%|     | 2921/6500 [8:50:21<11:45:00, 11.82s/it] 45%|     | 2922/6500 [8:50:31<11:19:35, 11.40s/it]                                                         45%|     | 2922/6500 [8:50:31<11:19:35, 11.40s/it] 45%|     | 2923/6500 [8:50:42<11:01:41, 11.10s/it]                                                         45%|     | 2923/6500 [8:50:42<11:01:41, 11.10s/it] 45%|     | 2924/6500 [8:50:52<10:49:20, 10.90s/it]                                                         45%|     | 2924/6500 [8:50:52<10:49:20, 10.90s/it] 45%|     | 2925/6500 [8:51:03<10:46:08, 10.84s/it]                                                         45%|     | 2925/6500 [8:51:03<10:46:08, 10.84s/it] 45%|     | 2926/6500 [8:51:13<10:38:08, 10.71s/{'loss': 0.3804, 'learning_rate': 5.782411083477046e-05, 'epoch': 0.45}
{'loss': 0.3801, 'learning_rate': 5.780023418512497e-05, 'epoch': 0.45}
{'loss': 0.4091, 'learning_rate': 5.7776355712222166e-05, 'epoch': 0.45}
{'loss': 0.3901, 'learning_rate': 5.775247542164349e-05, 'epoch': 0.45}
{'loss': 0.4197, 'learning_rate': 5.7728593318970825e-05, 'epoch': 0.45}
it]                                                         45%|     | 2926/6500 [8:51:13<10:38:08, 10.71s/it] 45%|     | 2927/6500 [8:51:24<10:32:24, 10.62s/it]                                                         45%|     | 2927/6500 [8:51:24<10:32:24, 10.62s/it] 45%|     | 2928/6500 [8:51:34<10:28:27, 10.56s/it]                                                         45%|     | 2928/6500 [8:51:34<10:28:27, 10.56s/it] 45%|     | 2929/6500 [8:51:45<10:25:49, 10.52s/it]                                                         45%|     | 2929/6500 [8:51:45<10:25:49, 10.52s/it] 45%|     | 2930/6500 [8:51:55<10:23:57, 10.49s/it]                                                         45%|     | 2930/6500 [8:51:55<10:23:57, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8400298357009888, 'eval_runtime': 4.328, 'eval_samples_per_second': 5.314, 'eval_steps_per_second': 1.386, 'epoch': 0.45}
                                                         45%|     | 2930/6500 [8:51:59<10:23:57, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.41, 'learning_rate': 5.7704709409786464e-05, 'epoch': 0.45}
{'loss': 0.3966, 'learning_rate': 5.768082369967312e-05, 'epoch': 0.45}
{'loss': 0.4023, 'learning_rate': 5.765693619421394e-05, 'epoch': 0.45}
{'loss': 0.3913, 'learning_rate': 5.763304689899249e-05, 'epoch': 0.45}
{'loss': 0.3897, 'learning_rate': 5.760915581959272e-05, 'epoch': 0.45}
 45%|     | 2931/6500 [8:52:10<11:48:08, 11.90s/it]                                                         45%|     | 2931/6500 [8:52:10<11:48:08, 11.90s/it] 45%|     | 2932/6500 [8:52:21<11:21:06, 11.45s/it]                                                         45%|     | 2932/6500 [8:52:21<11:21:06, 11.45s/it] 45%|     | 2933/6500 [8:52:31<11:02:09, 11.14s/it]                                                         45%|     | 2933/6500 [8:52:31<11:02:09, 11.14s/it] 45%|     | 2934/6500 [8:52:41<10:48:47, 10.92s/it]                                                         45%|     | 2934/6500 [8:52:41<10:48:47, 10.92s/it] 45%|     | 2935/6500 [8:52:52<10:38:50, 10.75s/it]                                                         45%|     | 2935/6500 [8:52:52<10:38:50, 10.75s/it] 45%|     | 2936/6500 [8:53:02<10:32:14, 10.64s/{'loss': 0.3964, 'learning_rate': 5.7585262961599054e-05, 'epoch': 0.45}
{'loss': 0.4125, 'learning_rate': 5.7561368330596275e-05, 'epoch': 0.45}
{'loss': 0.3707, 'learning_rate': 5.753747193216963e-05, 'epoch': 0.45}
{'loss': 0.3958, 'learning_rate': 5.751357377190475e-05, 'epoch': 0.45}
{'loss': 0.3825, 'learning_rate': 5.748967385538769e-05, 'epoch': 0.45}
it]                                                         45%|     | 2936/6500 [8:53:02<10:32:14, 10.64s/it] 45%|     | 2937/6500 [8:53:13<10:27:29, 10.57s/it]                                                         45%|     | 2937/6500 [8:53:13<10:27:29, 10.57s/it] 45%|     | 2938/6500 [8:53:23<10:24:13, 10.51s/it]                                                         45%|     | 2938/6500 [8:53:23<10:24:13, 10.51s/it] 45%|     | 2939/6500 [8:53:33<10:21:57, 10.48s/it]                                                         45%|     | 2939/6500 [8:53:33<10:21:57, 10.48s/it] 45%|     | 2940/6500 [8:53:44<10:20:24, 10.46s/it]                                                         45%|     | 2940/6500 [8:53:44<10:20:24, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8215518593788147, 'eval_runtime': 3.9805, 'eval_samples_per_second': 5.778, 'eval_steps_per_second': 1.507, 'epoch': 0.45}
                                                         45%|     | 2940/6500 [8:53:48<10:20:24, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4204, 'learning_rate': 5.7465772188204905e-05, 'epoch': 0.45}
{'loss': 0.4321, 'learning_rate': 5.744186877594325e-05, 'epoch': 0.45}
{'loss': 0.374, 'learning_rate': 5.741796362419003e-05, 'epoch': 0.45}
{'loss': 0.3942, 'learning_rate': 5.73940567385329e-05, 'epoch': 0.45}
{'loss': 0.4074, 'learning_rate': 5.737014812455999e-05, 'epoch': 0.45}
 45%|     | 2941/6500 [8:53:59<11:39:28, 11.79s/it]                                                         45%|     | 2941/6500 [8:53:59<11:39:28, 11.79s/it] 45%|     | 2942/6500 [8:54:09<11:18:49, 11.45s/it]                                                         45%|     | 2942/6500 [8:54:09<11:18:49, 11.45s/it] 45%|     | 2943/6500 [8:54:20<11:00:16, 11.14s/it]                                                         45%|     | 2943/6500 [8:54:20<11:00:16, 11.14s/it] 45%|     | 2944/6500 [8:54:30<10:47:04, 10.92s/it]                                                         45%|     | 2944/6500 [8:54:30<10:47:04, 10.92s/it] 45%|     | 2945/6500 [8:54:41<10:38:30, 10.78s/it]                                                         45%|     | 2945/6500 [8:54:41<10:38:30, 10.78s/it] 45%|     | 2946/6500 [8:54:51<10:35:17, 10.73s/{'loss': 0.8849, 'learning_rate': 5.7346237787859745e-05, 'epoch': 0.45}
{'loss': 0.4023, 'learning_rate': 5.7322325734021086e-05, 'epoch': 0.45}
{'loss': 0.3959, 'learning_rate': 5.7298411968633306e-05, 'epoch': 0.45}
{'loss': 0.3638, 'learning_rate': 5.72744964972861e-05, 'epoch': 0.45}
{'loss': 0.4029, 'learning_rate': 5.7250579325569574e-05, 'epoch': 0.45}
it]                                                         45%|     | 2946/6500 [8:54:51<10:35:17, 10.73s/it] 45%|     | 2947/6500 [8:55:02<10:29:30, 10.63s/it]                                                         45%|     | 2947/6500 [8:55:02<10:29:30, 10.63s/it] 45%|     | 2948/6500 [8:55:12<10:24:42, 10.55s/it]                                                         45%|     | 2948/6500 [8:55:12<10:24:42, 10.55s/it] 45%|     | 2949/6500 [8:55:22<10:21:26, 10.50s/it]                                                         45%|     | 2949/6500 [8:55:22<10:21:26, 10.50s/it] 45%|     | 2950/6500 [8:55:33<10:18:57, 10.46s/it]                                                         45%|     | 2950/6500 [8:55:33<10:18:57, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8319150805473328, 'eval_runtime': 4.1624, 'eval_samples_per_second': 5.526, 'eval_steps_per_second': 1.441, 'epoch': 0.45}
                                                         45%|     | 2950/6500 [8:55:37<10:18:57, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2950/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3946, 'learning_rate': 5.722666045907422e-05, 'epoch': 0.45}
{'loss': 0.3564, 'learning_rate': 5.720273990339092e-05, 'epoch': 0.45}
{'loss': 0.3835, 'learning_rate': 5.717881766411095e-05, 'epoch': 0.45}
{'loss': 0.3745, 'learning_rate': 5.7154893746826014e-05, 'epoch': 0.45}
{'loss': 0.3827, 'learning_rate': 5.7130968157128154e-05, 'epoch': 0.45}
 45%|     | 2951/6500 [8:55:48<11:40:59, 11.85s/it]                                                         45%|     | 2951/6500 [8:55:48<11:40:59, 11.85s/it] 45%|     | 2952/6500 [8:55:58<11:14:15, 11.40s/it]                                                         45%|     | 2952/6500 [8:55:58<11:14:15, 11.40s/it] 45%|     | 2953/6500 [8:56:09<10:55:22, 11.09s/it]                                                         45%|     | 2953/6500 [8:56:09<10:55:22, 11.09s/it] 45%|     | 2954/6500 [8:56:19<10:41:56, 10.86s/it]                                                         45%|     | 2954/6500 [8:56:19<10:41:56, 10.86s/it] 45%|     | 2955/6500 [8:56:29<10:32:42, 10.71s/it]                                                         45%|     | 2955/6500 [8:56:29<10:32:42, 10.71s/it] 45%|     | 2956/6500 [8:56:40<10:25:42, 10.59s/{'loss': 0.3806, 'learning_rate': 5.710704090060985e-05, 'epoch': 0.45}
{'loss': 0.3878, 'learning_rate': 5.7083111982863956e-05, 'epoch': 0.45}
{'loss': 0.3697, 'learning_rate': 5.7059181409483684e-05, 'epoch': 0.46}
{'loss': 0.3903, 'learning_rate': 5.703524918606269e-05, 'epoch': 0.46}
{'loss': 0.4079, 'learning_rate': 5.701131531819497e-05, 'epoch': 0.46}
it]                                                         45%|     | 2956/6500 [8:56:40<10:25:42, 10.59s/it] 45%|     | 2957/6500 [8:56:50<10:21:25, 10.52s/it]                                                         45%|     | 2957/6500 [8:56:50<10:21:25, 10.52s/it] 46%|     | 2958/6500 [8:57:01<10:25:17, 10.59s/it]                                                         46%|     | 2958/6500 [8:57:01<10:25:17, 10.59s/it] 46%|     | 2959/6500 [8:57:11<10:20:42, 10.52s/it]                                                         46%|     | 2959/6500 [8:57:11<10:20:42, 10.52s/it] 46%|     | 2960/6500 [8:57:21<10:17:35, 10.47s/it]                                                         46%|     | 2960/6500 [8:57:21<10:17:35, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8459052443504333, 'eval_runtime': 4.8725, 'eval_samples_per_second': 4.72, 'eval_steps_per_second': 1.231, 'epoch': 0.46}
                                                         46%|     | 2960/6500 [8:57:26<10:17:35, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2960/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2960/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3797, 'learning_rate': 5.698737981147493e-05, 'epoch': 0.46}
{'loss': 0.3995, 'learning_rate': 5.696344267149735e-05, 'epoch': 0.46}
{'loss': 0.3873, 'learning_rate': 5.693950390385736e-05, 'epoch': 0.46}
{'loss': 0.3766, 'learning_rate': 5.691556351415054e-05, 'epoch': 0.46}
{'loss': 0.3886, 'learning_rate': 5.6891621507972794e-05, 'epoch': 0.46}
 46%|     | 2961/6500 [8:57:37<11:51:51, 12.07s/it]                                                         46%|     | 2961/6500 [8:57:37<11:51:51, 12.07s/it] 46%|     | 2962/6500 [8:57:48<11:21:15, 11.55s/it]                                                         46%|     | 2962/6500 [8:57:48<11:21:15, 11.55s/it] 46%|     | 2963/6500 [8:57:58<10:59:47, 11.19s/it]                                                         46%|     | 2963/6500 [8:57:58<10:59:47, 11.19s/it] 46%|     | 2964/6500 [8:58:08<10:44:37, 10.94s/it]                                                         46%|     | 2964/6500 [8:58:08<10:44:37, 10.94s/it] 46%|     | 2965/6500 [8:58:19<10:33:47, 10.76s/it]                                                         46%|     | 2965/6500 [8:58:19<10:33:47, 10.76s/it] 46%|     | 2966/6500 [8:58:29<10:26:30, 10.64s/{'loss': 0.4017, 'learning_rate': 5.686767789092041e-05, 'epoch': 0.46}
{'loss': 0.3944, 'learning_rate': 5.684373266859009e-05, 'epoch': 0.46}
{'loss': 0.385, 'learning_rate': 5.681978584657886e-05, 'epoch': 0.46}
{'loss': 0.375, 'learning_rate': 5.679583743048416e-05, 'epoch': 0.46}
{'loss': 0.3864, 'learning_rate': 5.677188742590378e-05, 'epoch': 0.46}
it]                                                         46%|     | 2966/6500 [8:58:29<10:26:30, 10.64s/it] 46%|     | 2967/6500 [8:58:39<10:21:11, 10.55s/it]                                                         46%|     | 2967/6500 [8:58:39<10:21:11, 10.55s/it] 46%|     | 2968/6500 [8:58:50<10:17:47, 10.49s/it]                                                         46%|     | 2968/6500 [8:58:50<10:17:47, 10.49s/it] 46%|     | 2969/6500 [8:59:00<10:15:10, 10.45s/it]                                                         46%|     | 2969/6500 [8:59:00<10:15:10, 10.45s/it] 46%|     | 2970/6500 [8:59:10<10:14:51, 10.45s/it]                                                         46%|     | 2970/6500 [8:59:10<10:14:51, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8244302272796631, 'eval_runtime': 4.1022, 'eval_samples_per_second': 5.607, 'eval_steps_per_second': 1.463, 'epoch': 0.46}
                                                         46%|     | 2970/6500 [8:59:15<10:14:51, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2970/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4534, 'learning_rate': 5.674793583843588e-05, 'epoch': 0.46}
{'loss': 0.3793, 'learning_rate': 5.672398267367902e-05, 'epoch': 0.46}
{'loss': 0.3567, 'learning_rate': 5.670002793723209e-05, 'epoch': 0.46}
{'loss': 0.3976, 'learning_rate': 5.667607163469436e-05, 'epoch': 0.46}
{'loss': 0.9089, 'learning_rate': 5.665211377166548e-05, 'epoch': 0.46}
 46%|     | 2971/6500 [8:59:25<11:34:16, 11.80s/it]                                                         46%|     | 2971/6500 [8:59:25<11:34:16, 11.80s/it] 46%|     | 2972/6500 [8:59:36<11:08:14, 11.36s/it]                                                         46%|     | 2972/6500 [8:59:36<11:08:14, 11.36s/it] 46%|     | 2973/6500 [8:59:46<10:49:58, 11.06s/it]                                                         46%|     | 2973/6500 [8:59:46<10:49:58, 11.06s/it] 46%|     | 2974/6500 [8:59:57<10:41:08, 10.91s/it]                                                         46%|     | 2974/6500 [8:59:57<10:41:08, 10.91s/it] 46%|     | 2975/6500 [9:00:07<10:30:44, 10.74s/it]                                                         46%|     | 2975/6500 [9:00:07<10:30:44, 10.74s/it] 46%|     | 2976/6500 [9:00:17<10:23:46, 10.62s/{'loss': 0.4013, 'learning_rate': 5.662815435374544e-05, 'epoch': 0.46}
{'loss': 0.3944, 'learning_rate': 5.660419338653463e-05, 'epoch': 0.46}
{'loss': 0.3992, 'learning_rate': 5.658023087563379e-05, 'epoch': 0.46}
{'loss': 0.3583, 'learning_rate': 5.655626682664397e-05, 'epoch': 0.46}
{'loss': 0.41, 'learning_rate': 5.653230124516663e-05, 'epoch': 0.46}
it]                                                         46%|     | 2976/6500 [9:00:17<10:23:46, 10.62s/it] 46%|     | 2977/6500 [9:00:28<10:19:05, 10.54s/it]                                                         46%|     | 2977/6500 [9:00:28<10:19:05, 10.54s/it] 46%|     | 2978/6500 [9:00:38<10:15:22, 10.48s/it]                                                         46%|     | 2978/6500 [9:00:38<10:15:22, 10.48s/it] 46%|     | 2979/6500 [9:00:48<10:12:41, 10.44s/it]                                                         46%|     | 2979/6500 [9:00:48<10:12:41, 10.44s/it] 46%|     | 2980/6500 [9:00:59<10:10:49, 10.41s/it]                                                         46%|     | 2980/6500 [9:00:59<10:10:49, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8290287256240845, 'eval_runtime': 3.9816, 'eval_samples_per_second': 5.777, 'eval_steps_per_second': 1.507, 'epoch': 0.46}
                                                         46%|     | 2980/6500 [9:01:03<10:10:49, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2980/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3825, 'learning_rate': 5.650833413680361e-05, 'epoch': 0.46}
{'loss': 0.3702, 'learning_rate': 5.648436550715704e-05, 'epoch': 0.46}
{'loss': 0.3824, 'learning_rate': 5.646039536182949e-05, 'epoch': 0.46}
{'loss': 0.3732, 'learning_rate': 5.643642370642378e-05, 'epoch': 0.46}
{'loss': 0.3857, 'learning_rate': 5.641245054654316e-05, 'epoch': 0.46}
 46%|     | 2981/6500 [9:01:14<11:28:08, 11.73s/it]                                                         46%|     | 2981/6500 [9:01:14<11:28:08, 11.73s/it] 46%|     | 2982/6500 [9:01:24<11:04:07, 11.33s/it]                                                         46%|     | 2982/6500 [9:01:24<11:04:07, 11.33s/it] 46%|     | 2983/6500 [9:01:34<10:47:10, 11.04s/it]                                                         46%|     | 2983/6500 [9:01:34<10:47:10, 11.04s/it] 46%|     | 2984/6500 [9:01:45<10:35:01, 10.84s/it]                                                         46%|     | 2984/6500 [9:01:45<10:35:01, 10.84s/it] 46%|     | 2985/6500 [9:01:55<10:27:09, 10.71s/it]                                                         46%|     | 2985/6500 [9:01:55<10:27:09, 10.71s/it] 46%|     | 2986/6500 [9:02:05<10:21:00, 10.60s/{'loss': 0.3736, 'learning_rate': 5.638847588779121e-05, 'epoch': 0.46}
{'loss': 0.4026, 'learning_rate': 5.636449973577188e-05, 'epoch': 0.46}
{'loss': 0.3673, 'learning_rate': 5.6340522096089424e-05, 'epoch': 0.46}
{'loss': 0.4123, 'learning_rate': 5.631654297434849e-05, 'epoch': 0.46}
{'loss': 0.3985, 'learning_rate': 5.6292562376154037e-05, 'epoch': 0.46}
it]                                                         46%|     | 2986/6500 [9:02:05<10:21:00, 10.60s/it] 46%|     | 2987/6500 [9:02:16<10:16:32, 10.53s/it]                                                         46%|     | 2987/6500 [9:02:16<10:16:32, 10.53s/it] 46%|     | 2988/6500 [9:02:26<10:13:39, 10.48s/it]                                                         46%|     | 2988/6500 [9:02:26<10:13:39, 10.48s/it] 46%|     | 2989/6500 [9:02:37<10:11:34, 10.45s/it]                                                         46%|     | 2989/6500 [9:02:37<10:11:34, 10.45s/it] 46%|     | 2990/6500 [9:02:47<10:19:03, 10.58s/it]                                                         46%|     | 2990/6500 [9:02:47<10:19:03, 10.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8476159572601318, 'eval_runtime': 4.3469, 'eval_samples_per_second': 5.291, 'eval_steps_per_second': 1.38, 'epoch': 0.46}
                                                         46%|     | 2990/6500 [9:02:52<10:19:03, 10.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-2990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-2990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3777, 'learning_rate': 5.6268580307111376e-05, 'epoch': 0.46}
{'loss': 0.3976, 'learning_rate': 5.624459677282619e-05, 'epoch': 0.46}
{'loss': 0.3912, 'learning_rate': 5.622061177890447e-05, 'epoch': 0.46}
{'loss': 0.3882, 'learning_rate': 5.619662533095257e-05, 'epoch': 0.46}
{'loss': 0.3862, 'learning_rate': 5.617263743457719e-05, 'epoch': 0.46}
 46%|     | 2991/6500 [9:03:03<11:42:18, 12.01s/it]                                                         46%|     | 2991/6500 [9:03:03<11:42:18, 12.01s/it] 46%|     | 2992/6500 [9:03:13<11:13:40, 11.52s/it]                                                         46%|     | 2992/6500 [9:03:13<11:13:40, 11.52s/it] 46%|     | 2993/6500 [9:03:24<10:53:34, 11.18s/it]                                                         46%|     | 2993/6500 [9:03:24<10:53:34, 11.18s/it] 46%|     | 2994/6500 [9:03:34<10:39:28, 10.94s/it]                                                         46%|     | 2994/6500 [9:03:34<10:39:28, 10.94s/it] 46%|     | 2995/6500 [9:03:44<10:29:25, 10.77s/it]                                                         46%|     | 2995/6500 [9:03:44<10:29:25, 10.77s/it] 46%|     | 2996/6500 [9:03:55<10:22:58, 10.67s/{'loss': 0.4026, 'learning_rate': 5.6148648095385327e-05, 'epoch': 0.46}
{'loss': 0.3742, 'learning_rate': 5.612465731898435e-05, 'epoch': 0.46}
{'loss': 0.3847, 'learning_rate': 5.610066511098198e-05, 'epoch': 0.46}
{'loss': 0.3607, 'learning_rate': 5.607667147698622e-05, 'epoch': 0.46}
{'loss': 0.4114, 'learning_rate': 5.6052676422605467e-05, 'epoch': 0.46}
it]                                                         46%|     | 2996/6500 [9:03:55<10:22:58, 10.67s/it] 46%|     | 2997/6500 [9:04:05<10:17:39, 10.58s/it]                                                         46%|     | 2997/6500 [9:04:05<10:17:39, 10.58s/it] 46%|     | 2998/6500 [9:04:15<10:14:03, 10.52s/it]                                                         46%|     | 2998/6500 [9:04:15<10:14:03, 10.52s/it] 46%|     | 2999/6500 [9:04:26<10:11:40, 10.48s/it]                                                         46%|     | 2999/6500 [9:04:26<10:11:40, 10.48s/it] 46%|     | 3000/6500 [9:04:36<10:09:56, 10.46s/it]                                                         46%|     | 3000/6500 [9:04:36<10:09:56, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8255061507225037, 'eval_runtime': 3.9634, 'eval_samples_per_second': 5.803, 'eval_steps_per_second': 1.514, 'epoch': 0.46}
                                                         46%|     | 3000/6500 [9:04:40<10:09:56, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3000/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4361, 'learning_rate': 5.6028679953448384e-05, 'epoch': 0.46}
{'loss': 0.3951, 'learning_rate': 5.6004682075124016e-05, 'epoch': 0.46}
{'loss': 0.3933, 'learning_rate': 5.598068279324172e-05, 'epoch': 0.46}
{'loss': 0.3778, 'learning_rate': 5.595668211341118e-05, 'epoch': 0.46}
{'loss': 0.92, 'learning_rate': 5.593268004124243e-05, 'epoch': 0.46}
 46%|     | 3001/6500 [9:04:51<11:27:29, 11.79s/it]                                                         46%|     | 3001/6500 [9:04:51<11:27:29, 11.79s/it] 46%|     | 3002/6500 [9:05:02<11:02:34, 11.37s/it]                                                         46%|     | 3002/6500 [9:05:02<11:02:34, 11.37s/it] 46%|     | 3003/6500 [9:05:12<10:45:06, 11.07s/it]                                                         46%|     | 3003/6500 [9:05:12<10:45:06, 11.07s/it] 46%|     | 3004/6500 [9:05:22<10:32:53, 10.86s/it]                                                         46%|     | 3004/6500 [9:05:22<10:32:53, 10.86s/it] 46%|     | 3005/6500 [9:05:33<10:24:15, 10.72s/it]                                                         46%|     | 3005/6500 [9:05:33<10:24:15, 10.72s/it] 46%|     | 3006/6500 [9:05:43<10:25:04, 10.73s/{'loss': 0.3968, 'learning_rate': 5.5908676582345786e-05, 'epoch': 0.46}
{'loss': 0.3938, 'learning_rate': 5.588467174233192e-05, 'epoch': 0.46}
{'loss': 0.3655, 'learning_rate': 5.586066552681179e-05, 'epoch': 0.46}
{'loss': 0.3708, 'learning_rate': 5.583665794139675e-05, 'epoch': 0.46}
{'loss': 0.4012, 'learning_rate': 5.5812648991698415e-05, 'epoch': 0.46}
it]                                                         46%|     | 3006/6500 [9:05:43<10:25:04, 10.73s/it] 46%|     | 3007/6500 [9:05:54<10:18:37, 10.63s/it]                                                         46%|     | 3007/6500 [9:05:54<10:18:37, 10.63s/it] 46%|     | 3008/6500 [9:06:04<10:14:08, 10.55s/it]                                                         46%|     | 3008/6500 [9:06:04<10:14:08, 10.55s/it] 46%|     | 3009/6500 [9:06:15<10:10:56, 10.50s/it]                                                         46%|     | 3009/6500 [9:06:15<10:10:56, 10.50s/it] 46%|     | 3010/6500 [9:06:25<10:08:54, 10.47s/it]                                                         46%|     | 3010/6500 [9:06:25<10:08:54, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8310610055923462, 'eval_runtime': 3.965, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.46}
                                                         46%|     | 3010/6500 [9:06:29<10:08:54, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3010
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3646, 'learning_rate': 5.57886386833287e-05, 'epoch': 0.46}
{'loss': 0.3798, 'learning_rate': 5.576462702189989e-05, 'epoch': 0.46}
{'loss': 0.368, 'learning_rate': 5.574061401302456e-05, 'epoch': 0.46}
{'loss': 0.3787, 'learning_rate': 5.571659966231562e-05, 'epoch': 0.46}
{'loss': 0.3844, 'learning_rate': 5.569258397538626e-05, 'epoch': 0.46}
 46%|     | 3011/6500 [9:06:40<11:24:44, 11.78s/it]                                                         46%|     | 3011/6500 [9:06:40<11:24:44, 11.78s/it] 46%|     | 3012/6500 [9:06:50<11:00:10, 11.36s/it]                                                         46%|     | 3012/6500 [9:06:50<11:00:10, 11.36s/it] 46%|     | 3013/6500 [9:07:01<10:42:51, 11.06s/it]                                                         46%|     | 3013/6500 [9:07:01<10:42:51, 11.06s/it] 46%|     | 3014/6500 [9:07:11<10:30:50, 10.86s/it]                                                         46%|     | 3014/6500 [9:07:11<10:30:50, 10.86s/it] 46%|     | 3015/6500 [9:07:21<10:22:06, 10.71s/it]                                                         46%|     | 3015/6500 [9:07:21<10:22:06, 10.71s/it] 46%|     | 3016/6500 [9:07:32<10:16:21, 10.61s/{'loss': 0.3648, 'learning_rate': 5.566856695785001e-05, 'epoch': 0.46}
{'loss': 0.3929, 'learning_rate': 5.564454861532069e-05, 'epoch': 0.46}
{'loss': 0.3746, 'learning_rate': 5.5620528953412456e-05, 'epoch': 0.46}
{'loss': 0.4119, 'learning_rate': 5.5596507977739755e-05, 'epoch': 0.46}
{'loss': 0.3734, 'learning_rate': 5.5572485693917345e-05, 'epoch': 0.46}
it]                                                         46%|     | 3016/6500 [9:07:32<10:16:21, 10.61s/it] 46%|     | 3017/6500 [9:07:42<10:11:47, 10.54s/it]                                                         46%|     | 3017/6500 [9:07:42<10:11:47, 10.54s/it] 46%|     | 3018/6500 [9:07:52<10:09:18, 10.50s/it]                                                         46%|     | 3018/6500 [9:07:52<10:09:18, 10.50s/it] 46%|     | 3019/6500 [9:08:03<10:07:26, 10.47s/it]                                                         46%|     | 3019/6500 [9:08:03<10:07:26, 10.47s/it] 46%|     | 3020/6500 [9:08:13<10:05:56, 10.45s/it]                                                         46%|     | 3020/6500 [9:08:13<10:05:56, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8491997122764587, 'eval_runtime': 3.9571, 'eval_samples_per_second': 5.812, 'eval_steps_per_second': 1.516, 'epoch': 0.46}
                                                         46%|     | 3020/6500 [9:08:17<10:05:56, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3020I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3020

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3020
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3828, 'learning_rate': 5.5548462107560284e-05, 'epoch': 0.46}
{'loss': 0.3976, 'learning_rate': 5.552443722428393e-05, 'epoch': 0.46}
{'loss': 0.3726, 'learning_rate': 5.550041104970397e-05, 'epoch': 0.47}
{'loss': 0.3915, 'learning_rate': 5.547638358943637e-05, 'epoch': 0.47}
{'loss': 0.3811, 'learning_rate': 5.5452354849097396e-05, 'epoch': 0.47}
 46%|     | 3021/6500 [9:08:28<11:23:57, 11.80s/it]                                                         46%|     | 3021/6500 [9:08:28<11:23:57, 11.80s/it] 46%|     | 3022/6500 [9:08:39<11:08:07, 11.53s/it]                                                         46%|     | 3022/6500 [9:08:39<11:08:07, 11.53s/it] 47%|     | 3023/6500 [9:08:49<10:48:04, 11.18s/it]                                                         47%|     | 3023/6500 [9:08:49<10:48:04, 11.18s/it] 47%|     | 3024/6500 [9:09:00<10:33:58, 10.94s/it]                                                         47%|     | 3024/6500 [9:09:00<10:33:58, 10.94s/it] 47%|     | 3025/6500 [9:09:10<10:23:51, 10.77s/it]                                                         47%|     | 3025/6500 [9:09:10<10:23:51, 10.77s/it] 47%|     | 3026/6500 [9:09:21<10:16:49, 10.65s/{'loss': 0.3991, 'learning_rate': 5.542832483430363e-05, 'epoch': 0.47}
{'loss': 0.3689, 'learning_rate': 5.540429355067196e-05, 'epoch': 0.47}
{'loss': 0.3791, 'learning_rate': 5.538026100381951e-05, 'epoch': 0.47}
{'loss': 0.3679, 'learning_rate': 5.5356227199363764e-05, 'epoch': 0.47}
{'loss': 0.4633, 'learning_rate': 5.533219214292248e-05, 'epoch': 0.47}
it]                                                         47%|     | 3026/6500 [9:09:21<10:16:49, 10.65s/it] 47%|     | 3027/6500 [9:09:31<10:11:56, 10.57s/it]                                                         47%|     | 3027/6500 [9:09:31<10:11:56, 10.57s/it] 47%|     | 3028/6500 [9:09:41<10:08:32, 10.52s/it]                                                         47%|     | 3028/6500 [9:09:41<10:08:32, 10.52s/it] 47%|     | 3029/6500 [9:09:52<10:06:15, 10.48s/it]                                                         47%|     | 3029/6500 [9:09:52<10:06:15, 10.48s/it] 47%|     | 3030/6500 [9:10:02<10:04:27, 10.45s/it]                                                         47%|     | 3030/6500 [9:10:02<10:04:27, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8238617181777954, 'eval_runtime': 3.9581, 'eval_samples_per_second': 5.811, 'eval_steps_per_second': 1.516, 'epoch': 0.47}
                                                         47%|     | 3030/6500 [9:10:06<10:04:27, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3030
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3030/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3030/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3764, 'learning_rate': 5.530815584011371e-05, 'epoch': 0.47}
{'loss': 0.3646, 'learning_rate': 5.528411829655579e-05, 'epoch': 0.47}
{'loss': 0.3869, 'learning_rate': 5.5260079517867323e-05, 'epoch': 0.47}
{'loss': 0.905, 'learning_rate': 5.523603950966726e-05, 'epoch': 0.47}
{'loss': 0.3871, 'learning_rate': 5.5211998277574805e-05, 'epoch': 0.47}
 47%|     | 3031/6500 [9:10:17<11:21:08, 11.78s/it]                                                         47%|     | 3031/6500 [9:10:17<11:21:08, 11.78s/it] 47%|     | 3032/6500 [9:10:27<10:56:47, 11.36s/it]                                                         47%|     | 3032/6500 [9:10:27<10:56:47, 11.36s/it] 47%|     | 3033/6500 [9:10:38<10:39:42, 11.07s/it]                                                         47%|     | 3033/6500 [9:10:38<10:39:42, 11.07s/it] 47%|     | 3034/6500 [9:10:48<10:27:17, 10.86s/it]                                                         47%|     | 3034/6500 [9:10:48<10:27:17, 10.86s/it] 47%|     | 3035/6500 [9:10:59<10:19:19, 10.72s/it]                                                         47%|     | 3035/6500 [9:10:59<10:19:19, 10.72s/it] 47%|     | 3036/6500 [9:11:09<10:13:00, 10.62s/{'loss': 0.3905, 'learning_rate': 5.518795582720944e-05, 'epoch': 0.47}
{'loss': 0.3884, 'learning_rate': 5.5163912164190935e-05, 'epoch': 0.47}
{'loss': 0.3662, 'learning_rate': 5.513986729413937e-05, 'epoch': 0.47}
{'loss': 0.4004, 'learning_rate': 5.511582122267507e-05, 'epoch': 0.47}
{'loss': 0.3874, 'learning_rate': 5.509177395541866e-05, 'epoch': 0.47}
it]                                                         47%|     | 3036/6500 [9:11:09<10:13:00, 10.62s/it] 47%|     | 3037/6500 [9:11:19<10:07:50, 10.53s/it]                                                         47%|     | 3037/6500 [9:11:19<10:07:50, 10.53s/it] 47%|     | 3038/6500 [9:11:30<10:04:11, 10.47s/it]                                                         47%|     | 3038/6500 [9:11:30<10:04:11, 10.47s/it] 47%|     | 3039/6500 [9:11:40<10:10:19, 10.58s/it]                                                         47%|     | 3039/6500 [9:11:40<10:10:19, 10.58s/it] 47%|     | 3040/6500 [9:11:51<10:07:22, 10.53s/it]                                                         47%|     | 3040/6500 [9:11:51<10:07:22, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8301605582237244, 'eval_runtime': 4.194, 'eval_samples_per_second': 5.484, 'eval_steps_per_second': 1.431, 'epoch': 0.47}
                                                         47%|     | 3040/6500 [9:11:55<10:07:22, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3040
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3040/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3537, 'learning_rate': 5.506772549799105e-05, 'epoch': 0.47}
{'loss': 0.3776, 'learning_rate': 5.504367585601342e-05, 'epoch': 0.47}
{'loss': 0.3669, 'learning_rate': 5.501962503510721e-05, 'epoch': 0.47}
{'loss': 0.3776, 'learning_rate': 5.499557304089419e-05, 'epoch': 0.47}
{'loss': 0.37, 'learning_rate': 5.497151987899634e-05, 'epoch': 0.47}
 47%|     | 3041/6500 [9:12:06<11:25:04, 11.88s/it]                                                         47%|     | 3041/6500 [9:12:06<11:25:04, 11.88s/it] 47%|     | 3042/6500 [9:12:16<10:58:45, 11.43s/it]                                                         47%|     | 3042/6500 [9:12:16<10:58:45, 11.43s/it] 47%|     | 3043/6500 [9:12:27<10:39:44, 11.10s/it]                                                         47%|     | 3043/6500 [9:12:27<10:39:44, 11.10s/it] 47%|     | 3044/6500 [9:12:37<10:26:36, 10.88s/it]                                                         47%|     | 3044/6500 [9:12:37<10:26:36, 10.88s/it] 47%|     | 3045/6500 [9:12:47<10:17:24, 10.72s/it]                                                         47%|     | 3045/6500 [9:12:47<10:17:24, 10.72s/it] 47%|     | 3046/6500 [9:12:58<10:11:04, 10.61s/{'loss': 0.3781, 'learning_rate': 5.494746555503593e-05, 'epoch': 0.47}
{'loss': 0.3738, 'learning_rate': 5.492341007463554e-05, 'epoch': 0.47}
{'loss': 0.3872, 'learning_rate': 5.489935344341799e-05, 'epoch': 0.47}
{'loss': 0.3925, 'learning_rate': 5.4875295667006346e-05, 'epoch': 0.47}
{'loss': 0.3781, 'learning_rate': 5.4851236751023985e-05, 'epoch': 0.47}
it]                                                         47%|     | 3046/6500 [9:12:58<10:11:04, 10.61s/it] 47%|     | 3047/6500 [9:13:08<10:06:39, 10.54s/it]                                                         47%|     | 3047/6500 [9:13:08<10:06:39, 10.54s/it] 47%|     | 3048/6500 [9:13:18<10:03:12, 10.48s/it]                                                         47%|     | 3048/6500 [9:13:18<10:03:12, 10.48s/it] 47%|     | 3049/6500 [9:13:29<10:00:28, 10.44s/it]                                                         47%|     | 3049/6500 [9:13:29<10:00:28, 10.44s/it] 47%|     | 3050/6500 [9:13:39<9:58:41, 10.41s/it]                                                         47%|     | 3050/6500 [9:13:39<9:58:41, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8493995666503906, 'eval_runtime': 3.9622, 'eval_samples_per_second': 5.805, 'eval_steps_per_second': 1.514, 'epoch': 0.47}
                                                        47%|     | 3050/6500 [9:13:43<9:58:41, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3934, 'learning_rate': 5.482717670109453e-05, 'epoch': 0.47}
{'loss': 0.3877, 'learning_rate': 5.4803115522841866e-05, 'epoch': 0.47}
{'loss': 0.3793, 'learning_rate': 5.477905322189015e-05, 'epoch': 0.47}
{'loss': 0.3754, 'learning_rate': 5.475498980386382e-05, 'epoch': 0.47}
{'loss': 0.3964, 'learning_rate': 5.4730925274387524e-05, 'epoch': 0.47}
 47%|     | 3051/6500 [9:13:54<11:16:09, 11.76s/it]                                                         47%|     | 3051/6500 [9:13:54<11:16:09, 11.76s/it] 47%|     | 3052/6500 [9:14:04<10:52:09, 11.35s/it]                                                         47%|     | 3052/6500 [9:14:04<10:52:09, 11.35s/it] 47%|     | 3053/6500 [9:14:15<10:34:52, 11.05s/it]                                                         47%|     | 3053/6500 [9:14:15<10:34:52, 11.05s/it] 47%|     | 3054/6500 [9:14:25<10:22:36, 10.84s/it]                                                         47%|     | 3054/6500 [9:14:25<10:22:36, 10.84s/it] 47%|     | 3055/6500 [9:14:36<10:23:43, 10.86s/it]                                                         47%|     | 3055/6500 [9:14:36<10:23:43, 10.86s/it] 47%|     | 3056/6500 [9:14:46<10:14:58, 10.71s/{'loss': 0.3847, 'learning_rate': 5.470685963908621e-05, 'epoch': 0.47}
{'loss': 0.3736, 'learning_rate': 5.468279290358507e-05, 'epoch': 0.47}
{'loss': 0.3638, 'learning_rate': 5.465872507350955e-05, 'epoch': 0.47}
{'loss': 0.3904, 'learning_rate': 5.46346561544854e-05, 'epoch': 0.47}
{'loss': 0.4424, 'learning_rate': 5.461058615213852e-05, 'epoch': 0.47}
it]                                                         47%|     | 3056/6500 [9:14:46<10:14:58, 10.71s/it] 47%|     | 3057/6500 [9:14:57<10:08:33, 10.61s/it]                                                         47%|     | 3057/6500 [9:14:57<10:08:33, 10.61s/it] 47%|     | 3058/6500 [9:15:07<10:04:12, 10.53s/it]                                                         47%|     | 3058/6500 [9:15:07<10:04:12, 10.53s/it] 47%|     | 3059/6500 [9:15:17<10:01:16, 10.48s/it]                                                         47%|     | 3059/6500 [9:15:17<10:01:16, 10.48s/it] 47%|     | 3060/6500 [9:15:28<9:58:55, 10.45s/it]                                                         47%|     | 3060/6500 [9:15:28<9:58:55, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8266682624816895, 'eval_runtime': 3.9716, 'eval_samples_per_second': 5.791, 'eval_steps_per_second': 1.511, 'epoch': 0.47}
                                                        47%|     | 3060/6500 [9:15:32<9:58:55, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3060I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3060

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3060
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3789, 'learning_rate': 5.458651507209518e-05, 'epoch': 0.47}
{'loss': 0.3651, 'learning_rate': 5.4562442919981816e-05, 'epoch': 0.47}
{'loss': 0.383, 'learning_rate': 5.453836970142516e-05, 'epoch': 0.47}
{'loss': 0.9079, 'learning_rate': 5.45142954220522e-05, 'epoch': 0.47}
{'loss': 0.3962, 'learning_rate': 5.449022008749012e-05, 'epoch': 0.47}
 47%|     | 3061/6500 [9:15:43<11:15:27, 11.78s/it]                                                         47%|     | 3061/6500 [9:15:43<11:15:27, 11.78s/it] 47%|     | 3062/6500 [9:15:53<10:50:59, 11.36s/it]                                                         47%|     | 3062/6500 [9:15:53<10:50:59, 11.36s/it] 47%|     | 3063/6500 [9:16:03<10:33:14, 11.05s/it]                                                         47%|     | 3063/6500 [9:16:03<10:33:14, 11.05s/it] 47%|     | 3064/6500 [9:16:14<10:20:51, 10.84s/it]                                                         47%|     | 3064/6500 [9:16:14<10:20:51, 10.84s/it] 47%|     | 3065/6500 [9:16:24<10:12:14, 10.69s/it]                                                         47%|     | 3065/6500 [9:16:24<10:12:14, 10.69s/it] 47%|     | 3066/6500 [9:16:35<10:06:03, 10.59s/{'loss': 0.3843, 'learning_rate': 5.446614370336639e-05, 'epoch': 0.47}
{'loss': 0.3734, 'learning_rate': 5.444206627530873e-05, 'epoch': 0.47}
{'loss': 0.3676, 'learning_rate': 5.441798780894508e-05, 'epoch': 0.47}
{'loss': 0.4155, 'learning_rate': 5.439390830990365e-05, 'epoch': 0.47}
{'loss': 0.3617, 'learning_rate': 5.4369827783812864e-05, 'epoch': 0.47}
it]                                                         47%|     | 3066/6500 [9:16:35<10:06:03, 10.59s/it] 47%|     | 3067/6500 [9:16:45<10:01:47, 10.52s/it]                                                         47%|     | 3067/6500 [9:16:45<10:01:47, 10.52s/it] 47%|     | 3068/6500 [9:16:55<9:58:45, 10.47s/it]                                                         47%|     | 3068/6500 [9:16:55<9:58:45, 10.47s/it] 47%|     | 3069/6500 [9:17:06<9:56:44, 10.44s/it]                                                        47%|     | 3069/6500 [9:17:06<9:56:44, 10.44s/it] 47%|     | 3070/6500 [9:17:16<9:55:16, 10.41s/it]                                                        47%|     | 3070/6500 [9:17:16<9:55:16, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8301864862442017, 'eval_runtime': 3.9614, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.515, 'epoch': 0.47}
                                                        47%|     | 3070/6500 [9:17:20<9:55:16, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3070I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3070/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3070/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3791, 'learning_rate': 5.434574623630141e-05, 'epoch': 0.47}
{'loss': 0.3696, 'learning_rate': 5.432166367299818e-05, 'epoch': 0.47}
{'loss': 0.3854, 'learning_rate': 5.429758009953235e-05, 'epoch': 0.47}
{'loss': 0.3742, 'learning_rate': 5.4273495521533304e-05, 'epoch': 0.47}
{'loss': 0.3817, 'learning_rate': 5.424940994463066e-05, 'epoch': 0.47}
 47%|     | 3071/6500 [9:17:31<11:17:45, 11.86s/it]                                                         47%|     | 3071/6500 [9:17:31<11:17:45, 11.86s/it] 47%|     | 3072/6500 [9:17:42<10:51:39, 11.41s/it]                                                         47%|     | 3072/6500 [9:17:42<10:51:39, 11.41s/it] 47%|     | 3073/6500 [9:17:52<10:33:17, 11.09s/it]                                                         47%|     | 3073/6500 [9:17:52<10:33:17, 11.09s/it] 47%|     | 3074/6500 [9:18:02<10:20:26, 10.87s/it]                                                         47%|     | 3074/6500 [9:18:02<10:20:26, 10.87s/it] 47%|     | 3075/6500 [9:18:13<10:11:28, 10.71s/it]                                                         47%|     | 3075/6500 [9:18:13<10:11:28, 10.71s/it] 47%|     | 3076/6500 [9:18:23<10:05:08, 10.60s/{'loss': 0.4004, 'learning_rate': 5.4225323374454286e-05, 'epoch': 0.47}
{'loss': 0.3729, 'learning_rate': 5.420123581663426e-05, 'epoch': 0.47}
{'loss': 0.4012, 'learning_rate': 5.4177147276800896e-05, 'epoch': 0.47}
{'loss': 0.3894, 'learning_rate': 5.4153057760584755e-05, 'epoch': 0.47}
{'loss': 0.3732, 'learning_rate': 5.4128967273616625e-05, 'epoch': 0.47}
it]                                                         47%|     | 3076/6500 [9:18:23<10:05:08, 10.60s/it] 47%|     | 3077/6500 [9:18:33<10:00:45, 10.53s/it]                                                         47%|     | 3077/6500 [9:18:33<10:00:45, 10.53s/it] 47%|     | 3078/6500 [9:18:44<9:57:34, 10.48s/it]                                                         47%|     | 3078/6500 [9:18:44<9:57:34, 10.48s/it] 47%|     | 3079/6500 [9:18:54<9:55:08, 10.44s/it]                                                        47%|     | 3079/6500 [9:18:54<9:55:08, 10.44s/it] 47%|     | 3080/6500 [9:19:04<9:53:08, 10.41s/it]                                                        47%|     | 3080/6500 [9:19:04<9:53:08, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8472849726676941, 'eval_runtime': 3.9654, 'eval_samples_per_second': 5.8, 'eval_steps_per_second': 1.513, 'epoch': 0.47}
                                                        47%|     | 3080/6500 [9:19:08<9:53:08, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3080/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3935, 'learning_rate': 5.410487582152749e-05, 'epoch': 0.47}
{'loss': 0.3874, 'learning_rate': 5.408078340994859e-05, 'epoch': 0.47}
{'loss': 0.3789, 'learning_rate': 5.4056690044511385e-05, 'epoch': 0.47}
{'loss': 0.3898, 'learning_rate': 5.403259573084753e-05, 'epoch': 0.47}
{'loss': 0.4091, 'learning_rate': 5.4008500474588965e-05, 'epoch': 0.47}
 47%|     | 3081/6500 [9:19:19<11:08:32, 11.73s/it]                                                         47%|     | 3081/6500 [9:19:19<11:08:32, 11.73s/it] 47%|     | 3082/6500 [9:19:29<10:44:25, 11.31s/it]                                                         47%|     | 3082/6500 [9:19:29<10:44:25, 11.31s/it] 47%|     | 3083/6500 [9:19:40<10:27:36, 11.02s/it]                                                         47%|     | 3083/6500 [9:19:40<10:27:36, 11.02s/it] 47%|     | 3084/6500 [9:19:50<10:15:42, 10.81s/it]                                                         47%|     | 3084/6500 [9:19:50<10:15:42, 10.81s/it] 47%|     | 3085/6500 [9:20:00<10:07:15, 10.67s/it]                                                         47%|     | 3085/6500 [9:20:00<10:07:15, 10.67s/it] 47%|     | 3086/6500 [9:20:11<10:01:34, 10.57s/{'loss': 0.3605, 'learning_rate': 5.3984404281367786e-05, 'epoch': 0.47}
{'loss': 0.3815, 'learning_rate': 5.3960307156816324e-05, 'epoch': 0.47}
{'loss': 0.3568, 'learning_rate': 5.393620910656714e-05, 'epoch': 0.48}
{'loss': 0.4134, 'learning_rate': 5.391211013625301e-05, 'epoch': 0.48}
{'loss': 0.4229, 'learning_rate': 5.3888010251506915e-05, 'epoch': 0.48}
it]                                                         47%|     | 3086/6500 [9:20:11<10:01:34, 10.57s/it] 47%|     | 3087/6500 [9:20:21<10:01:25, 10.57s/it]                                                         47%|     | 3087/6500 [9:20:21<10:01:25, 10.57s/it] 48%|     | 3088/6500 [9:20:32<9:57:06, 10.50s/it]                                                         48%|     | 3088/6500 [9:20:32<9:57:06, 10.50s/it] 48%|     | 3089/6500 [9:20:42<9:54:02, 10.45s/it]                                                        48%|     | 3089/6500 [9:20:42<9:54:02, 10.45s/it] 48%|     | 3090/6500 [9:20:52<9:51:59, 10.42s/it]                                                        48%|     | 3090/6500 [9:20:52<9:51:59, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8243617415428162, 'eval_runtime': 4.1773, 'eval_samples_per_second': 5.506, 'eval_steps_per_second': 1.436, 'epoch': 0.48}
                                                        48%|     | 3090/6500 [9:20:57<9:51:59, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3704, 'learning_rate': 5.3863909457962094e-05, 'epoch': 0.48}
{'loss': 0.3866, 'learning_rate': 5.3839807761251906e-05, 'epoch': 0.48}
{'loss': 0.3751, 'learning_rate': 5.381570516701e-05, 'epoch': 0.48}
{'loss': 0.9171, 'learning_rate': 5.379160168087021e-05, 'epoch': 0.48}
{'loss': 0.3837, 'learning_rate': 5.376749730846657e-05, 'epoch': 0.48}
 48%|     | 3091/6500 [9:21:07<11:10:37, 11.80s/it]                                                         48%|     | 3091/6500 [9:21:07<11:10:37, 11.80s/it] 48%|     | 3092/6500 [9:21:18<10:45:15, 11.36s/it]                                                         48%|     | 3092/6500 [9:21:18<10:45:15, 11.36s/it] 48%|     | 3093/6500 [9:21:28<10:27:18, 11.05s/it]                                                         48%|     | 3093/6500 [9:21:28<10:27:18, 11.05s/it] 48%|     | 3094/6500 [9:21:38<10:14:50, 10.83s/it]                                                         48%|     | 3094/6500 [9:21:38<10:14:50, 10.83s/it] 48%|     | 3095/6500 [9:21:49<10:06:30, 10.69s/it]                                                         48%|     | 3095/6500 [9:21:49<10:06:30, 10.69s/it] 48%|     | 3096/6500 [9:21:59<10:00:02, 10.58s/{'loss': 0.3899, 'learning_rate': 5.374339205543336e-05, 'epoch': 0.48}
{'loss': 0.3522, 'learning_rate': 5.371928592740503e-05, 'epoch': 0.48}
{'loss': 0.3853, 'learning_rate': 5.3695178930016196e-05, 'epoch': 0.48}
{'loss': 0.3868, 'learning_rate': 5.367107106890177e-05, 'epoch': 0.48}
{'loss': 0.3462, 'learning_rate': 5.3646962349696806e-05, 'epoch': 0.48}
it]                                                         48%|     | 3096/6500 [9:21:59<10:00:02, 10.58s/it] 48%|     | 3097/6500 [9:22:09<9:55:24, 10.50s/it]                                                         48%|     | 3097/6500 [9:22:09<9:55:24, 10.50s/it] 48%|     | 3098/6500 [9:22:20<9:52:14, 10.45s/it]                                                        48%|     | 3098/6500 [9:22:20<9:52:14, 10.45s/it] 48%|     | 3099/6500 [9:22:30<9:49:55, 10.41s/it]                                                        48%|     | 3099/6500 [9:22:30<9:49:55, 10.41s/it] 48%|     | 3100/6500 [9:22:40<9:48:20, 10.38s/it]                                                        48%|     | 3100/6500 [9:22:40<9:48:20, 10.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8337832689285278, 'eval_runtime': 3.9639, 'eval_samples_per_second': 5.802, 'eval_steps_per_second': 1.514, 'epoch': 0.48}
                                                        48%|     | 3100/6500 [9:22:44<9:48:20, 10.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3100
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3100

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3678, 'learning_rate': 5.362285277803656e-05, 'epoch': 0.48}
{'loss': 0.3656, 'learning_rate': 5.3598742359556495e-05, 'epoch': 0.48}
{'loss': 0.3777, 'learning_rate': 5.35746310998923e-05, 'epoch': 0.48}
{'loss': 0.3782, 'learning_rate': 5.3550519004679823e-05, 'epoch': 0.48}
{'loss': 0.38, 'learning_rate': 5.35264060795551e-05, 'epoch': 0.48}
 48%|     | 3101/6500 [9:22:55<11:04:01, 11.72s/it]                                                         48%|     | 3101/6500 [9:22:55<11:04:01, 11.72s/it] 48%|     | 3102/6500 [9:23:06<10:40:15, 11.31s/it]                                                         48%|     | 3102/6500 [9:23:06<10:40:15, 11.31s/it] 48%|     | 3103/6500 [9:23:16<10:29:41, 11.12s/it]                                                         48%|     | 3103/6500 [9:23:16<10:29:41, 11.12s/it] 48%|     | 3104/6500 [9:23:27<10:16:08, 10.89s/it]                                                         48%|     | 3104/6500 [9:23:27<10:16:08, 10.89s/it] 48%|     | 3105/6500 [9:23:37<10:06:32, 10.72s/it]                                                         48%|     | 3105/6500 [9:23:37<10:06:32, 10.72s/it] 48%|     | 3106/6500 [9:23:47<9:59:32, 10.60s/i{'loss': 0.383, 'learning_rate': 5.3502292330154404e-05, 'epoch': 0.48}
{'loss': 0.3767, 'learning_rate': 5.347817776211417e-05, 'epoch': 0.48}
{'loss': 0.4143, 'learning_rate': 5.3454062381071046e-05, 'epoch': 0.48}
{'loss': 0.386, 'learning_rate': 5.342994619266182e-05, 'epoch': 0.48}
{'loss': 0.3924, 'learning_rate': 5.340582920252354e-05, 'epoch': 0.48}
t]                                                         48%|     | 3106/6500 [9:23:47<9:59:32, 10.60s/it] 48%|     | 3107/6500 [9:23:58<9:54:33, 10.51s/it]                                                        48%|     | 3107/6500 [9:23:58<9:54:33, 10.51s/it] 48%|     | 3108/6500 [9:24:08<9:51:07, 10.46s/it]                                                        48%|     | 3108/6500 [9:24:08<9:51:07, 10.46s/it] 48%|     | 3109/6500 [9:24:18<9:48:55, 10.42s/it]                                                        48%|     | 3109/6500 [9:24:18<9:48:55, 10.42s/it] 48%|     | 3110/6500 [9:24:29<9:47:05, 10.39s/it]                                                        48%|     | 3110/6500 [9:24:29<9:47:05, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8472012281417847, 'eval_runtime': 3.9528, 'eval_samples_per_second': 5.819, 'eval_steps_per_second': 1.518, 'epoch': 0.48}
                                                        48%|     | 3110/6500 [9:24:32<9:47:05, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3110the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3110

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3110
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4024, 'learning_rate': 5.338171141629338e-05, 'epoch': 0.48}
{'loss': 0.3683, 'learning_rate': 5.335759283960874e-05, 'epoch': 0.48}
{'loss': 0.3786, 'learning_rate': 5.3333473478107184e-05, 'epoch': 0.48}
{'loss': 0.4031, 'learning_rate': 5.330935333742649e-05, 'epoch': 0.48}
{'loss': 0.3883, 'learning_rate': 5.328523242320456e-05, 'epoch': 0.48}
 48%|     | 3111/6500 [9:24:43<11:01:18, 11.71s/it]                                                         48%|     | 3111/6500 [9:24:43<11:01:18, 11.71s/it] 48%|     | 3112/6500 [9:24:54<10:37:46, 11.29s/it]                                                         48%|     | 3112/6500 [9:24:54<10:37:46, 11.29s/it] 48%|     | 3113/6500 [9:25:04<10:21:16, 11.01s/it]                                                         48%|     | 3113/6500 [9:25:04<10:21:16, 11.01s/it] 48%|     | 3114/6500 [9:25:14<10:09:40, 10.80s/it]                                                         48%|     | 3114/6500 [9:25:14<10:09:40, 10.80s/it] 48%|     | 3115/6500 [9:25:25<10:01:40, 10.66s/it]                                                         48%|     | 3115/6500 [9:25:25<10:01:40, 10.66s/it] 48%|     | 3116/6500 [9:25:35<9:55:54, 10.57s/i{'loss': 0.3591, 'learning_rate': 5.3261110741079525e-05, 'epoch': 0.48}
{'loss': 0.3791, 'learning_rate': 5.323698829668968e-05, 'epoch': 0.48}
{'loss': 0.3772, 'learning_rate': 5.3212865095673514e-05, 'epoch': 0.48}
{'loss': 0.4648, 'learning_rate': 5.318874114366965e-05, 'epoch': 0.48}
{'loss': 0.3716, 'learning_rate': 5.316461644631694e-05, 'epoch': 0.48}
t]                                                         48%|     | 3116/6500 [9:25:35<9:55:54, 10.57s/it] 48%|     | 3117/6500 [9:25:45<9:51:47, 10.50s/it]                                                        48%|     | 3117/6500 [9:25:45<9:51:47, 10.50s/it] 48%|     | 3118/6500 [9:25:56<9:48:56, 10.45s/it]                                                        48%|     | 3118/6500 [9:25:56<9:48:56, 10.45s/it] 48%|     | 3119/6500 [9:26:06<9:50:24, 10.48s/it]                                                        48%|     | 3119/6500 [9:26:06<9:50:24, 10.48s/it] 48%|     | 3120/6500 [9:26:17<9:47:49, 10.43s/it]                                                        48%|     | 3120/6500 [9:26:17<9:47:49, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8235064148902893, 'eval_runtime': 4.1846, 'eval_samples_per_second': 5.496, 'eval_steps_per_second': 1.434, 'epoch': 0.48}
                                                        48%|     | 3120/6500 [9:26:21<9:47:49, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3120
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3120

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3120/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3120/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3676, 'learning_rate': 5.3140491009254376e-05, 'epoch': 0.48}
{'loss': 0.3858, 'learning_rate': 5.311636483812114e-05, 'epoch': 0.48}
{'loss': 0.8956, 'learning_rate': 5.309223793855655e-05, 'epoch': 0.48}
{'loss': 0.3793, 'learning_rate': 5.306811031620017e-05, 'epoch': 0.48}
{'loss': 0.3864, 'learning_rate': 5.3043981976691645e-05, 'epoch': 0.48}
 48%|     | 3121/6500 [9:26:32<11:04:57, 11.81s/it]                                                         48%|     | 3121/6500 [9:26:32<11:04:57, 11.81s/it] 48%|     | 3122/6500 [9:26:42<10:39:35, 11.36s/it]                                                         48%|     | 3122/6500 [9:26:42<10:39:35, 11.36s/it] 48%|     | 3123/6500 [9:26:52<10:21:30, 11.04s/it]                                                         48%|     | 3123/6500 [9:26:52<10:21:30, 11.04s/it] 48%|     | 3124/6500 [9:27:02<10:09:05, 10.83s/it]                                                         48%|     | 3124/6500 [9:27:02<10:09:05, 10.83s/it] 48%|     | 3125/6500 [9:27:13<10:00:16, 10.67s/it]                                                         48%|     | 3125/6500 [9:27:13<10:00:16, 10.67s/it] 48%|     | 3126/6500 [9:27:23<9:54:21, 10.57s/i{'loss': 0.3838, 'learning_rate': 5.301985292567084e-05, 'epoch': 0.48}
{'loss': 0.3642, 'learning_rate': 5.299572316877778e-05, 'epoch': 0.48}
{'loss': 0.4007, 'learning_rate': 5.297159271165264e-05, 'epoch': 0.48}
{'loss': 0.3667, 'learning_rate': 5.2947461559935786e-05, 'epoch': 0.48}
{'loss': 0.3619, 'learning_rate': 5.292332971926769e-05, 'epoch': 0.48}
t]                                                         48%|     | 3126/6500 [9:27:23<9:54:21, 10.57s/it] 48%|     | 3127/6500 [9:27:33<9:49:44, 10.49s/it]                                                        48%|     | 3127/6500 [9:27:33<9:49:44, 10.49s/it] 48%|     | 3128/6500 [9:27:44<9:46:42, 10.44s/it]                                                        48%|     | 3128/6500 [9:27:44<9:46:42, 10.44s/it] 48%|     | 3129/6500 [9:27:54<9:44:16, 10.40s/it]                                                        48%|     | 3129/6500 [9:27:54<9:44:16, 10.40s/it] 48%|     | 3130/6500 [9:28:04<9:42:45, 10.38s/it]                                                        48%|     | 3130/6500 [9:28:04<9:42:45, 10.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8314828276634216, 'eval_runtime': 3.9586, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.48}
                                                        48%|     | 3130/6500 [9:28:08<9:42:45, 10.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3130/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3617, 'learning_rate': 5.289919719528905e-05, 'epoch': 0.48}
{'loss': 0.3654, 'learning_rate': 5.2875063993640707e-05, 'epoch': 0.48}
{'loss': 0.3832, 'learning_rate': 5.285093011996362e-05, 'epoch': 0.48}
{'loss': 0.3659, 'learning_rate': 5.2826795579898956e-05, 'epoch': 0.48}
{'loss': 0.3858, 'learning_rate': 5.280266037908802e-05, 'epoch': 0.48}
 48%|     | 3131/6500 [9:28:19<10:57:22, 11.71s/it]                                                         48%|     | 3131/6500 [9:28:19<10:57:22, 11.71s/it] 48%|     | 3132/6500 [9:28:29<10:33:56, 11.29s/it]                                                         48%|     | 3132/6500 [9:28:29<10:33:56, 11.29s/it] 48%|     | 3133/6500 [9:28:40<10:17:30, 11.00s/it]                                                         48%|     | 3133/6500 [9:28:40<10:17:30, 11.00s/it] 48%|     | 3134/6500 [9:28:50<10:05:59, 10.80s/it]                                                         48%|     | 3134/6500 [9:28:50<10:05:59, 10.80s/it] 48%|     | 3135/6500 [9:29:00<9:57:49, 10.66s/it]                                                         48%|     | 3135/6500 [9:29:00<9:57:49, 10.66s/it] 48%|     | 3136/6500 [9:29:11<9:58:31, 10.68s/it]{'loss': 0.3585, 'learning_rate': 5.277852452317226e-05, 'epoch': 0.48}
{'loss': 0.3987, 'learning_rate': 5.2754388017793274e-05, 'epoch': 0.48}
{'loss': 0.3804, 'learning_rate': 5.2730250868592845e-05, 'epoch': 0.48}
{'loss': 0.3643, 'learning_rate': 5.270611308121287e-05, 'epoch': 0.48}
{'loss': 0.3835, 'learning_rate': 5.268197466129542e-05, 'epoch': 0.48}
                                                        48%|     | 3136/6500 [9:29:11<9:58:31, 10.68s/it] 48%|     | 3137/6500 [9:29:22<9:52:39, 10.57s/it]                                                        48%|     | 3137/6500 [9:29:22<9:52:39, 10.57s/it] 48%|     | 3138/6500 [9:29:32<9:48:17, 10.50s/it]                                                        48%|     | 3138/6500 [9:29:32<9:48:17, 10.50s/it] 48%|     | 3139/6500 [9:29:42<9:45:07, 10.45s/it]                                                        48%|     | 3139/6500 [9:29:42<9:45:07, 10.45s/it] 48%|     | 3140/6500 [9:29:53<9:43:48, 10.43s/it]                                                        48%|     | 3140/6500 [9:29:53<9:43:48, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8492940664291382, 'eval_runtime': 3.9344, 'eval_samples_per_second': 5.846, 'eval_steps_per_second': 1.525, 'epoch': 0.48}
                                                        48%|     | 3140/6500 [9:29:56<9:43:48, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3140
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.371, 'learning_rate': 5.2657835614482706e-05, 'epoch': 0.48}
{'loss': 0.3803, 'learning_rate': 5.2633695946417075e-05, 'epoch': 0.48}
{'loss': 0.3601, 'learning_rate': 5.260955566274103e-05, 'epoch': 0.48}
{'loss': 0.4033, 'learning_rate': 5.2585414769097207e-05, 'epoch': 0.48}
{'loss': 0.3762, 'learning_rate': 5.2561273271128396e-05, 'epoch': 0.48}
 48%|     | 3141/6500 [9:30:07<10:57:17, 11.74s/it]                                                         48%|     | 3141/6500 [9:30:07<10:57:17, 11.74s/it] 48%|     | 3142/6500 [9:30:18<10:33:30, 11.32s/it]                                                         48%|     | 3142/6500 [9:30:18<10:33:30, 11.32s/it] 48%|     | 3143/6500 [9:30:28<10:16:46, 11.02s/it]                                                         48%|     | 3143/6500 [9:30:28<10:16:46, 11.02s/it] 48%|     | 3144/6500 [9:30:38<10:05:20, 10.82s/it]                                                         48%|     | 3144/6500 [9:30:38<10:05:20, 10.82s/it] 48%|     | 3145/6500 [9:30:49<9:56:59, 10.68s/it]                                                         48%|     | 3145/6500 [9:30:49<9:56:59, 10.68s/it] 48%|     | 3146/6500 [9:30:59<9:51:10, 10.58s/it]{'loss': 0.3721, 'learning_rate': 5.253713117447755e-05, 'epoch': 0.48}
{'loss': 0.3591, 'learning_rate': 5.2512988484787704e-05, 'epoch': 0.48}
{'loss': 0.3927, 'learning_rate': 5.248884520770209e-05, 'epoch': 0.48}
{'loss': 0.4401, 'learning_rate': 5.246470134886403e-05, 'epoch': 0.48}
{'loss': 0.3676, 'learning_rate': 5.2440556913917014e-05, 'epoch': 0.48}
                                                        48%|     | 3146/6500 [9:30:59<9:51:10, 10.58s/it] 48%|     | 3147/6500 [9:31:09<9:47:04, 10.51s/it]                                                        48%|     | 3147/6500 [9:31:09<9:47:04, 10.51s/it] 48%|     | 3148/6500 [9:31:20<9:44:19, 10.46s/it]                                                        48%|     | 3148/6500 [9:31:20<9:44:19, 10.46s/it] 48%|     | 3149/6500 [9:31:30<9:42:23, 10.43s/it]                                                        48%|     | 3149/6500 [9:31:30<9:42:23, 10.43s/it] 48%|     | 3150/6500 [9:31:40<9:40:22, 10.39s/it]                                                        48%|     | 3150/6500 [9:31:40<9:40:22, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8284992575645447, 'eval_runtime': 3.9622, 'eval_samples_per_second': 5.805, 'eval_steps_per_second': 1.514, 'epoch': 0.48}
                                                        48%|     | 3150/6500 [9:31:44<9:40:22, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3150
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3579, 'learning_rate': 5.241641190850466e-05, 'epoch': 0.48}
{'loss': 0.3875, 'learning_rate': 5.2392266338270736e-05, 'epoch': 0.48}
{'loss': 0.8904, 'learning_rate': 5.236812020885907e-05, 'epoch': 0.49}
{'loss': 0.3978, 'learning_rate': 5.2343973525913716e-05, 'epoch': 0.49}
{'loss': 0.38, 'learning_rate': 5.23198262950788e-05, 'epoch': 0.49}
 48%|     | 3151/6500 [9:31:55<10:56:12, 11.76s/it]                                                         48%|     | 3151/6500 [9:31:55<10:56:12, 11.76s/it] 48%|     | 3152/6500 [9:32:06<10:41:04, 11.49s/it]                                                         48%|     | 3152/6500 [9:32:06<10:41:04, 11.49s/it] 49%|     | 3153/6500 [9:32:17<10:22:31, 11.16s/it]                                                         49%|     | 3153/6500 [9:32:17<10:22:31, 11.16s/it] 49%|     | 3154/6500 [9:32:27<10:09:30, 10.93s/it]                                                         49%|     | 3154/6500 [9:32:27<10:09:30, 10.93s/it] 49%|     | 3155/6500 [9:32:37<10:00:16, 10.77s/it]                                                         49%|     | 3155/6500 [9:32:37<10:00:16, 10.77s/it] 49%|     | 3156/6500 [9:32:48<9:53:47, 10.65s/i{'loss': 0.3622, 'learning_rate': 5.229567852199859e-05, 'epoch': 0.49}
{'loss': 0.3673, 'learning_rate': 5.2271530212317487e-05, 'epoch': 0.49}
{'loss': 0.3972, 'learning_rate': 5.2247381371680014e-05, 'epoch': 0.49}
{'loss': 0.3712, 'learning_rate': 5.222323200573081e-05, 'epoch': 0.49}
{'loss': 0.3739, 'learning_rate': 5.219908212011463e-05, 'epoch': 0.49}
t]                                                         49%|     | 3156/6500 [9:32:48<9:53:47, 10.65s/it] 49%|     | 3157/6500 [9:32:58<9:49:09, 10.57s/it]                                                        49%|     | 3157/6500 [9:32:58<9:49:09, 10.57s/it] 49%|     | 3158/6500 [9:33:09<9:45:35, 10.51s/it]                                                        49%|     | 3158/6500 [9:33:09<9:45:35, 10.51s/it] 49%|     | 3159/6500 [9:33:19<9:43:15, 10.47s/it]                                                        49%|     | 3159/6500 [9:33:19<9:43:15, 10.47s/it] 49%|     | 3160/6500 [9:33:29<9:41:48, 10.45s/it]                                                        49%|     | 3160/6500 [9:33:29<9:41:48, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8372477889060974, 'eval_runtime': 3.9585, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.49}
                                                        49%|     | 3160/6500 [9:33:33<9:41:48, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3160the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3160
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3160/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3160/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3707, 'learning_rate': 5.217493172047637e-05, 'epoch': 0.49}
{'loss': 0.3673, 'learning_rate': 5.2150780812461075e-05, 'epoch': 0.49}
{'loss': 0.3703, 'learning_rate': 5.2126629401713814e-05, 'epoch': 0.49}
{'loss': 0.3667, 'learning_rate': 5.210247749387986e-05, 'epoch': 0.49}
{'loss': 0.385, 'learning_rate': 5.2078325094604596e-05, 'epoch': 0.49}
 49%|     | 3161/6500 [9:33:44<10:55:26, 11.78s/it]                                                         49%|     | 3161/6500 [9:33:44<10:55:26, 11.78s/it] 49%|     | 3162/6500 [9:33:55<10:32:10, 11.36s/it]                                                         49%|     | 3162/6500 [9:33:55<10:32:10, 11.36s/it] 49%|     | 3163/6500 [9:34:05<10:15:35, 11.07s/it]                                                         49%|     | 3163/6500 [9:34:05<10:15:35, 11.07s/it] 49%|     | 3164/6500 [9:34:15<10:03:56, 10.86s/it]                                                         49%|     | 3164/6500 [9:34:15<10:03:56, 10.86s/it] 49%|     | 3165/6500 [9:34:26<9:55:40, 10.72s/it]                                                         49%|     | 3165/6500 [9:34:26<9:55:40, 10.72s/it] 49%|     | 3166/6500 [9:34:36<9:50:04, 10.62s/it]{'loss': 0.371, 'learning_rate': 5.205417220953346e-05, 'epoch': 0.49}
{'loss': 0.3955, 'learning_rate': 5.203001884431208e-05, 'epoch': 0.49}
{'loss': 0.383, 'learning_rate': 5.200586500458612e-05, 'epoch': 0.49}
{'loss': 0.3764, 'learning_rate': 5.198171069600141e-05, 'epoch': 0.49}
{'loss': 0.3882, 'learning_rate': 5.195755592420387e-05, 'epoch': 0.49}
                                                        49%|     | 3166/6500 [9:34:36<9:50:04, 10.62s/it] 49%|     | 3167/6500 [9:34:47<9:46:04, 10.55s/it]                                                        49%|     | 3167/6500 [9:34:47<9:46:04, 10.55s/it] 49%|     | 3168/6500 [9:34:57<9:49:17, 10.61s/it]                                                        49%|     | 3168/6500 [9:34:57<9:49:17, 10.61s/it] 49%|     | 3169/6500 [9:35:08<9:44:36, 10.53s/it]                                                        49%|     | 3169/6500 [9:35:08<9:44:36, 10.53s/it] 49%|     | 3170/6500 [9:35:18<9:41:39, 10.48s/it]                                                        49%|     | 3170/6500 [9:35:18<9:41:39, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.84660404920578, 'eval_runtime': 3.964, 'eval_samples_per_second': 5.802, 'eval_steps_per_second': 1.514, 'epoch': 0.49}
                                                        49%|     | 3170/6500 [9:35:22<9:41:39, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3170I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3170
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3170/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3170/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3843, 'learning_rate': 5.193340069483955e-05, 'epoch': 0.49}
{'loss': 0.383, 'learning_rate': 5.1909245013554564e-05, 'epoch': 0.49}
{'loss': 0.3766, 'learning_rate': 5.188508888599517e-05, 'epoch': 0.49}
{'loss': 0.3867, 'learning_rate': 5.186093231780771e-05, 'epoch': 0.49}
{'loss': 0.369, 'learning_rate': 5.183677531463863e-05, 'epoch': 0.49}
 49%|     | 3171/6500 [9:35:33<10:53:41, 11.78s/it]                                                         49%|     | 3171/6500 [9:35:33<10:53:41, 11.78s/it] 49%|     | 3172/6500 [9:35:43<10:29:37, 11.35s/it]                                                         49%|     | 3172/6500 [9:35:43<10:29:37, 11.35s/it] 49%|     | 3173/6500 [9:35:53<10:12:45, 11.05s/it]                                                         49%|     | 3173/6500 [9:35:53<10:12:45, 11.05s/it] 49%|     | 3174/6500 [9:36:04<10:00:59, 10.84s/it]                                                         49%|     | 3174/6500 [9:36:04<10:00:59, 10.84s/it] 49%|     | 3175/6500 [9:36:14<9:52:30, 10.69s/it]                                                         49%|     | 3175/6500 [9:36:14<9:52:30, 10.69s/it] 49%|     | 3176/6500 [9:36:25<9:46:38, 10.59s/it]{'loss': 0.3747, 'learning_rate': 5.1812617882134486e-05, 'epoch': 0.49}
{'loss': 0.3519, 'learning_rate': 5.1788460025941934e-05, 'epoch': 0.49}
{'loss': 0.4077, 'learning_rate': 5.1764301751707735e-05, 'epoch': 0.49}
{'loss': 0.4214, 'learning_rate': 5.174014306507873e-05, 'epoch': 0.49}
{'loss': 0.3751, 'learning_rate': 5.171598397170184e-05, 'epoch': 0.49}
                                                        49%|     | 3176/6500 [9:36:25<9:46:38, 10.59s/it] 49%|     | 3177/6500 [9:36:35<9:42:50, 10.52s/it]                                                        49%|     | 3177/6500 [9:36:35<9:42:50, 10.52s/it] 49%|     | 3178/6500 [9:36:45<9:39:55, 10.47s/it]                                                        49%|     | 3178/6500 [9:36:45<9:39:55, 10.47s/it] 49%|     | 3179/6500 [9:36:56<9:37:48, 10.44s/it]                                                        49%|     | 3179/6500 [9:36:56<9:37:48, 10.44s/it] 49%|     | 3180/6500 [9:37:06<9:36:20, 10.42s/it]                                                        49%|     | 3180/6500 [9:37:06<9:36:20, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8275449872016907, 'eval_runtime': 3.9318, 'eval_samples_per_second': 5.85, 'eval_steps_per_second': 1.526, 'epoch': 0.49}
                                                        49%|     | 3180/6500 [9:37:10<9:36:20, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3807, 'learning_rate': 5.169182447722415e-05, 'epoch': 0.49}
{'loss': 0.3792, 'learning_rate': 5.1667664587292776e-05, 'epoch': 0.49}
{'loss': 0.9101, 'learning_rate': 5.164350430755494e-05, 'epoch': 0.49}
{'loss': 0.3844, 'learning_rate': 5.161934364365796e-05, 'epoch': 0.49}
{'loss': 0.379, 'learning_rate': 5.159518260124925e-05, 'epoch': 0.49}
 49%|     | 3181/6500 [9:37:21<10:49:48, 11.75s/it]                                                         49%|     | 3181/6500 [9:37:21<10:49:48, 11.75s/it] 49%|     | 3182/6500 [9:37:31<10:26:13, 11.32s/it]                                                         49%|     | 3182/6500 [9:37:31<10:26:13, 11.32s/it] 49%|     | 3183/6500 [9:37:42<10:09:28, 11.02s/it]                                                         49%|     | 3183/6500 [9:37:42<10:09:28, 11.02s/it] 49%|     | 3184/6500 [9:37:52<10:05:57, 10.96s/it]                                                         49%|     | 3184/6500 [9:37:52<10:05:57, 10.96s/it] 49%|     | 3185/6500 [9:38:03<9:55:30, 10.78s/it]                                                         49%|     | 3185/6500 [9:38:03<9:55:30, 10.78s/it] 49%|     | 3186/6500 [9:38:13<9:47:53, 10.64s/it]{'loss': 0.3477, 'learning_rate': 5.157102118597631e-05, 'epoch': 0.49}
{'loss': 0.3829, 'learning_rate': 5.154685940348671e-05, 'epoch': 0.49}
{'loss': 0.3768, 'learning_rate': 5.1522697259428146e-05, 'epoch': 0.49}
{'loss': 0.3436, 'learning_rate': 5.1498534759448346e-05, 'epoch': 0.49}
{'loss': 0.3763, 'learning_rate': 5.147437190919516e-05, 'epoch': 0.49}
                                                        49%|     | 3186/6500 [9:38:13<9:47:53, 10.64s/it] 49%|     | 3187/6500 [9:38:23<9:42:52, 10.56s/it]                                                        49%|     | 3187/6500 [9:38:23<9:42:52, 10.56s/it] 49%|     | 3188/6500 [9:38:34<9:39:09, 10.49s/it]                                                        49%|     | 3188/6500 [9:38:34<9:39:09, 10.49s/it] 49%|     | 3189/6500 [9:38:44<9:36:31, 10.45s/it]                                                        49%|     | 3189/6500 [9:38:44<9:36:31, 10.45s/it] 49%|     | 3190/6500 [9:38:54<9:35:38, 10.43s/it]                                                        49%|     | 3190/6500 [9:38:54<9:35:38, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.841433584690094, 'eval_runtime': 4.1767, 'eval_samples_per_second': 5.507, 'eval_steps_per_second': 1.437, 'epoch': 0.49}
                                                        49%|     | 3190/6500 [9:38:59<9:35:38, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3566, 'learning_rate': 5.1450208714316504e-05, 'epoch': 0.49}
{'loss': 0.3651, 'learning_rate': 5.142604518046038e-05, 'epoch': 0.49}
{'loss': 0.3731, 'learning_rate': 5.140188131327486e-05, 'epoch': 0.49}
{'loss': 0.3742, 'learning_rate': 5.1377717118408105e-05, 'epoch': 0.49}
{'loss': 0.368, 'learning_rate': 5.1353552601508356e-05, 'epoch': 0.49}
 49%|     | 3191/6500 [9:39:09<10:51:22, 11.81s/it]                                                         49%|     | 3191/6500 [9:39:09<10:51:22, 11.81s/it] 49%|     | 3192/6500 [9:39:20<10:27:02, 11.37s/it]                                                         49%|     | 3192/6500 [9:39:20<10:27:02, 11.37s/it] 49%|     | 3193/6500 [9:39:30<10:09:50, 11.06s/it]                                                         49%|     | 3193/6500 [9:39:30<10:09:50, 11.06s/it] 49%|     | 3194/6500 [9:39:40<9:57:27, 10.84s/it]                                                         49%|     | 3194/6500 [9:39:40<9:57:27, 10.84s/it] 49%|     | 3195/6500 [9:39:51<9:50:04, 10.71s/it]                                                        49%|     | 3195/6500 [9:39:51<9:50:04, 10.71s/it] 49%|     | 3196/6500 [9:40:01<9:44:18, 10.61s/it]  {'loss': 0.377, 'learning_rate': 5.132938776822391e-05, 'epoch': 0.49}
{'loss': 0.395, 'learning_rate': 5.130522262420316e-05, 'epoch': 0.49}
{'loss': 0.3642, 'learning_rate': 5.128105717509456e-05, 'epoch': 0.49}
{'loss': 0.3895, 'learning_rate': 5.1256891426546625e-05, 'epoch': 0.49}
{'loss': 0.3681, 'learning_rate': 5.123272538420798e-05, 'epoch': 0.49}
                                                      49%|     | 3196/6500 [9:40:01<9:44:18, 10.61s/it] 49%|     | 3197/6500 [9:40:12<9:39:40, 10.53s/it]                                                        49%|     | 3197/6500 [9:40:12<9:39:40, 10.53s/it] 49%|     | 3198/6500 [9:40:22<9:36:56, 10.48s/it]                                                        49%|     | 3198/6500 [9:40:22<9:36:56, 10.48s/it] 49%|     | 3199/6500 [9:40:32<9:34:49, 10.45s/it]                                                        49%|     | 3199/6500 [9:40:32<9:34:49, 10.45s/it] 49%|     | 3200/6500 [9:40:43<9:37:51, 10.51s/it]                                                        49%|     | 3200/6500 [9:40:43<9:37:51, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8462586402893066, 'eval_runtime': 3.9728, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.49}
                                                        49%|     | 3200/6500 [9:40:47<9:37:51, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3628, 'learning_rate': 5.1208559053727257e-05, 'epoch': 0.49}
{'loss': 0.375, 'learning_rate': 5.11843924407532e-05, 'epoch': 0.49}
{'loss': 0.3876, 'learning_rate': 5.1160225550934624e-05, 'epoch': 0.49}
{'loss': 0.3797, 'learning_rate': 5.1136058389920374e-05, 'epoch': 0.49}
{'loss': 0.3714, 'learning_rate': 5.111189096335939e-05, 'epoch': 0.49}
 49%|     | 3201/6500 [9:40:58<10:49:11, 11.81s/it]                                                         49%|     | 3201/6500 [9:40:58<10:49:11, 11.81s/it] 49%|     | 3202/6500 [9:41:08<10:24:55, 11.37s/it]                                                         49%|     | 3202/6500 [9:41:08<10:24:55, 11.37s/it] 49%|     | 3203/6500 [9:41:19<10:07:57, 11.06s/it]                                                         49%|     | 3203/6500 [9:41:19<10:07:57, 11.06s/it] 49%|     | 3204/6500 [9:41:29<9:56:10, 10.85s/it]                                                         49%|     | 3204/6500 [9:41:29<9:56:10, 10.85s/it] 49%|     | 3205/6500 [9:41:39<9:47:48, 10.70s/it]                                                        49%|     | 3205/6500 [9:41:39<9:47:48, 10.70s/it] 49%|     | 3206/6500 [9:41:50<9:41:46, 10.60s/it]  {'loss': 0.3667, 'learning_rate': 5.1087723276900646e-05, 'epoch': 0.49}
{'loss': 0.3778, 'learning_rate': 5.106355533619319e-05, 'epoch': 0.49}
{'loss': 0.4397, 'learning_rate': 5.1039387146886154e-05, 'epoch': 0.49}
{'loss': 0.3671, 'learning_rate': 5.101521871462869e-05, 'epoch': 0.49}
{'loss': 0.3508, 'learning_rate': 5.0991050045070024e-05, 'epoch': 0.49}
                                                      49%|     | 3206/6500 [9:41:50<9:41:46, 10.60s/it] 49%|     | 3207/6500 [9:42:00<9:38:00, 10.53s/it]                                                        49%|     | 3207/6500 [9:42:00<9:38:00, 10.53s/it] 49%|     | 3208/6500 [9:42:10<9:34:48, 10.48s/it]                                                        49%|     | 3208/6500 [9:42:10<9:34:48, 10.48s/it] 49%|     | 3209/6500 [9:42:21<9:32:34, 10.44s/it]                                                        49%|     | 3209/6500 [9:42:21<9:32:34, 10.44s/it] 49%|     | 3210/6500 [9:42:31<9:31:23, 10.42s/it]                                                        49%|     | 3210/6500 [9:42:31<9:31:23, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8297706246376038, 'eval_runtime': 3.9537, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 1.518, 'epoch': 0.49}
                                                        49%|     | 3210/6500 [9:42:35<9:31:23, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3210/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.378, 'learning_rate': 5.0966881143859435e-05, 'epoch': 0.49}
{'loss': 0.8871, 'learning_rate': 5.094271201664625e-05, 'epoch': 0.49}
{'loss': 0.3829, 'learning_rate': 5.091854266907987e-05, 'epoch': 0.49}
{'loss': 0.369, 'learning_rate': 5.089437310680972e-05, 'epoch': 0.49}
{'loss': 0.3766, 'learning_rate': 5.08702033354853e-05, 'epoch': 0.49}
 49%|     | 3211/6500 [9:42:46<10:46:07, 11.79s/it]                                                         49%|     | 3211/6500 [9:42:46<10:46:07, 11.79s/it] 49%|     | 3212/6500 [9:42:56<10:22:55, 11.37s/it]                                                         49%|     | 3212/6500 [9:42:56<10:22:55, 11.37s/it] 49%|     | 3213/6500 [9:43:07<10:06:57, 11.08s/it]                                                         49%|     | 3213/6500 [9:43:07<10:06:57, 11.08s/it] 49%|     | 3214/6500 [9:43:17<9:55:28, 10.87s/it]                                                         49%|     | 3214/6500 [9:43:17<9:55:28, 10.87s/it] 49%|     | 3215/6500 [9:43:28<9:47:45, 10.74s/it]                                                        49%|     | 3215/6500 [9:43:28<9:47:45, 10.74s/it] 49%|     | 3216/6500 [9:43:38<9:47:59, 10.74s/it]  {'loss': 0.3497, 'learning_rate': 5.0846033360756155e-05, 'epoch': 0.49}
{'loss': 0.3916, 'learning_rate': 5.082186318827184e-05, 'epoch': 0.49}
{'loss': 0.3617, 'learning_rate': 5.0797692823682e-05, 'epoch': 0.5}
{'loss': 0.356, 'learning_rate': 5.077352227263632e-05, 'epoch': 0.5}
{'loss': 0.3615, 'learning_rate': 5.07493515407845e-05, 'epoch': 0.5}
                                                      49%|     | 3216/6500 [9:43:38<9:47:59, 10.74s/it] 49%|     | 3217/6500 [9:43:49<9:42:26, 10.64s/it]                                                        49%|     | 3217/6500 [9:43:49<9:42:26, 10.64s/it] 50%|     | 3218/6500 [9:43:59<9:38:20, 10.57s/it]                                                        50%|     | 3218/6500 [9:43:59<9:38:20, 10.57s/it] 50%|     | 3219/6500 [9:44:10<9:35:31, 10.52s/it]                                                        50%|     | 3219/6500 [9:44:10<9:35:31, 10.52s/it] 50%|     | 3220/6500 [9:44:20<9:33:33, 10.49s/it]                                                        50%|     | 3220/6500 [9:44:20<9:33:33, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8381326794624329, 'eval_runtime': 4.1905, 'eval_samples_per_second': 5.489, 'eval_steps_per_second': 1.432, 'epoch': 0.5}
                                                        50%|     | 3220/6500 [9:44:24<9:33:33, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3678, 'learning_rate': 5.0725180633776315e-05, 'epoch': 0.5}
{'loss': 0.3709, 'learning_rate': 5.070100955726159e-05, 'epoch': 0.5}
{'loss': 0.3581, 'learning_rate': 5.0676838316890116e-05, 'epoch': 0.5}
{'loss': 0.3842, 'learning_rate': 5.065266691831181e-05, 'epoch': 0.5}
{'loss': 0.3642, 'learning_rate': 5.0628495367176576e-05, 'epoch': 0.5}
 50%|     | 3221/6500 [9:44:38<11:31:06, 12.65s/it]                                                         50%|     | 3221/6500 [9:44:38<11:31:06, 12.65s/it] 50%|     | 3222/6500 [9:44:48<10:57:03, 12.03s/it]                                                         50%|     | 3222/6500 [9:44:48<10:57:03, 12.03s/it] 50%|     | 3223/6500 [9:44:59<10:29:20, 11.52s/it]                                                         50%|     | 3223/6500 [9:44:59<10:29:20, 11.52s/it] 50%|     | 3224/6500 [9:45:09<10:09:45, 11.17s/it]                                                         50%|     | 3224/6500 [9:45:09<10:09:45, 11.17s/it] 50%|     | 3225/6500 [9:45:19<9:56:07, 10.92s/it]                                                         50%|     | 3225/6500 [9:45:19<9:56:07, 10.92s/it] 50%|     | 3226/6500 [9:45:30<9:46:38, 10.75s/it]{'loss': 0.3828, 'learning_rate': 5.060432366913438e-05, 'epoch': 0.5}
{'loss': 0.372, 'learning_rate': 5.058015182983519e-05, 'epoch': 0.5}
{'loss': 0.3666, 'learning_rate': 5.055597985492906e-05, 'epoch': 0.5}
{'loss': 0.3783, 'learning_rate': 5.053180775006599e-05, 'epoch': 0.5}
{'loss': 0.377, 'learning_rate': 5.050763552089611e-05, 'epoch': 0.5}
                                                        50%|     | 3226/6500 [9:45:30<9:46:38, 10.75s/it] 50%|     | 3227/6500 [9:45:40<9:39:54, 10.63s/it]                                                        50%|     | 3227/6500 [9:45:40<9:39:54, 10.63s/it] 50%|     | 3228/6500 [9:45:50<9:35:03, 10.55s/it]                                                        50%|     | 3228/6500 [9:45:50<9:35:03, 10.55s/it] 50%|     | 3229/6500 [9:46:01<9:31:51, 10.49s/it]                                                        50%|     | 3229/6500 [9:46:01<9:31:51, 10.49s/it] 50%|     | 3230/6500 [9:46:11<9:29:46, 10.45s/it]                                                        50%|     | 3230/6500 [9:46:11<9:29:46, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8489766120910645, 'eval_runtime': 4.0292, 'eval_samples_per_second': 5.708, 'eval_steps_per_second': 1.489, 'epoch': 0.5}
                                                        50%|     | 3230/6500 [9:46:15<9:29:46, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3230
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3724, 'learning_rate': 5.04834631730695e-05, 'epoch': 0.5}
{'loss': 0.3729, 'learning_rate': 5.0459290712236326e-05, 'epoch': 0.5}
{'loss': 0.3872, 'learning_rate': 5.043511814404673e-05, 'epoch': 0.5}
{'loss': 0.3589, 'learning_rate': 5.0410945474150916e-05, 'epoch': 0.5}
{'loss': 0.3606, 'learning_rate': 5.0386772708199104e-05, 'epoch': 0.5}
 50%|     | 3231/6500 [9:46:26<10:45:25, 11.85s/it]                                                         50%|     | 3231/6500 [9:46:26<10:45:25, 11.85s/it] 50%|     | 3232/6500 [9:46:37<10:21:02, 11.40s/it]                                                         50%|     | 3232/6500 [9:46:37<10:21:02, 11.40s/it] 50%|     | 3233/6500 [9:46:47<10:08:22, 11.17s/it]                                                         50%|     | 3233/6500 [9:46:47<10:08:22, 11.17s/it] 50%|     | 3234/6500 [9:46:58<9:54:53, 10.93s/it]                                                         50%|     | 3234/6500 [9:46:58<9:54:53, 10.93s/it] 50%|     | 3235/6500 [9:47:08<9:45:24, 10.76s/it]                                                        50%|     | 3235/6500 [9:47:08<9:45:24, 10.76s/it] 50%|     | 3236/6500 [9:47:18<9:41:21, 10.69s/it]  {'loss': 0.346, 'learning_rate': 5.036259985184151e-05, 'epoch': 0.5}
{'loss': 0.3949, 'learning_rate': 5.033842691072841e-05, 'epoch': 0.5}
{'loss': 0.4275, 'learning_rate': 5.031425389051009e-05, 'epoch': 0.5}
{'loss': 0.3623, 'learning_rate': 5.0290080796836826e-05, 'epoch': 0.5}
{'loss': 0.3741, 'learning_rate': 5.0265907635358934e-05, 'epoch': 0.5}
                                                      50%|     | 3236/6500 [9:47:18<9:41:21, 10.69s/it] 50%|     | 3237/6500 [9:47:29<9:36:15, 10.60s/it]                                                        50%|     | 3237/6500 [9:47:29<9:36:15, 10.60s/it] 50%|     | 3238/6500 [9:47:39<9:32:43, 10.53s/it]                                                        50%|     | 3238/6500 [9:47:39<9:32:43, 10.53s/it] 50%|     | 3239/6500 [9:47:50<9:30:18, 10.49s/it]                                                        50%|     | 3239/6500 [9:47:50<9:30:18, 10.49s/it] 50%|     | 3240/6500 [9:48:00<9:28:30, 10.46s/it]                                                        50%|     | 3240/6500 [9:48:00<9:28:30, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.83162921667099, 'eval_runtime': 4.1652, 'eval_samples_per_second': 5.522, 'eval_steps_per_second': 1.44, 'epoch': 0.5}
                                                        50%|     | 3240/6500 [9:48:04<9:28:30, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3618, 'learning_rate': 5.024173441172675e-05, 'epoch': 0.5}
{'loss': 0.8925, 'learning_rate': 5.021756113159062e-05, 'epoch': 0.5}
{'loss': 0.3822, 'learning_rate': 5.01933878006009e-05, 'epoch': 0.5}
{'loss': 0.3781, 'learning_rate': 5.0169214424407965e-05, 'epoch': 0.5}
{'loss': 0.3484, 'learning_rate': 5.0145041008662166e-05, 'epoch': 0.5}
 50%|     | 3241/6500 [9:48:15<10:45:04, 11.88s/it]                                                         50%|     | 3241/6500 [9:48:15<10:45:04, 11.88s/it] 50%|     | 3242/6500 [9:48:26<10:20:28, 11.43s/it]                                                         50%|     | 3242/6500 [9:48:26<10:20:28, 11.43s/it] 50%|     | 3243/6500 [9:48:36<10:03:24, 11.12s/it]                                                         50%|     | 3243/6500 [9:48:36<10:03:24, 11.12s/it] 50%|     | 3244/6500 [9:48:46<9:51:12, 10.89s/it]                                                         50%|     | 3244/6500 [9:48:46<9:51:12, 10.89s/it] 50%|     | 3245/6500 [9:48:57<9:42:38, 10.74s/it]                                                        50%|     | 3245/6500 [9:48:57<9:42:38, 10.74s/it] 50%|     | 3246/6500 [9:49:07<9:36:45, 10.63s/it]  {'loss': 0.3676, 'learning_rate': 5.012086755901393e-05, 'epoch': 0.5}
{'loss': 0.3954, 'learning_rate': 5.0096694081113625e-05, 'epoch': 0.5}
{'loss': 0.3449, 'learning_rate': 5.007252058061167e-05, 'epoch': 0.5}
{'loss': 0.3728, 'learning_rate': 5.0048347063158485e-05, 'epoch': 0.5}
{'loss': 0.3641, 'learning_rate': 5.002417353440445e-05, 'epoch': 0.5}
                                                      50%|     | 3246/6500 [9:49:07<9:36:45, 10.63s/it] 50%|     | 3247/6500 [9:49:17<9:32:17, 10.56s/it]                                                        50%|     | 3247/6500 [9:49:17<9:32:17, 10.56s/it] 50%|     | 3248/6500 [9:49:28<9:29:24, 10.51s/it]                                                        50%|     | 3248/6500 [9:49:28<9:29:24, 10.51s/it] 50%|     | 3249/6500 [9:49:39<9:34:33, 10.60s/it]                                                        50%|     | 3249/6500 [9:49:39<9:34:33, 10.60s/it] 50%|     | 3250/6500 [9:49:49<9:30:43, 10.54s/it]                                                        50%|     | 3250/6500 [9:49:49<9:30:43, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8373556733131409, 'eval_runtime': 4.3926, 'eval_samples_per_second': 5.236, 'eval_steps_per_second': 1.366, 'epoch': 0.5}
                                                        50%|     | 3250/6500 [9:49:53<9:30:43, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.377, 'learning_rate': 5e-05, 'epoch': 0.5}
{'loss': 0.3701, 'learning_rate': 4.997582646559556e-05, 'epoch': 0.5}
{'loss': 0.3745, 'learning_rate': 4.9951652936841527e-05, 'epoch': 0.5}
{'loss': 0.3875, 'learning_rate': 4.992747941938834e-05, 'epoch': 0.5}
{'loss': 0.3639, 'learning_rate': 4.990330591888639e-05, 'epoch': 0.5}
 50%|     | 3251/6500 [9:50:04<10:47:48, 11.96s/it]                                                         50%|     | 3251/6500 [9:50:04<10:47:48, 11.96s/it] 50%|     | 3252/6500 [9:50:15<10:21:47, 11.49s/it]                                                         50%|     | 3252/6500 [9:50:15<10:21:47, 11.49s/it] 50%|     | 3253/6500 [9:50:25<10:05:54, 11.20s/it]                                                         50%|     | 3253/6500 [9:50:25<10:05:54, 11.20s/it] 50%|     | 3254/6500 [9:50:36<9:52:17, 10.95s/it]                                                         50%|     | 3254/6500 [9:50:36<9:52:17, 10.95s/it] 50%|     | 3255/6500 [9:50:46<9:42:47, 10.78s/it]                                                        50%|     | 3255/6500 [9:50:46<9:42:47, 10.78s/it] 50%|     | 3256/6500 [9:50:56<9:36:13, 10.66s/it]  {'loss': 0.4058, 'learning_rate': 4.987913244098609e-05, 'epoch': 0.5}
{'loss': 0.3732, 'learning_rate': 4.985495899133784e-05, 'epoch': 0.5}
{'loss': 0.3696, 'learning_rate': 4.9830785575592054e-05, 'epoch': 0.5}
{'loss': 0.3898, 'learning_rate': 4.98066121993991e-05, 'epoch': 0.5}
{'loss': 0.3716, 'learning_rate': 4.978243886840939e-05, 'epoch': 0.5}
                                                      50%|     | 3256/6500 [9:50:56<9:36:13, 10.66s/it] 50%|     | 3257/6500 [9:51:07<9:31:26, 10.57s/it]                                                        50%|     | 3257/6500 [9:51:07<9:31:26, 10.57s/it] 50%|     | 3258/6500 [9:51:17<9:28:15, 10.52s/it]                                                        50%|     | 3258/6500 [9:51:17<9:28:15, 10.52s/it] 50%|     | 3259/6500 [9:51:28<9:26:01, 10.48s/it]                                                        50%|     | 3259/6500 [9:51:28<9:26:01, 10.48s/it] 50%|     | 3260/6500 [9:51:38<9:24:21, 10.45s/it]                                                        50%|     | 3260/6500 [9:51:38<9:24:21, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8464929461479187, 'eval_runtime': 3.9634, 'eval_samples_per_second': 5.803, 'eval_steps_per_second': 1.514, 'epoch': 0.5}
                                                        50%|     | 3260/6500 [9:51:42<9:24:21, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3260/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3858, 'learning_rate': 4.9758265588273264e-05, 'epoch': 0.5}
{'loss': 0.366, 'learning_rate': 4.973409236464108e-05, 'epoch': 0.5}
{'loss': 0.3837, 'learning_rate': 4.9709919203163185e-05, 'epoch': 0.5}
{'loss': 0.359, 'learning_rate': 4.968574610948992e-05, 'epoch': 0.5}
{'loss': 0.3675, 'learning_rate': 4.96615730892716e-05, 'epoch': 0.5}
 50%|     | 3261/6500 [9:51:53<10:36:35, 11.79s/it]                                                         50%|     | 3261/6500 [9:51:53<10:36:35, 11.79s/it] 50%|     | 3262/6500 [9:52:03<10:13:29, 11.37s/it]                                                         50%|     | 3262/6500 [9:52:03<10:13:29, 11.37s/it] 50%|     | 3263/6500 [9:52:14<9:57:14, 11.07s/it]                                                         50%|     | 3263/6500 [9:52:14<9:57:14, 11.07s/it] 50%|     | 3264/6500 [9:52:24<9:45:43, 10.86s/it]                                                        50%|     | 3264/6500 [9:52:24<9:45:43, 10.86s/it] 50%|     | 3265/6500 [9:52:35<9:41:33, 10.79s/it]                                                        50%|     | 3265/6500 [9:52:35<9:41:33, 10.79s/it] 50%|     | 3266/6500 [9:52:45<9:34:42, 10.66s/it]     {'loss': 0.3523, 'learning_rate': 4.9637400148158504e-05, 'epoch': 0.5}
{'loss': 0.4455, 'learning_rate': 4.9613227291800914e-05, 'epoch': 0.5}
{'loss': 0.3728, 'learning_rate': 4.9589054525849096e-05, 'epoch': 0.5}
{'loss': 0.358, 'learning_rate': 4.956488185595328e-05, 'epoch': 0.5}
{'loss': 0.3767, 'learning_rate': 4.9540709287763685e-05, 'epoch': 0.5}
                                                   50%|     | 3266/6500 [9:52:45<9:34:42, 10.66s/it] 50%|     | 3267/6500 [9:52:55<9:29:45, 10.57s/it]                                                        50%|     | 3267/6500 [9:52:55<9:29:45, 10.57s/it] 50%|     | 3268/6500 [9:53:06<9:26:05, 10.51s/it]                                                        50%|     | 3268/6500 [9:53:06<9:26:05, 10.51s/it] 50%|     | 3269/6500 [9:53:16<9:23:24, 10.46s/it]                                                        50%|     | 3269/6500 [9:53:16<9:23:24, 10.46s/it] 50%|     | 3270/6500 [9:53:26<9:21:43, 10.43s/it]                                                        50%|     | 3270/6500 [9:53:26<9:21:43, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8318712115287781, 'eval_runtime': 3.975, 'eval_samples_per_second': 5.786, 'eval_steps_per_second': 1.509, 'epoch': 0.5}
                                                        50%|     | 3270/6500 [9:53:30<9:21:43, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8921, 'learning_rate': 4.9516536826930515e-05, 'epoch': 0.5}
{'loss': 0.3807, 'learning_rate': 4.9492364479103914e-05, 'epoch': 0.5}
{'loss': 0.3735, 'learning_rate': 4.9468192249934025e-05, 'epoch': 0.5}
{'loss': 0.3736, 'learning_rate': 4.944402014507097e-05, 'epoch': 0.5}
{'loss': 0.3456, 'learning_rate': 4.9419848170164815e-05, 'epoch': 0.5}
 50%|     | 3271/6500 [9:53:41<10:33:09, 11.77s/it]                                                         50%|     | 3271/6500 [9:53:41<10:33:09, 11.77s/it] 50%|     | 3272/6500 [9:53:52<10:10:27, 11.35s/it]                                                         50%|     | 3272/6500 [9:53:52<10:10:27, 11.35s/it] 50%|     | 3273/6500 [9:54:02<9:54:36, 11.06s/it]                                                         50%|     | 3273/6500 [9:54:02<9:54:36, 11.06s/it] 50%|     | 3274/6500 [9:54:12<9:43:20, 10.85s/it]                                                        50%|     | 3274/6500 [9:54:12<9:43:20, 10.85s/it] 50%|     | 3275/6500 [9:54:23<9:35:20, 10.70s/it]                                                        50%|     | 3275/6500 [9:54:23<9:35:20, 10.70s/it] 50%|     | 3276/6500 [9:54:33<9:29:43, 10.60s/it]     {'loss': 0.3754, 'learning_rate': 4.939567633086563e-05, 'epoch': 0.5}
{'loss': 0.3688, 'learning_rate': 4.937150463282344e-05, 'epoch': 0.5}
{'loss': 0.3311, 'learning_rate': 4.934733308168821e-05, 'epoch': 0.5}
{'loss': 0.3691, 'learning_rate': 4.93231616831099e-05, 'epoch': 0.5}
{'loss': 0.3468, 'learning_rate': 4.929899044273843e-05, 'epoch': 0.5}
                                                   50%|     | 3276/6500 [9:54:33<9:29:43, 10.60s/it] 50%|     | 3277/6500 [9:54:43<9:25:37, 10.53s/it]                                                        50%|     | 3277/6500 [9:54:43<9:25:37, 10.53s/it] 50%|     | 3278/6500 [9:54:54<9:22:42, 10.48s/it]                                                        50%|     | 3278/6500 [9:54:54<9:22:42, 10.48s/it] 50%|     | 3279/6500 [9:55:04<9:20:44, 10.45s/it]                                                        50%|     | 3279/6500 [9:55:04<9:20:44, 10.45s/it] 50%|     | 3280/6500 [9:55:15<9:19:17, 10.42s/it]                                                        50%|     | 3280/6500 [9:55:15<9:19:17, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8389554023742676, 'eval_runtime': 3.9448, 'eval_samples_per_second': 5.83, 'eval_steps_per_second': 1.521, 'epoch': 0.5}
                                                        50%|     | 3280/6500 [9:55:19<9:19:17, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3791, 'learning_rate': 4.927481936622369e-05, 'epoch': 0.5}
{'loss': 0.3545, 'learning_rate': 4.925064845921552e-05, 'epoch': 0.5}
{'loss': 0.3756, 'learning_rate': 4.922647772736371e-05, 'epoch': 0.51}
{'loss': 0.3635, 'learning_rate': 4.920230717631802e-05, 'epoch': 0.51}
{'loss': 0.3782, 'learning_rate': 4.917813681172818e-05, 'epoch': 0.51}
 50%|     | 3281/6500 [9:55:30<10:44:47, 12.02s/it]                                                         50%|     | 3281/6500 [9:55:30<10:44:47, 12.02s/it] 50%|     | 3282/6500 [9:55:41<10:17:57, 11.52s/it]                                                         50%|     | 3282/6500 [9:55:41<10:17:57, 11.52s/it] 51%|     | 3283/6500 [9:55:51<9:58:53, 11.17s/it]                                                         51%|     | 3283/6500 [9:55:51<9:58:53, 11.17s/it] 51%|     | 3284/6500 [9:56:01<9:45:42, 10.93s/it]                                                        51%|     | 3284/6500 [9:56:01<9:45:42, 10.93s/it] 51%|     | 3285/6500 [9:56:12<9:36:20, 10.76s/it]                                                        51%|     | 3285/6500 [9:56:12<9:36:20, 10.76s/it] 51%|     | 3286/6500 [9:56:22<9:29:47, 10.64s/it]     {'loss': 0.4065, 'learning_rate': 4.9153966639243864e-05, 'epoch': 0.51}
{'loss': 0.3824, 'learning_rate': 4.91297966645147e-05, 'epoch': 0.51}
{'loss': 0.3821, 'learning_rate': 4.910562689319029e-05, 'epoch': 0.51}
{'loss': 0.3763, 'learning_rate': 4.908145733092013e-05, 'epoch': 0.51}
{'loss': 0.3621, 'learning_rate': 4.9057287983353745e-05, 'epoch': 0.51}
                                                   51%|     | 3286/6500 [9:56:22<9:29:47, 10.64s/it] 51%|     | 3287/6500 [9:56:32<9:25:14, 10.56s/it]                                                        51%|     | 3287/6500 [9:56:32<9:25:14, 10.56s/it] 51%|     | 3288/6500 [9:56:43<9:21:53, 10.50s/it]                                                        51%|     | 3288/6500 [9:56:43<9:21:53, 10.50s/it] 51%|     | 3289/6500 [9:56:53<9:19:55, 10.46s/it]                                                        51%|     | 3289/6500 [9:56:53<9:19:55, 10.46s/it] 51%|     | 3290/6500 [9:57:04<9:18:06, 10.43s/it]                                                        51%|     | 3290/6500 [9:57:04<9:18:06, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8433348536491394, 'eval_runtime': 3.9633, 'eval_samples_per_second': 5.803, 'eval_steps_per_second': 1.514, 'epoch': 0.51}
                                                        51%|     | 3290/6500 [9:57:08<9:18:06, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.365, 'learning_rate': 4.903311885614058e-05, 'epoch': 0.51}
{'loss': 0.394, 'learning_rate': 4.900894995492998e-05, 'epoch': 0.51}
{'loss': 0.3692, 'learning_rate': 4.898478128537131e-05, 'epoch': 0.51}
{'loss': 0.3617, 'learning_rate': 4.8960612853113844e-05, 'epoch': 0.51}
{'loss': 0.3561, 'learning_rate': 4.89364446638068e-05, 'epoch': 0.51}
 51%|     | 3291/6500 [9:57:18<10:29:16, 11.77s/it]                                                         51%|     | 3291/6500 [9:57:18<10:29:16, 11.77s/it] 51%|     | 3292/6500 [9:57:29<10:07:15, 11.36s/it]                                                         51%|     | 3292/6500 [9:57:29<10:07:15, 11.36s/it] 51%|     | 3293/6500 [9:57:39<9:52:01, 11.08s/it]                                                         51%|     | 3293/6500 [9:57:39<9:52:01, 11.08s/it] 51%|     | 3294/6500 [9:57:50<9:41:11, 10.88s/it]                                                        51%|     | 3294/6500 [9:57:50<9:41:11, 10.88s/it] 51%|     | 3295/6500 [9:58:00<9:33:36, 10.74s/it]                                                        51%|     | 3295/6500 [9:58:00<9:33:36, 10.74s/it] 51%|     | 3296/6500 [9:58:10<9:27:58, 10.64s/it]     {'loss': 0.3882, 'learning_rate': 4.891227672309935e-05, 'epoch': 0.51}
{'loss': 0.4299, 'learning_rate': 4.888810903664062e-05, 'epoch': 0.51}
{'loss': 0.3672, 'learning_rate': 4.886394161007963e-05, 'epoch': 0.51}
{'loss': 0.3621, 'learning_rate': 4.883977444906538e-05, 'epoch': 0.51}
{'loss': 0.3665, 'learning_rate': 4.881560755924679e-05, 'epoch': 0.51}
                                                   51%|     | 3296/6500 [9:58:10<9:27:58, 10.64s/it] 51%|     | 3297/6500 [9:58:21<9:28:24, 10.65s/it]                                                        51%|     | 3297/6500 [9:58:21<9:28:24, 10.65s/it] 51%|     | 3298/6500 [9:58:32<9:24:36, 10.58s/it]                                                        51%|     | 3298/6500 [9:58:32<9:24:36, 10.58s/it] 51%|     | 3299/6500 [9:58:42<9:21:32, 10.53s/it]                                                        51%|     | 3299/6500 [9:58:42<9:21:32, 10.53s/it] 51%|     | 3300/6500 [9:58:52<9:19:26, 10.49s/it]                                                        51%|     | 3300/6500 [9:58:52<9:19:26, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8348893523216248, 'eval_runtime': 3.9698, 'eval_samples_per_second': 5.794, 'eval_steps_per_second': 1.511, 'epoch': 0.51}
                                                        51%|     | 3300/6500 [9:58:56<9:19:26, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3300/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8821, 'learning_rate': 4.8791440946272735e-05, 'epoch': 0.51}
{'loss': 0.3782, 'learning_rate': 4.876727461579203e-05, 'epoch': 0.51}
{'loss': 0.3653, 'learning_rate': 4.874310857345337e-05, 'epoch': 0.51}
{'loss': 0.3614, 'learning_rate': 4.8718942824905445e-05, 'epoch': 0.51}
{'loss': 0.3606, 'learning_rate': 4.8694777375796844e-05, 'epoch': 0.51}
 51%|     | 3301/6500 [9:59:07<10:29:54, 11.81s/it]                                                         51%|     | 3301/6500 [9:59:07<10:29:54, 11.81s/it] 51%|     | 3302/6500 [9:59:18<10:07:10, 11.39s/it]                                                         51%|     | 3302/6500 [9:59:18<10:07:10, 11.39s/it] 51%|     | 3303/6500 [9:59:28<9:51:07, 11.09s/it]                                                         51%|     | 3303/6500 [9:59:28<9:51:07, 11.09s/it] 51%|     | 3304/6500 [9:59:38<9:39:31, 10.88s/it]                                                        51%|     | 3304/6500 [9:59:38<9:39:31, 10.88s/it] 51%|     | 3305/6500 [9:59:49<9:31:31, 10.73s/it]                                                        51%|     | 3305/6500 [9:59:49<9:31:31, 10.73s/it] 51%|     | 3306/6500 [9:59:59<9:25:43, 10.63s/it]     {'loss': 0.3921, 'learning_rate': 4.867061223177609e-05, 'epoch': 0.51}
{'loss': 0.3545, 'learning_rate': 4.864644739849165e-05, 'epoch': 0.51}
{'loss': 0.3637, 'learning_rate': 4.8622282881591906e-05, 'epoch': 0.51}
{'loss': 0.3471, 'learning_rate': 4.859811868672515e-05, 'epoch': 0.51}
{'loss': 0.36, 'learning_rate': 4.8573954819539634e-05, 'epoch': 0.51}
                                                   51%|     | 3306/6500 [9:59:59<9:25:43, 10.63s/it] 51%|     | 3307/6500 [10:00:10<9:21:40, 10.55s/it]                                                         51%|     | 3307/6500 [10:00:10<9:21:40, 10.55s/it] 51%|     | 3308/6500 [10:00:20<9:19:03, 10.51s/it]                                                         51%|     | 3308/6500 [10:00:20<9:19:03, 10.51s/it] 51%|     | 3309/6500 [10:00:30<9:17:00, 10.47s/it]                                                         51%|     | 3309/6500 [10:00:30<9:17:00, 10.47s/it] 51%|     | 3310/6500 [10:00:41<9:15:31, 10.45s/it]                                                         51%|     | 3310/6500 [10:00:41<9:15:31, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8393617868423462, 'eval_runtime': 3.9584, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.51}
                                                         51%|     | 3310/6500 [10:00:45<9:15:31, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3677, 'learning_rate': 4.854979128568351e-05, 'epoch': 0.51}
{'loss': 0.3606, 'learning_rate': 4.852562809080485e-05, 'epoch': 0.51}
{'loss': 0.3844, 'learning_rate': 4.8501465240551666e-05, 'epoch': 0.51}
{'loss': 0.363, 'learning_rate': 4.8477302740571866e-05, 'epoch': 0.51}
{'loss': 0.3772, 'learning_rate': 4.84531405965133e-05, 'epoch': 0.51}
 51%|     | 3311/6500 [10:00:56<10:25:17, 11.76s/it]                                                          51%|     | 3311/6500 [10:00:56<10:25:17, 11.76s/it] 51%|     | 3312/6500 [10:01:06<10:02:39, 11.34s/it]                                                          51%|     | 3312/6500 [10:01:06<10:02:39, 11.34s/it] 51%|     | 3313/6500 [10:01:17<9:52:51, 11.16s/it]                                                          51%|     | 3313/6500 [10:01:17<9:52:51, 11.16s/it] 51%|     | 3314/6500 [10:01:27<9:39:57, 10.92s/it]                                                         51%|     | 3314/6500 [10:01:27<9:39:57, 10.92s/it] 51%|     | 3315/6500 [10:01:37<9:30:51, 10.75s/it]                                                         51%|     | 3315/6500 [10:01:37<9:30:51, 10.75s/it] 51%|     | 3316/6500 [10:01:48<9:24:58, {'loss': 0.3617, 'learning_rate': 4.84289788140237e-05, 'epoch': 0.51}
{'loss': 0.3522, 'learning_rate': 4.8404817398750756e-05, 'epoch': 0.51}
{'loss': 0.3709, 'learning_rate': 4.838065635634205e-05, 'epoch': 0.51}
{'loss': 0.3609, 'learning_rate': 4.835649569244508e-05, 'epoch': 0.51}
{'loss': 0.3603, 'learning_rate': 4.833233541270724e-05, 'epoch': 0.51}
10.65s/it]                                                         51%|     | 3316/6500 [10:01:48<9:24:58, 10.65s/it] 51%|     | 3317/6500 [10:01:58<9:20:20, 10.56s/it]                                                         51%|     | 3317/6500 [10:01:58<9:20:20, 10.56s/it] 51%|     | 3318/6500 [10:02:09<9:17:06, 10.50s/it]                                                         51%|     | 3318/6500 [10:02:09<9:17:06, 10.50s/it] 51%|     | 3319/6500 [10:02:19<9:14:30, 10.46s/it]                                                         51%|     | 3319/6500 [10:02:19<9:14:30, 10.46s/it] 51%|     | 3320/6500 [10:02:29<9:15:06, 10.47s/it]                                                         51%|     | 3320/6500 [10:02:30<9:15:06, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8486696481704712, 'eval_runtime': 4.9964, 'eval_samples_per_second': 4.603, 'eval_steps_per_second': 1.201, 'epoch': 0.51}
                                                         51%|     | 3320/6500 [10:02:34<9:15:06, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3320
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3320/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3665, 'learning_rate': 4.8308175522775865e-05, 'epoch': 0.51}
{'loss': 0.3899, 'learning_rate': 4.828401602829816e-05, 'epoch': 0.51}
{'loss': 0.3515, 'learning_rate': 4.825985693492129e-05, 'epoch': 0.51}
{'loss': 0.3704, 'learning_rate': 4.823569824829227e-05, 'epoch': 0.51}
{'loss': 0.344, 'learning_rate': 4.821153997405807e-05, 'epoch': 0.51}
 51%|     | 3321/6500 [10:02:45<10:40:55, 12.10s/it]                                                          51%|     | 3321/6500 [10:02:45<10:40:55, 12.10s/it] 51%|     | 3322/6500 [10:02:56<10:13:21, 11.58s/it]                                                          51%|     | 3322/6500 [10:02:56<10:13:21, 11.58s/it] 51%|     | 3323/6500 [10:03:06<9:53:46, 11.21s/it]                                                          51%|     | 3323/6500 [10:03:06<9:53:46, 11.21s/it] 51%|     | 3324/6500 [10:03:16<9:39:57, 10.96s/it]                                                         51%|     | 3324/6500 [10:03:16<9:39:57, 10.96s/it] 51%|     | 3325/6500 [10:03:27<9:30:13, 10.78s/it]                                                         51%|     | 3325/6500 [10:03:27<9:30:13, 10.78s/it] 51%|     | 3326/6500 [10:03:37<9:23:32, {'loss': 0.401, 'learning_rate': 4.8187382117865525e-05, 'epoch': 0.51}
{'loss': 0.4097, 'learning_rate': 4.816322468536139e-05, 'epoch': 0.51}
{'loss': 0.3649, 'learning_rate': 4.8139067682192303e-05, 'epoch': 0.51}
{'loss': 0.365, 'learning_rate': 4.811491111400484e-05, 'epoch': 0.51}
{'loss': 0.368, 'learning_rate': 4.8090754986445454e-05, 'epoch': 0.51}
10.65s/it]                                                         51%|     | 3326/6500 [10:03:37<9:23:32, 10.65s/it] 51%|     | 3327/6500 [10:03:48<9:18:46, 10.57s/it]                                                         51%|     | 3327/6500 [10:03:48<9:18:46, 10.57s/it] 51%|     | 3328/6500 [10:03:58<9:15:36, 10.51s/it]                                                         51%|     | 3328/6500 [10:03:58<9:15:36, 10.51s/it] 51%|     | 3329/6500 [10:04:09<9:17:02, 10.54s/it]                                                         51%|     | 3329/6500 [10:04:09<9:17:02, 10.54s/it] 51%|     | 3330/6500 [10:04:20<9:25:31, 10.70s/it]                                                         51%|     | 3330/6500 [10:04:20<9:25:31, 10.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8369520306587219, 'eval_runtime': 3.9672, 'eval_samples_per_second': 5.798, 'eval_steps_per_second': 1.512, 'epoch': 0.51}
                                                         51%|     | 3330/6500 [10:04:24<9:25:31, 10.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3330the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3330

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3330/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8812, 'learning_rate': 4.806659930516047e-05, 'epoch': 0.51}
{'loss': 0.3844, 'learning_rate': 4.804244407579613e-05, 'epoch': 0.51}
{'loss': 0.3845, 'learning_rate': 4.8018289303998604e-05, 'epoch': 0.51}
{'loss': 0.3451, 'learning_rate': 4.79941349954139e-05, 'epoch': 0.51}
{'loss': 0.381, 'learning_rate': 4.796998115568794e-05, 'epoch': 0.51}
 51%|     | 3331/6500 [10:04:34<10:31:15, 11.95s/it]                                                          51%|     | 3331/6500 [10:04:34<10:31:15, 11.95s/it] 51%|    | 3332/6500 [10:04:45<10:05:47, 11.47s/it]                                                          51%|    | 3332/6500 [10:04:45<10:05:47, 11.47s/it] 51%|    | 3333/6500 [10:04:55<9:47:39, 11.13s/it]                                                          51%|    | 3333/6500 [10:04:55<9:47:39, 11.13s/it] 51%|    | 3334/6500 [10:05:06<9:35:01, 10.90s/it]                                                         51%|    | 3334/6500 [10:05:06<9:35:01, 10.90s/it] 51%|    | 3335/6500 [10:05:16<9:26:23, 10.74s/it]                                                         51%|    | 3335/6500 [10:05:16<9:26:23, 10.74s/it] 51%|    | 3336/6500 [{'loss': 0.3681, 'learning_rate': 4.7945827790466554e-05, 'epoch': 0.51}
{'loss': 0.3402, 'learning_rate': 4.792167490539542e-05, 'epoch': 0.51}
{'loss': 0.3613, 'learning_rate': 4.789752250612014e-05, 'epoch': 0.51}
{'loss': 0.3636, 'learning_rate': 4.787337059828619e-05, 'epoch': 0.51}
{'loss': 0.3607, 'learning_rate': 4.7849219187538944e-05, 'epoch': 0.51}
10:05:26<9:20:06, 10.62s/it]                                                         51%|    | 3336/6500 [10:05:26<9:20:06, 10.62s/it] 51%|    | 3337/6500 [10:05:37<9:15:50, 10.54s/it]                                                         51%|    | 3337/6500 [10:05:37<9:15:50, 10.54s/it] 51%|    | 3338/6500 [10:05:47<9:13:03, 10.49s/it]                                                         51%|    | 3338/6500 [10:05:47<9:13:03, 10.49s/it] 51%|    | 3339/6500 [10:05:57<9:10:39, 10.45s/it]                                                         51%|    | 3339/6500 [10:05:57<9:10:39, 10.45s/it] 51%|    | 3340/6500 [10:06:08<9:09:12, 10.43s/it]                                                         51%|    | 3340/6500 [10:06:08<9:09:12, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8378474116325378, 'eval_runtime': 3.9423, 'eval_samples_per_second': 5.834, 'eval_steps_per_second': 1.522, 'epoch': 0.51}
                                                         51%|    | 3340/6500 [10:06:12<9:09:12, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3340
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3340/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3622, 'learning_rate': 4.782506827952364e-05, 'epoch': 0.51}
{'loss': 0.3624, 'learning_rate': 4.780091787988539e-05, 'epoch': 0.51}
{'loss': 0.364, 'learning_rate': 4.777676799426921e-05, 'epoch': 0.51}
{'loss': 0.3624, 'learning_rate': 4.775261862832e-05, 'epoch': 0.51}
{'loss': 0.3937, 'learning_rate': 4.772846978768252e-05, 'epoch': 0.51}
 51%|    | 3341/6500 [10:06:23<10:19:47, 11.77s/it]                                                          51%|    | 3341/6500 [10:06:23<10:19:47, 11.77s/it] 51%|    | 3342/6500 [10:06:33<9:57:09, 11.35s/it]                                                          51%|    | 3342/6500 [10:06:33<9:57:09, 11.35s/it] 51%|    | 3343/6500 [10:06:43<9:41:10, 11.05s/it]                                                         51%|    | 3343/6500 [10:06:43<9:41:10, 11.05s/it] 51%|    | 3344/6500 [10:06:54<9:30:14, 10.84s/it]                                                         51%|    | 3344/6500 [10:06:54<9:30:14, 10.84s/it] 51%|    | 3345/6500 [10:07:04<9:22:28, 10.70s/it]                                                         51%|    | 3345/6500 [10:07:04<9:22:28, 10.70s/it] 51%|    | 3346/6500 {'loss': 0.3657, 'learning_rate': 4.7704321478001415e-05, 'epoch': 0.51}
{'loss': 0.3752, 'learning_rate': 4.768017370492121e-05, 'epoch': 0.51}
{'loss': 0.3829, 'learning_rate': 4.76560264740863e-05, 'epoch': 0.52}
{'loss': 0.3567, 'learning_rate': 4.7631879791140946e-05, 'epoch': 0.52}
{'loss': 0.3691, 'learning_rate': 4.760773366172929e-05, 'epoch': 0.52}
[10:07:15<9:22:52, 10.71s/it]                                                         51%|    | 3346/6500 [10:07:15<9:22:52, 10.71s/it] 51%|    | 3347/6500 [10:07:25<9:17:20, 10.61s/it]                                                         51%|    | 3347/6500 [10:07:25<9:17:20, 10.61s/it] 52%|    | 3348/6500 [10:07:36<9:13:37, 10.54s/it]                                                         52%|    | 3348/6500 [10:07:36<9:13:37, 10.54s/it] 52%|    | 3349/6500 [10:07:46<9:10:54, 10.49s/it]                                                         52%|    | 3349/6500 [10:07:46<9:10:54, 10.49s/it] 52%|    | 3350/6500 [10:07:57<9:14:04, 10.55s/it]                                                         52%|    | 3350/6500 [10:07:57<9:14:04, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8471946120262146, 'eval_runtime': 3.9675, 'eval_samples_per_second': 5.797, 'eval_steps_per_second': 1.512, 'epoch': 0.52}
                                                         52%|    | 3350/6500 [10:08:01<9:14:04, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.372, 'learning_rate': 4.7583588091495344e-05, 'epoch': 0.52}
{'loss': 0.3772, 'learning_rate': 4.7559443086083005e-05, 'epoch': 0.52}
{'loss': 0.3529, 'learning_rate': 4.7535298651136e-05, 'epoch': 0.52}
{'loss': 0.3572, 'learning_rate': 4.751115479229794e-05, 'epoch': 0.52}
{'loss': 0.3532, 'learning_rate': 4.7487011515212315e-05, 'epoch': 0.52}
 52%|    | 3351/6500 [10:08:12<10:29:38, 12.00s/it]                                                          52%|    | 3351/6500 [10:08:12<10:29:38, 12.00s/it] 52%|    | 3352/6500 [10:08:22<10:03:46, 11.51s/it]                                                          52%|    | 3352/6500 [10:08:22<10:03:46, 11.51s/it] 52%|    | 3353/6500 [10:08:33<9:45:44, 11.17s/it]                                                          52%|    | 3353/6500 [10:08:33<9:45:44, 11.17s/it] 52%|    | 3354/6500 [10:08:43<9:33:07, 10.93s/it]                                                         52%|    | 3354/6500 [10:08:43<9:33:07, 10.93s/it] 52%|    | 3355/6500 [10:08:53<9:23:52, 10.76s/it]                                                         52%|    | 3355/6500 [10:08:53<9:23:52, 10.76s/it] 52%|    | 3356/65{'loss': 0.4535, 'learning_rate': 4.7462868825522466e-05, 'epoch': 0.52}
{'loss': 0.3681, 'learning_rate': 4.7438726728871615e-05, 'epoch': 0.52}
{'loss': 0.3538, 'learning_rate': 4.741458523090282e-05, 'epoch': 0.52}
{'loss': 0.3764, 'learning_rate': 4.7390444337259e-05, 'epoch': 0.52}
{'loss': 0.8889, 'learning_rate': 4.7366304053582943e-05, 'epoch': 0.52}
00 [10:09:04<9:17:38, 10.64s/it]                                                         52%|    | 3356/6500 [10:09:04<9:17:38, 10.64s/it] 52%|    | 3357/6500 [10:09:14<9:13:15, 10.56s/it]                                                         52%|    | 3357/6500 [10:09:14<9:13:15, 10.56s/it] 52%|    | 3358/6500 [10:09:25<9:10:08, 10.51s/it]                                                         52%|    | 3358/6500 [10:09:25<9:10:08, 10.51s/it] 52%|    | 3359/6500 [10:09:35<9:07:30, 10.46s/it]                                                         52%|    | 3359/6500 [10:09:35<9:07:30, 10.46s/it] 52%|    | 3360/6500 [10:09:45<9:05:21, 10.42s/it]                                                         52%|    | 3360/6500 [10:09:45<9:05:21, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8365800976753235, 'eval_runtime': 3.9774, 'eval_samples_per_second': 5.783, 'eval_steps_per_second': 1.509, 'epoch': 0.52}
                                                         52%|    | 3360/6500 [10:09:49<9:05:21, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3829, 'learning_rate': 4.7342164385517305e-05, 'epoch': 0.52}
{'loss': 0.3676, 'learning_rate': 4.731802533870459e-05, 'epoch': 0.52}
{'loss': 0.3683, 'learning_rate': 4.729388691878714e-05, 'epoch': 0.52}
{'loss': 0.3415, 'learning_rate': 4.726974913140717e-05, 'epoch': 0.52}
{'loss': 0.3842, 'learning_rate': 4.7245611982206724e-05, 'epoch': 0.52}
 52%|    | 3361/6500 [10:10:00<10:15:22, 11.76s/it]                                                          52%|    | 3361/6500 [10:10:00<10:15:22, 11.76s/it] 52%|    | 3362/6500 [10:10:11<10:00:20, 11.48s/it]                                                          52%|    | 3362/6500 [10:10:11<10:00:20, 11.48s/it] 52%|    | 3363/6500 [10:10:21<9:42:23, 11.14s/it]                                                          52%|    | 3363/6500 [10:10:21<9:42:23, 11.14s/it] 52%|    | 3364/6500 [10:10:32<9:29:59, 10.91s/it]                                                         52%|    | 3364/6500 [10:10:32<9:29:59, 10.91s/it] 52%|    | 3365/6500 [10:10:42<9:21:03, 10.74s/it]                                                         52%|    | 3365/6500 [10:10:42<9:21:03, 10.74s/it] 52%|    | 3366/65{'loss': 0.3479, 'learning_rate': 4.7221475476827745e-05, 'epoch': 0.52}
{'loss': 0.3465, 'learning_rate': 4.719733962091198e-05, 'epoch': 0.52}
{'loss': 0.352, 'learning_rate': 4.717320442010105e-05, 'epoch': 0.52}
{'loss': 0.3608, 'learning_rate': 4.714906988003638e-05, 'epoch': 0.52}
{'loss': 0.3587, 'learning_rate': 4.71249360063593e-05, 'epoch': 0.52}
00 [10:10:52<9:15:07, 10.63s/it]                                                         52%|    | 3366/6500 [10:10:52<9:15:07, 10.63s/it] 52%|    | 3367/6500 [10:11:03<9:10:45, 10.55s/it]                                                         52%|    | 3367/6500 [10:11:03<9:10:45, 10.55s/it] 52%|    | 3368/6500 [10:11:13<9:07:21, 10.49s/it]                                                         52%|    | 3368/6500 [10:11:13<9:07:21, 10.49s/it] 52%|    | 3369/6500 [10:11:23<9:05:03, 10.44s/it]                                                         52%|    | 3369/6500 [10:11:23<9:05:03, 10.44s/it] 52%|    | 3370/6500 [10:11:34<9:03:30, 10.42s/it]                                                         52%|    | 3370/6500 [10:11:34<9:03:30, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8408457636833191, 'eval_runtime': 3.9619, 'eval_samples_per_second': 5.805, 'eval_steps_per_second': 1.514, 'epoch': 0.52}
                                                         52%|    | 3370/6500 [10:11:38<9:03:30, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3481, 'learning_rate': 4.7100802804710946e-05, 'epoch': 0.52}
{'loss': 0.3767, 'learning_rate': 4.707667028073232e-05, 'epoch': 0.52}
{'loss': 0.3452, 'learning_rate': 4.705253844006423e-05, 'epoch': 0.52}
{'loss': 0.3831, 'learning_rate': 4.702840728834736e-05, 'epoch': 0.52}
{'loss': 0.3655, 'learning_rate': 4.7004276831222224e-05, 'epoch': 0.52}
 52%|    | 3371/6500 [10:11:49<10:12:45, 11.75s/it]                                                          52%|    | 3371/6500 [10:11:49<10:12:45, 11.75s/it] 52%|    | 3372/6500 [10:11:59<9:51:15, 11.34s/it]                                                          52%|    | 3372/6500 [10:11:59<9:51:15, 11.34s/it] 52%|    | 3373/6500 [10:12:09<9:36:20, 11.06s/it]                                                         52%|    | 3373/6500 [10:12:09<9:36:20, 11.06s/it] 52%|    | 3374/6500 [10:12:20<9:25:50, 10.86s/it]                                                         52%|    | 3374/6500 [10:12:20<9:25:50, 10.86s/it] 52%|    | 3375/6500 [10:12:30<9:21:52, 10.79s/it]                                                         52%|    | 3375/6500 [10:12:30<9:21:52, 10.79s/it] 52%|    | 3376/6500 {'loss': 0.3628, 'learning_rate': 4.698014707432916e-05, 'epoch': 0.52}
{'loss': 0.3692, 'learning_rate': 4.695601802330835e-05, 'epoch': 0.52}
{'loss': 0.3561, 'learning_rate': 4.693188968379983e-05, 'epoch': 0.52}
{'loss': 0.3684, 'learning_rate': 4.6907762061443446e-05, 'epoch': 0.52}
{'loss': 0.3444, 'learning_rate': 4.688363516187886e-05, 'epoch': 0.52}
[10:12:41<9:15:43, 10.67s/it]                                                         52%|    | 3376/6500 [10:12:41<9:15:43, 10.67s/it] 52%|    | 3377/6500 [10:12:51<9:11:29, 10.60s/it]                                                         52%|    | 3377/6500 [10:12:51<9:11:29, 10.60s/it] 52%|    | 3378/6500 [10:13:02<9:12:28, 10.62s/it]                                                         52%|    | 3378/6500 [10:13:02<9:12:28, 10.62s/it] 52%|    | 3379/6500 [10:13:12<9:08:51, 10.55s/it]                                                         52%|    | 3379/6500 [10:13:12<9:08:51, 10.55s/it] 52%|    | 3380/6500 [10:13:23<9:06:13, 10.50s/it]                                                         52%|    | 3380/6500 [10:13:23<9:06:13, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8504558801651001, 'eval_runtime': 4.1956, 'eval_samples_per_second': 5.482, 'eval_steps_per_second': 1.43, 'epoch': 0.52}
                                                         52%|    | 3380/6500 [10:13:27<9:06:13, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3380/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3953, 'learning_rate': 4.685950899074562e-05, 'epoch': 0.52}
{'loss': 0.3582, 'learning_rate': 4.683538355368306e-05, 'epoch': 0.52}
{'loss': 0.3623, 'learning_rate': 4.681125885633035e-05, 'epoch': 0.52}
{'loss': 0.3475, 'learning_rate': 4.6787134904326504e-05, 'epoch': 0.52}
{'loss': 0.3834, 'learning_rate': 4.676301170331033e-05, 'epoch': 0.52}
 52%|    | 3381/6500 [10:13:38<10:17:48, 11.88s/it]                                                          52%|    | 3381/6500 [10:13:38<10:17:48, 11.88s/it] 52%|    | 3382/6500 [10:13:48<9:54:26, 11.44s/it]                                                          52%|    | 3382/6500 [10:13:48<9:54:26, 11.44s/it] 52%|    | 3383/6500 [10:13:59<9:37:52, 11.12s/it]                                                         52%|    | 3383/6500 [10:13:59<9:37:52, 11.12s/it] 52%|    | 3384/6500 [10:14:09<9:26:17, 10.90s/it]                                                         52%|    | 3384/6500 [10:14:09<9:26:17, 10.90s/it] 52%|    | 3385/6500 [10:14:19<9:18:03, 10.75s/it]                                                         52%|    | 3385/6500 [10:14:19<9:18:03, 10.75s/it] 52%|    | 3386/6500 {'loss': 0.4287, 'learning_rate': 4.673888925892048e-05, 'epoch': 0.52}
{'loss': 0.3548, 'learning_rate': 4.6714767576795446e-05, 'epoch': 0.52}
{'loss': 0.3491, 'learning_rate': 4.669064666257352e-05, 'epoch': 0.52}
{'loss': 0.3637, 'learning_rate': 4.666652652189282e-05, 'epoch': 0.52}
{'loss': 0.8778, 'learning_rate': 4.664240716039127e-05, 'epoch': 0.52}
[10:14:30<9:12:36, 10.65s/it]                                                         52%|    | 3386/6500 [10:14:30<9:12:36, 10.65s/it] 52%|    | 3387/6500 [10:14:40<9:08:41, 10.58s/it]                                                         52%|    | 3387/6500 [10:14:40<9:08:41, 10.58s/it] 52%|    | 3388/6500 [10:14:51<9:05:47, 10.52s/it]                                                         52%|    | 3388/6500 [10:14:51<9:05:47, 10.52s/it] 52%|    | 3389/6500 [10:15:01<9:03:32, 10.48s/it]                                                         52%|    | 3389/6500 [10:15:01<9:03:32, 10.48s/it] 52%|    | 3390/6500 [10:15:11<9:01:39, 10.45s/it]                                                         52%|    | 3390/6500 [10:15:11<9:01:39, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8421368598937988, 'eval_runtime': 3.9587, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.52}
                                                         52%|    | 3390/6500 [10:15:15<9:01:39, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3390/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3390/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3766, 'learning_rate': 4.6618288583706634e-05, 'epoch': 0.52}
{'loss': 0.3585, 'learning_rate': 4.659417079747648e-05, 'epoch': 0.52}
{'loss': 0.3485, 'learning_rate': 4.6570053807338186e-05, 'epoch': 0.52}
{'loss': 0.3534, 'learning_rate': 4.654593761892897e-05, 'epoch': 0.52}
{'loss': 0.3792, 'learning_rate': 4.652182223788584e-05, 'epoch': 0.52}
 52%|    | 3391/6500 [10:15:26<10:11:34, 11.80s/it]                                                          52%|    | 3391/6500 [10:15:26<10:11:34, 11.80s/it] 52%|    | 3392/6500 [10:15:37<9:49:46, 11.39s/it]                                                          52%|    | 3392/6500 [10:15:37<9:49:46, 11.39s/it] 52%|    | 3393/6500 [10:15:47<9:34:07, 11.09s/it]                                                         52%|    | 3393/6500 [10:15:47<9:34:07, 11.09s/it] 52%|    | 3394/6500 [10:15:58<9:28:45, 10.99s/it]                                                         52%|    | 3394/6500 [10:15:58<9:28:45, 10.99s/it] 52%|    | 3395/6500 [10:16:08<9:19:11, 10.81s/it]                                                         52%|    | 3395/6500 [10:16:08<9:19:11, 10.81s/it] 52%|    | 3396/6500 {'loss': 0.3513, 'learning_rate': 4.64977076698456e-05, 'epoch': 0.52}
{'loss': 0.3489, 'learning_rate': 4.647359392044491e-05, 'epoch': 0.52}
{'loss': 0.346, 'learning_rate': 4.644948099532019e-05, 'epoch': 0.52}
{'loss': 0.357, 'learning_rate': 4.64253689001077e-05, 'epoch': 0.52}
{'loss': 0.3622, 'learning_rate': 4.640125764044351e-05, 'epoch': 0.52}
[10:16:19<9:12:31, 10.68s/it]                                                         52%|    | 3396/6500 [10:16:19<9:12:31, 10.68s/it] 52%|    | 3397/6500 [10:16:29<9:07:40, 10.59s/it]                                                         52%|    | 3397/6500 [10:16:29<9:07:40, 10.59s/it] 52%|    | 3398/6500 [10:16:39<9:04:08, 10.53s/it]                                                         52%|    | 3398/6500 [10:16:39<9:04:08, 10.53s/it] 52%|    | 3399/6500 [10:16:50<9:01:43, 10.48s/it]                                                         52%|    | 3399/6500 [10:16:50<9:01:43, 10.48s/it] 52%|    | 3400/6500 [10:17:00<8:59:58, 10.45s/it]                                                         52%|    | 3400/6500 [10:17:00<8:59:58, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8428991436958313, 'eval_runtime': 3.9443, 'eval_samples_per_second': 5.831, 'eval_steps_per_second': 1.521, 'epoch': 0.52}
                                                         52%|    | 3400/6500 [10:17:04<8:59:58, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3505, 'learning_rate': 4.6377147221963455e-05, 'epoch': 0.52}
{'loss': 0.373, 'learning_rate': 4.635303765030321e-05, 'epoch': 0.52}
{'loss': 0.3557, 'learning_rate': 4.6328928931098236e-05, 'epoch': 0.52}
{'loss': 0.3785, 'learning_rate': 4.630482106998381e-05, 'epoch': 0.52}
{'loss': 0.3621, 'learning_rate': 4.628071407259499e-05, 'epoch': 0.52}
 52%|    | 3401/6500 [10:17:15<10:07:01, 11.75s/it]                                                          52%|    | 3401/6500 [10:17:15<10:07:01, 11.75s/it] 52%|    | 3402/6500 [10:17:25<9:45:21, 11.34s/it]                                                          52%|    | 3402/6500 [10:17:25<9:45:21, 11.34s/it] 52%|    | 3403/6500 [10:17:36<9:30:27, 11.05s/it]                                                         52%|    | 3403/6500 [10:17:36<9:30:27, 11.05s/it] 52%|    | 3404/6500 [10:17:46<9:19:44, 10.85s/it]                                                         52%|    | 3404/6500 [10:17:46<9:19:44, 10.85s/it] 52%|    | 3405/6500 [10:17:56<9:12:07, 10.70s/it]                                                         52%|    | 3405/6500 [10:17:56<9:12:07, 10.70s/it] 52%|    | 3406/6500 {'loss': 0.3563, 'learning_rate': 4.625660794456665e-05, 'epoch': 0.52}
{'loss': 0.3735, 'learning_rate': 4.623250269153343e-05, 'epoch': 0.52}
{'loss': 0.3591, 'learning_rate': 4.6208398319129804e-05, 'epoch': 0.52}
{'loss': 0.3699, 'learning_rate': 4.6184294832990016e-05, 'epoch': 0.52}
{'loss': 0.3558, 'learning_rate': 4.616019223874811e-05, 'epoch': 0.52}
[10:18:07<9:07:20, 10.61s/it]                                                         52%|    | 3406/6500 [10:18:07<9:07:20, 10.61s/it] 52%|    | 3407/6500 [10:18:17<9:03:46, 10.55s/it]                                                         52%|    | 3407/6500 [10:18:17<9:03:46, 10.55s/it] 52%|    | 3408/6500 [10:18:28<9:00:59, 10.50s/it]                                                         52%|    | 3408/6500 [10:18:28<9:00:59, 10.50s/it] 52%|    | 3409/6500 [10:18:38<8:59:09, 10.47s/it]                                                         52%|    | 3409/6500 [10:18:38<8:59:09, 10.47s/it] 52%|    | 3410/6500 [10:18:49<9:01:47, 10.52s/it]                                                         52%|    | 3410/6500 [10:18:49<9:01:47, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8493192791938782, 'eval_runtime': 4.1907, 'eval_samples_per_second': 5.488, 'eval_steps_per_second': 1.432, 'epoch': 0.52}
                                                         52%|    | 3410/6500 [10:18:53<9:01:47, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3410
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3788, 'learning_rate': 4.6136090542037924e-05, 'epoch': 0.52}
{'loss': 0.3489, 'learning_rate': 4.611198974849309e-05, 'epoch': 0.52}
{'loss': 0.3547, 'learning_rate': 4.6087889863747e-05, 'epoch': 0.53}
{'loss': 0.3347, 'learning_rate': 4.6063790893432874e-05, 'epoch': 0.53}
{'loss': 0.3983, 'learning_rate': 4.603969284318369e-05, 'epoch': 0.53}
 52%|    | 3411/6500 [10:19:04<10:12:06, 11.89s/it]                                                          52%|    | 3411/6500 [10:19:04<10:12:06, 11.89s/it] 52%|    | 3412/6500 [10:19:14<9:48:42, 11.44s/it]                                                          52%|    | 3412/6500 [10:19:14<9:48:42, 11.44s/it] 53%|    | 3413/6500 [10:19:25<9:32:12, 11.12s/it]                                                         53%|    | 3413/6500 [10:19:25<9:32:12, 11.12s/it] 53%|    | 3414/6500 [10:19:35<9:20:51, 10.90s/it]                                                         53%|    | 3414/6500 [10:19:35<9:20:51, 10.90s/it] 53%|    | 3415/6500 [10:19:45<9:13:04, 10.76s/it]                                                         53%|    | 3415/6500 [10:19:45<9:13:04, 10.76s/it] 53%|    | 3416/6500 {'loss': 0.4057, 'learning_rate': 4.6015595718632226e-05, 'epoch': 0.53}
{'loss': 0.3519, 'learning_rate': 4.5991499525411046e-05, 'epoch': 0.53}
{'loss': 0.3663, 'learning_rate': 4.596740426915247e-05, 'epoch': 0.53}
{'loss': 0.36, 'learning_rate': 4.594330995548863e-05, 'epoch': 0.53}
{'loss': 0.8932, 'learning_rate': 4.591921659005142e-05, 'epoch': 0.53}
[10:19:56<9:07:14, 10.65s/it]                                                         53%|    | 3416/6500 [10:19:56<9:07:14, 10.65s/it] 53%|    | 3417/6500 [10:20:06<9:03:06, 10.57s/it]                                                         53%|    | 3417/6500 [10:20:06<9:03:06, 10.57s/it] 53%|    | 3418/6500 [10:20:16<8:59:51, 10.51s/it]                                                         53%|    | 3418/6500 [10:20:16<8:59:51, 10.51s/it] 53%|    | 3419/6500 [10:20:27<8:57:55, 10.48s/it]                                                         53%|    | 3419/6500 [10:20:27<8:57:55, 10.48s/it] 53%|    | 3420/6500 [10:20:37<8:56:51, 10.46s/it]                                                         53%|    | 3420/6500 [10:20:37<8:56:51, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8433473706245422, 'eval_runtime': 4.5839, 'eval_samples_per_second': 5.018, 'eval_steps_per_second': 1.309, 'epoch': 0.53}
                                                         53%|    | 3420/6500 [10:20:42<8:56:51, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3420
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3420/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3420/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3652, 'learning_rate': 4.589512417847252e-05, 'epoch': 0.53}
{'loss': 0.3726, 'learning_rate': 4.5871032726383386e-05, 'epoch': 0.53}
{'loss': 0.3409, 'learning_rate': 4.584694223941526e-05, 'epoch': 0.53}
{'loss': 0.3667, 'learning_rate': 4.582285272319913e-05, 'epoch': 0.53}
{'loss': 0.3696, 'learning_rate': 4.579876418336577e-05, 'epoch': 0.53}
 53%|    | 3421/6500 [10:20:53<10:15:42, 12.00s/it]                                                          53%|    | 3421/6500 [10:20:53<10:15:42, 12.00s/it] 53%|    | 3422/6500 [10:21:03<9:50:44, 11.52s/it]                                                          53%|    | 3422/6500 [10:21:03<9:50:44, 11.52s/it] 53%|    | 3423/6500 [10:21:14<9:33:08, 11.18s/it]                                                         53%|    | 3423/6500 [10:21:14<9:33:08, 11.18s/it] 53%|    | 3424/6500 [10:21:24<9:20:31, 10.93s/it]                                                         53%|    | 3424/6500 [10:21:24<9:20:31, 10.93s/it] 53%|    | 3425/6500 [10:21:34<9:11:37, 10.76s/it]                                                         53%|    | 3425/6500 [10:21:34<9:11:37, 10.76s/it] 53%|    | 3426/6500 {'loss': 0.3239, 'learning_rate': 4.577467662554574e-05, 'epoch': 0.53}
{'loss': 0.3718, 'learning_rate': 4.575059005536935e-05, 'epoch': 0.53}
{'loss': 0.3497, 'learning_rate': 4.572650447846672e-05, 'epoch': 0.53}
{'loss': 0.3681, 'learning_rate': 4.570241990046767e-05, 'epoch': 0.53}
{'loss': 0.3699, 'learning_rate': 4.5678336327001844e-05, 'epoch': 0.53}
[10:21:45<9:11:22, 10.76s/it]                                                         53%|    | 3426/6500 [10:21:45<9:11:22, 10.76s/it] 53%|    | 3427/6500 [10:21:56<9:06:08, 10.66s/it]                                                         53%|    | 3427/6500 [10:21:56<9:06:08, 10.66s/it] 53%|    | 3428/6500 [10:22:06<9:01:52, 10.58s/it]                                                         53%|    | 3428/6500 [10:22:06<9:01:52, 10.58s/it] 53%|    | 3429/6500 [10:22:16<8:58:49, 10.53s/it]                                                         53%|    | 3429/6500 [10:22:16<8:58:49, 10.53s/it] 53%|    | 3430/6500 [10:22:27<8:56:26, 10.48s/it]                                                         53%|    | 3430/6500 [10:22:27<8:56:26, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8407262563705444, 'eval_runtime': 3.9577, 'eval_samples_per_second': 5.811, 'eval_steps_per_second': 1.516, 'epoch': 0.53}
                                                         53%|    | 3430/6500 [10:22:31<8:56:26, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3713, 'learning_rate': 4.565425376369862e-05, 'epoch': 0.53}
{'loss': 0.3561, 'learning_rate': 4.563017221618715e-05, 'epoch': 0.53}
{'loss': 0.3572, 'learning_rate': 4.560609169009636e-05, 'epoch': 0.53}
{'loss': 0.3973, 'learning_rate': 4.558201219105494e-05, 'epoch': 0.53}
{'loss': 0.3563, 'learning_rate': 4.555793372469129e-05, 'epoch': 0.53}
 53%|    | 3431/6500 [10:22:42<10:03:50, 11.81s/it]                                                          53%|    | 3431/6500 [10:22:42<10:03:50, 11.81s/it] 53%|    | 3432/6500 [10:22:52<9:42:13, 11.39s/it]                                                          53%|    | 3432/6500 [10:22:52<9:42:13, 11.39s/it] 53%|    | 3433/6500 [10:23:02<9:26:35, 11.08s/it]                                                         53%|    | 3433/6500 [10:23:02<9:26:35, 11.08s/it] 53%|    | 3434/6500 [10:23:13<9:15:14, 10.87s/it]                                                         53%|    | 3434/6500 [10:23:13<9:15:14, 10.87s/it] 53%|    | 3435/6500 [10:23:23<9:07:36, 10.72s/it]                                                         53%|    | 3435/6500 [10:23:23<9:07:36, 10.72s/it] 53%|    | 3436/6500 {'loss': 0.3719, 'learning_rate': 4.553385629663363e-05, 'epoch': 0.53}
{'loss': 0.3739, 'learning_rate': 4.55097799125099e-05, 'epoch': 0.53}
{'loss': 0.3571, 'learning_rate': 4.548570457794782e-05, 'epoch': 0.53}
{'loss': 0.3595, 'learning_rate': 4.546163029857485e-05, 'epoch': 0.53}
{'loss': 0.374, 'learning_rate': 4.5437557080018175e-05, 'epoch': 0.53}
[10:23:34<9:02:22, 10.62s/it]                                                         53%|    | 3436/6500 [10:23:34<9:02:22, 10.62s/it] 53%|    | 3437/6500 [10:23:44<8:58:32, 10.55s/it]                                                         53%|    | 3437/6500 [10:23:44<8:58:32, 10.55s/it] 53%|    | 3438/6500 [10:23:54<8:55:49, 10.50s/it]                                                         53%|    | 3438/6500 [10:23:54<8:55:49, 10.50s/it] 53%|    | 3439/6500 [10:24:05<8:53:53, 10.46s/it]                                                         53%|    | 3439/6500 [10:24:05<8:53:53, 10.46s/it] 53%|    | 3440/6500 [10:24:15<8:52:29, 10.44s/it]                                                         53%|    | 3440/6500 [10:24:15<8:52:29, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8474633693695068, 'eval_runtime': 3.9561, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.53}
                                                         53%|    | 3440/6500 [10:24:19<8:52:29, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3440
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3621, 'learning_rate': 4.541348492790482e-05, 'epoch': 0.53}
{'loss': 0.3518, 'learning_rate': 4.538941384786147e-05, 'epoch': 0.53}
{'loss': 0.3479, 'learning_rate': 4.536534384551462e-05, 'epoch': 0.53}
{'loss': 0.3694, 'learning_rate': 4.5341274926490446e-05, 'epoch': 0.53}
{'loss': 0.4248, 'learning_rate': 4.5317207096414934e-05, 'epoch': 0.53}
 53%|    | 3441/6500 [10:24:30<10:00:15, 11.77s/it]                                                          53%|    | 3441/6500 [10:24:30<10:00:15, 11.77s/it] 53%|    | 3442/6500 [10:24:40<9:39:08, 11.36s/it]                                                          53%|    | 3442/6500 [10:24:40<9:39:08, 11.36s/it] 53%|    | 3443/6500 [10:24:51<9:33:31, 11.26s/it]                                                         53%|    | 3443/6500 [10:24:51<9:33:31, 11.26s/it] 53%|    | 3444/6500 [10:25:02<9:20:07, 11.00s/it]                                                         53%|    | 3444/6500 [10:25:02<9:20:07, 11.00s/it] 53%|    | 3445/6500 [10:25:12<9:10:38, 10.81s/it]                                                         53%|    | 3445/6500 [10:25:12<9:10:38, 10.81s/it] 53%|    | 3446/6500 {'loss': 0.3466, 'learning_rate': 4.529314036091379e-05, 'epoch': 0.53}
{'loss': 0.3507, 'learning_rate': 4.5269074725612474e-05, 'epoch': 0.53}
{'loss': 0.3765, 'learning_rate': 4.524501019613619e-05, 'epoch': 0.53}
{'loss': 0.885, 'learning_rate': 4.522094677810985e-05, 'epoch': 0.53}
{'loss': 0.3658, 'learning_rate': 4.519688447715814e-05, 'epoch': 0.53}
[10:25:23<9:03:59, 10.69s/it]                                                         53%|    | 3446/6500 [10:25:23<9:03:59, 10.69s/it] 53%|    | 3447/6500 [10:25:33<8:59:15, 10.60s/it]                                                         53%|    | 3447/6500 [10:25:33<8:59:15, 10.60s/it] 53%|    | 3448/6500 [10:25:43<8:56:02, 10.54s/it]                                                         53%|    | 3448/6500 [10:25:43<8:56:02, 10.54s/it] 53%|    | 3449/6500 [10:25:54<8:54:02, 10.50s/it]                                                         53%|    | 3449/6500 [10:25:54<8:54:02, 10.50s/it] 53%|    | 3450/6500 [10:26:04<8:52:13, 10.47s/it]                                                         53%|    | 3450/6500 [10:26:04<8:52:13, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8444809913635254, 'eval_runtime': 3.9448, 'eval_samples_per_second': 5.83, 'eval_steps_per_second': 1.521, 'epoch': 0.53}
                                                         53%|    | 3450/6500 [10:26:08<8:52:13, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3649, 'learning_rate': 4.517282329890548e-05, 'epoch': 0.53}
{'loss': 0.3659, 'learning_rate': 4.514876324897602e-05, 'epoch': 0.53}
{'loss': 0.3346, 'learning_rate': 4.512470433299366e-05, 'epoch': 0.53}
{'loss': 0.3723, 'learning_rate': 4.510064655658203e-05, 'epoch': 0.53}
{'loss': 0.349, 'learning_rate': 4.5076589925364465e-05, 'epoch': 0.53}
 53%|    | 3451/6500 [10:26:19<9:58:34, 11.78s/it]                                                         53%|    | 3451/6500 [10:26:19<9:58:34, 11.78s/it] 53%|    | 3452/6500 [10:26:29<9:37:11, 11.36s/it]                                                         53%|    | 3452/6500 [10:26:29<9:37:11, 11.36s/it] 53%|    | 3453/6500 [10:26:40<9:22:00, 11.07s/it]                                                         53%|    | 3453/6500 [10:26:40<9:22:00, 11.07s/it] 53%|    | 3454/6500 [10:26:50<9:11:41, 10.87s/it]                                                         53%|    | 3454/6500 [10:26:50<9:11:41, 10.87s/it] 53%|    | 3455/6500 [10:27:01<9:03:40, 10.71s/it]                                                         53%|    | 3455/6500 [10:27:01<9:03:40, 10.71s/it] 53%|    | 3456/6500 [10:{'loss': 0.3383, 'learning_rate': 4.505253444496407e-05, 'epoch': 0.53}
{'loss': 0.3466, 'learning_rate': 4.502848012100367e-05, 'epoch': 0.53}
{'loss': 0.3463, 'learning_rate': 4.500442695910582e-05, 'epoch': 0.53}
{'loss': 0.3619, 'learning_rate': 4.4980374964892794e-05, 'epoch': 0.53}
{'loss': 0.3458, 'learning_rate': 4.4956324143986596e-05, 'epoch': 0.53}
27:11<8:58:22, 10.61s/it]                                                         53%|    | 3456/6500 [10:27:11<8:58:22, 10.61s/it] 53%|    | 3457/6500 [10:27:21<8:54:28, 10.54s/it]                                                         53%|    | 3457/6500 [10:27:21<8:54:28, 10.54s/it] 53%|    | 3458/6500 [10:27:32<8:52:00, 10.49s/it]                                                         53%|    | 3458/6500 [10:27:32<8:52:00, 10.49s/it] 53%|    | 3459/6500 [10:27:42<8:55:57, 10.57s/it]                                                         53%|    | 3459/6500 [10:27:42<8:55:57, 10.57s/it] 53%|    | 3460/6500 [10:27:53<8:53:45, 10.53s/it]                                                         53%|    | 3460/6500 [10:27:53<8:53:45, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8436794281005859, 'eval_runtime': 3.953, 'eval_samples_per_second': 5.818, 'eval_steps_per_second': 1.518, 'epoch': 0.53}
                                                         53%|    | 3460/6500 [10:27:57<8:53:45, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3460/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3819, 'learning_rate': 4.4932274502008956e-05, 'epoch': 0.53}
{'loss': 0.3565, 'learning_rate': 4.4908226044581346e-05, 'epoch': 0.53}
{'loss': 0.3894, 'learning_rate': 4.488417877732494e-05, 'epoch': 0.53}
{'loss': 0.3696, 'learning_rate': 4.4860132705860644e-05, 'epoch': 0.53}
{'loss': 0.3685, 'learning_rate': 4.4836087835809083e-05, 'epoch': 0.53}
 53%|    | 3461/6500 [10:28:08<10:00:41, 11.86s/it]                                                          53%|    | 3461/6500 [10:28:08<10:00:41, 11.86s/it] 53%|    | 3462/6500 [10:28:18<9:38:26, 11.42s/it]                                                          53%|    | 3462/6500 [10:28:18<9:38:26, 11.42s/it] 53%|    | 3463/6500 [10:28:29<9:22:35, 11.11s/it]                                                         53%|    | 3463/6500 [10:28:29<9:22:35, 11.11s/it] 53%|    | 3464/6500 [10:28:39<9:11:33, 10.90s/it]                                                         53%|    | 3464/6500 [10:28:39<9:11:33, 10.90s/it] 53%|    | 3465/6500 [10:28:49<9:03:50, 10.75s/it]                                                         53%|    | 3465/6500 [10:28:49<9:03:50, 10.75s/it] 53%|    | 3466/6500 {'loss': 0.3695, 'learning_rate': 4.481204417279058e-05, 'epoch': 0.53}
{'loss': 0.3657, 'learning_rate': 4.478800172242521e-05, 'epoch': 0.53}
{'loss': 0.3571, 'learning_rate': 4.476396049033275e-05, 'epoch': 0.53}
{'loss': 0.3693, 'learning_rate': 4.473992048213269e-05, 'epoch': 0.53}
{'loss': 0.376, 'learning_rate': 4.471588170344423e-05, 'epoch': 0.53}
[10:29:00<8:58:30, 10.65s/it]                                                         53%|    | 3466/6500 [10:29:00<8:58:30, 10.65s/it] 53%|    | 3467/6500 [10:29:10<8:54:30, 10.57s/it]                                                         53%|    | 3467/6500 [10:29:10<8:54:30, 10.57s/it] 53%|    | 3468/6500 [10:29:21<8:51:51, 10.53s/it]                                                         53%|    | 3468/6500 [10:29:21<8:51:51, 10.53s/it] 53%|    | 3469/6500 [10:29:31<8:50:10, 10.50s/it]                                                         53%|    | 3469/6500 [10:29:31<8:50:10, 10.50s/it] 53%|    | 3470/6500 [10:29:41<8:48:44, 10.47s/it]                                                         53%|    | 3470/6500 [10:29:41<8:48:44, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8459945321083069, 'eval_runtime': 3.9587, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.53}
                                                         53%|    | 3470/6500 [10:29:45<8:48:44, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3470/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3448, 'learning_rate': 4.469184415988631e-05, 'epoch': 0.53}
{'loss': 0.359, 'learning_rate': 4.466780785707752e-05, 'epoch': 0.53}
{'loss': 0.347, 'learning_rate': 4.464377280063624e-05, 'epoch': 0.53}
{'loss': 0.3822, 'learning_rate': 4.46197389961805e-05, 'epoch': 0.53}
{'loss': 0.4199, 'learning_rate': 4.459570644932805e-05, 'epoch': 0.53}
 53%|    | 3471/6500 [10:29:56<9:55:24, 11.79s/it]                                                         53%|    | 3471/6500 [10:29:56<9:55:24, 11.79s/it] 53%|    | 3472/6500 [10:30:07<9:34:35, 11.39s/it]                                                         53%|    | 3472/6500 [10:30:07<9:34:35, 11.39s/it] 53%|    | 3473/6500 [10:30:17<9:19:40, 11.09s/it]                                                         53%|    | 3473/6500 [10:30:17<9:19:40, 11.09s/it] 53%|    | 3474/6500 [10:30:28<9:08:58, 10.89s/it]                                                         53%|    | 3474/6500 [10:30:28<9:08:58, 10.89s/it] 53%|    | 3475/6500 [10:30:38<9:05:17, 10.82s/it]                                                         53%|    | 3475/6500 [10:30:38<9:05:17, 10.82s/it] 53%|    | 3476/6500 [10:{'loss': 0.3526, 'learning_rate': 4.457167516569637e-05, 'epoch': 0.53}
{'loss': 0.3556, 'learning_rate': 4.454764515090261e-05, 'epoch': 0.53}
{'loss': 0.3627, 'learning_rate': 4.452361641056364e-05, 'epoch': 0.54}
{'loss': 0.8773, 'learning_rate': 4.449958895029604e-05, 'epoch': 0.54}
{'loss': 0.3706, 'learning_rate': 4.447556277571608e-05, 'epoch': 0.54}
30:49<8:58:54, 10.69s/it]                                                         53%|    | 3476/6500 [10:30:49<8:58:54, 10.69s/it] 53%|    | 3477/6500 [10:30:59<8:54:36, 10.61s/it]                                                         53%|    | 3477/6500 [10:30:59<8:54:36, 10.61s/it] 54%|    | 3478/6500 [10:31:09<8:51:21, 10.55s/it]                                                         54%|    | 3478/6500 [10:31:09<8:51:21, 10.55s/it] 54%|    | 3479/6500 [10:31:20<8:49:03, 10.51s/it]                                                         54%|    | 3479/6500 [10:31:20<8:49:03, 10.51s/it] 54%|    | 3480/6500 [10:31:30<8:47:14, 10.48s/it]                                                         54%|    | 3480/6500 [10:31:30<8:47:14, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8467011451721191, 'eval_runtime': 4.1984, 'eval_samples_per_second': 5.478, 'eval_steps_per_second': 1.429, 'epoch': 0.54}
                                                         54%|    | 3480/6500 [10:31:34<8:47:14, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3480the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3480

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3480/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.373, 'learning_rate': 4.4451537892439735e-05, 'epoch': 0.54}
{'loss': 0.3345, 'learning_rate': 4.442751430608267e-05, 'epoch': 0.54}
{'loss': 0.358, 'learning_rate': 4.440349202226026e-05, 'epoch': 0.54}
{'loss': 0.3771, 'learning_rate': 4.437947104658755e-05, 'epoch': 0.54}
{'loss': 0.345, 'learning_rate': 4.4355451384679313e-05, 'epoch': 0.54}
 54%|    | 3481/6500 [10:31:46<9:59:02, 11.91s/it]                                                         54%|    | 3481/6500 [10:31:46<9:59:02, 11.91s/it] 54%|    | 3482/6500 [10:31:56<9:35:55, 11.45s/it]                                                         54%|    | 3482/6500 [10:31:56<9:35:55, 11.45s/it] 54%|    | 3483/6500 [10:32:06<9:20:01, 11.14s/it]                                                         54%|    | 3483/6500 [10:32:06<9:20:01, 11.14s/it] 54%|    | 3484/6500 [10:32:17<9:08:29, 10.91s/it]                                                         54%|    | 3484/6500 [10:32:17<9:08:29, 10.91s/it] 54%|    | 3485/6500 [10:32:27<9:00:27, 10.76s/it]                                                         54%|    | 3485/6500 [10:32:27<9:00:27, 10.76s/it] 54%|    | 3486/6500 [10:{'loss': 0.3549, 'learning_rate': 4.4331433042150003e-05, 'epoch': 0.54}
{'loss': 0.3428, 'learning_rate': 4.430741602461376e-05, 'epoch': 0.54}
{'loss': 0.355, 'learning_rate': 4.428340033768439e-05, 'epoch': 0.54}
{'loss': 0.3481, 'learning_rate': 4.4259385986975446e-05, 'epoch': 0.54}
{'loss': 0.3572, 'learning_rate': 4.423537297810012e-05, 'epoch': 0.54}
32:38<8:55:04, 10.65s/it]                                                         54%|    | 3486/6500 [10:32:38<8:55:04, 10.65s/it] 54%|    | 3487/6500 [10:32:48<8:51:07, 10.58s/it]                                                         54%|    | 3487/6500 [10:32:48<8:51:07, 10.58s/it] 54%|    | 3488/6500 [10:32:58<8:48:29, 10.53s/it]                                                         54%|    | 3488/6500 [10:32:58<8:48:29, 10.53s/it] 54%|    | 3489/6500 [10:33:09<8:46:43, 10.50s/it]                                                         54%|    | 3489/6500 [10:33:09<8:46:43, 10.50s/it] 54%|    | 3490/6500 [10:33:19<8:45:17, 10.47s/it]                                                         54%|    | 3490/6500 [10:33:19<8:45:17, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.841427743434906, 'eval_runtime': 3.9583, 'eval_samples_per_second': 5.811, 'eval_steps_per_second': 1.516, 'epoch': 0.54}
                                                         54%|    | 3490/6500 [10:33:23<8:45:17, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3656, 'learning_rate': 4.421136131667131e-05, 'epoch': 0.54}
{'loss': 0.3557, 'learning_rate': 4.4187351008301596e-05, 'epoch': 0.54}
{'loss': 0.3869, 'learning_rate': 4.4163342058603255e-05, 'epoch': 0.54}
{'loss': 0.3515, 'learning_rate': 4.413933447318821e-05, 'epoch': 0.54}
{'loss': 0.3482, 'learning_rate': 4.41153282576681e-05, 'epoch': 0.54}
 54%|    | 3491/6500 [10:33:34<9:57:13, 11.91s/it]                                                         54%|    | 3491/6500 [10:33:34<9:57:13, 11.91s/it] 54%|    | 3492/6500 [10:33:45<9:34:33, 11.46s/it]                                                         54%|    | 3492/6500 [10:33:45<9:34:33, 11.46s/it] 54%|    | 3493/6500 [10:33:55<9:18:18, 11.14s/it]                                                         54%|    | 3493/6500 [10:33:55<9:18:18, 11.14s/it] 54%|    | 3494/6500 [10:34:06<9:07:16, 10.92s/it]                                                         54%|    | 3494/6500 [10:34:06<9:07:16, 10.92s/it] 54%|    | 3495/6500 [10:34:16<8:59:26, 10.77s/it]                                                         54%|    | 3495/6500 [10:34:16<8:59:26, 10.77s/it] 54%|    | 3496/6500 [10:{'loss': 0.3679, 'learning_rate': 4.4091323417654225e-05, 'epoch': 0.54}
{'loss': 0.3506, 'learning_rate': 4.406731995875758e-05, 'epoch': 0.54}
{'loss': 0.3629, 'learning_rate': 4.404331788658882e-05, 'epoch': 0.54}
{'loss': 0.3507, 'learning_rate': 4.4019317206758297e-05, 'epoch': 0.54}
{'loss': 0.3734, 'learning_rate': 4.399531792487601e-05, 'epoch': 0.54}
34:26<8:53:57, 10.66s/it]                                                         54%|    | 3496/6500 [10:34:26<8:53:57, 10.66s/it] 54%|    | 3497/6500 [10:34:37<8:50:01, 10.59s/it]                                                         54%|    | 3497/6500 [10:34:37<8:50:01, 10.59s/it] 54%|    | 3498/6500 [10:34:47<8:47:25, 10.54s/it]                                                         54%|    | 3498/6500 [10:34:47<8:47:25, 10.54s/it] 54%|    | 3499/6500 [10:34:58<8:45:36, 10.51s/it]                                                         54%|    | 3499/6500 [10:34:58<8:45:36, 10.51s/it] 54%|    | 3500/6500 [10:35:08<8:44:04, 10.48s/it]                                                         54%|    | 3500/6500 [10:35:08<8:44:04, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8490537405014038, 'eval_runtime': 3.957, 'eval_samples_per_second': 5.812, 'eval_steps_per_second': 1.516, 'epoch': 0.54}
                                                         54%|    | 3500/6500 [10:35:12<8:44:04, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3500
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.355, 'learning_rate': 4.397132004655165e-05, 'epoch': 0.54}
{'loss': 0.3471, 'learning_rate': 4.394732357739456e-05, 'epoch': 0.54}
{'loss': 0.3391, 'learning_rate': 4.392332852301379e-05, 'epoch': 0.54}
{'loss': 0.4342, 'learning_rate': 4.389933488901805e-05, 'epoch': 0.54}
{'loss': 0.3603, 'learning_rate': 4.387534268101566e-05, 'epoch': 0.54}
 54%|    | 3501/6500 [10:35:23<9:49:41, 11.80s/it]                                                         54%|    | 3501/6500 [10:35:23<9:49:41, 11.80s/it] 54%|    | 3502/6500 [10:35:33<9:28:44, 11.38s/it]                                                         54%|    | 3502/6500 [10:35:33<9:28:44, 11.38s/it] 54%|    | 3503/6500 [10:35:44<9:14:13, 11.10s/it]                                                         54%|    | 3503/6500 [10:35:44<9:14:13, 11.10s/it] 54%|    | 3504/6500 [10:35:54<9:03:20, 10.88s/it]                                                         54%|    | 3504/6500 [10:35:54<9:03:20, 10.88s/it] 54%|    | 3505/6500 [10:36:05<8:55:55, 10.74s/it]                                                         54%|    | 3505/6500 [10:36:05<8:55:55, 10.74s/it] 54%|    | 3506/6500 [10:{'loss': 0.3393, 'learning_rate': 4.38513519046147e-05, 'epoch': 0.54}
{'loss': 0.3563, 'learning_rate': 4.382736256542283e-05, 'epoch': 0.54}
{'loss': 0.7831, 'learning_rate': 4.3803374669047436e-05, 'epoch': 0.54}
{'loss': 0.4607, 'learning_rate': 4.377938822109554e-05, 'epoch': 0.54}
{'loss': 0.3752, 'learning_rate': 4.3755403227173836e-05, 'epoch': 0.54}
36:15<8:50:53, 10.64s/it]                                                         54%|    | 3506/6500 [10:36:15<8:50:53, 10.64s/it] 54%|    | 3507/6500 [10:36:26<8:54:48, 10.72s/it]                                                         54%|    | 3507/6500 [10:36:26<8:54:48, 10.72s/it] 54%|    | 3508/6500 [10:36:36<8:49:49, 10.62s/it]                                                         54%|    | 3508/6500 [10:36:36<8:49:49, 10.62s/it] 54%|    | 3509/6500 [10:36:47<8:46:32, 10.56s/it]                                                         54%|    | 3509/6500 [10:36:47<8:46:32, 10.56s/it] 54%|    | 3510/6500 [10:36:57<8:43:59, 10.52s/it]                                                         54%|    | 3510/6500 [10:36:57<8:43:59, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8508055210113525, 'eval_runtime': 3.958, 'eval_samples_per_second': 5.811, 'eval_steps_per_second': 1.516, 'epoch': 0.54}
                                                         54%|    | 3510/6500 [10:37:01<8:43:59, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3674, 'learning_rate': 4.373141969288865e-05, 'epoch': 0.54}
{'loss': 0.3314, 'learning_rate': 4.3707437623845995e-05, 'epoch': 0.54}
{'loss': 0.373, 'learning_rate': 4.368345702565153e-05, 'epoch': 0.54}
{'loss': 0.3623, 'learning_rate': 4.365947790391059e-05, 'epoch': 0.54}
{'loss': 0.328, 'learning_rate': 4.3635500264228146e-05, 'epoch': 0.54}
 54%|    | 3511/6500 [10:37:12<9:50:29, 11.85s/it]                                                         54%|    | 3511/6500 [10:37:12<9:50:29, 11.85s/it] 54%|    | 3512/6500 [10:37:23<9:28:27, 11.41s/it]                                                         54%|    | 3512/6500 [10:37:23<9:28:27, 11.41s/it] 54%|    | 3513/6500 [10:37:33<9:13:16, 11.11s/it]                                                         54%|    | 3513/6500 [10:37:33<9:13:16, 11.11s/it] 54%|    | 3514/6500 [10:37:43<9:02:19, 10.90s/it]                                                         54%|    | 3514/6500 [10:37:43<9:02:19, 10.90s/it] 54%|    | 3515/6500 [10:37:54<8:54:40, 10.75s/it]                                                         54%|    | 3515/6500 [10:37:54<8:54:40, 10.75s/it] 54%|    | 3516/6500 [10:{'loss': 0.3607, 'learning_rate': 4.361152411220878e-05, 'epoch': 0.54}
{'loss': 0.3423, 'learning_rate': 4.358754945345684e-05, 'epoch': 0.54}
{'loss': 0.356, 'learning_rate': 4.356357629357624e-05, 'epoch': 0.54}
{'loss': 0.3525, 'learning_rate': 4.353960463817053e-05, 'epoch': 0.54}
{'loss': 0.3574, 'learning_rate': 4.3515634492842956e-05, 'epoch': 0.54}
38:04<8:49:34, 10.65s/it]                                                         54%|    | 3516/6500 [10:38:04<8:49:34, 10.65s/it] 54%|    | 3517/6500 [10:38:15<8:45:49, 10.58s/it]                                                         54%|    | 3517/6500 [10:38:15<8:45:49, 10.58s/it] 54%|    | 3518/6500 [10:38:25<8:43:16, 10.53s/it]                                                         54%|    | 3518/6500 [10:38:25<8:43:16, 10.53s/it] 54%|    | 3519/6500 [10:38:35<8:41:34, 10.50s/it]                                                         54%|    | 3519/6500 [10:38:35<8:41:34, 10.50s/it] 54%|    | 3520/6500 [10:38:46<8:40:09, 10.47s/it]                                                         54%|    | 3520/6500 [10:38:46<8:40:09, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8416589498519897, 'eval_runtime': 3.9465, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.54}
                                                         54%|    | 3520/6500 [10:38:50<8:40:09, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3520
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3491, 'learning_rate': 4.3491665863196396e-05, 'epoch': 0.54}
{'loss': 0.3555, 'learning_rate': 4.346769875483336e-05, 'epoch': 0.54}
{'loss': 0.3849, 'learning_rate': 4.3443733173356035e-05, 'epoch': 0.54}
{'loss': 0.3584, 'learning_rate': 4.3419769124366225e-05, 'epoch': 0.54}
{'loss': 0.3732, 'learning_rate': 4.339580661346536e-05, 'epoch': 0.54}
 54%|    | 3521/6500 [10:39:01<9:45:33, 11.79s/it]                                                         54%|    | 3521/6500 [10:39:01<9:45:33, 11.79s/it] 54%|    | 3522/6500 [10:39:11<9:24:33, 11.37s/it]                                                         54%|    | 3522/6500 [10:39:11<9:24:33, 11.37s/it] 54%|    | 3523/6500 [10:39:22<9:15:54, 11.20s/it]                                                         54%|    | 3523/6500 [10:39:22<9:15:54, 11.20s/it] 54%|    | 3524/6500 [10:39:32<9:04:04, 10.97s/it]                                                         54%|    | 3524/6500 [10:39:32<9:04:04, 10.97s/it] 54%|    | 3525/6500 [10:39:43<8:55:40, 10.80s/it]                                                         54%|    | 3525/6500 [10:39:43<8:55:40, 10.80s/it] 54%|    | 3526/6500 [10:{'loss': 0.3656, 'learning_rate': 4.337184564625455e-05, 'epoch': 0.54}
{'loss': 0.3583, 'learning_rate': 4.334788622833452e-05, 'epoch': 0.54}
{'loss': 0.354, 'learning_rate': 4.3323928365305636e-05, 'epoch': 0.54}
{'loss': 0.3659, 'learning_rate': 4.3299972062767905e-05, 'epoch': 0.54}
{'loss': 0.3615, 'learning_rate': 4.3276017326320985e-05, 'epoch': 0.54}
39:53<8:50:08, 10.70s/it]                                                         54%|    | 3526/6500 [10:39:53<8:50:08, 10.70s/it] 54%|    | 3527/6500 [10:40:04<8:45:57, 10.61s/it]                                                         54%|    | 3527/6500 [10:40:04<8:45:57, 10.61s/it] 54%|    | 3528/6500 [10:40:14<8:43:03, 10.56s/it]                                                         54%|    | 3528/6500 [10:40:14<8:43:03, 10.56s/it] 54%|    | 3529/6500 [10:40:25<8:40:55, 10.52s/it]                                                         54%|    | 3529/6500 [10:40:25<8:40:55, 10.52s/it] 54%|    | 3530/6500 [10:40:35<8:39:13, 10.49s/it]                                                         54%|    | 3530/6500 [10:40:35<8:39:13, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8503511548042297, 'eval_runtime': 3.9481, 'eval_samples_per_second': 5.826, 'eval_steps_per_second': 1.52, 'epoch': 0.54}
                                                         54%|    | 3530/6500 [10:40:39<8:39:13, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3530/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3530/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3530/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3554, 'learning_rate': 4.3252064161564115e-05, 'epoch': 0.54}
{'loss': 0.3398, 'learning_rate': 4.322811257409622e-05, 'epoch': 0.54}
{'loss': 0.3686, 'learning_rate': 4.320416256951584e-05, 'epoch': 0.54}
{'loss': 0.4186, 'learning_rate': 4.3180214153421135e-05, 'epoch': 0.54}
{'loss': 0.3688, 'learning_rate': 4.315626733140992e-05, 'epoch': 0.54}
 54%|    | 3531/6500 [10:40:50<9:45:02, 11.82s/it]                                                         54%|    | 3531/6500 [10:40:50<9:45:02, 11.82s/it] 54%|    | 3532/6500 [10:41:00<9:23:43, 11.40s/it]                                                         54%|    | 3532/6500 [10:41:00<9:23:43, 11.40s/it] 54%|    | 3533/6500 [10:41:11<9:08:27, 11.09s/it]                                                         54%|    | 3533/6500 [10:41:11<9:08:27, 11.09s/it] 54%|    | 3534/6500 [10:41:21<8:57:45, 10.88s/it]                                                         54%|    | 3534/6500 [10:41:21<8:57:45, 10.88s/it] 54%|    | 3535/6500 [10:41:31<8:50:11, 10.73s/it]                                                         54%|    | 3535/6500 [10:41:31<8:50:11, 10.73s/it] 54%|    | 3536/6500 [10:{'loss': 0.3437, 'learning_rate': 4.3132322109079596e-05, 'epoch': 0.54}
{'loss': 0.368, 'learning_rate': 4.3108378492027224e-05, 'epoch': 0.54}
{'loss': 0.8792, 'learning_rate': 4.3084436485849475e-05, 'epoch': 0.54}
{'loss': 0.3763, 'learning_rate': 4.306049609614265e-05, 'epoch': 0.54}
{'loss': 0.348, 'learning_rate': 4.303655732850267e-05, 'epoch': 0.54}
41:42<8:44:50, 10.62s/it]                                                         54%|    | 3536/6500 [10:41:42<8:44:50, 10.62s/it] 54%|    | 3537/6500 [10:41:52<8:41:08, 10.55s/it]                                                         54%|    | 3537/6500 [10:41:52<8:41:08, 10.55s/it] 54%|    | 3538/6500 [10:42:03<8:38:11, 10.50s/it]                                                         54%|    | 3538/6500 [10:42:03<8:38:11, 10.50s/it] 54%|    | 3539/6500 [10:42:13<8:36:29, 10.47s/it]                                                         54%|    | 3539/6500 [10:42:13<8:36:29, 10.47s/it] 54%|    | 3540/6500 [10:42:24<8:44:36, 10.63s/it]                                                         54%|    | 3540/6500 [10:42:24<8:44:36, 10.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8490535020828247, 'eval_runtime': 3.9335, 'eval_samples_per_second': 5.847, 'eval_steps_per_second': 1.525, 'epoch': 0.54}
                                                         54%|    | 3540/6500 [10:42:28<8:44:36, 10.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3540I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3540

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3555, 'learning_rate': 4.301262018852509e-05, 'epoch': 0.54}
{'loss': 0.3397, 'learning_rate': 4.2988684681805036e-05, 'epoch': 0.54}
{'loss': 0.3751, 'learning_rate': 4.296475081393731e-05, 'epoch': 0.55}
{'loss': 0.3399, 'learning_rate': 4.294081859051632e-05, 'epoch': 0.55}
{'loss': 0.3432, 'learning_rate': 4.2916888017136055e-05, 'epoch': 0.55}
 54%|    | 3541/6500 [10:42:39<9:47:14, 11.91s/it]                                                         54%|    | 3541/6500 [10:42:39<9:47:14, 11.91s/it] 54%|    | 3542/6500 [10:42:49<9:24:20, 11.45s/it]                                                         54%|    | 3542/6500 [10:42:49<9:24:20, 11.45s/it] 55%|    | 3543/6500 [10:43:00<9:08:24, 11.13s/it]                                                         55%|    | 3543/6500 [10:43:00<9:08:24, 11.13s/it] 55%|    | 3544/6500 [10:43:10<8:57:01, 10.90s/it]                                                         55%|    | 3544/6500 [10:43:10<8:57:01, 10.90s/it] 55%|    | 3545/6500 [10:43:20<8:49:38, 10.75s/it]                                                         55%|    | 3545/6500 [10:43:20<8:49:38, 10.75s/it] 55%|    | 3546/6500 [10:{'loss': 0.3395, 'learning_rate': 4.289295909939016e-05, 'epoch': 0.55}
{'loss': 0.3528, 'learning_rate': 4.286903184287185e-05, 'epoch': 0.55}
{'loss': 0.3543, 'learning_rate': 4.2845106253174e-05, 'epoch': 0.55}
{'loss': 0.3401, 'learning_rate': 4.282118233588905e-05, 'epoch': 0.55}
{'loss': 0.3728, 'learning_rate': 4.279726009660909e-05, 'epoch': 0.55}
43:31<8:44:12, 10.65s/it]                                                         55%|    | 3546/6500 [10:43:31<8:44:12, 10.65s/it] 55%|    | 3547/6500 [10:43:41<8:40:28, 10.58s/it]                                                         55%|    | 3547/6500 [10:43:41<8:40:28, 10.58s/it] 55%|    | 3548/6500 [10:43:52<8:38:07, 10.53s/it]                                                         55%|    | 3548/6500 [10:43:52<8:38:07, 10.53s/it] 55%|    | 3549/6500 [10:44:02<8:36:19, 10.50s/it]                                                         55%|    | 3549/6500 [10:44:02<8:36:19, 10.50s/it] 55%|    | 3550/6500 [10:44:12<8:34:40, 10.47s/it]                                                         55%|    | 3550/6500 [10:44:12<8:34:40, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8456382751464844, 'eval_runtime': 3.9562, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.55}
                                                         55%|    | 3550/6500 [10:44:16<8:34:40, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3550
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3487, 'learning_rate': 4.277333954092579e-05, 'epoch': 0.55}
{'loss': 0.3626, 'learning_rate': 4.274942067443044e-05, 'epoch': 0.55}
{'loss': 0.3555, 'learning_rate': 4.272550350271391e-05, 'epoch': 0.55}
{'loss': 0.3457, 'learning_rate': 4.2701588031366706e-05, 'epoch': 0.55}
{'loss': 0.3615, 'learning_rate': 4.267767426597893e-05, 'epoch': 0.55}
 55%|    | 3551/6500 [10:44:27<9:40:13, 11.81s/it]                                                         55%|    | 3551/6500 [10:44:27<9:40:13, 11.81s/it] 55%|    | 3552/6500 [10:44:38<9:19:15, 11.38s/it]                                                         55%|    | 3552/6500 [10:44:38<9:19:15, 11.38s/it] 55%|    | 3553/6500 [10:44:48<9:04:46, 11.09s/it]                                                         55%|    | 3553/6500 [10:44:48<9:04:46, 11.09s/it] 55%|    | 3554/6500 [10:44:59<8:54:36, 10.89s/it]                                                         55%|    | 3554/6500 [10:44:59<8:54:36, 10.89s/it] 55%|    | 3555/6500 [10:45:09<8:47:15, 10.74s/it]                                                         55%|    | 3555/6500 [10:45:09<8:47:15, 10.74s/it] 55%|    | 3556/6500 [10:{'loss': 0.3518, 'learning_rate': 4.2653762212140266e-05, 'epoch': 0.55}
{'loss': 0.3482, 'learning_rate': 4.262985187544003e-05, 'epoch': 0.55}
{'loss': 0.3576, 'learning_rate': 4.2605943261467106e-05, 'epoch': 0.55}
{'loss': 0.3799, 'learning_rate': 4.2582036375809984e-05, 'epoch': 0.55}
{'loss': 0.3341, 'learning_rate': 4.2558131224056755e-05, 'epoch': 0.55}
45:20<8:46:28, 10.73s/it]                                                         55%|    | 3556/6500 [10:45:20<8:46:28, 10.73s/it] 55%|    | 3557/6500 [10:45:30<8:41:52, 10.64s/it]                                                         55%|    | 3557/6500 [10:45:30<8:41:52, 10.64s/it] 55%|    | 3558/6500 [10:45:41<8:38:29, 10.57s/it]                                                         55%|    | 3558/6500 [10:45:41<8:38:29, 10.57s/it] 55%|    | 3559/6500 [10:45:51<8:36:03, 10.53s/it]                                                         55%|    | 3559/6500 [10:45:51<8:36:03, 10.53s/it] 55%|    | 3560/6500 [10:46:01<8:34:33, 10.50s/it]                                                         55%|    | 3560/6500 [10:46:01<8:34:33, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8483265042304993, 'eval_runtime': 3.9574, 'eval_samples_per_second': 5.812, 'eval_steps_per_second': 1.516, 'epoch': 0.55}
                                                         55%|    | 3560/6500 [10:46:05<8:34:33, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3560
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3560/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3569, 'learning_rate': 4.253422781179511e-05, 'epoch': 0.55}
{'loss': 0.3345, 'learning_rate': 4.251032614461232e-05, 'epoch': 0.55}
{'loss': 0.3802, 'learning_rate': 4.248642622809526e-05, 'epoch': 0.55}
{'loss': 0.4106, 'learning_rate': 4.246252806783038e-05, 'epoch': 0.55}
{'loss': 0.3482, 'learning_rate': 4.243863166940374e-05, 'epoch': 0.55}
 55%|    | 3561/6500 [10:46:16<9:39:08, 11.82s/it]                                                         55%|    | 3561/6500 [10:46:16<9:39:08, 11.82s/it] 55%|    | 3562/6500 [10:46:27<9:17:58, 11.40s/it]                                                         55%|    | 3562/6500 [10:46:27<9:17:58, 11.40s/it] 55%|    | 3563/6500 [10:46:37<9:03:41, 11.11s/it]                                                         55%|    | 3563/6500 [10:46:37<9:03:41, 11.11s/it] 55%|    | 3564/6500 [10:46:48<8:53:26, 10.90s/it]                                                         55%|    | 3564/6500 [10:46:48<8:53:26, 10.90s/it] 55%|    | 3565/6500 [10:46:58<8:46:12, 10.76s/it]                                                         55%|    | 3565/6500 [10:46:58<8:46:12, 10.76s/it] 55%|    | 3566/6500 [10:{'loss': 0.3615, 'learning_rate': 4.2414737038400964e-05, 'epoch': 0.55}
{'loss': 0.3398, 'learning_rate': 4.2390844180407285e-05, 'epoch': 0.55}
{'loss': 0.8779, 'learning_rate': 4.236695310100752e-05, 'epoch': 0.55}
{'loss': 0.3634, 'learning_rate': 4.234306380578607e-05, 'epoch': 0.55}
{'loss': 0.3621, 'learning_rate': 4.231917630032689e-05, 'epoch': 0.55}
47:08<8:41:00, 10.65s/it]                                                         55%|    | 3566/6500 [10:47:08<8:41:00, 10.65s/it] 55%|    | 3567/6500 [10:47:19<8:37:29, 10.59s/it]                                                         55%|    | 3567/6500 [10:47:19<8:37:29, 10.59s/it] 55%|    | 3568/6500 [10:47:29<8:34:43, 10.53s/it]                                                         55%|    | 3568/6500 [10:47:29<8:34:43, 10.53s/it] 55%|    | 3569/6500 [10:47:40<8:32:48, 10.50s/it]                                                         55%|    | 3569/6500 [10:47:40<8:32:48, 10.50s/it] 55%|    | 3570/6500 [10:47:50<8:31:28, 10.47s/it]                                                         55%|    | 3570/6500 [10:47:50<8:31:28, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8521965742111206, 'eval_runtime': 3.9551, 'eval_samples_per_second': 5.815, 'eval_steps_per_second': 1.517, 'epoch': 0.55}
                                                         55%|    | 3570/6500 [10:47:54<8:31:28, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3347, 'learning_rate': 4.229529059021354e-05, 'epoch': 0.55}
{'loss': 0.3668, 'learning_rate': 4.227140668102918e-05, 'epoch': 0.55}
{'loss': 0.3559, 'learning_rate': 4.224752457835652e-05, 'epoch': 0.55}
{'loss': 0.3295, 'learning_rate': 4.222364428777786e-05, 'epoch': 0.55}
{'loss': 0.3461, 'learning_rate': 4.219976581487505e-05, 'epoch': 0.55}
 55%|    | 3571/6500 [10:48:05<9:37:01, 11.82s/it]                                                         55%|    | 3571/6500 [10:48:05<9:37:01, 11.82s/it] 55%|    | 3572/6500 [10:48:16<9:25:34, 11.59s/it]                                                         55%|    | 3572/6500 [10:48:16<9:25:34, 11.59s/it] 55%|    | 3573/6500 [10:48:26<9:07:58, 11.23s/it]                                                         55%|    | 3573/6500 [10:48:26<9:07:58, 11.23s/it] 55%|    | 3574/6500 [10:48:37<8:55:29, 10.98s/it]                                                         55%|    | 3574/6500 [10:48:37<8:55:29, 10.98s/it] 55%|    | 3575/6500 [10:48:47<8:46:48, 10.81s/it]                                                         55%|    | 3575/6500 [10:48:47<8:46:48, 10.81s/it] 55%|    | 3576/6500 [10:{'loss': 0.337, 'learning_rate': 4.217588916522956e-05, 'epoch': 0.55}
{'loss': 0.3512, 'learning_rate': 4.215201434442241e-05, 'epoch': 0.55}
{'loss': 0.3558, 'learning_rate': 4.2128141358034186e-05, 'epoch': 0.55}
{'loss': 0.3495, 'learning_rate': 4.210427021164506e-05, 'epoch': 0.55}
{'loss': 0.3552, 'learning_rate': 4.2080400910834773e-05, 'epoch': 0.55}
48:58<8:40:39, 10.68s/it]                                                         55%|    | 3576/6500 [10:48:58<8:40:39, 10.68s/it] 55%|    | 3577/6500 [10:49:08<8:36:24, 10.60s/it]                                                         55%|    | 3577/6500 [10:49:08<8:36:24, 10.60s/it] 55%|    | 3578/6500 [10:49:18<8:33:39, 10.55s/it]                                                         55%|    | 3578/6500 [10:49:18<8:33:39, 10.55s/it] 55%|    | 3579/6500 [10:49:29<8:31:37, 10.51s/it]                                                         55%|    | 3579/6500 [10:49:29<8:31:37, 10.51s/it] 55%|    | 3580/6500 [10:49:39<8:30:03, 10.48s/it]                                                         55%|    | 3580/6500 [10:49:39<8:30:03, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8427597284317017, 'eval_runtime': 3.9564, 'eval_samples_per_second': 5.813, 'eval_steps_per_second': 1.517, 'epoch': 0.55}
                                                         55%|    | 3580/6500 [10:49:43<8:30:03, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3580
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3580
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3415, 'learning_rate': 4.205653346118261e-05, 'epoch': 0.55}
{'loss': 0.3761, 'learning_rate': 4.203266786826745e-05, 'epoch': 0.55}
{'loss': 0.3464, 'learning_rate': 4.2008804137667744e-05, 'epoch': 0.55}
{'loss': 0.3611, 'learning_rate': 4.198494227496148e-05, 'epoch': 0.55}
{'loss': 0.3749, 'learning_rate': 4.1961082285726234e-05, 'epoch': 0.55}
 55%|    | 3581/6500 [10:49:54<9:34:32, 11.81s/it]                                                         55%|    | 3581/6500 [10:49:54<9:34:32, 11.81s/it] 55%|    | 3582/6500 [10:50:05<9:13:54, 11.39s/it]                                                         55%|    | 3582/6500 [10:50:05<9:13:54, 11.39s/it] 55%|    | 3583/6500 [10:50:15<8:59:26, 11.10s/it]                                                         55%|    | 3583/6500 [10:50:15<8:59:26, 11.10s/it] 55%|    | 3584/6500 [10:50:25<8:49:22, 10.89s/it]                                                         55%|    | 3584/6500 [10:50:25<8:49:22, 10.89s/it] 55%|    | 3585/6500 [10:50:36<8:42:16, 10.75s/it]                                                         55%|    | 3585/6500 [10:50:36<8:42:16, 10.75s/it] 55%|    | 3586/6500 [10:{'loss': 0.3405, 'learning_rate': 4.1937224175539116e-05, 'epoch': 0.55}
{'loss': 0.3605, 'learning_rate': 4.1913367949976826e-05, 'epoch': 0.55}
{'loss': 0.3594, 'learning_rate': 4.188951361461561e-05, 'epoch': 0.55}
{'loss': 0.358, 'learning_rate': 4.1865661175031276e-05, 'epoch': 0.55}
{'loss': 0.3395, 'learning_rate': 4.184181063679918e-05, 'epoch': 0.55}
50:46<8:37:16, 10.65s/it]                                                         55%|    | 3586/6500 [10:50:46<8:37:16, 10.65s/it] 55%|    | 3587/6500 [10:50:57<8:33:34, 10.58s/it]                                                         55%|    | 3587/6500 [10:50:57<8:33:34, 10.58s/it] 55%|    | 3588/6500 [10:51:07<8:34:52, 10.61s/it]                                                         55%|    | 3588/6500 [10:51:07<8:34:52, 10.61s/it] 55%|    | 3589/6500 [10:51:18<8:31:46, 10.55s/it]                                                         55%|    | 3589/6500 [10:51:18<8:31:46, 10.55s/it] 55%|    | 3590/6500 [10:51:28<8:30:20, 10.52s/it]                                                         55%|    | 3590/6500 [10:51:28<8:30:20, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8508092761039734, 'eval_runtime': 3.9545, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.55}
                                                         55%|    | 3590/6500 [10:51:32<8:30:20, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3590I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3404, 'learning_rate': 4.181796200549426e-05, 'epoch': 0.55}
{'loss': 0.3295, 'learning_rate': 4.179411528669094e-05, 'epoch': 0.55}
{'loss': 0.4399, 'learning_rate': 4.17702704859633e-05, 'epoch': 0.55}
{'loss': 0.3487, 'learning_rate': 4.174642760888491e-05, 'epoch': 0.55}
{'loss': 0.3366, 'learning_rate': 4.172258666102887e-05, 'epoch': 0.55}
 55%|    | 3591/6500 [10:51:43<9:34:27, 11.85s/it]                                                         55%|    | 3591/6500 [10:51:43<9:34:27, 11.85s/it] 55%|    | 3592/6500 [10:51:54<9:13:19, 11.42s/it]                                                         55%|    | 3592/6500 [10:51:54<9:13:19, 11.42s/it] 55%|    | 3593/6500 [10:52:04<8:58:20, 11.11s/it]                                                         55%|    | 3593/6500 [10:52:04<8:58:20, 11.11s/it] 55%|    | 3594/6500 [10:52:14<8:47:53, 10.90s/it]                                                         55%|    | 3594/6500 [10:52:14<8:47:53, 10.90s/it] 55%|    | 3595/6500 [10:52:25<8:40:28, 10.75s/it]                                                         55%|    | 3595/6500 [10:52:25<8:40:28, 10.75s/it] 55%|    | 3596/6500 [10:{'loss': 0.3596, 'learning_rate': 4.169874764796787e-05, 'epoch': 0.55}
{'loss': 0.8722, 'learning_rate': 4.167491057527413e-05, 'epoch': 0.55}
{'loss': 0.3681, 'learning_rate': 4.165107544851944e-05, 'epoch': 0.55}
{'loss': 0.3587, 'learning_rate': 4.162724227327509e-05, 'epoch': 0.55}
{'loss': 0.3582, 'learning_rate': 4.160341105511196e-05, 'epoch': 0.55}
52:35<8:35:20, 10.65s/it]                                                         55%|    | 3596/6500 [10:52:35<8:35:20, 10.65s/it] 55%|    | 3597/6500 [10:52:46<8:31:11, 10.57s/it]                                                         55%|    | 3597/6500 [10:52:46<8:31:11, 10.57s/it] 55%|    | 3598/6500 [10:52:56<8:28:42, 10.52s/it]                                                         55%|    | 3598/6500 [10:52:56<8:28:42, 10.52s/it] 55%|    | 3599/6500 [10:53:06<8:26:52, 10.48s/it]                                                         55%|    | 3599/6500 [10:53:06<8:26:52, 10.48s/it] 55%|    | 3600/6500 [10:53:17<8:25:21, 10.46s/it]                                                         55%|    | 3600/6500 [10:53:17<8:25:21, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8544365167617798, 'eval_runtime': 3.9536, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 1.518, 'epoch': 0.55}
                                                         55%|    | 3600/6500 [10:53:21<8:25:21, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3600/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3600/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3392, 'learning_rate': 4.157958179960044e-05, 'epoch': 0.55}
{'loss': 0.369, 'learning_rate': 4.155575451231048e-05, 'epoch': 0.55}
{'loss': 0.3483, 'learning_rate': 4.1531929198811556e-05, 'epoch': 0.55}
{'loss': 0.3275, 'learning_rate': 4.15081058646727e-05, 'epoch': 0.55}
{'loss': 0.3546, 'learning_rate': 4.148428451546247e-05, 'epoch': 0.55}
 55%|    | 3601/6500 [10:53:32<9:28:54, 11.77s/it]                                                         55%|    | 3601/6500 [10:53:32<9:28:54, 11.77s/it] 55%|    | 3602/6500 [10:53:42<9:08:43, 11.36s/it]                                                         55%|    | 3602/6500 [10:53:42<9:08:43, 11.36s/it] 55%|    | 3603/6500 [10:53:52<8:54:41, 11.07s/it]                                                         55%|    | 3603/6500 [10:53:52<8:54:41, 11.07s/it] 55%|    | 3604/6500 [10:54:04<8:55:20, 11.09s/it]                                                         55%|    | 3604/6500 [10:54:04<8:55:20, 11.09s/it] 55%|    | 3605/6500 [10:54:14<8:45:11, 10.88s/it]                                                         55%|    | 3605/6500 [10:54:14<8:45:11, 10.88s/it] 55%|    | 3606/6500 [10:{'loss': 0.3447, 'learning_rate': 4.1460465156748954e-05, 'epoch': 0.55}
{'loss': 0.3616, 'learning_rate': 4.143664779409978e-05, 'epoch': 0.55}
{'loss': 0.3537, 'learning_rate': 4.1412832433082124e-05, 'epoch': 0.56}
{'loss': 0.3652, 'learning_rate': 4.138901907926267e-05, 'epoch': 0.56}
{'loss': 0.3416, 'learning_rate': 4.136520773820765e-05, 'epoch': 0.56}
54:24<8:38:09, 10.74s/it]                                                         55%|    | 3606/6500 [10:54:24<8:38:09, 10.74s/it] 55%|    | 3607/6500 [10:54:35<8:34:53, 10.68s/it]                                                         55%|    | 3607/6500 [10:54:35<8:34:53, 10.68s/it] 56%|    | 3608/6500 [10:54:45<8:31:03, 10.60s/it]                                                         56%|    | 3608/6500 [10:54:45<8:31:03, 10.60s/it] 56%|    | 3609/6500 [10:54:56<8:28:04, 10.54s/it]                                                         56%|    | 3609/6500 [10:54:56<8:28:04, 10.54s/it] 56%|    | 3610/6500 [10:55:06<8:25:50, 10.50s/it]                                                         56%|    | 3610/6500 [10:55:06<8:25:50, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8435842990875244, 'eval_runtime': 3.9606, 'eval_samples_per_second': 5.807, 'eval_steps_per_second': 1.515, 'epoch': 0.56}
                                                         56%|    | 3610/6500 [10:55:10<8:25:50, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3610
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3610/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3610/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3719, 'learning_rate': 4.134139841548283e-05, 'epoch': 0.56}
{'loss': 0.3668, 'learning_rate': 4.131759111665349e-05, 'epoch': 0.56}
{'loss': 0.3491, 'learning_rate': 4.129378584728442e-05, 'epoch': 0.56}
{'loss': 0.3671, 'learning_rate': 4.1269982612939983e-05, 'epoch': 0.56}
{'loss': 0.3591, 'learning_rate': 4.124618141918403e-05, 'epoch': 0.56}
 56%|    | 3611/6500 [10:55:21<9:29:29, 11.83s/it]                                                         56%|    | 3611/6500 [10:55:21<9:29:29, 11.83s/it] 56%|    | 3612/6500 [10:55:31<9:08:49, 11.40s/it]                                                         56%|    | 3612/6500 [10:55:31<9:08:49, 11.40s/it] 56%|    | 3613/6500 [10:55:42<8:54:09, 11.10s/it]                                                         56%|    | 3613/6500 [10:55:42<8:54:09, 11.10s/it] 56%|    | 3614/6500 [10:55:52<8:44:06, 10.90s/it]                                                         56%|    | 3614/6500 [10:55:52<8:44:06, 10.90s/it] 56%|    | 3615/6500 [10:56:03<8:36:55, 10.75s/it]                                                         56%|    | 3615/6500 [10:56:03<8:36:55, 10.75s/it] 56%|    | 3616/6500 [10:{'loss': 0.3617, 'learning_rate': 4.122238227157994e-05, 'epoch': 0.56}
{'loss': 0.3421, 'learning_rate': 4.119858517569064e-05, 'epoch': 0.56}
{'loss': 0.3782, 'learning_rate': 4.117479013707854e-05, 'epoch': 0.56}
{'loss': 0.3497, 'learning_rate': 4.115099716130557e-05, 'epoch': 0.56}
{'loss': 0.3497, 'learning_rate': 4.112720625393322e-05, 'epoch': 0.56}
56:13<8:31:30, 10.64s/it]                                                         56%|    | 3616/6500 [10:56:13<8:31:30, 10.64s/it] 56%|    | 3617/6500 [10:56:24<8:27:53, 10.57s/it]                                                         56%|    | 3617/6500 [10:56:24<8:27:53, 10.57s/it] 56%|    | 3618/6500 [10:56:34<8:24:54, 10.51s/it]                                                         56%|    | 3618/6500 [10:56:34<8:24:54, 10.51s/it] 56%|    | 3619/6500 [10:56:44<8:22:51, 10.47s/it]                                                         56%|    | 3619/6500 [10:56:44<8:22:51, 10.47s/it] 56%|    | 3620/6500 [10:56:55<8:25:30, 10.53s/it]                                                         56%|    | 3620/6500 [10:56:55<8:25:30, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8449100852012634, 'eval_runtime': 3.9845, 'eval_samples_per_second': 5.772, 'eval_steps_per_second': 1.506, 'epoch': 0.56}
                                                         56%|    | 3620/6500 [10:56:59<8:25:30, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3620/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3394, 'learning_rate': 4.110341742052245e-05, 'epoch': 0.56}
{'loss': 0.3623, 'learning_rate': 4.1079630666633796e-05, 'epoch': 0.56}
{'loss': 0.4206, 'learning_rate': 4.105584599782721e-05, 'epoch': 0.56}
{'loss': 0.342, 'learning_rate': 4.1032063419662245e-05, 'epoch': 0.56}
{'loss': 0.3431, 'learning_rate': 4.100828293769794e-05, 'epoch': 0.56}
 56%|    | 3621/6500 [10:57:10<9:28:32, 11.85s/it]                                                         56%|    | 3621/6500 [10:57:10<9:28:32, 11.85s/it] 56%|    | 3622/6500 [10:57:20<9:06:23, 11.39s/it]                                                         56%|    | 3622/6500 [10:57:20<9:06:23, 11.39s/it] 56%|    | 3623/6500 [10:57:31<8:51:11, 11.08s/it]                                                         56%|    | 3623/6500 [10:57:31<8:51:11, 11.08s/it] 56%|    | 3624/6500 [10:57:41<8:40:26, 10.86s/it]                                                         56%|    | 3624/6500 [10:57:41<8:40:26, 10.86s/it] 56%|    | 3625/6500 [10:57:51<8:33:19, 10.71s/it]                                                         56%|    | 3625/6500 [10:57:51<8:33:19, 10.71s/it] 56%|    | 3626/6500 [10:{'loss': 0.3547, 'learning_rate': 4.098450455749281e-05, 'epoch': 0.56}
{'loss': 0.8741, 'learning_rate': 4.096072828460494e-05, 'epoch': 0.56}
{'loss': 0.3678, 'learning_rate': 4.093695412459188e-05, 'epoch': 0.56}
{'loss': 0.354, 'learning_rate': 4.0913182083010676e-05, 'epoch': 0.56}
{'loss': 0.3355, 'learning_rate': 4.088941216541791e-05, 'epoch': 0.56}
58:02<8:27:36, 10.60s/it]                                                         56%|    | 3626/6500 [10:58:02<8:27:36, 10.60s/it] 56%|    | 3627/6500 [10:58:12<8:23:30, 10.52s/it]                                                         56%|    | 3627/6500 [10:58:12<8:23:30, 10.52s/it] 56%|    | 3628/6500 [10:58:22<8:20:54, 10.46s/it]                                                         56%|    | 3628/6500 [10:58:22<8:20:54, 10.46s/it] 56%|    | 3629/6500 [10:58:33<8:18:56, 10.43s/it]                                                         56%|    | 3629/6500 [10:58:33<8:18:56, 10.43s/it] 56%|    | 3630/6500 [10:58:43<8:17:20, 10.40s/it]                                                         56%|    | 3630/6500 [10:58:43<8:17:20, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8560128211975098, 'eval_runtime': 3.9535, 'eval_samples_per_second': 5.818, 'eval_steps_per_second': 1.518, 'epoch': 0.56}
                                                         56%|    | 3630/6500 [10:58:47<8:17:20, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3630
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3630/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3352, 'learning_rate': 4.086564437736966e-05, 'epoch': 0.56}
{'loss': 0.363, 'learning_rate': 4.08418787244215e-05, 'epoch': 0.56}
{'loss': 0.3372, 'learning_rate': 4.081811521212851e-05, 'epoch': 0.56}
{'loss': 0.3397, 'learning_rate': 4.079435384604526e-05, 'epoch': 0.56}
{'loss': 0.3373, 'learning_rate': 4.077059463172582e-05, 'epoch': 0.56}
 56%|    | 3631/6500 [10:58:58<9:20:52, 11.73s/it]                                                         56%|    | 3631/6500 [10:58:58<9:20:52, 11.73s/it] 56%|    | 3632/6500 [10:59:08<9:00:42, 11.31s/it]                                                         56%|    | 3632/6500 [10:59:08<9:00:42, 11.31s/it] 56%|    | 3633/6500 [10:59:18<8:46:24, 11.02s/it]                                                         56%|    | 3633/6500 [10:59:18<8:46:24, 11.02s/it] 56%|    | 3634/6500 [10:59:29<8:36:29, 10.81s/it]                                                         56%|    | 3634/6500 [10:59:29<8:36:29, 10.81s/it] 56%|    | 3635/6500 [10:59:39<8:29:12, 10.66s/it]                                                         56%|    | 3635/6500 [10:59:39<8:29:12, 10.66s/it] 56%|    | 3636/6500 [10:{'loss': 0.347, 'learning_rate': 4.0746837574723776e-05, 'epoch': 0.56}
{'loss': 0.3475, 'learning_rate': 4.072308268059219e-05, 'epoch': 0.56}
{'loss': 0.344, 'learning_rate': 4.069932995488361e-05, 'epoch': 0.56}
{'loss': 0.3736, 'learning_rate': 4.0675579403150125e-05, 'epoch': 0.56}
{'loss': 0.3507, 'learning_rate': 4.0651831030943246e-05, 'epoch': 0.56}
59:49<8:24:14, 10.56s/it]                                                         56%|    | 3636/6500 [10:59:49<8:24:14, 10.56s/it] 56%|    | 3637/6500 [11:00:00<8:26:31, 10.62s/it]                                                         56%|    | 3637/6500 [11:00:00<8:26:31, 10.62s/it] 56%|    | 3638/6500 [11:00:10<8:22:32, 10.54s/it]                                                         56%|    | 3638/6500 [11:00:10<8:22:32, 10.54s/it] 56%|    | 3639/6500 [11:00:21<8:19:45, 10.48s/it]                                                         56%|    | 3639/6500 [11:00:21<8:19:45, 10.48s/it] 56%|    | 3640/6500 [11:00:33<8:36:38, 10.84s/it]                                                         56%|    | 3640/6500 [11:00:33<8:36:38, 10.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8433212041854858, 'eval_runtime': 4.1064, 'eval_samples_per_second': 5.601, 'eval_steps_per_second': 1.461, 'epoch': 0.56}
                                                         56%|    | 3640/6500 [11:00:37<8:36:38, 10.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3753, 'learning_rate': 4.062808484381403e-05, 'epoch': 0.56}
{'loss': 0.3721, 'learning_rate': 4.0604340847313e-05, 'epoch': 0.56}
{'loss': 0.3591, 'learning_rate': 4.058059904699017e-05, 'epoch': 0.56}
{'loss': 0.3626, 'learning_rate': 4.0556859448395037e-05, 'epoch': 0.56}
{'loss': 0.3551, 'learning_rate': 4.0533122057076604e-05, 'epoch': 0.56}
 56%|    | 3641/6500 [11:00:48<9:37:13, 12.11s/it]                                                         56%|    | 3641/6500 [11:00:48<9:37:13, 12.11s/it] 56%|    | 3642/6500 [11:00:58<9:11:47, 11.58s/it]                                                         56%|    | 3642/6500 [11:00:58<9:11:47, 11.58s/it] 56%|    | 3643/6500 [11:01:08<8:54:03, 11.22s/it]                                                         56%|    | 3643/6500 [11:01:08<8:54:03, 11.22s/it] 56%|    | 3644/6500 [11:01:19<8:41:25, 10.95s/it]                                                         56%|    | 3644/6500 [11:01:19<8:41:25, 10.95s/it] 56%|    | 3645/6500 [11:01:29<8:32:47, 10.78s/it]                                                         56%|    | 3645/6500 [11:01:29<8:32:47, 10.78s/it] 56%|    | 3646/6500 [11:{'loss': 0.3529, 'learning_rate': 4.050938687858333e-05, 'epoch': 0.56}
{'loss': 0.355, 'learning_rate': 4.048565391846316e-05, 'epoch': 0.56}
{'loss': 0.38, 'learning_rate': 4.046192318226354e-05, 'epoch': 0.56}
{'loss': 0.3312, 'learning_rate': 4.043819467553138e-05, 'epoch': 0.56}
{'loss': 0.3573, 'learning_rate': 4.0414468403813095e-05, 'epoch': 0.56}
01:39<8:26:38, 10.65s/it]                                                         56%|    | 3646/6500 [11:01:39<8:26:38, 10.65s/it] 56%|    | 3647/6500 [11:01:50<8:22:38, 10.57s/it]                                                         56%|    | 3647/6500 [11:01:50<8:22:38, 10.57s/it] 56%|    | 3648/6500 [11:02:00<8:19:20, 10.51s/it]                                                         56%|    | 3648/6500 [11:02:00<8:19:20, 10.51s/it] 56%|    | 3649/6500 [11:02:10<8:17:08, 10.46s/it]                                                         56%|    | 3649/6500 [11:02:10<8:17:08, 10.46s/it] 56%|    | 3650/6500 [11:02:21<8:15:18, 10.43s/it]                                                         56%|    | 3650/6500 [11:02:21<8:15:18, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.849271297454834, 'eval_runtime': 4.5823, 'eval_samples_per_second': 5.019, 'eval_steps_per_second': 1.309, 'epoch': 0.56}
                                                         56%|    | 3650/6500 [11:02:25<8:15:18, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3650I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3650

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3650/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3429, 'learning_rate': 4.039074437265452e-05, 'epoch': 0.56}
{'loss': 0.3838, 'learning_rate': 4.0367022587601035e-05, 'epoch': 0.56}
{'loss': 0.4006, 'learning_rate': 4.034330305419744e-05, 'epoch': 0.56}
{'loss': 0.3397, 'learning_rate': 4.031958577798805e-05, 'epoch': 0.56}
{'loss': 0.3601, 'learning_rate': 4.029587076451662e-05, 'epoch': 0.56}
 56%|    | 3651/6500 [11:02:36<9:26:43, 11.94s/it]                                                         56%|    | 3651/6500 [11:02:36<9:26:43, 11.94s/it] 56%|    | 3652/6500 [11:02:47<9:04:07, 11.46s/it]                                                         56%|    | 3652/6500 [11:02:47<9:04:07, 11.46s/it] 56%|    | 3653/6500 [11:02:57<8:52:06, 11.21s/it]                                                         56%|    | 3653/6500 [11:02:57<8:52:06, 11.21s/it] 56%|    | 3654/6500 [11:03:08<8:39:48, 10.96s/it]                                                         56%|    | 3654/6500 [11:03:08<8:39:48, 10.96s/it] 56%|    | 3655/6500 [11:03:18<8:30:54, 10.77s/it]                                                         56%|    | 3655/6500 [11:03:18<8:30:54, 10.77s/it] 56%|    | 3656/6500 [11:{'loss': 0.3452, 'learning_rate': 4.0272158019326414e-05, 'epoch': 0.56}
{'loss': 0.87, 'learning_rate': 4.024844754796011e-05, 'epoch': 0.56}
{'loss': 0.354, 'learning_rate': 4.0224739355959905e-05, 'epoch': 0.56}
{'loss': 0.368, 'learning_rate': 4.020103344886744e-05, 'epoch': 0.56}
{'loss': 0.3311, 'learning_rate': 4.017732983222382e-05, 'epoch': 0.56}
03:28<8:24:38, 10.65s/it]                                                         56%|    | 3656/6500 [11:03:28<8:24:38, 10.65s/it] 56%|    | 3657/6500 [11:03:39<8:20:11, 10.56s/it]                                                         56%|    | 3657/6500 [11:03:39<8:20:11, 10.56s/it] 56%|    | 3658/6500 [11:03:49<8:18:17, 10.52s/it]                                                         56%|    | 3658/6500 [11:03:49<8:18:17, 10.52s/it] 56%|    | 3659/6500 [11:03:59<8:15:51, 10.47s/it]                                                         56%|    | 3659/6500 [11:03:59<8:15:51, 10.47s/it] 56%|    | 3660/6500 [11:04:10<8:14:07, 10.44s/it]                                                         56%|    | 3660/6500 [11:04:10<8:14:07, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8564622402191162, 'eval_runtime': 3.9523, 'eval_samples_per_second': 5.819, 'eval_steps_per_second': 1.518, 'epoch': 0.56}
                                                         56%|    | 3660/6500 [11:04:14<8:14:07, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3660/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3668, 'learning_rate': 4.0153628511569646e-05, 'epoch': 0.56}
{'loss': 0.3506, 'learning_rate': 4.012992949244493e-05, 'epoch': 0.56}
{'loss': 0.3271, 'learning_rate': 4.010623278038919e-05, 'epoch': 0.56}
{'loss': 0.3478, 'learning_rate': 4.008253838094137e-05, 'epoch': 0.56}
{'loss': 0.3395, 'learning_rate': 4.005884629963991e-05, 'epoch': 0.56}
 56%|    | 3661/6500 [11:04:25<9:16:04, 11.75s/it]                                                         56%|    | 3661/6500 [11:04:25<9:16:04, 11.75s/it] 56%|    | 3662/6500 [11:04:35<8:56:03, 11.33s/it]                                                         56%|    | 3662/6500 [11:04:35<8:56:03, 11.33s/it] 56%|    | 3663/6500 [11:04:45<8:41:44, 11.03s/it]                                                         56%|    | 3663/6500 [11:04:45<8:41:44, 11.03s/it] 56%|    | 3664/6500 [11:04:56<8:31:57, 10.83s/it]                                                         56%|    | 3664/6500 [11:04:56<8:31:57, 10.83s/it] 56%|    | 3665/6500 [11:05:06<8:24:44, 10.68s/it]                                                         56%|    | 3665/6500 [11:05:06<8:24:44, 10.68s/it] 56%|    | 3666/6500 [11:{'loss': 0.3532, 'learning_rate': 4.0035156542022684e-05, 'epoch': 0.56}
{'loss': 0.3509, 'learning_rate': 4.001146911362702e-05, 'epoch': 0.56}
{'loss': 0.3578, 'learning_rate': 3.998778401998973e-05, 'epoch': 0.56}
{'loss': 0.3438, 'learning_rate': 3.9964101266647044e-05, 'epoch': 0.56}
{'loss': 0.3478, 'learning_rate': 3.9940420859134686e-05, 'epoch': 0.56}
05:16<8:20:01, 10.59s/it]                                                         56%|    | 3666/6500 [11:05:16<8:20:01, 10.59s/it] 56%|    | 3667/6500 [11:05:27<8:16:28, 10.51s/it]                                                         56%|    | 3667/6500 [11:05:27<8:16:28, 10.51s/it] 56%|    | 3668/6500 [11:05:37<8:13:58, 10.47s/it]                                                         56%|    | 3668/6500 [11:05:37<8:13:58, 10.47s/it] 56%|    | 3669/6500 [11:05:48<8:17:48, 10.55s/it]                                                         56%|    | 3669/6500 [11:05:48<8:17:48, 10.55s/it] 56%|    | 3670/6500 [11:05:58<8:14:55, 10.49s/it]                                                         56%|    | 3670/6500 [11:05:58<8:14:55, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8445848226547241, 'eval_runtime': 3.9634, 'eval_samples_per_second': 5.803, 'eval_steps_per_second': 1.514, 'epoch': 0.56}
                                                         56%|    | 3670/6500 [11:06:02<8:14:55, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3670I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3670/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3670/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3751, 'learning_rate': 3.9916742802987774e-05, 'epoch': 0.56}
{'loss': 0.336, 'learning_rate': 3.9893067103740925e-05, 'epoch': 0.56}
{'loss': 0.3513, 'learning_rate': 3.986939376692819e-05, 'epoch': 0.57}
{'loss': 0.3678, 'learning_rate': 3.9845722798083066e-05, 'epoch': 0.57}
{'loss': 0.3346, 'learning_rate': 3.98220542027385e-05, 'epoch': 0.57}
 56%|    | 3671/6500 [11:06:13<9:15:57, 11.79s/it]                                                         56%|    | 3671/6500 [11:06:13<9:15:57, 11.79s/it] 56%|    | 3672/6500 [11:06:23<8:55:38, 11.36s/it]                                                         56%|    | 3672/6500 [11:06:23<8:55:38, 11.36s/it] 57%|    | 3673/6500 [11:06:34<8:41:13, 11.06s/it]                                                         57%|    | 3673/6500 [11:06:34<8:41:13, 11.06s/it] 57%|    | 3674/6500 [11:06:44<8:31:13, 10.85s/it]                                                         57%|    | 3674/6500 [11:06:44<8:31:13, 10.85s/it] 57%|    | 3675/6500 [11:06:54<8:24:04, 10.71s/it]                                                         57%|    | 3675/6500 [11:06:54<8:24:04, 10.71s/it] 57%|    | 3676/6500 [11:{'loss': 0.3501, 'learning_rate': 3.97983879864269e-05, 'epoch': 0.57}
{'loss': 0.3672, 'learning_rate': 3.977472415468006e-05, 'epoch': 0.57}
{'loss': 0.3512, 'learning_rate': 3.975106271302928e-05, 'epoch': 0.57}
{'loss': 0.3496, 'learning_rate': 3.972740366700528e-05, 'epoch': 0.57}
{'loss': 0.3399, 'learning_rate': 3.970374702213821e-05, 'epoch': 0.57}
07:05<8:18:52, 10.60s/it]                                                         57%|    | 3676/6500 [11:07:05<8:18:52, 10.60s/it] 57%|    | 3677/6500 [11:07:15<8:15:14, 10.53s/it]                                                         57%|    | 3677/6500 [11:07:15<8:15:14, 10.53s/it] 57%|    | 3678/6500 [11:07:26<8:12:32, 10.47s/it]                                                         57%|    | 3678/6500 [11:07:26<8:12:32, 10.47s/it] 57%|    | 3679/6500 [11:07:36<8:10:44, 10.44s/it]                                                         57%|    | 3679/6500 [11:07:36<8:10:44, 10.44s/it] 57%|    | 3680/6500 [11:07:46<8:09:53, 10.42s/it]                                                         57%|    | 3680/6500 [11:07:46<8:09:53, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8488936424255371, 'eval_runtime': 4.1248, 'eval_samples_per_second': 5.576, 'eval_steps_per_second': 1.455, 'epoch': 0.57}
                                                         57%|    | 3680/6500 [11:07:50<8:09:53, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3571, 'learning_rate': 3.968009278395768e-05, 'epoch': 0.57}
{'loss': 0.4139, 'learning_rate': 3.9656440957992716e-05, 'epoch': 0.57}
{'loss': 0.3445, 'learning_rate': 3.9632791549771776e-05, 'epoch': 0.57}
{'loss': 0.3288, 'learning_rate': 3.960914456482278e-05, 'epoch': 0.57}
{'loss': 0.3627, 'learning_rate': 3.958550000867307e-05, 'epoch': 0.57}
 57%|    | 3681/6500 [11:08:02<9:25:38, 12.04s/it]                                                         57%|    | 3681/6500 [11:08:02<9:25:38, 12.04s/it] 57%|    | 3682/6500 [11:08:12<9:01:36, 11.53s/it]                                                         57%|    | 3682/6500 [11:08:12<9:01:36, 11.53s/it] 57%|    | 3683/6500 [11:08:23<8:44:52, 11.18s/it]                                                         57%|    | 3683/6500 [11:08:23<8:44:52, 11.18s/it] 57%|    | 3684/6500 [11:08:33<8:33:06, 10.93s/it]                                                         57%|    | 3684/6500 [11:08:33<8:33:06, 10.93s/it] 57%|    | 3685/6500 [11:08:44<8:28:07, 10.83s/it]                                                         57%|    | 3685/6500 [11:08:44<8:28:07, 10.83s/it] 57%|    | 3686/6500 [11:{'loss': 0.8733, 'learning_rate': 3.9561857886849405e-05, 'epoch': 0.57}
{'loss': 0.3607, 'learning_rate': 3.9538218204878015e-05, 'epoch': 0.57}
{'loss': 0.3636, 'learning_rate': 3.951458096828449e-05, 'epoch': 0.57}
{'loss': 0.3559, 'learning_rate': 3.9490946182593914e-05, 'epoch': 0.57}
{'loss': 0.3301, 'learning_rate': 3.9467313853330776e-05, 'epoch': 0.57}
08:54<8:20:52, 10.68s/it]                                                         57%|    | 3686/6500 [11:08:54<8:20:52, 10.68s/it] 57%|    | 3687/6500 [11:09:04<8:16:20, 10.59s/it]                                                         57%|    | 3687/6500 [11:09:04<8:16:20, 10.59s/it] 57%|    | 3688/6500 [11:09:15<8:13:01, 10.52s/it]                                                         57%|    | 3688/6500 [11:09:15<8:13:01, 10.52s/it] 57%|    | 3689/6500 [11:09:25<8:10:26, 10.47s/it]                                                         57%|    | 3689/6500 [11:09:25<8:10:26, 10.47s/it] 57%|    | 3690/6500 [11:09:35<8:08:32, 10.43s/it]                                                         57%|    | 3690/6500 [11:09:35<8:08:32, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8533867001533508, 'eval_runtime': 3.9394, 'eval_samples_per_second': 5.839, 'eval_steps_per_second': 1.523, 'epoch': 0.57}
                                                         57%|    | 3690/6500 [11:09:39<8:08:32, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3690
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3690/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3690/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3715, 'learning_rate': 3.944368398601898e-05, 'epoch': 0.57}
{'loss': 0.3459, 'learning_rate': 3.942005658618188e-05, 'epoch': 0.57}
{'loss': 0.3382, 'learning_rate': 3.9396431659342216e-05, 'epoch': 0.57}
{'loss': 0.349, 'learning_rate': 3.937280921102218e-05, 'epoch': 0.57}
{'loss': 0.3392, 'learning_rate': 3.934918924674338e-05, 'epoch': 0.57}
 57%|    | 3691/6500 [11:09:50<9:09:50, 11.74s/it]                                                         57%|    | 3691/6500 [11:09:50<9:09:50, 11.74s/it] 57%|    | 3692/6500 [11:10:01<8:50:02, 11.33s/it]                                                         57%|    | 3692/6500 [11:10:01<8:50:02, 11.33s/it] 57%|    | 3693/6500 [11:10:11<8:36:31, 11.04s/it]                                                         57%|    | 3693/6500 [11:10:11<8:36:31, 11.04s/it] 57%|    | 3694/6500 [11:10:21<8:26:43, 10.84s/it]                                                         57%|    | 3694/6500 [11:10:21<8:26:43, 10.84s/it] 57%|    | 3695/6500 [11:10:32<8:19:48, 10.69s/it]                                                         57%|    | 3695/6500 [11:10:32<8:19:48, 10.69s/it] 57%|    | 3696/6500 [11:{'loss': 0.3504, 'learning_rate': 3.9325571772026834e-05, 'epoch': 0.57}
{'loss': 0.3408, 'learning_rate': 3.930195679239298e-05, 'epoch': 0.57}
{'loss': 0.3605, 'learning_rate': 3.9278344313361696e-05, 'epoch': 0.57}
{'loss': 0.3411, 'learning_rate': 3.925473434045223e-05, 'epoch': 0.57}
{'loss': 0.3694, 'learning_rate': 3.923112687918328e-05, 'epoch': 0.57}
10:42<8:14:59, 10.59s/it]                                                         57%|    | 3696/6500 [11:10:42<8:14:59, 10.59s/it] 57%|    | 3697/6500 [11:10:52<8:11:21, 10.52s/it]                                                         57%|    | 3697/6500 [11:10:52<8:11:21, 10.52s/it] 57%|    | 3698/6500 [11:11:03<8:08:55, 10.47s/it]                                                         57%|    | 3698/6500 [11:11:03<8:08:55, 10.47s/it] 57%|    | 3699/6500 [11:11:13<8:07:18, 10.44s/it]                                                         57%|    | 3699/6500 [11:11:13<8:07:18, 10.44s/it] 57%|    | 3700/6500 [11:11:24<8:06:08, 10.42s/it]                                                         57%|    | 3700/6500 [11:11:24<8:06:08, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8422595858573914, 'eval_runtime': 3.9574, 'eval_samples_per_second': 5.812, 'eval_steps_per_second': 1.516, 'epoch': 0.57}
                                                         57%|    | 3700/6500 [11:11:27<8:06:08, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3700/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3533, 'learning_rate': 3.9207521935072946e-05, 'epoch': 0.57}
{'loss': 0.3549, 'learning_rate': 3.9183919513638734e-05, 'epoch': 0.57}
{'loss': 0.3596, 'learning_rate': 3.916031962039758e-05, 'epoch': 0.57}
{'loss': 0.3552, 'learning_rate': 3.913672226086581e-05, 'epoch': 0.57}
{'loss': 0.354, 'learning_rate': 3.911312744055917e-05, 'epoch': 0.57}
 57%|    | 3701/6500 [11:11:39<9:13:06, 11.86s/it]                                                         57%|    | 3701/6500 [11:11:39<9:13:06, 11.86s/it] 57%|    | 3702/6500 [11:11:49<8:52:06, 11.41s/it]                                                         57%|    | 3702/6500 [11:11:49<8:52:06, 11.41s/it] 57%|    | 3703/6500 [11:11:59<8:37:11, 11.09s/it]                                                         57%|    | 3703/6500 [11:11:59<8:37:11, 11.09s/it] 57%|    | 3704/6500 [11:12:10<8:27:06, 10.88s/it]                                                         57%|    | 3704/6500 [11:12:10<8:27:06, 10.88s/it] 57%|    | 3705/6500 [11:12:20<8:19:38, 10.73s/it]                                                         57%|    | 3705/6500 [11:12:20<8:19:38, 10.73s/it] 57%|    | 3706/6500 [11:{'loss': 0.3581, 'learning_rate': 3.908953516499278e-05, 'epoch': 0.57}
{'loss': 0.3616, 'learning_rate': 3.9065945439681214e-05, 'epoch': 0.57}
{'loss': 0.3422, 'learning_rate': 3.904235827013843e-05, 'epoch': 0.57}
{'loss': 0.344, 'learning_rate': 3.901877366187777e-05, 'epoch': 0.57}
{'loss': 0.3291, 'learning_rate': 3.8995191620412e-05, 'epoch': 0.57}
12:31<8:14:30, 10.62s/it]                                                         57%|    | 3706/6500 [11:12:31<8:14:30, 10.62s/it] 57%|    | 3707/6500 [11:12:41<8:10:43, 10.54s/it]                                                         57%|    | 3707/6500 [11:12:41<8:10:43, 10.54s/it] 57%|    | 3708/6500 [11:12:51<8:08:04, 10.49s/it]                                                         57%|    | 3708/6500 [11:12:51<8:08:04, 10.49s/it] 57%|    | 3709/6500 [11:13:02<8:06:14, 10.45s/it]                                                         57%|    | 3709/6500 [11:13:02<8:06:14, 10.45s/it] 57%|    | 3710/6500 [11:13:18<9:25:45, 12.17s/it]                                                         57%|    | 3710/6500 [11:13:18<9:25:45, 12.17s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8489845395088196, 'eval_runtime': 4.5435, 'eval_samples_per_second': 5.062, 'eval_steps_per_second': 1.321, 'epoch': 0.57}
                                                         57%|    | 3710/6500 [11:13:23<9:25:45, 12.17s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3710I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3710/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3683, 'learning_rate': 3.8971612151253275e-05, 'epoch': 0.57}
{'loss': 0.4145, 'learning_rate': 3.8948035259913154e-05, 'epoch': 0.57}
{'loss': 0.354, 'learning_rate': 3.89244609519026e-05, 'epoch': 0.57}
{'loss': 0.3388, 'learning_rate': 3.8900889232731954e-05, 'epoch': 0.57}
{'loss': 0.361, 'learning_rate': 3.887732010791098e-05, 'epoch': 0.57}
 57%|    | 3711/6500 [11:13:33<10:13:07, 13.19s/it]                                                          57%|    | 3711/6500 [11:13:33<10:13:07, 13.19s/it] 57%|    | 3712/6500 [11:13:44<9:33:15, 12.34s/it]                                                          57%|    | 3712/6500 [11:13:44<9:33:15, 12.34s/it] 57%|    | 3713/6500 [11:13:54<9:05:13, 11.74s/it]                                                         57%|    | 3713/6500 [11:13:54<9:05:13, 11.74s/it] 57%|    | 3714/6500 [11:14:04<8:45:43, 11.32s/it]                                                         57%|    | 3714/6500 [11:14:04<8:45:43, 11.32s/it] 57%|    | 3715/6500 [11:14:15<8:31:56, 11.03s/it]                                                         57%|    | 3715/6500 [11:14:15<8:31:56, 11.03s/it] 57%|    | 3716/6500 {'loss': 0.8828, 'learning_rate': 3.8853753582948785e-05, 'epoch': 0.57}
{'loss': 0.3674, 'learning_rate': 3.883018966335393e-05, 'epoch': 0.57}
{'loss': 0.3544, 'learning_rate': 3.880662835463432e-05, 'epoch': 0.57}
{'loss': 0.3225, 'learning_rate': 3.878306966229728e-05, 'epoch': 0.57}
{'loss': 0.3399, 'learning_rate': 3.875951359184951e-05, 'epoch': 0.57}
[11:14:25<8:22:20, 10.83s/it]                                                         57%|    | 3716/6500 [11:14:25<8:22:20, 10.83s/it] 57%|    | 3717/6500 [11:14:36<8:21:37, 10.81s/it]                                                         57%|    | 3717/6500 [11:14:36<8:21:37, 10.81s/it] 57%|    | 3718/6500 [11:14:46<8:15:08, 10.68s/it]                                                         57%|    | 3718/6500 [11:14:46<8:15:08, 10.68s/it] 57%|    | 3719/6500 [11:14:57<8:10:51, 10.59s/it]                                                         57%|    | 3719/6500 [11:14:57<8:10:51, 10.59s/it] 57%|    | 3720/6500 [11:15:07<8:07:35, 10.52s/it]                                                         57%|    | 3720/6500 [11:15:07<8:07:35, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8577142953872681, 'eval_runtime': 3.9549, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.57}
                                                         57%|    | 3720/6500 [11:15:11<8:07:35, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3720/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3720/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3720/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3659, 'learning_rate': 3.873596014879709e-05, 'epoch': 0.57}
{'loss': 0.3317, 'learning_rate': 3.87124093386455e-05, 'epoch': 0.57}
{'loss': 0.3395, 'learning_rate': 3.868886116689959e-05, 'epoch': 0.57}
{'loss': 0.3293, 'learning_rate': 3.866531563906362e-05, 'epoch': 0.57}
{'loss': 0.35, 'learning_rate': 3.86417727606412e-05, 'epoch': 0.57}
 57%|    | 3721/6500 [11:15:22<9:06:15, 11.79s/it]                                                         57%|    | 3721/6500 [11:15:22<9:06:15, 11.79s/it] 57%|    | 3722/6500 [11:15:32<8:46:10, 11.36s/it]                                                         57%|    | 3722/6500 [11:15:32<8:46:10, 11.36s/it] 57%|    | 3723/6500 [11:15:43<8:31:53, 11.06s/it]                                                         57%|    | 3723/6500 [11:15:43<8:31:53, 11.06s/it] 57%|    | 3724/6500 [11:15:53<8:21:46, 10.85s/it]                                                         57%|    | 3724/6500 [11:15:53<8:21:46, 10.85s/it] 57%|    | 3725/6500 [11:16:03<8:14:45, 10.70s/it]                                                         57%|    | 3725/6500 [11:16:03<8:14:45, 10.70s/it] 57%|    | 3726/6500 [11:{'loss': 0.3433, 'learning_rate': 3.861823253713535e-05, 'epoch': 0.57}
{'loss': 0.3397, 'learning_rate': 3.8594694974048426e-05, 'epoch': 0.57}
{'loss': 0.3581, 'learning_rate': 3.8571160076882204e-05, 'epoch': 0.57}
{'loss': 0.3455, 'learning_rate': 3.8547627851137836e-05, 'epoch': 0.57}
{'loss': 0.3703, 'learning_rate': 3.852409830231582e-05, 'epoch': 0.57}
16:14<8:09:45, 10.59s/it]                                                         57%|    | 3726/6500 [11:16:14<8:09:45, 10.59s/it] 57%|    | 3727/6500 [11:16:24<8:06:18, 10.52s/it]                                                         57%|    | 3727/6500 [11:16:24<8:06:18, 10.52s/it] 57%|    | 3728/6500 [11:16:34<8:03:47, 10.47s/it]                                                         57%|    | 3728/6500 [11:16:34<8:03:47, 10.47s/it] 57%|    | 3729/6500 [11:16:45<8:02:03, 10.44s/it]                                                         57%|    | 3729/6500 [11:16:45<8:02:03, 10.44s/it] 57%|    | 3730/6500 [11:16:55<8:01:28, 10.43s/it]                                                         57%|    | 3730/6500 [11:16:55<8:01:28, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8466129899024963, 'eval_runtime': 3.9396, 'eval_samples_per_second': 5.838, 'eval_steps_per_second': 1.523, 'epoch': 0.57}
                                                         57%|    | 3730/6500 [11:16:59<8:01:28, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3730
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3730/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3467, 'learning_rate': 3.850057143591605e-05, 'epoch': 0.57}
{'loss': 0.3372, 'learning_rate': 3.84770472574378e-05, 'epoch': 0.57}
{'loss': 0.3594, 'learning_rate': 3.845352577237969e-05, 'epoch': 0.57}
{'loss': 0.3413, 'learning_rate': 3.843000698623972e-05, 'epoch': 0.57}
{'loss': 0.3565, 'learning_rate': 3.840649090451527e-05, 'epoch': 0.57}
 57%|    | 3731/6500 [11:17:10<9:01:35, 11.74s/it]                                                         57%|    | 3731/6500 [11:17:10<9:01:35, 11.74s/it] 57%|    | 3732/6500 [11:17:20<8:42:23, 11.32s/it]                                                         57%|    | 3732/6500 [11:17:20<8:42:23, 11.32s/it] 57%|    | 3733/6500 [11:17:31<8:28:44, 11.03s/it]                                                         57%|    | 3733/6500 [11:17:31<8:28:44, 11.03s/it] 57%|    | 3734/6500 [11:17:41<8:22:52, 10.91s/it]                                                         57%|    | 3734/6500 [11:17:41<8:22:52, 10.91s/it] 57%|    | 3735/6500 [11:17:52<8:15:35, 10.75s/it]                                                         57%|    | 3735/6500 [11:17:52<8:15:35, 10.75s/it] 57%|    | 3736/6500 [11:{'loss': 0.3439, 'learning_rate': 3.838297753270308e-05, 'epoch': 0.57}
{'loss': 0.3637, 'learning_rate': 3.835946687629927e-05, 'epoch': 0.57}
{'loss': 0.3391, 'learning_rate': 3.83359589407993e-05, 'epoch': 0.58}
{'loss': 0.3444, 'learning_rate': 3.8312453731698e-05, 'epoch': 0.58}
{'loss': 0.3355, 'learning_rate': 3.8288951254489583e-05, 'epoch': 0.58}
18:02<8:09:50, 10.63s/it]                                                         57%|    | 3736/6500 [11:18:02<8:09:50, 10.63s/it] 57%|    | 3737/6500 [11:18:12<8:06:01, 10.55s/it]                                                         57%|    | 3737/6500 [11:18:12<8:06:01, 10.55s/it] 58%|    | 3738/6500 [11:18:23<8:03:16, 10.50s/it]                                                         58%|    | 3738/6500 [11:18:23<8:03:16, 10.50s/it] 58%|    | 3739/6500 [11:18:33<8:01:05, 10.45s/it]                                                         58%|    | 3739/6500 [11:18:33<8:01:05, 10.45s/it] 58%|    | 3740/6500 [11:18:43<7:59:30, 10.42s/it]                                                         58%|    | 3740/6500 [11:18:43<7:59:30, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8483185172080994, 'eval_runtime': 4.1958, 'eval_samples_per_second': 5.482, 'eval_steps_per_second': 1.43, 'epoch': 0.58}
                                                         58%|    | 3740/6500 [11:18:48<7:59:30, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3740I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3740

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3740/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4189, 'learning_rate': 3.82654515146676e-05, 'epoch': 0.58}
{'loss': 0.3506, 'learning_rate': 3.824195451772499e-05, 'epoch': 0.58}
{'loss': 0.3353, 'learning_rate': 3.8218460269154e-05, 'epoch': 0.58}
{'loss': 0.3536, 'learning_rate': 3.81949687744463e-05, 'epoch': 0.58}
{'loss': 0.6898, 'learning_rate': 3.817148003909288e-05, 'epoch': 0.58}
 58%|    | 3741/6500 [11:18:58<9:02:25, 11.80s/it]                                                         58%|    | 3741/6500 [11:18:58<9:02:25, 11.80s/it] 58%|    | 3742/6500 [11:19:09<8:42:30, 11.37s/it]                                                         58%|    | 3742/6500 [11:19:09<8:42:30, 11.37s/it] 58%|    | 3743/6500 [11:19:19<8:28:26, 11.07s/it]                                                         58%|    | 3743/6500 [11:19:19<8:28:26, 11.07s/it] 58%|    | 3744/6500 [11:19:29<8:18:26, 10.85s/it]                                                         58%|    | 3744/6500 [11:19:29<8:18:26, 10.85s/it] 58%|    | 3745/6500 [11:19:40<8:11:12, 10.70s/it]                                                         58%|    | 3745/6500 [11:19:40<8:11:12, 10.70s/it] 58%|    | 3746/6500 [11:{'loss': 0.5303, 'learning_rate': 3.8147994068584087e-05, 'epoch': 0.58}
{'loss': 0.3574, 'learning_rate': 3.812451086840961e-05, 'epoch': 0.58}
{'loss': 0.3501, 'learning_rate': 3.8101030444058515e-05, 'epoch': 0.58}
{'loss': 0.3309, 'learning_rate': 3.807755280101921e-05, 'epoch': 0.58}
{'loss': 0.3579, 'learning_rate': 3.8054077944779434e-05, 'epoch': 0.58}
19:50<8:06:30, 10.60s/it]                                                         58%|    | 3746/6500 [11:19:50<8:06:30, 10.60s/it] 58%|    | 3747/6500 [11:20:01<8:03:14, 10.53s/it]                                                         58%|    | 3747/6500 [11:20:01<8:03:14, 10.53s/it] 58%|    | 3748/6500 [11:20:11<8:00:27, 10.48s/it]                                                         58%|    | 3748/6500 [11:20:11<8:00:27, 10.48s/it] 58%|    | 3749/6500 [11:20:21<7:58:43, 10.44s/it]                                                         58%|    | 3749/6500 [11:20:21<7:58:43, 10.44s/it] 58%|    | 3750/6500 [11:20:32<8:02:25, 10.53s/it]                                                         58%|    | 3750/6500 [11:20:32<8:02:25, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.862272322177887, 'eval_runtime': 3.947, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.58}
                                                         58%|    | 3750/6500 [11:20:36<8:02:25, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3750the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3750

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3750/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3516, 'learning_rate': 3.803060588082633e-05, 'epoch': 0.58}
{'loss': 0.3142, 'learning_rate': 3.800713661464631e-05, 'epoch': 0.58}
{'loss': 0.3484, 'learning_rate': 3.7983670151725195e-05, 'epoch': 0.58}
{'loss': 0.3235, 'learning_rate': 3.7960206497548115e-05, 'epoch': 0.58}
{'loss': 0.3492, 'learning_rate': 3.793674565759957e-05, 'epoch': 0.58}
 58%|    | 3751/6500 [11:20:47<9:01:02, 11.81s/it]                                                         58%|    | 3751/6500 [11:20:47<9:01:02, 11.81s/it] 58%|    | 3752/6500 [11:20:57<8:40:48, 11.37s/it]                                                         58%|    | 3752/6500 [11:20:57<8:40:48, 11.37s/it] 58%|    | 3753/6500 [11:21:07<8:26:55, 11.07s/it]                                                         58%|    | 3753/6500 [11:21:07<8:26:55, 11.07s/it] 58%|    | 3754/6500 [11:21:18<8:16:51, 10.86s/it]                                                         58%|    | 3754/6500 [11:21:18<8:16:51, 10.86s/it] 58%|    | 3755/6500 [11:21:28<8:09:53, 10.71s/it]                                                         58%|    | 3755/6500 [11:21:28<8:09:53, 10.71s/it] 58%|    | 3756/6500 [11:{'loss': 0.3392, 'learning_rate': 3.791328763736337e-05, 'epoch': 0.58}
{'loss': 0.3478, 'learning_rate': 3.788983244232272e-05, 'epoch': 0.58}
{'loss': 0.3481, 'learning_rate': 3.7866380077960085e-05, 'epoch': 0.58}
{'loss': 0.3406, 'learning_rate': 3.784293054975734e-05, 'epoch': 0.58}
{'loss': 0.3649, 'learning_rate': 3.781948386319566e-05, 'epoch': 0.58}
21:39<8:05:04, 10.61s/it]                                                         58%|    | 3756/6500 [11:21:39<8:05:04, 10.61s/it] 58%|    | 3757/6500 [11:21:49<8:01:37, 10.53s/it]                                                         58%|    | 3757/6500 [11:21:49<8:01:37, 10.53s/it] 58%|    | 3758/6500 [11:21:59<7:59:02, 10.48s/it]                                                         58%|    | 3758/6500 [11:21:59<7:59:02, 10.48s/it] 58%|    | 3759/6500 [11:22:10<7:57:10, 10.45s/it]                                                         58%|    | 3759/6500 [11:22:10<7:57:10, 10.45s/it] 58%|    | 3760/6500 [11:22:20<7:55:47, 10.42s/it]                                                         58%|    | 3760/6500 [11:22:20<7:55:47, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8461377620697021, 'eval_runtime': 3.9642, 'eval_samples_per_second': 5.802, 'eval_steps_per_second': 1.514, 'epoch': 0.58}
                                                         58%|    | 3760/6500 [11:22:24<7:55:47, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3760
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3760/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3438, 'learning_rate': 3.7796040023755566e-05, 'epoch': 0.58}
{'loss': 0.358, 'learning_rate': 3.777259903691692e-05, 'epoch': 0.58}
{'loss': 0.3564, 'learning_rate': 3.774916090815891e-05, 'epoch': 0.58}
{'loss': 0.3417, 'learning_rate': 3.772572564296005e-05, 'epoch': 0.58}
{'loss': 0.345, 'learning_rate': 3.770229324679818e-05, 'epoch': 0.58}
 58%|    | 3761/6500 [11:22:35<8:56:11, 11.75s/it]                                                         58%|    | 3761/6500 [11:22:35<8:56:11, 11.75s/it] 58%|    | 3762/6500 [11:22:45<8:37:17, 11.34s/it]                                                         58%|    | 3762/6500 [11:22:45<8:37:17, 11.34s/it] 58%|    | 3763/6500 [11:22:56<8:23:45, 11.04s/it]                                                         58%|    | 3763/6500 [11:22:56<8:23:45, 11.04s/it] 58%|    | 3764/6500 [11:23:06<8:14:29, 10.84s/it]                                                         58%|    | 3764/6500 [11:23:06<8:14:29, 10.84s/it] 58%|    | 3765/6500 [11:23:16<8:07:55, 10.70s/it]                                                         58%|    | 3765/6500 [11:23:16<8:07:55, 10.70s/it] 58%|    | 3766/6500 [11:{'loss': 0.3598, 'learning_rate': 3.7678863725150505e-05, 'epoch': 0.58}
{'loss': 0.3494, 'learning_rate': 3.765543708349351e-05, 'epoch': 0.58}
{'loss': 0.3429, 'learning_rate': 3.7632013327303055e-05, 'epoch': 0.58}
{'loss': 0.3263, 'learning_rate': 3.760859246205427e-05, 'epoch': 0.58}
{'loss': 0.3543, 'learning_rate': 3.7585174493221664e-05, 'epoch': 0.58}
23:27<8:06:51, 10.68s/it]                                                         58%|    | 3766/6500 [11:23:27<8:06:51, 10.68s/it] 58%|    | 3767/6500 [11:23:37<8:02:27, 10.59s/it]                                                         58%|    | 3767/6500 [11:23:37<8:02:27, 10.59s/it] 58%|    | 3768/6500 [11:23:48<7:59:20, 10.53s/it]                                                         58%|    | 3768/6500 [11:23:48<7:59:20, 10.53s/it] 58%|    | 3769/6500 [11:23:58<7:57:00, 10.48s/it]                                                         58%|    | 3769/6500 [11:23:58<7:57:00, 10.48s/it] 58%|    | 3770/6500 [11:24:08<7:55:15, 10.45s/it]                                                         58%|    | 3770/6500 [11:24:08<7:55:15, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8487945795059204, 'eval_runtime': 4.1784, 'eval_samples_per_second': 5.505, 'eval_steps_per_second': 1.436, 'epoch': 0.58}
                                                         58%|    | 3770/6500 [11:24:13<7:55:15, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3770/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3770/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3770/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4079, 'learning_rate': 3.756175942627904e-05, 'epoch': 0.58}
{'loss': 0.3406, 'learning_rate': 3.753834726669951e-05, 'epoch': 0.58}
{'loss': 0.3242, 'learning_rate': 3.7514938019955554e-05, 'epoch': 0.58}
{'loss': 0.3611, 'learning_rate': 3.7491531691518935e-05, 'epoch': 0.58}
{'loss': 0.8662, 'learning_rate': 3.74681282868607e-05, 'epoch': 0.58}
 58%|    | 3771/6500 [11:24:23<8:57:26, 11.82s/it]                                                         58%|    | 3771/6500 [11:24:23<8:57:26, 11.82s/it] 58%|    | 3772/6500 [11:24:34<8:37:35, 11.38s/it]                                                         58%|    | 3772/6500 [11:24:34<8:37:35, 11.38s/it] 58%|    | 3773/6500 [11:24:44<8:23:36, 11.08s/it]                                                         58%|    | 3773/6500 [11:24:44<8:23:36, 11.08s/it] 58%|    | 3774/6500 [11:24:55<8:14:19, 10.88s/it]                                                         58%|    | 3774/6500 [11:24:55<8:14:19, 10.88s/it] 58%|    | 3775/6500 [11:25:05<8:06:43, 10.72s/it]                                                         58%|    | 3775/6500 [11:25:05<8:06:43, 10.72s/it] 58%|    | 3776/6500 [11:{'loss': 0.3602, 'learning_rate': 3.744472781145131e-05, 'epoch': 0.58}
{'loss': 0.3464, 'learning_rate': 3.742133027076043e-05, 'epoch': 0.58}
{'loss': 0.3537, 'learning_rate': 3.739793567025714e-05, 'epoch': 0.58}
{'loss': 0.32, 'learning_rate': 3.737454401540977e-05, 'epoch': 0.58}
{'loss': 0.3727, 'learning_rate': 3.735115531168596e-05, 'epoch': 0.58}
25:15<8:01:52, 10.61s/it]                                                         58%|    | 3776/6500 [11:25:15<8:01:52, 10.61s/it] 58%|    | 3777/6500 [11:25:26<7:58:14, 10.54s/it]                                                         58%|    | 3777/6500 [11:25:26<7:58:14, 10.54s/it] 58%|    | 3778/6500 [11:25:36<7:55:40, 10.49s/it]                                                         58%|    | 3778/6500 [11:25:36<7:55:40, 10.49s/it] 58%|    | 3779/6500 [11:25:47<7:55:22, 10.48s/it]                                                         58%|    | 3779/6500 [11:25:47<7:55:22, 10.48s/it] 58%|    | 3780/6500 [11:25:57<7:54:03, 10.46s/it]                                                         58%|    | 3780/6500 [11:25:57<7:54:03, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8623568415641785, 'eval_runtime': 4.5437, 'eval_samples_per_second': 5.062, 'eval_steps_per_second': 1.321, 'epoch': 0.58}
                                                         58%|    | 3780/6500 [11:26:01<7:54:03, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3780
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.327, 'learning_rate': 3.73277695645527e-05, 'epoch': 0.58}
{'loss': 0.3349, 'learning_rate': 3.730438677947624e-05, 'epoch': 0.58}
{'loss': 0.3406, 'learning_rate': 3.728100696192218e-05, 'epoch': 0.58}
{'loss': 0.3517, 'learning_rate': 3.725763011735542e-05, 'epoch': 0.58}
{'loss': 0.3496, 'learning_rate': 3.723425625124015e-05, 'epoch': 0.58}
 58%|    | 3781/6500 [11:26:12<9:00:09, 11.92s/it]                                                         58%|    | 3781/6500 [11:26:12<9:00:09, 11.92s/it] 58%|    | 3782/6500 [11:26:23<8:43:33, 11.56s/it]                                                         58%|    | 3782/6500 [11:26:23<8:43:33, 11.56s/it] 58%|    | 3783/6500 [11:26:33<8:26:43, 11.19s/it]                                                         58%|    | 3783/6500 [11:26:33<8:26:43, 11.19s/it] 58%|    | 3784/6500 [11:26:44<8:15:09, 10.94s/it]                                                         58%|    | 3784/6500 [11:26:44<8:15:09, 10.94s/it] 58%|    | 3785/6500 [11:26:54<8:07:08, 10.77s/it]                                                         58%|    | 3785/6500 [11:26:54<8:07:08, 10.77s/it] 58%|    | 3786/6500 [11:{'loss': 0.3466, 'learning_rate': 3.721088536903986e-05, 'epoch': 0.58}
{'loss': 0.3692, 'learning_rate': 3.718751747621735e-05, 'epoch': 0.58}
{'loss': 0.339, 'learning_rate': 3.7164152578234734e-05, 'epoch': 0.58}
{'loss': 0.37, 'learning_rate': 3.714079068055341e-05, 'epoch': 0.58}
{'loss': 0.3553, 'learning_rate': 3.711743178863407e-05, 'epoch': 0.58}
27:04<8:01:20, 10.64s/it]                                                         58%|    | 3786/6500 [11:27:04<8:01:20, 10.64s/it] 58%|    | 3787/6500 [11:27:15<7:57:05, 10.55s/it]                                                         58%|    | 3787/6500 [11:27:15<7:57:05, 10.55s/it] 58%|    | 3788/6500 [11:27:25<7:54:17, 10.49s/it]                                                         58%|    | 3788/6500 [11:27:25<7:54:17, 10.49s/it] 58%|    | 3789/6500 [11:27:35<7:52:01, 10.45s/it]                                                         58%|    | 3789/6500 [11:27:35<7:52:01, 10.45s/it] 58%|    | 3790/6500 [11:27:46<7:50:27, 10.42s/it]                                                         58%|    | 3790/6500 [11:27:46<7:50:27, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8430207967758179, 'eval_runtime': 3.9563, 'eval_samples_per_second': 5.813, 'eval_steps_per_second': 1.517, 'epoch': 0.58}
                                                         58%|    | 3790/6500 [11:27:50<7:50:27, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3790/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3790/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3404, 'learning_rate': 3.709407590793673e-05, 'epoch': 0.58}
{'loss': 0.3607, 'learning_rate': 3.707072304392068e-05, 'epoch': 0.58}
{'loss': 0.355, 'learning_rate': 3.70473732020445e-05, 'epoch': 0.58}
{'loss': 0.3473, 'learning_rate': 3.702402638776609e-05, 'epoch': 0.58}
{'loss': 0.3472, 'learning_rate': 3.7000682606542605e-05, 'epoch': 0.58}
 58%|    | 3791/6500 [11:28:01<8:49:22, 11.72s/it]                                                         58%|    | 3791/6500 [11:28:01<8:49:22, 11.72s/it] 58%|    | 3792/6500 [11:28:11<8:30:40, 11.31s/it]                                                         58%|    | 3792/6500 [11:28:11<8:30:40, 11.31s/it] 58%|    | 3793/6500 [11:28:21<8:17:36, 11.03s/it]                                                         58%|    | 3793/6500 [11:28:21<8:17:36, 11.03s/it] 58%|    | 3794/6500 [11:28:32<8:08:21, 10.83s/it]                                                         58%|    | 3794/6500 [11:28:32<8:08:21, 10.83s/it] 58%|    | 3795/6500 [11:28:42<8:01:52, 10.69s/it]                                                         58%|    | 3795/6500 [11:28:42<8:01:52, 10.69s/it] 58%|    | 3796/6500 [11:{'loss': 0.3689, 'learning_rate': 3.6977341863830534e-05, 'epoch': 0.58}
{'loss': 0.3359, 'learning_rate': 3.695400416508562e-05, 'epoch': 0.58}
{'loss': 0.3442, 'learning_rate': 3.6930669515762906e-05, 'epoch': 0.58}
{'loss': 0.3291, 'learning_rate': 3.690733792131673e-05, 'epoch': 0.58}
{'loss': 0.3679, 'learning_rate': 3.6884009387200714e-05, 'epoch': 0.58}
28:52<7:57:37, 10.60s/it]                                                         58%|    | 3796/6500 [11:28:52<7:57:37, 10.60s/it] 58%|    | 3797/6500 [11:29:03<7:54:17, 10.53s/it]                                                         58%|    | 3797/6500 [11:29:03<7:54:17, 10.53s/it] 58%|    | 3798/6500 [11:29:13<7:54:59, 10.55s/it]                                                         58%|    | 3798/6500 [11:29:13<7:54:59, 10.55s/it] 58%|    | 3799/6500 [11:29:24<7:52:08, 10.49s/it]                                                         58%|    | 3799/6500 [11:29:24<7:52:08, 10.49s/it] 58%|    | 3800/6500 [11:29:34<7:50:04, 10.45s/it]                                                         58%|    | 3800/6500 [11:29:34<7:50:04, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8490192294120789, 'eval_runtime': 3.9501, 'eval_samples_per_second': 5.823, 'eval_steps_per_second': 1.519, 'epoch': 0.58}
                                                         58%|    | 3800/6500 [11:29:38<7:50:04, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3800
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3800/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3952, 'learning_rate': 3.6860683918867756e-05, 'epoch': 0.58}
{'loss': 0.3395, 'learning_rate': 3.683736152177005e-05, 'epoch': 0.58}
{'loss': 0.3569, 'learning_rate': 3.6814042201359056e-05, 'epoch': 0.59}
{'loss': 0.3358, 'learning_rate': 3.6790725963085516e-05, 'epoch': 0.59}
{'loss': 0.8653, 'learning_rate': 3.6767412812399473e-05, 'epoch': 0.59}
 58%|    | 3801/6500 [11:29:49<8:53:04, 11.85s/it]                                                         58%|    | 3801/6500 [11:29:49<8:53:04, 11.85s/it] 58%|    | 3802/6500 [11:30:00<8:33:42, 11.42s/it]                                                         58%|    | 3802/6500 [11:30:00<8:33:42, 11.42s/it] 59%|    | 3803/6500 [11:30:10<8:18:54, 11.10s/it]                                                         59%|    | 3803/6500 [11:30:10<8:18:54, 11.10s/it] 59%|    | 3804/6500 [11:30:20<8:08:42, 10.88s/it]                                                         59%|    | 3804/6500 [11:30:20<8:08:42, 10.88s/it] 59%|    | 3805/6500 [11:30:31<8:01:34, 10.72s/it]                                                         59%|    | 3805/6500 [11:30:31<8:01:34, 10.72s/it] 59%|    | 3806/6500 [11:{'loss': 0.3625, 'learning_rate': 3.674410275475023e-05, 'epoch': 0.59}
{'loss': 0.3517, 'learning_rate': 3.6720795795586384e-05, 'epoch': 0.59}
{'loss': 0.3218, 'learning_rate': 3.6697491940355765e-05, 'epoch': 0.59}
{'loss': 0.344, 'learning_rate': 3.667419119450553e-05, 'epoch': 0.59}
{'loss': 0.354, 'learning_rate': 3.665089356348208e-05, 'epoch': 0.59}
30:41<7:56:22, 10.61s/it]                                                         59%|    | 3806/6500 [11:30:41<7:56:22, 10.61s/it] 59%|    | 3807/6500 [11:30:51<7:52:46, 10.53s/it]                                                         59%|    | 3807/6500 [11:30:51<7:52:46, 10.53s/it] 59%|    | 3808/6500 [11:31:02<7:50:12, 10.48s/it]                                                         59%|    | 3808/6500 [11:31:02<7:50:12, 10.48s/it] 59%|    | 3809/6500 [11:31:12<7:48:13, 10.44s/it]                                                         59%|    | 3809/6500 [11:31:12<7:48:13, 10.44s/it] 59%|    | 3810/6500 [11:31:22<7:46:42, 10.41s/it]                                                         59%|    | 3810/6500 [11:31:22<7:46:42, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.862022340297699, 'eval_runtime': 3.959, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.59}
                                                         59%|    | 3810/6500 [11:31:26<7:46:42, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3163, 'learning_rate': 3.662759905273109e-05, 'epoch': 0.59}
{'loss': 0.3358, 'learning_rate': 3.660430766769752e-05, 'epoch': 0.59}
{'loss': 0.3336, 'learning_rate': 3.65810194138256e-05, 'epoch': 0.59}
{'loss': 0.3427, 'learning_rate': 3.655773429655879e-05, 'epoch': 0.59}
{'loss': 0.3455, 'learning_rate': 3.6534452321339854e-05, 'epoch': 0.59}
 59%|    | 3811/6500 [11:31:37<8:45:57, 11.74s/it]                                                         59%|    | 3811/6500 [11:31:37<8:45:57, 11.74s/it] 59%|    | 3812/6500 [11:31:48<8:27:19, 11.32s/it]                                                         59%|    | 3812/6500 [11:31:48<8:27:19, 11.32s/it] 59%|    | 3813/6500 [11:31:58<8:14:05, 11.03s/it]                                                         59%|    | 3813/6500 [11:31:58<8:14:05, 11.03s/it] 59%|    | 3814/6500 [11:32:09<8:09:47, 10.94s/it]                                                         59%|    | 3814/6500 [11:32:09<8:09:47, 10.94s/it] 59%|    | 3815/6500 [11:32:19<8:01:41, 10.76s/it]                                                         59%|    | 3815/6500 [11:32:19<8:01:41, 10.76s/it] 59%|    | 3816/6500 [11:{'loss': 0.3434, 'learning_rate': 3.6511173493610825e-05, 'epoch': 0.59}
{'loss': 0.3531, 'learning_rate': 3.648789781881297e-05, 'epoch': 0.59}
{'loss': 0.3473, 'learning_rate': 3.646462530238684e-05, 'epoch': 0.59}
{'loss': 0.3812, 'learning_rate': 3.6441355949772253e-05, 'epoch': 0.59}
{'loss': 0.3503, 'learning_rate': 3.641808976640828e-05, 'epoch': 0.59}
32:29<7:56:09, 10.64s/it]                                                         59%|    | 3816/6500 [11:32:29<7:56:09, 10.64s/it] 59%|    | 3817/6500 [11:32:40<7:52:18, 10.56s/it]                                                         59%|    | 3817/6500 [11:32:40<7:52:18, 10.56s/it] 59%|    | 3818/6500 [11:32:50<7:49:32, 10.50s/it]                                                         59%|    | 3818/6500 [11:32:50<7:49:32, 10.50s/it] 59%|    | 3819/6500 [11:33:00<7:47:22, 10.46s/it]                                                         59%|    | 3819/6500 [11:33:00<7:47:22, 10.46s/it] 59%|    | 3820/6500 [11:33:11<7:45:54, 10.43s/it]                                                         59%|    | 3820/6500 [11:33:11<7:45:54, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8436095118522644, 'eval_runtime': 3.9598, 'eval_samples_per_second': 5.808, 'eval_steps_per_second': 1.515, 'epoch': 0.59}
                                                         59%|    | 3820/6500 [11:33:15<7:45:54, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3820/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3627, 'learning_rate': 3.639482675773324e-05, 'epoch': 0.59}
{'loss': 0.3649, 'learning_rate': 3.6371566929184744e-05, 'epoch': 0.59}
{'loss': 0.3369, 'learning_rate': 3.634831028619959e-05, 'epoch': 0.59}
{'loss': 0.3528, 'learning_rate': 3.632505683421392e-05, 'epoch': 0.59}
{'loss': 0.3581, 'learning_rate': 3.630180657866306e-05, 'epoch': 0.59}
 59%|    | 3821/6500 [11:33:26<8:44:00, 11.74s/it]                                                         59%|    | 3821/6500 [11:33:26<8:44:00, 11.74s/it] 59%|    | 3822/6500 [11:33:36<8:25:38, 11.33s/it]                                                         59%|    | 3822/6500 [11:33:36<8:25:38, 11.33s/it] 59%|    | 3823/6500 [11:33:46<8:12:32, 11.04s/it]                                                         59%|    | 3823/6500 [11:33:46<8:12:32, 11.04s/it] 59%|    | 3824/6500 [11:33:57<8:03:35, 10.84s/it]                                                         59%|    | 3824/6500 [11:33:57<8:03:35, 10.84s/it] 59%|    | 3825/6500 [11:34:07<7:57:22, 10.71s/it]                                                         59%|    | 3825/6500 [11:34:07<7:57:22, 10.71s/it] 59%|    | 3826/6500 [11:{'loss': 0.3599, 'learning_rate': 3.627855952498163e-05, 'epoch': 0.59}
{'loss': 0.3262, 'learning_rate': 3.6255315678603494e-05, 'epoch': 0.59}
{'loss': 0.3474, 'learning_rate': 3.6232075044961735e-05, 'epoch': 0.59}
{'loss': 0.343, 'learning_rate': 3.620883762948873e-05, 'epoch': 0.59}
{'loss': 0.4245, 'learning_rate': 3.6185603437616065e-05, 'epoch': 0.59}
34:18<7:52:48, 10.61s/it]                                                         59%|    | 3826/6500 [11:34:18<7:52:48, 10.61s/it] 59%|    | 3827/6500 [11:34:28<7:49:43, 10.54s/it]                                                         59%|    | 3827/6500 [11:34:28<7:49:43, 10.54s/it] 59%|    | 3828/6500 [11:34:38<7:47:30, 10.50s/it]                                                         59%|    | 3828/6500 [11:34:38<7:47:30, 10.50s/it] 59%|    | 3829/6500 [11:34:49<7:45:54, 10.47s/it]                                                         59%|    | 3829/6500 [11:34:49<7:45:54, 10.47s/it] 59%|    | 3830/6500 [11:34:59<7:47:11, 10.50s/it]                                                         59%|    | 3830/6500 [11:34:59<7:47:11, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8489000201225281, 'eval_runtime': 4.1996, 'eval_samples_per_second': 5.477, 'eval_steps_per_second': 1.429, 'epoch': 0.59}
                                                         59%|    | 3830/6500 [11:35:03<7:47:11, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3393, 'learning_rate': 3.616237247477462e-05, 'epoch': 0.59}
{'loss': 0.3342, 'learning_rate': 3.6139144746394464e-05, 'epoch': 0.59}
{'loss': 0.3498, 'learning_rate': 3.6115920257904963e-05, 'epoch': 0.59}
{'loss': 0.8549, 'learning_rate': 3.609269901473467e-05, 'epoch': 0.59}
{'loss': 0.3551, 'learning_rate': 3.606948102231143e-05, 'epoch': 0.59}
 59%|    | 3831/6500 [11:35:14<8:47:59, 11.87s/it]                                                         59%|    | 3831/6500 [11:35:14<8:47:59, 11.87s/it] 59%|    | 3832/6500 [11:35:25<8:27:54, 11.42s/it]                                                         59%|    | 3832/6500 [11:35:25<8:27:54, 11.42s/it] 59%|    | 3833/6500 [11:35:35<8:13:49, 11.11s/it]                                                         59%|    | 3833/6500 [11:35:35<8:13:49, 11.11s/it] 59%|    | 3834/6500 [11:35:45<8:03:35, 10.88s/it]                                                         59%|    | 3834/6500 [11:35:45<8:03:35, 10.88s/it] 59%|    | 3835/6500 [11:35:56<7:56:42, 10.73s/it]                                                         59%|    | 3835/6500 [11:35:56<7:56:42, 10.73s/it] 59%|    | 3836/6500 [11:{'loss': 0.354, 'learning_rate': 3.60462662860623e-05, 'epoch': 0.59}
{'loss': 0.351, 'learning_rate': 3.6023054811413584e-05, 'epoch': 0.59}
{'loss': 0.3287, 'learning_rate': 3.599984660379084e-05, 'epoch': 0.59}
{'loss': 0.3576, 'learning_rate': 3.5976641668618816e-05, 'epoch': 0.59}
{'loss': 0.3441, 'learning_rate': 3.595344001132154e-05, 'epoch': 0.59}
36:06<7:51:40, 10.62s/it]                                                         59%|    | 3836/6500 [11:36:06<7:51:40, 10.62s/it] 59%|    | 3837/6500 [11:36:17<7:48:10, 10.55s/it]                                                         59%|    | 3837/6500 [11:36:17<7:48:10, 10.55s/it] 59%|    | 3838/6500 [11:36:27<7:45:43, 10.50s/it]                                                         59%|    | 3838/6500 [11:36:27<7:45:43, 10.50s/it] 59%|    | 3839/6500 [11:36:37<7:43:55, 10.46s/it]                                                         59%|    | 3839/6500 [11:36:37<7:43:55, 10.46s/it] 59%|    | 3840/6500 [11:36:48<7:42:38, 10.44s/it]                                                         59%|    | 3840/6500 [11:36:48<7:42:38, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8615737557411194, 'eval_runtime': 3.9795, 'eval_samples_per_second': 5.78, 'eval_steps_per_second': 1.508, 'epoch': 0.59}
                                                         59%|    | 3840/6500 [11:36:52<7:42:38, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3840
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3840/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3206, 'learning_rate': 3.593024163732225e-05, 'epoch': 0.59}
{'loss': 0.347, 'learning_rate': 3.5907046552043436e-05, 'epoch': 0.59}
{'loss': 0.3313, 'learning_rate': 3.588385476090681e-05, 'epoch': 0.59}
{'loss': 0.3551, 'learning_rate': 3.586066626933331e-05, 'epoch': 0.59}
{'loss': 0.3382, 'learning_rate': 3.583748108274309e-05, 'epoch': 0.59}
 59%|    | 3841/6500 [11:37:03<8:41:11, 11.76s/it]                                                         59%|    | 3841/6500 [11:37:03<8:41:11, 11.76s/it] 59%|    | 3842/6500 [11:37:13<8:22:19, 11.34s/it]                                                         59%|    | 3842/6500 [11:37:13<8:22:19, 11.34s/it] 59%|    | 3843/6500 [11:37:23<8:09:13, 11.05s/it]                                                         59%|    | 3843/6500 [11:37:23<8:09:13, 11.05s/it] 59%|    | 3844/6500 [11:37:34<7:59:54, 10.84s/it]                                                         59%|    | 3844/6500 [11:37:34<7:59:54, 10.84s/it] 59%|    | 3845/6500 [11:37:44<7:53:38, 10.70s/it]                                                         59%|    | 3845/6500 [11:37:44<7:53:38, 10.70s/it] 59%|    | 3846/6500 [11:{'loss': 0.3502, 'learning_rate': 3.5814299206555555e-05, 'epoch': 0.59}
{'loss': 0.3274, 'learning_rate': 3.579112064618934e-05, 'epoch': 0.59}
{'loss': 0.3624, 'learning_rate': 3.576794540706227e-05, 'epoch': 0.59}
{'loss': 0.35, 'learning_rate': 3.5744773494591445e-05, 'epoch': 0.59}
{'loss': 0.3414, 'learning_rate': 3.5721604914193144e-05, 'epoch': 0.59}
37:54<7:49:12, 10.61s/it]                                                         59%|    | 3846/6500 [11:37:54<7:49:12, 10.61s/it] 59%|    | 3847/6500 [11:38:05<7:50:48, 10.65s/it]                                                         59%|    | 3847/6500 [11:38:05<7:50:48, 10.65s/it] 59%|    | 3848/6500 [11:38:16<7:47:02, 10.57s/it]                                                         59%|    | 3848/6500 [11:38:16<7:47:02, 10.57s/it] 59%|    | 3849/6500 [11:38:26<7:44:12, 10.51s/it]                                                         59%|    | 3849/6500 [11:38:26<7:44:12, 10.51s/it] 59%|    | 3850/6500 [11:38:36<7:42:12, 10.47s/it]                                                         59%|    | 3850/6500 [11:38:36<7:42:12, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8462210893630981, 'eval_runtime': 3.9674, 'eval_samples_per_second': 5.797, 'eval_steps_per_second': 1.512, 'epoch': 0.59}
                                                         59%|    | 3850/6500 [11:38:40<7:42:12, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3850
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3850/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3850/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3518, 'learning_rate': 3.569843967128287e-05, 'epoch': 0.59}
{'loss': 0.3388, 'learning_rate': 3.567527777127536e-05, 'epoch': 0.59}
{'loss': 0.3437, 'learning_rate': 3.5652119219584586e-05, 'epoch': 0.59}
{'loss': 0.3321, 'learning_rate': 3.56289640216237e-05, 'epoch': 0.59}
{'loss': 0.3655, 'learning_rate': 3.5605812182805116e-05, 'epoch': 0.59}
 59%|    | 3851/6500 [11:38:51<8:39:55, 11.78s/it]                                                         59%|    | 3851/6500 [11:38:51<8:39:55, 11.78s/it] 59%|    | 3852/6500 [11:39:01<8:21:10, 11.36s/it]                                                         59%|    | 3852/6500 [11:39:01<8:21:10, 11.36s/it] 59%|    | 3853/6500 [11:39:12<8:08:06, 11.06s/it]                                                         59%|    | 3853/6500 [11:39:12<8:08:06, 11.06s/it] 59%|    | 3854/6500 [11:39:22<7:58:59, 10.86s/it]                                                         59%|    | 3854/6500 [11:39:22<7:58:59, 10.86s/it] 59%|    | 3855/6500 [11:39:33<7:52:09, 10.71s/it]                                                         59%|    | 3855/6500 [11:39:33<7:52:09, 10.71s/it] 59%|    | 3856/6500 [11:{'loss': 0.3418, 'learning_rate': 3.55826637085404e-05, 'epoch': 0.59}
{'loss': 0.3474, 'learning_rate': 3.5559518604240385e-05, 'epoch': 0.59}
{'loss': 0.3326, 'learning_rate': 3.5536376875315095e-05, 'epoch': 0.59}
{'loss': 0.3489, 'learning_rate': 3.551323852717378e-05, 'epoch': 0.59}
{'loss': 0.4036, 'learning_rate': 3.5490103565224865e-05, 'epoch': 0.59}
39:43<7:47:23, 10.61s/it]                                                         59%|    | 3856/6500 [11:39:43<7:47:23, 10.61s/it] 59%|    | 3857/6500 [11:39:53<7:44:08, 10.54s/it]                                                         59%|    | 3857/6500 [11:39:53<7:44:08, 10.54s/it] 59%|    | 3858/6500 [11:40:04<7:41:30, 10.48s/it]                                                         59%|    | 3858/6500 [11:40:04<7:41:30, 10.48s/it] 59%|    | 3859/6500 [11:40:14<7:39:41, 10.44s/it]                                                         59%|    | 3859/6500 [11:40:14<7:39:41, 10.44s/it] 59%|    | 3860/6500 [11:40:24<7:38:19, 10.42s/it]                                                         59%|    | 3860/6500 [11:40:24<7:38:19, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8511543869972229, 'eval_runtime': 3.9646, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.59}
                                                         59%|    | 3860/6500 [11:40:28<7:38:19, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3860/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3426, 'learning_rate': 3.546697199487603e-05, 'epoch': 0.59}
{'loss': 0.3263, 'learning_rate': 3.544384382153413e-05, 'epoch': 0.59}
{'loss': 0.3579, 'learning_rate': 3.542071905060522e-05, 'epoch': 0.59}
{'loss': 0.8582, 'learning_rate': 3.5397597687494596e-05, 'epoch': 0.59}
{'loss': 0.3608, 'learning_rate': 3.5374479737606733e-05, 'epoch': 0.59}
 59%|    | 3861/6500 [11:40:39<8:36:31, 11.74s/it]                                                         59%|    | 3861/6500 [11:40:39<8:36:31, 11.74s/it] 59%|    | 3862/6500 [11:40:50<8:17:59, 11.33s/it]                                                         59%|    | 3862/6500 [11:40:50<8:17:59, 11.33s/it] 59%|    | 3863/6500 [11:41:00<8:11:24, 11.18s/it]                                                         59%|    | 3863/6500 [11:41:00<8:11:24, 11.18s/it] 59%|    | 3864/6500 [11:41:11<8:00:25, 10.94s/it]                                                         59%|    | 3864/6500 [11:41:11<8:00:25, 10.94s/it] 59%|    | 3865/6500 [11:41:21<7:52:34, 10.76s/it]                                                         59%|    | 3865/6500 [11:41:21<7:52:34, 10.76s/it] 59%|    | 3866/6500 [11:{'loss': 0.3555, 'learning_rate': 3.535136520634531e-05, 'epoch': 0.59}
{'loss': 0.336, 'learning_rate': 3.53282540991132e-05, 'epoch': 0.59}
{'loss': 0.3331, 'learning_rate': 3.530514642131249e-05, 'epoch': 0.6}
{'loss': 0.3541, 'learning_rate': 3.528204217834444e-05, 'epoch': 0.6}
{'loss': 0.338, 'learning_rate': 3.5258941375609565e-05, 'epoch': 0.6}
41:32<7:47:04, 10.64s/it]                                                         59%|    | 3866/6500 [11:41:32<7:47:04, 10.64s/it] 59%|    | 3867/6500 [11:41:42<7:43:12, 10.56s/it]                                                         59%|    | 3867/6500 [11:41:42<7:43:12, 10.56s/it] 60%|    | 3868/6500 [11:41:52<7:41:04, 10.51s/it]                                                         60%|    | 3868/6500 [11:41:52<7:41:04, 10.51s/it] 60%|    | 3869/6500 [11:42:03<7:38:49, 10.46s/it]                                                         60%|    | 3869/6500 [11:42:03<7:38:49, 10.46s/it] 60%|    | 3870/6500 [11:42:13<7:37:09, 10.43s/it]                                                         60%|    | 3870/6500 [11:42:13<7:37:09, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.857377827167511, 'eval_runtime': 3.97, 'eval_samples_per_second': 5.793, 'eval_steps_per_second': 1.511, 'epoch': 0.6}
                                                         60%|    | 3870/6500 [11:42:17<7:37:09, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3870
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3870/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3403, 'learning_rate': 3.523584401850751e-05, 'epoch': 0.6}
{'loss': 0.3442, 'learning_rate': 3.521275011243715e-05, 'epoch': 0.6}
{'loss': 0.3414, 'learning_rate': 3.5189659662796546e-05, 'epoch': 0.6}
{'loss': 0.3383, 'learning_rate': 3.5166572674982944e-05, 'epoch': 0.6}
{'loss': 0.3304, 'learning_rate': 3.5143489154392785e-05, 'epoch': 0.6}
 60%|    | 3871/6500 [11:42:28<8:34:53, 11.75s/it]                                                         60%|    | 3871/6500 [11:42:28<8:34:53, 11.75s/it] 60%|    | 3872/6500 [11:42:38<8:16:10, 11.33s/it]                                                         60%|    | 3872/6500 [11:42:38<8:16:10, 11.33s/it] 60%|    | 3873/6500 [11:42:49<8:03:07, 11.03s/it]                                                         60%|    | 3873/6500 [11:42:49<8:03:07, 11.03s/it] 60%|    | 3874/6500 [11:42:59<7:53:58, 10.83s/it]                                                         60%|    | 3874/6500 [11:42:59<7:53:58, 10.83s/it] 60%|    | 3875/6500 [11:43:09<7:47:21, 10.68s/it]                                                         60%|    | 3875/6500 [11:43:09<7:47:21, 10.68s/it] 60%|    | 3876/6500 [11:{'loss': 0.3565, 'learning_rate': 3.5120409106421716e-05, 'epoch': 0.6}
{'loss': 0.3373, 'learning_rate': 3.509733253646454e-05, 'epoch': 0.6}
{'loss': 0.3596, 'learning_rate': 3.5074259449915284e-05, 'epoch': 0.6}
{'loss': 0.3532, 'learning_rate': 3.505118985216713e-05, 'epoch': 0.6}
{'loss': 0.3454, 'learning_rate': 3.502812374861245e-05, 'epoch': 0.6}
43:20<7:42:38, 10.58s/it]                                                         60%|    | 3876/6500 [11:43:20<7:42:38, 10.58s/it] 60%|    | 3877/6500 [11:43:30<7:39:30, 10.51s/it]                                                         60%|    | 3877/6500 [11:43:30<7:39:30, 10.51s/it] 60%|    | 3878/6500 [11:43:40<7:37:25, 10.47s/it]                                                         60%|    | 3878/6500 [11:43:40<7:37:25, 10.47s/it] 60%|    | 3879/6500 [11:43:51<7:41:10, 10.56s/it]                                                         60%|    | 3879/6500 [11:43:51<7:41:10, 10.56s/it] 60%|    | 3880/6500 [11:44:01<7:38:17, 10.50s/it]                                                         60%|    | 3880/6500 [11:44:01<7:38:17, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8472177386283875, 'eval_runtime': 3.9539, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 1.517, 'epoch': 0.6}
                                                         60%|    | 3880/6500 [11:44:05<7:38:17, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3880
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3517, 'learning_rate': 3.500506114464282e-05, 'epoch': 0.6}
{'loss': 0.356, 'learning_rate': 3.498200204564897e-05, 'epoch': 0.6}
{'loss': 0.3471, 'learning_rate': 3.495894645702084e-05, 'epoch': 0.6}
{'loss': 0.3442, 'learning_rate': 3.4935894384147516e-05, 'epoch': 0.6}
{'loss': 0.3557, 'learning_rate': 3.49128458324173e-05, 'epoch': 0.6}
 60%|    | 3881/6500 [11:44:16<8:35:13, 11.80s/it]                                                         60%|    | 3881/6500 [11:44:16<8:35:13, 11.80s/it] 60%|    | 3882/6500 [11:44:27<8:16:00, 11.37s/it]                                                         60%|    | 3882/6500 [11:44:27<8:16:00, 11.37s/it] 60%|    | 3883/6500 [11:44:37<8:02:38, 11.07s/it]                                                         60%|    | 3883/6500 [11:44:37<8:02:38, 11.07s/it] 60%|    | 3884/6500 [11:44:47<7:53:17, 10.86s/it]                                                         60%|    | 3884/6500 [11:44:47<7:53:17, 10.86s/it] 60%|    | 3885/6500 [11:44:58<7:46:43, 10.71s/it]                                                         60%|    | 3885/6500 [11:44:58<7:46:43, 10.71s/it] 60%|    | 3886/6500 [11:{'loss': 0.3303, 'learning_rate': 3.488980080721762e-05, 'epoch': 0.6}
{'loss': 0.3444, 'learning_rate': 3.486675931393514e-05, 'epoch': 0.6}
{'loss': 0.3226, 'learning_rate': 3.484372135795566e-05, 'epoch': 0.6}
{'loss': 0.3708, 'learning_rate': 3.482068694466417e-05, 'epoch': 0.6}
{'loss': 0.3911, 'learning_rate': 3.4797656079444806e-05, 'epoch': 0.6}
45:08<7:42:00, 10.60s/it]                                                         60%|    | 3886/6500 [11:45:08<7:42:00, 10.60s/it] 60%|    | 3887/6500 [11:45:18<7:38:43, 10.53s/it]                                                         60%|    | 3887/6500 [11:45:18<7:38:43, 10.53s/it] 60%|    | 3888/6500 [11:45:29<7:36:11, 10.48s/it]                                                         60%|    | 3888/6500 [11:45:29<7:36:11, 10.48s/it] 60%|    | 3889/6500 [11:45:39<7:34:22, 10.44s/it]                                                         60%|    | 3889/6500 [11:45:39<7:34:22, 10.44s/it] 60%|    | 3890/6500 [11:45:49<7:33:03, 10.42s/it]                                                         60%|    | 3890/6500 [11:45:49<7:33:03, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.851399302482605, 'eval_runtime': 3.9548, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.6}
                                                         60%|    | 3890/6500 [11:45:53<7:33:03, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3433, 'learning_rate': 3.47746287676809e-05, 'epoch': 0.6}
{'loss': 0.3496, 'learning_rate': 3.475160501475495e-05, 'epoch': 0.6}
{'loss': 0.3507, 'learning_rate': 3.472858482604861e-05, 'epoch': 0.6}
{'loss': 0.8772, 'learning_rate': 3.4705568206942706e-05, 'epoch': 0.6}
{'loss': 0.3483, 'learning_rate': 3.468255516281725e-05, 'epoch': 0.6}
 60%|    | 3891/6500 [11:46:04<8:30:32, 11.74s/it]                                                         60%|    | 3891/6500 [11:46:04<8:30:32, 11.74s/it] 60%|    | 3892/6500 [11:46:15<8:12:20, 11.33s/it]                                                         60%|    | 3892/6500 [11:46:15<8:12:20, 11.33s/it] 60%|    | 3893/6500 [11:46:25<7:59:33, 11.04s/it]                                                         60%|    | 3893/6500 [11:46:25<7:59:33, 11.04s/it] 60%|    | 3894/6500 [11:46:35<7:50:23, 10.83s/it]                                                         60%|    | 3894/6500 [11:46:35<7:50:23, 10.83s/it] 60%|    | 3895/6500 [11:46:46<7:52:03, 10.87s/it]                                                         60%|    | 3895/6500 [11:46:46<7:52:03, 10.87s/it] 60%|    | 3896/6500 [11:{'loss': 0.3518, 'learning_rate': 3.465954569905141e-05, 'epoch': 0.6}
{'loss': 0.3176, 'learning_rate': 3.463653982102347e-05, 'epoch': 0.6}
{'loss': 0.3545, 'learning_rate': 3.461353753411096e-05, 'epoch': 0.6}
{'loss': 0.3409, 'learning_rate': 3.4590538843690485e-05, 'epoch': 0.6}
{'loss': 0.3105, 'learning_rate': 3.456754375513786e-05, 'epoch': 0.6}
46:57<7:44:57, 10.71s/it]                                                         60%|    | 3896/6500 [11:46:57<7:44:57, 10.71s/it] 60%|    | 3897/6500 [11:47:07<7:40:12, 10.61s/it]                                                         60%|    | 3897/6500 [11:47:07<7:40:12, 10.61s/it] 60%|    | 3898/6500 [11:47:17<7:36:52, 10.54s/it]                                                         60%|    | 3898/6500 [11:47:17<7:36:52, 10.54s/it] 60%|    | 3899/6500 [11:47:28<7:34:10, 10.48s/it]                                                         60%|    | 3899/6500 [11:47:28<7:34:10, 10.48s/it] 60%|    | 3900/6500 [11:47:38<7:32:34, 10.44s/it]                                                         60%|    | 3900/6500 [11:47:38<7:32:34, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8631861209869385, 'eval_runtime': 3.9674, 'eval_samples_per_second': 5.797, 'eval_steps_per_second': 1.512, 'epoch': 0.6}
                                                         60%|    | 3900/6500 [11:47:42<7:32:34, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3900the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3900

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3900
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.342, 'learning_rate': 3.4544552273828056e-05, 'epoch': 0.6}
{'loss': 0.3288, 'learning_rate': 3.452156440513519e-05, 'epoch': 0.6}
{'loss': 0.3497, 'learning_rate': 3.449858015443254e-05, 'epoch': 0.6}
{'loss': 0.3404, 'learning_rate': 3.447559952709252e-05, 'epoch': 0.6}
{'loss': 0.3451, 'learning_rate': 3.445262252848672e-05, 'epoch': 0.6}
 60%|    | 3901/6500 [11:47:53<8:29:45, 11.77s/it]                                                         60%|    | 3901/6500 [11:47:53<8:29:45, 11.77s/it] 60%|    | 3902/6500 [11:48:03<8:11:26, 11.35s/it]                                                         60%|    | 3902/6500 [11:48:03<8:11:26, 11.35s/it] 60%|    | 3903/6500 [11:48:14<7:58:30, 11.06s/it]                                                         60%|    | 3903/6500 [11:48:14<7:58:30, 11.06s/it] 60%|    | 3904/6500 [11:48:24<7:49:33, 10.85s/it]                                                         60%|    | 3904/6500 [11:48:24<7:49:33, 10.85s/it] 60%|    | 3905/6500 [11:48:34<7:43:05, 10.71s/it]                                                         60%|    | 3905/6500 [11:48:34<7:43:05, 10.71s/it] 60%|    | 3906/6500 [11:{'loss': 0.3379, 'learning_rate': 3.442964916398588e-05, 'epoch': 0.6}
{'loss': 0.335, 'learning_rate': 3.440667943895986e-05, 'epoch': 0.6}
{'loss': 0.3618, 'learning_rate': 3.4383713358777735e-05, 'epoch': 0.6}
{'loss': 0.3372, 'learning_rate': 3.4360750928807664e-05, 'epoch': 0.6}
{'loss': 0.3453, 'learning_rate': 3.4337792154416966e-05, 'epoch': 0.6}
48:45<7:38:24, 10.60s/it]                                                         60%|    | 3906/6500 [11:48:45<7:38:24, 10.60s/it] 60%|    | 3907/6500 [11:48:55<7:34:56, 10.53s/it]                                                         60%|    | 3907/6500 [11:48:55<7:34:56, 10.53s/it] 60%|    | 3908/6500 [11:49:06<7:32:37, 10.48s/it]                                                         60%|    | 3908/6500 [11:49:06<7:32:37, 10.48s/it] 60%|    | 3909/6500 [11:49:16<7:31:09, 10.45s/it]                                                         60%|    | 3909/6500 [11:49:16<7:31:09, 10.45s/it] 60%|    | 3910/6500 [11:49:26<7:29:50, 10.42s/it]                                                         60%|    | 3910/6500 [11:49:26<7:29:50, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.848225474357605, 'eval_runtime': 3.9543, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.6}
                                                         60%|    | 3910/6500 [11:49:30<7:29:50, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3910I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3910/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3516, 'learning_rate': 3.431483704097212e-05, 'epoch': 0.6}
{'loss': 0.3332, 'learning_rate': 3.4291885593838755e-05, 'epoch': 0.6}
{'loss': 0.3422, 'learning_rate': 3.426893781838162e-05, 'epoch': 0.6}
{'loss': 0.3583, 'learning_rate': 3.4245993719964634e-05, 'epoch': 0.6}
{'loss': 0.3418, 'learning_rate': 3.4223053303950827e-05, 'epoch': 0.6}
 60%|    | 3911/6500 [11:49:41<8:30:30, 11.83s/it]                                                         60%|    | 3911/6500 [11:49:41<8:30:30, 11.83s/it] 60%|    | 3912/6500 [11:49:52<8:12:14, 11.41s/it]                                                         60%|    | 3912/6500 [11:49:52<8:12:14, 11.41s/it] 60%|    | 3913/6500 [11:50:02<7:58:55, 11.11s/it]                                                         60%|    | 3913/6500 [11:50:02<7:58:55, 11.11s/it] 60%|    | 3914/6500 [11:50:13<7:49:45, 10.90s/it]                                                         60%|    | 3914/6500 [11:50:13<7:49:45, 10.90s/it] 60%|    | 3915/6500 [11:50:23<7:43:24, 10.76s/it]                                                         60%|    | 3915/6500 [11:50:23<7:43:24, 10.76s/it] 60%|    | 3916/6500 [11:{'loss': 0.3365, 'learning_rate': 3.420011657570238e-05, 'epoch': 0.6}
{'loss': 0.3329, 'learning_rate': 3.417718354058062e-05, 'epoch': 0.6}
{'loss': 0.3467, 'learning_rate': 3.4154254203946e-05, 'epoch': 0.6}
{'loss': 0.4139, 'learning_rate': 3.413132857115812e-05, 'epoch': 0.6}
{'loss': 0.3241, 'learning_rate': 3.4108406647575706e-05, 'epoch': 0.6}
50:33<7:38:56, 10.66s/it]                                                         60%|    | 3916/6500 [11:50:33<7:38:56, 10.66s/it] 60%|    | 3917/6500 [11:50:44<7:35:40, 10.58s/it]                                                         60%|    | 3917/6500 [11:50:44<7:35:40, 10.58s/it] 60%|    | 3918/6500 [11:50:54<7:33:35, 10.54s/it]                                                         60%|    | 3918/6500 [11:50:54<7:33:35, 10.54s/it] 60%|    | 3919/6500 [11:51:05<7:31:45, 10.50s/it]                                                         60%|    | 3919/6500 [11:51:05<7:31:45, 10.50s/it] 60%|    | 3920/6500 [11:51:15<7:30:31, 10.48s/it]                                                         60%|    | 3920/6500 [11:51:15<7:30:31, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8495675921440125, 'eval_runtime': 3.961, 'eval_samples_per_second': 5.807, 'eval_steps_per_second': 1.515, 'epoch': 0.6}
                                                         60%|    | 3920/6500 [11:51:19<7:30:31, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3920
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3259, 'learning_rate': 3.408548843855661e-05, 'epoch': 0.6}
{'loss': 0.3554, 'learning_rate': 3.406257394945783e-05, 'epoch': 0.6}
{'loss': 0.8566, 'learning_rate': 3.403966318563549e-05, 'epoch': 0.6}
{'loss': 0.3492, 'learning_rate': 3.401675615244483e-05, 'epoch': 0.6}
{'loss': 0.3432, 'learning_rate': 3.399385285524026e-05, 'epoch': 0.6}
 60%|    | 3921/6500 [11:51:30<8:27:17, 11.80s/it]                                                         60%|    | 3921/6500 [11:51:30<8:27:17, 11.80s/it] 60%|    | 3922/6500 [11:51:40<8:09:09, 11.38s/it]                                                         60%|    | 3922/6500 [11:51:40<8:09:09, 11.38s/it] 60%|    | 3923/6500 [11:51:51<7:56:09, 11.09s/it]                                                         60%|    | 3923/6500 [11:51:51<7:56:09, 11.09s/it] 60%|    | 3924/6500 [11:52:01<7:47:24, 10.89s/it]                                                         60%|    | 3924/6500 [11:52:01<7:47:24, 10.89s/it] 60%|    | 3925/6500 [11:52:12<7:41:04, 10.74s/it]                                                         60%|    | 3925/6500 [11:52:12<7:41:04, 10.74s/it] 60%|    | 3926/6500 [11:{'loss': 0.3475, 'learning_rate': 3.397095329937526e-05, 'epoch': 0.6}
{'loss': 0.327, 'learning_rate': 3.394805749020246e-05, 'epoch': 0.6}
{'loss': 0.3569, 'learning_rate': 3.3925165433073624e-05, 'epoch': 0.6}
{'loss': 0.3275, 'learning_rate': 3.3902277133339635e-05, 'epoch': 0.6}
{'loss': 0.324, 'learning_rate': 3.387939259635049e-05, 'epoch': 0.6}
52:22<7:36:40, 10.64s/it]                                                         60%|    | 3926/6500 [11:52:22<7:36:40, 10.64s/it] 60%|    | 3927/6500 [11:52:33<7:37:26, 10.67s/it]                                                         60%|    | 3927/6500 [11:52:33<7:37:26, 10.67s/it] 60%|    | 3928/6500 [11:52:43<7:33:19, 10.58s/it]                                                         60%|    | 3928/6500 [11:52:43<7:33:19, 10.58s/it] 60%|    | 3929/6500 [11:52:54<7:30:09, 10.51s/it]                                                         60%|    | 3929/6500 [11:52:54<7:30:09, 10.51s/it] 60%|    | 3930/6500 [11:53:04<7:28:04, 10.46s/it]                                                         60%|    | 3930/6500 [11:53:04<7:28:04, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.865826427936554, 'eval_runtime': 3.9458, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.6}
                                                         60%|    | 3930/6500 [11:53:08<7:28:04, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3930
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3349, 'learning_rate': 3.385651182745532e-05, 'epoch': 0.6}
{'loss': 0.3312, 'learning_rate': 3.383363483200235e-05, 'epoch': 0.6}
{'loss': 0.3413, 'learning_rate': 3.3810761615338934e-05, 'epoch': 0.61}
{'loss': 0.331, 'learning_rate': 3.3787892182811564e-05, 'epoch': 0.61}
{'loss': 0.3553, 'learning_rate': 3.3765026539765834e-05, 'epoch': 0.61}
 60%|    | 3931/6500 [11:53:19<8:23:00, 11.75s/it]                                                         60%|    | 3931/6500 [11:53:19<8:23:00, 11.75s/it] 60%|    | 3932/6500 [11:53:29<8:04:46, 11.33s/it]                                                         60%|    | 3932/6500 [11:53:29<8:04:46, 11.33s/it] 61%|    | 3933/6500 [11:53:39<7:52:02, 11.03s/it]                                                         61%|    | 3933/6500 [11:53:39<7:52:02, 11.03s/it] 61%|    | 3934/6500 [11:53:50<7:43:05, 10.83s/it]                                                         61%|    | 3934/6500 [11:53:50<7:43:05, 10.83s/it] 61%|    | 3935/6500 [11:54:00<7:36:44, 10.68s/it]                                                         61%|    | 3935/6500 [11:54:00<7:36:44, 10.68s/it] 61%|    | 3936/6500 [11:{'loss': 0.3367, 'learning_rate': 3.374216469154643e-05, 'epoch': 0.61}
{'loss': 0.3495, 'learning_rate': 3.371930664349719e-05, 'epoch': 0.61}
{'loss': 0.3389, 'learning_rate': 3.3696452400961023e-05, 'epoch': 0.61}
{'loss': 0.3427, 'learning_rate': 3.3673601969279986e-05, 'epoch': 0.61}
{'loss': 0.3498, 'learning_rate': 3.3650755353795216e-05, 'epoch': 0.61}
54:10<7:32:08, 10.58s/it]                                                         61%|    | 3936/6500 [11:54:10<7:32:08, 10.58s/it] 61%|    | 3937/6500 [11:54:21<7:28:56, 10.51s/it]                                                         61%|    | 3937/6500 [11:54:21<7:28:56, 10.51s/it] 61%|    | 3938/6500 [11:54:31<7:26:30, 10.46s/it]                                                         61%|    | 3938/6500 [11:54:31<7:26:30, 10.46s/it] 61%|    | 3939/6500 [11:54:41<7:24:56, 10.42s/it]                                                         61%|    | 3939/6500 [11:54:41<7:24:56, 10.42s/it] 61%|    | 3940/6500 [11:54:52<7:23:42, 10.40s/it]                                                         61%|    | 3940/6500 [11:54:52<7:23:42, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8466292023658752, 'eval_runtime': 4.1776, 'eval_samples_per_second': 5.506, 'eval_steps_per_second': 1.436, 'epoch': 0.61}
                                                         61%|    | 3940/6500 [11:54:56<7:23:42, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3940
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3488, 'learning_rate': 3.3627912559846985e-05, 'epoch': 0.61}
{'loss': 0.3419, 'learning_rate': 3.360507359277466e-05, 'epoch': 0.61}
{'loss': 0.3446, 'learning_rate': 3.358223845791668e-05, 'epoch': 0.61}
{'loss': 0.3526, 'learning_rate': 3.355940716061064e-05, 'epoch': 0.61}
{'loss': 0.3339, 'learning_rate': 3.3536579706193224e-05, 'epoch': 0.61}
 61%|    | 3941/6500 [11:55:07<8:23:20, 11.80s/it]                                                         61%|    | 3941/6500 [11:55:07<8:23:20, 11.80s/it] 61%|    | 3942/6500 [11:55:17<8:04:32, 11.37s/it]                                                         61%|    | 3942/6500 [11:55:17<8:04:32, 11.37s/it] 61%|    | 3943/6500 [11:55:28<7:51:30, 11.06s/it]                                                         61%|    | 3943/6500 [11:55:28<7:51:30, 11.06s/it] 61%|    | 3944/6500 [11:55:38<7:45:32, 10.93s/it]                                                         61%|    | 3944/6500 [11:55:38<7:45:32, 10.93s/it] 61%|    | 3945/6500 [11:55:48<7:38:02, 10.76s/it]                                                         61%|    | 3945/6500 [11:55:48<7:38:02, 10.76s/it] 61%|    | 3946/6500 [11:{'loss': 0.3281, 'learning_rate': 3.351375610000019e-05, 'epoch': 0.61}
{'loss': 0.3209, 'learning_rate': 3.349093634736644e-05, 'epoch': 0.61}
{'loss': 0.3569, 'learning_rate': 3.346812045362595e-05, 'epoch': 0.61}
{'loss': 0.4022, 'learning_rate': 3.344530842411178e-05, 'epoch': 0.61}
{'loss': 0.3336, 'learning_rate': 3.342250026415611e-05, 'epoch': 0.61}
55:59<7:32:27, 10.63s/it]                                                         61%|    | 3946/6500 [11:55:59<7:32:27, 10.63s/it] 61%|    | 3947/6500 [11:56:09<7:28:47, 10.55s/it]                                                         61%|    | 3947/6500 [11:56:09<7:28:47, 10.55s/it] 61%|    | 3948/6500 [11:56:20<7:26:06, 10.49s/it]                                                         61%|    | 3948/6500 [11:56:20<7:26:06, 10.49s/it] 61%|    | 3949/6500 [11:56:30<7:24:05, 10.44s/it]                                                         61%|    | 3949/6500 [11:56:30<7:24:05, 10.44s/it] 61%|    | 3950/6500 [11:56:40<7:22:42, 10.42s/it]                                                         61%|    | 3950/6500 [11:56:40<7:22:42, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8540791273117065, 'eval_runtime': 3.9459, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.61}
                                                         61%|    | 3950/6500 [11:56:44<7:22:42, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3950
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3257, 'learning_rate': 3.339969597909021e-05, 'epoch': 0.61}
{'loss': 0.3454, 'learning_rate': 3.337689557424445e-05, 'epoch': 0.61}
{'loss': 0.8615, 'learning_rate': 3.335409905494828e-05, 'epoch': 0.61}
{'loss': 0.3573, 'learning_rate': 3.333130642653024e-05, 'epoch': 0.61}
{'loss': 0.3405, 'learning_rate': 3.330851769431798e-05, 'epoch': 0.61}
 61%|    | 3951/6500 [11:56:55<8:19:02, 11.75s/it]                                                         61%|    | 3951/6500 [11:56:55<8:19:02, 11.75s/it] 61%|    | 3952/6500 [11:57:05<8:01:12, 11.33s/it]                                                         61%|    | 3952/6500 [11:57:05<8:01:12, 11.33s/it] 61%|    | 3953/6500 [11:57:16<7:48:13, 11.03s/it]                                                         61%|    | 3953/6500 [11:57:16<7:48:13, 11.03s/it] 61%|    | 3954/6500 [11:57:26<7:39:31, 10.83s/it]                                                         61%|    | 3954/6500 [11:57:26<7:39:31, 10.83s/it] 61%|    | 3955/6500 [11:57:36<7:33:13, 10.68s/it]                                                         61%|    | 3955/6500 [11:57:36<7:33:13, 10.68s/it] 61%|    | 3956/6500 [11:{'loss': 0.3255, 'learning_rate': 3.3285732863638215e-05, 'epoch': 0.61}
{'loss': 0.3311, 'learning_rate': 3.326295193981677e-05, 'epoch': 0.61}
{'loss': 0.3664, 'learning_rate': 3.3240174928178544e-05, 'epoch': 0.61}
{'loss': 0.3198, 'learning_rate': 3.321740183404755e-05, 'epoch': 0.61}
{'loss': 0.3419, 'learning_rate': 3.319463266274682e-05, 'epoch': 0.61}
57:47<7:28:52, 10.59s/it]                                                         61%|    | 3956/6500 [11:57:47<7:28:52, 10.59s/it] 61%|    | 3957/6500 [11:57:57<7:26:28, 10.53s/it]                                                         61%|    | 3957/6500 [11:57:57<7:26:28, 10.53s/it] 61%|    | 3958/6500 [11:58:08<7:23:54, 10.48s/it]                                                         61%|    | 3958/6500 [11:58:08<7:23:54, 10.48s/it] 61%|    | 3959/6500 [11:58:18<7:22:05, 10.44s/it]                                                         61%|    | 3959/6500 [11:58:18<7:22:05, 10.44s/it] 61%|    | 3960/6500 [11:58:29<7:25:48, 10.53s/it]                                                         61%|    | 3960/6500 [11:58:29<7:25:48, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.864387035369873, 'eval_runtime': 4.1834, 'eval_samples_per_second': 5.498, 'eval_steps_per_second': 1.434, 'epoch': 0.61}
                                                         61%|    | 3960/6500 [11:58:33<7:25:48, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3960I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3960/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3291, 'learning_rate': 3.317186741959852e-05, 'epoch': 0.61}
{'loss': 0.3472, 'learning_rate': 3.31491061099239e-05, 'epoch': 0.61}
{'loss': 0.3487, 'learning_rate': 3.312634873904327e-05, 'epoch': 0.61}
{'loss': 0.3437, 'learning_rate': 3.3103595312276035e-05, 'epoch': 0.61}
{'loss': 0.3583, 'learning_rate': 3.3080845834940664e-05, 'epoch': 0.61}
 61%|    | 3961/6500 [11:58:44<8:22:47, 11.88s/it]                                                         61%|    | 3961/6500 [11:58:44<8:22:47, 11.88s/it] 61%|    | 3962/6500 [11:58:54<8:03:09, 11.42s/it]                                                         61%|    | 3962/6500 [11:58:54<8:03:09, 11.42s/it] 61%|    | 3963/6500 [11:59:04<7:49:38, 11.11s/it]                                                         61%|    | 3963/6500 [11:59:04<7:49:38, 11.11s/it] 61%|    | 3964/6500 [11:59:16<7:49:57, 11.12s/it]                                                         61%|    | 3964/6500 [11:59:16<7:49:57, 11.12s/it] 61%|    | 3965/6500 [11:59:26<7:40:23, 10.90s/it]                                                         61%|    | 3965/6500 [11:59:26<7:40:23, 10.90s/it] 61%|    | 3966/6500 [11:{'loss': 0.3414, 'learning_rate': 3.305810031235471e-05, 'epoch': 0.61}
{'loss': 0.3627, 'learning_rate': 3.303535874983479e-05, 'epoch': 0.61}
{'loss': 0.3433, 'learning_rate': 3.301262115269662e-05, 'epoch': 0.61}
{'loss': 0.3396, 'learning_rate': 3.298988752625496e-05, 'epoch': 0.61}
{'loss': 0.3557, 'learning_rate': 3.296715787582367e-05, 'epoch': 0.61}
59:36<7:34:57, 10.77s/it]                                                         61%|    | 3966/6500 [11:59:36<7:34:57, 10.77s/it] 61%|    | 3967/6500 [11:59:47<7:29:35, 10.65s/it]                                                         61%|    | 3967/6500 [11:59:47<7:29:35, 10.65s/it] 61%|    | 3968/6500 [11:59:57<7:25:32, 10.56s/it]                                                         61%|    | 3968/6500 [11:59:57<7:25:32, 10.56s/it] 61%|    | 3969/6500 [12:00:08<7:22:57, 10.50s/it]                                                         61%|    | 3969/6500 [12:00:08<7:22:57, 10.50s/it] 61%|    | 3970/6500 [12:00:18<7:23:03, 10.51s/it]                                                         61%|    | 3970/6500 [12:00:18<7:23:03, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8467122912406921, 'eval_runtime': 3.9898, 'eval_samples_per_second': 5.765, 'eval_steps_per_second': 1.504, 'epoch': 0.61}
                                                         61%|    | 3970/6500 [12:00:22<7:23:03, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3970
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3463, 'learning_rate': 3.2944432206715684e-05, 'epoch': 0.61}
{'loss': 0.3483, 'learning_rate': 3.2921710524242956e-05, 'epoch': 0.61}
{'loss': 0.3364, 'learning_rate': 3.289899283371657e-05, 'epoch': 0.61}
{'loss': 0.3588, 'learning_rate': 3.287627914044662e-05, 'epoch': 0.61}
{'loss': 0.328, 'learning_rate': 3.28535694497423e-05, 'epoch': 0.61}
 61%|    | 3971/6500 [12:00:33<8:19:26, 11.85s/it]                                                         61%|    | 3971/6500 [12:00:33<8:19:26, 11.85s/it] 61%|    | 3972/6500 [12:00:43<8:01:09, 11.42s/it]                                                         61%|    | 3972/6500 [12:00:43<8:01:09, 11.42s/it] 61%|    | 3973/6500 [12:00:54<7:48:14, 11.12s/it]                                                         61%|    | 3973/6500 [12:00:54<7:48:14, 11.12s/it] 61%|    | 3974/6500 [12:01:04<7:39:00, 10.90s/it]                                                         61%|    | 3974/6500 [12:01:04<7:39:00, 10.90s/it] 61%|    | 3975/6500 [12:01:15<7:32:32, 10.75s/it]                                                         61%|    | 3975/6500 [12:01:15<7:32:32, 10.75s/it] 61%|    | 3976/6500 [12:{'loss': 0.3388, 'learning_rate': 3.283086376691188e-05, 'epoch': 0.61}
{'loss': 0.3176, 'learning_rate': 3.2808162097262664e-05, 'epoch': 0.61}
{'loss': 0.3746, 'learning_rate': 3.278546444610103e-05, 'epoch': 0.61}
{'loss': 0.3817, 'learning_rate': 3.276277081873243e-05, 'epoch': 0.61}
{'loss': 0.3254, 'learning_rate': 3.274008122046132e-05, 'epoch': 0.61}
01:25<7:31:47, 10.74s/it]                                                         61%|    | 3976/6500 [12:01:25<7:31:47, 10.74s/it] 61%|    | 3977/6500 [12:01:36<7:27:27, 10.64s/it]                                                         61%|    | 3977/6500 [12:01:36<7:27:27, 10.64s/it] 61%|    | 3978/6500 [12:01:46<7:24:14, 10.57s/it]                                                         61%|    | 3978/6500 [12:01:46<7:24:14, 10.57s/it] 61%|    | 3979/6500 [12:01:57<7:22:08, 10.52s/it]                                                         61%|    | 3979/6500 [12:01:57<7:22:08, 10.52s/it] 61%|    | 3980/6500 [12:02:07<7:20:41, 10.49s/it]                                                         61%|    | 3980/6500 [12:02:07<7:20:41, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8521408438682556, 'eval_runtime': 3.9668, 'eval_samples_per_second': 5.798, 'eval_steps_per_second': 1.513, 'epoch': 0.61}
                                                         61%|    | 3980/6500 [12:02:11<7:20:41, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3980
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3507, 'learning_rate': 3.271739565659129e-05, 'epoch': 0.61}
{'loss': 0.5276, 'learning_rate': 3.269471413242495e-05, 'epoch': 0.61}
{'loss': 0.6781, 'learning_rate': 3.267203665326396e-05, 'epoch': 0.61}
{'loss': 0.3469, 'learning_rate': 3.264936322440905e-05, 'epoch': 0.61}
{'loss': 0.3412, 'learning_rate': 3.262669385116001e-05, 'epoch': 0.61}
 61%|    | 3981/6500 [12:02:22<8:15:50, 11.81s/it]                                                         61%|    | 3981/6500 [12:02:22<8:15:50, 11.81s/it] 61%|   | 3982/6500 [12:02:32<7:57:47, 11.39s/it]                                                         61%|   | 3982/6500 [12:02:32<7:57:47, 11.39s/it] 61%|   | 3983/6500 [12:02:43<7:45:08, 11.09s/it]                                                         61%|   | 3983/6500 [12:02:43<7:45:08, 11.09s/it] 61%|   | 3984/6500 [12:02:53<7:36:35, 10.89s/it]                                                         61%|   | 3984/6500 [12:02:53<7:36:35, 10.89s/it] 61%|   | 3985/6500 [12:03:04<7:30:20, 10.74s/it]                                                         61%|   | 3985/6500 [12:03:04<7:30:20, 10.74s/it] 61%| {'loss': 0.3193, 'learning_rate': 3.260402853881562e-05, 'epoch': 0.61}
{'loss': 0.3427, 'learning_rate': 3.2581367292673806e-05, 'epoch': 0.61}
{'loss': 0.3413, 'learning_rate': 3.255871011803148e-05, 'epoch': 0.61}
{'loss': 0.3021, 'learning_rate': 3.253605702018461e-05, 'epoch': 0.61}
{'loss': 0.3445, 'learning_rate': 3.251340800442825e-05, 'epoch': 0.61}
  | 3986/6500 [12:03:14<7:25:56, 10.64s/it]                                                         61%|   | 3986/6500 [12:03:14<7:25:56, 10.64s/it] 61%|   | 3987/6500 [12:03:25<7:26:35, 10.66s/it]                                                         61%|   | 3987/6500 [12:03:25<7:26:35, 10.66s/it] 61%|   | 3988/6500 [12:03:35<7:22:40, 10.57s/it]                                                         61%|   | 3988/6500 [12:03:35<7:22:40, 10.57s/it] 61%|   | 3989/6500 [12:03:45<7:19:48, 10.51s/it]                                                         61%|   | 3989/6500 [12:03:45<7:19:48, 10.51s/it] 61%|   | 3990/6500 [12:03:56<7:17:53, 10.47s/it]                                                         61%|   | 3990/6500 [12:03:56<7:17:53, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8641228675842285, 'eval_runtime': 4.0005, 'eval_samples_per_second': 5.749, 'eval_steps_per_second': 1.5, 'epoch': 0.61}
                                                         61%|   | 3990/6500 [12:04:00<7:17:53, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-3990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3990I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3990
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3990/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-3990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3151, 'learning_rate': 3.249076307605643e-05, 'epoch': 0.61}
{'loss': 0.3411, 'learning_rate': 3.2468122240362284e-05, 'epoch': 0.61}
{'loss': 0.3354, 'learning_rate': 3.2445485502637976e-05, 'epoch': 0.61}
{'loss': 0.3458, 'learning_rate': 3.242285286817469e-05, 'epoch': 0.61}
{'loss': 0.337, 'learning_rate': 3.240022434226268e-05, 'epoch': 0.61}
 61%|   | 3991/6500 [12:04:11<8:12:12, 11.77s/it]                                                         61%|   | 3991/6500 [12:04:11<8:12:12, 11.77s/it] 61%|   | 3992/6500 [12:04:22<8:04:38, 11.59s/it]                                                         61%|   | 3992/6500 [12:04:22<8:04:38, 11.59s/it] 61%|   | 3993/6500 [12:04:32<7:48:58, 11.22s/it]                                                         61%|   | 3993/6500 [12:04:32<7:48:58, 11.22s/it] 61%|   | 3994/6500 [12:04:42<7:37:43, 10.96s/it]                                                         61%|   | 3994/6500 [12:04:42<7:37:43, 10.96s/it] 61%|   | 3995/6500 [12:04:53<7:29:59, 10.78s/it]                                                         61%|   | 3995/6500 [12:04:53<7:29:59, 10.78s/it] 61%|{'loss': 0.3472, 'learning_rate': 3.2377599930191224e-05, 'epoch': 0.61}
{'loss': 0.3712, 'learning_rate': 3.2354979637248636e-05, 'epoch': 0.61}
{'loss': 0.3532, 'learning_rate': 3.233236346872227e-05, 'epoch': 0.62}
{'loss': 0.3544, 'learning_rate': 3.230975142989853e-05, 'epoch': 0.62}
{'loss': 0.3496, 'learning_rate': 3.2287143526062825e-05, 'epoch': 0.62}
   | 3996/6500 [12:05:03<7:24:31, 10.65s/it]                                                         61%|   | 3996/6500 [12:05:03<7:24:31, 10.65s/it] 61%|   | 3997/6500 [12:05:13<7:20:33, 10.56s/it]                                                         61%|   | 3997/6500 [12:05:13<7:20:33, 10.56s/it] 62%|   | 3998/6500 [12:05:24<7:17:59, 10.50s/it]                                                         62%|   | 3998/6500 [12:05:24<7:17:59, 10.50s/it] 62%|   | 3999/6500 [12:05:34<7:15:54, 10.46s/it]                                                         62%|   | 3999/6500 [12:05:34<7:15:54, 10.46s/it] 62%|   | 4000/6500 [12:05:45<7:14:49, 10.44s/it]                                                         62%|   | 4000/6500 [12:05:45<7:14:49, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8494593501091003, 'eval_runtime': 3.9491, 'eval_samples_per_second': 5.824, 'eval_steps_per_second': 1.519, 'epoch': 0.62}
                                                         62%|   | 4000/6500 [12:05:49<7:14:49, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4000
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3343, 'learning_rate': 3.2264539762499644e-05, 'epoch': 0.62}
{'loss': 0.346, 'learning_rate': 3.224194014449245e-05, 'epoch': 0.62}
{'loss': 0.3563, 'learning_rate': 3.221934467732377e-05, 'epoch': 0.62}
{'loss': 0.3365, 'learning_rate': 3.219675336627516e-05, 'epoch': 0.62}
{'loss': 0.3317, 'learning_rate': 3.2174166216627214e-05, 'epoch': 0.62}
 62%|   | 4001/6500 [12:05:59<8:09:25, 11.75s/it]                                                         62%|   | 4001/6500 [12:05:59<8:09:25, 11.75s/it] 62%|   | 4002/6500 [12:06:10<7:52:05, 11.34s/it]                                                         62%|   | 4002/6500 [12:06:10<7:52:05, 11.34s/it] 62%|   | 4003/6500 [12:06:20<7:39:40, 11.05s/it]                                                         62%|   | 4003/6500 [12:06:20<7:39:40, 11.05s/it] 62%|   | 4004/6500 [12:06:31<7:31:03, 10.84s/it]                                                         62%|   | 4004/6500 [12:06:31<7:31:03, 10.84s/it] 62%|   | 4005/6500 [12:06:41<7:25:05, 10.70s/it]                                                         62%|   | 4005/6500 [12:06:41<7:25:05, 10.70s/it] 62%|{'loss': 0.3292, 'learning_rate': 3.2151583233659526e-05, 'epoch': 0.62}
{'loss': 0.3511, 'learning_rate': 3.212900442265075e-05, 'epoch': 0.62}
{'loss': 0.4127, 'learning_rate': 3.2106429788878525e-05, 'epoch': 0.62}
{'loss': 0.3311, 'learning_rate': 3.2083859337619534e-05, 'epoch': 0.62}
{'loss': 0.3259, 'learning_rate': 3.20612930741495e-05, 'epoch': 0.62}
   | 4006/6500 [12:06:51<7:20:47, 10.60s/it]                                                         62%|   | 4006/6500 [12:06:51<7:20:47, 10.60s/it] 62%|   | 4007/6500 [12:07:02<7:17:51, 10.54s/it]                                                         62%|   | 4007/6500 [12:07:02<7:17:51, 10.54s/it] 62%|   | 4008/6500 [12:07:12<7:18:44, 10.56s/it]                                                         62%|   | 4008/6500 [12:07:12<7:18:44, 10.56s/it] 62%|   | 4009/6500 [12:07:25<7:40:45, 11.10s/it]                                                         62%|   | 4009/6500 [12:07:25<7:40:45, 11.10s/it] 62%|   | 4010/6500 [12:07:35<7:33:26, 10.93s/it]                                                         62%|   | 4010/6500 [12:07:35<7:33:26, 10.93s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8515955209732056, 'eval_runtime': 4.2159, 'eval_samples_per_second': 5.456, 'eval_steps_per_second': 1.423, 'epoch': 0.62}
                                                         62%|   | 4010/6500 [12:07:39<7:33:26, 10.93s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3493, 'learning_rate': 3.203873100374314e-05, 'epoch': 0.62}
{'loss': 0.8481, 'learning_rate': 3.201617313167421e-05, 'epoch': 0.62}
{'loss': 0.3505, 'learning_rate': 3.1993619463215454e-05, 'epoch': 0.62}
{'loss': 0.3436, 'learning_rate': 3.197107000363867e-05, 'epoch': 0.62}
{'loss': 0.3519, 'learning_rate': 3.194852475821465e-05, 'epoch': 0.62}
 62%|   | 4011/6500 [12:07:50<8:25:12, 12.18s/it]                                                         62%|   | 4011/6500 [12:07:50<8:25:12, 12.18s/it] 62%|   | 4012/6500 [12:08:01<8:02:30, 11.64s/it]                                                         62%|   | 4012/6500 [12:08:01<8:02:30, 11.64s/it] 62%|   | 4013/6500 [12:08:11<7:46:49, 11.26s/it]                                                         62%|   | 4013/6500 [12:08:11<7:46:49, 11.26s/it] 62%|   | 4014/6500 [12:08:21<7:35:54, 11.00s/it]                                                         62%|   | 4014/6500 [12:08:21<7:35:54, 11.00s/it] 62%|   | 4015/6500 [12:08:32<7:28:06, 10.82s/it]                                                         62%|   | 4015/6500 [12:08:32<7:28:06, 10.82s/it] 62%|{'loss': 0.3212, 'learning_rate': 3.192598373221322e-05, 'epoch': 0.62}
{'loss': 0.362, 'learning_rate': 3.1903446930903205e-05, 'epoch': 0.62}
{'loss': 0.3202, 'learning_rate': 3.188091435955244e-05, 'epoch': 0.62}
{'loss': 0.3228, 'learning_rate': 3.1858386023427774e-05, 'epoch': 0.62}
{'loss': 0.3302, 'learning_rate': 3.183586192779507e-05, 'epoch': 0.62}
   | 4016/6500 [12:08:42<7:22:29, 10.69s/it]                                                         62%|   | 4016/6500 [12:08:42<7:22:29, 10.69s/it] 62%|   | 4017/6500 [12:08:53<7:18:39, 10.60s/it]                                                         62%|   | 4017/6500 [12:08:53<7:18:39, 10.60s/it] 62%|   | 4018/6500 [12:09:03<7:15:42, 10.53s/it]                                                         62%|   | 4018/6500 [12:09:03<7:15:42, 10.53s/it] 62%|   | 4019/6500 [12:09:13<7:14:00, 10.50s/it]                                                         62%|   | 4019/6500 [12:09:13<7:14:00, 10.50s/it] 62%|   | 4020/6500 [12:09:24<7:12:42, 10.47s/it]                                                         62%|   | 4020/6500 [12:09:24<7:12:42, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8631839156150818, 'eval_runtime': 3.9531, 'eval_samples_per_second': 5.818, 'eval_steps_per_second': 1.518, 'epoch': 0.62}
                                                         62%|   | 4020/6500 [12:09:28<7:12:42, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3309, 'learning_rate': 3.18133420779192e-05, 'epoch': 0.62}
{'loss': 0.3463, 'learning_rate': 3.1790826479064046e-05, 'epoch': 0.62}
{'loss': 0.3287, 'learning_rate': 3.176831513649249e-05, 'epoch': 0.62}
{'loss': 0.3493, 'learning_rate': 3.174580805546642e-05, 'epoch': 0.62}
{'loss': 0.3283, 'learning_rate': 3.172330524124673e-05, 'epoch': 0.62}
 62%|   | 4021/6500 [12:09:39<8:07:17, 11.79s/it]                                                         62%|   | 4021/6500 [12:09:39<8:07:17, 11.79s/it] 62%|   | 4022/6500 [12:09:49<7:49:50, 11.38s/it]                                                         62%|   | 4022/6500 [12:09:49<7:49:50, 11.38s/it] 62%|   | 4023/6500 [12:09:59<7:37:08, 11.07s/it]                                                         62%|   | 4023/6500 [12:09:59<7:37:08, 11.07s/it] 62%|   | 4024/6500 [12:10:10<7:33:05, 10.98s/it]                                                         62%|   | 4024/6500 [12:10:10<7:33:05, 10.98s/it] 62%|   | 4025/6500 [12:10:21<7:25:40, 10.80s/it]                                                         62%|   | 4025/6500 [12:10:21<7:25:40, 10.80s/it] 62%|{'loss': 0.3551, 'learning_rate': 3.170080669909331e-05, 'epoch': 0.62}
{'loss': 0.3381, 'learning_rate': 3.167831243426507e-05, 'epoch': 0.62}
{'loss': 0.3239, 'learning_rate': 3.165582245201989e-05, 'epoch': 0.62}
{'loss': 0.3397, 'learning_rate': 3.1633336757614694e-05, 'epoch': 0.62}
{'loss': 0.3382, 'learning_rate': 3.1610855356305354e-05, 'epoch': 0.62}
   | 4026/6500 [12:10:31<7:20:32, 10.68s/it]                                                         62%|   | 4026/6500 [12:10:31<7:20:32, 10.68s/it] 62%|   | 4027/6500 [12:10:41<7:16:44, 10.60s/it]                                                         62%|   | 4027/6500 [12:10:41<7:16:44, 10.60s/it] 62%|   | 4028/6500 [12:10:52<7:14:27, 10.55s/it]                                                         62%|   | 4028/6500 [12:10:52<7:14:27, 10.55s/it] 62%|   | 4029/6500 [12:11:02<7:12:28, 10.50s/it]                                                         62%|   | 4029/6500 [12:11:02<7:12:28, 10.50s/it] 62%|   | 4030/6500 [12:11:13<7:10:52, 10.47s/it]                                                         62%|   | 4030/6500 [12:11:13<7:10:52, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8514913320541382, 'eval_runtime': 4.6982, 'eval_samples_per_second': 4.895, 'eval_steps_per_second': 1.277, 'epoch': 0.62}
                                                         62%|   | 4030/6500 [12:11:17<7:10:52, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4030I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3336, 'learning_rate': 3.158837825334676e-05, 'epoch': 0.62}
{'loss': 0.3385, 'learning_rate': 3.15659054539928e-05, 'epoch': 0.62}
{'loss': 0.3611, 'learning_rate': 3.154343696349638e-05, 'epoch': 0.62}
{'loss': 0.3269, 'learning_rate': 3.152097278710933e-05, 'epoch': 0.62}
{'loss': 0.3359, 'learning_rate': 3.149851293008256e-05, 'epoch': 0.62}
 62%|   | 4031/6500 [12:11:28<8:13:31, 11.99s/it]                                                         62%|   | 4031/6500 [12:11:28<8:13:31, 11.99s/it] 62%|   | 4032/6500 [12:11:39<7:53:29, 11.51s/it]                                                         62%|   | 4032/6500 [12:11:39<7:53:29, 11.51s/it] 62%|   | 4033/6500 [12:11:49<7:39:24, 11.17s/it]                                                         62%|   | 4033/6500 [12:11:49<7:39:24, 11.17s/it] 62%|   | 4034/6500 [12:11:59<7:29:32, 10.94s/it]                                                         62%|   | 4034/6500 [12:11:59<7:29:32, 10.94s/it] 62%|   | 4035/6500 [12:12:10<7:22:30, 10.77s/it]                                                         62%|   | 4035/6500 [12:12:10<7:22:30, 10.77s/it] 62%|{'loss': 0.3161, 'learning_rate': 3.147605739766588e-05, 'epoch': 0.62}
{'loss': 0.3655, 'learning_rate': 3.145360619510817e-05, 'epoch': 0.62}
{'loss': 0.3879, 'learning_rate': 3.143115932765723e-05, 'epoch': 0.62}
{'loss': 0.3368, 'learning_rate': 3.140871680055991e-05, 'epoch': 0.62}
{'loss': 0.3338, 'learning_rate': 3.1386278619062006e-05, 'epoch': 0.62}
   | 4036/6500 [12:12:20<7:17:43, 10.66s/it]                                                         62%|   | 4036/6500 [12:12:20<7:17:43, 10.66s/it] 62%|   | 4037/6500 [12:12:30<7:14:22, 10.58s/it]                                                         62%|   | 4037/6500 [12:12:30<7:14:22, 10.58s/it] 62%|   | 4038/6500 [12:12:41<7:12:10, 10.53s/it]                                                         62%|   | 4038/6500 [12:12:41<7:12:10, 10.53s/it] 62%|   | 4039/6500 [12:12:51<7:10:45, 10.50s/it]                                                         62%|   | 4039/6500 [12:12:51<7:10:45, 10.50s/it] 62%|   | 4040/6500 [12:13:02<7:13:43, 10.58s/it]                                                         62%|   | 4040/6500 [12:13:02<7:13:43, 10.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8541162610054016, 'eval_runtime': 4.0511, 'eval_samples_per_second': 5.677, 'eval_steps_per_second': 1.481, 'epoch': 0.62}
                                                         62%|   | 4040/6500 [12:13:06<7:13:43, 10.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4040/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3325, 'learning_rate': 3.13638447884083e-05, 'epoch': 0.62}
{'loss': 0.8576, 'learning_rate': 3.134141531384256e-05, 'epoch': 0.62}
{'loss': 0.3547, 'learning_rate': 3.131899020060754e-05, 'epoch': 0.62}
{'loss': 0.3623, 'learning_rate': 3.1296569453944977e-05, 'epoch': 0.62}
{'loss': 0.3165, 'learning_rate': 3.127415307909558e-05, 'epoch': 0.62}
 62%|   | 4041/6500 [12:13:18<8:13:10, 12.03s/it]                                                         62%|   | 4041/6500 [12:13:18<8:13:10, 12.03s/it] 62%|   | 4042/6500 [12:13:28<7:52:27, 11.53s/it]                                                         62%|   | 4042/6500 [12:13:28<7:52:27, 11.53s/it] 62%|   | 4043/6500 [12:13:38<7:38:12, 11.19s/it]                                                         62%|   | 4043/6500 [12:13:38<7:38:12, 11.19s/it] 62%|   | 4044/6500 [12:13:49<7:28:00, 10.94s/it]                                                         62%|   | 4044/6500 [12:13:49<7:28:00, 10.94s/it] 62%|   | 4045/6500 [12:13:59<7:20:32, 10.77s/it]                                                         62%|   | 4045/6500 [12:13:59<7:20:32, 10.77s/it] 62%|{'loss': 0.3321, 'learning_rate': 3.125174108129906e-05, 'epoch': 0.62}
{'loss': 0.3547, 'learning_rate': 3.122933346579406e-05, 'epoch': 0.62}
{'loss': 0.3208, 'learning_rate': 3.1206930237818245e-05, 'epoch': 0.62}
{'loss': 0.3338, 'learning_rate': 3.118453140260823e-05, 'epoch': 0.62}
{'loss': 0.335, 'learning_rate': 3.116213696539959e-05, 'epoch': 0.62}
   | 4046/6500 [12:14:09<7:15:44, 10.65s/it]                                                         62%|   | 4046/6500 [12:14:09<7:15:44, 10.65s/it] 62%|   | 4047/6500 [12:14:20<7:12:21, 10.58s/it]                                                         62%|   | 4047/6500 [12:14:20<7:12:21, 10.58s/it] 62%|   | 4048/6500 [12:14:30<7:10:08, 10.53s/it]                                                         62%|   | 4048/6500 [12:14:30<7:10:08, 10.53s/it] 62%|   | 4049/6500 [12:14:41<7:08:26, 10.49s/it]                                                         62%|   | 4049/6500 [12:14:41<7:08:26, 10.49s/it] 62%|   | 4050/6500 [12:14:51<7:07:28, 10.47s/it]                                                         62%|   | 4050/6500 [12:14:51<7:07:28, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8590275049209595, 'eval_runtime': 4.0469, 'eval_samples_per_second': 5.683, 'eval_steps_per_second': 1.483, 'epoch': 0.62}
                                                         62%|   | 4050/6500 [12:14:55<7:07:28, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4050/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3329, 'learning_rate': 3.1139746931426894e-05, 'epoch': 0.62}
{'loss': 0.3406, 'learning_rate': 3.1117361305923684e-05, 'epoch': 0.62}
{'loss': 0.3279, 'learning_rate': 3.109498009412246e-05, 'epoch': 0.62}
{'loss': 0.345, 'learning_rate': 3.10726033012547e-05, 'epoch': 0.62}
{'loss': 0.3329, 'learning_rate': 3.105023093255084e-05, 'epoch': 0.62}
 62%|   | 4051/6500 [12:15:06<8:02:48, 11.83s/it]                                                         62%|   | 4051/6500 [12:15:06<8:02:48, 11.83s/it] 62%|   | 4052/6500 [12:15:16<7:44:58, 11.40s/it]                                                         62%|   | 4052/6500 [12:15:16<7:44:58, 11.40s/it] 62%|   | 4053/6500 [12:15:27<7:32:37, 11.10s/it]                                                         62%|   | 4053/6500 [12:15:27<7:32:37, 11.10s/it] 62%|   | 4054/6500 [12:15:37<7:23:48, 10.89s/it]                                                         62%|   | 4054/6500 [12:15:37<7:23:48, 10.89s/it] 62%|   | 4055/6500 [12:15:48<7:17:55, 10.75s/it]                                                         62%|   | 4055/6500 [12:15:48<7:17:55, 10.75s/it] 62%|{'loss': 0.3697, 'learning_rate': 3.102786299324028e-05, 'epoch': 0.62}
{'loss': 0.3334, 'learning_rate': 3.100549948855138e-05, 'epoch': 0.62}
{'loss': 0.347, 'learning_rate': 3.0983140423711495e-05, 'epoch': 0.62}
{'loss': 0.3573, 'learning_rate': 3.096078580394691e-05, 'epoch': 0.62}
{'loss': 0.3333, 'learning_rate': 3.09384356344829e-05, 'epoch': 0.62}
   | 4056/6500 [12:15:58<7:13:08, 10.63s/it]                                                         62%|   | 4056/6500 [12:15:58<7:13:08, 10.63s/it] 62%|   | 4057/6500 [12:16:09<7:14:28, 10.67s/it]                                                         62%|   | 4057/6500 [12:16:09<7:14:28, 10.67s/it] 62%|   | 4058/6500 [12:16:19<7:10:44, 10.58s/it]                                                         62%|   | 4058/6500 [12:16:19<7:10:44, 10.58s/it] 62%|   | 4059/6500 [12:16:29<7:08:05, 10.52s/it]                                                         62%|   | 4059/6500 [12:16:29<7:08:05, 10.52s/it] 62%|   | 4060/6500 [12:16:40<7:06:02, 10.48s/it]                                                         62%|   | 4060/6500 [12:16:40<7:06:02, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8515011072158813, 'eval_runtime': 4.4488, 'eval_samples_per_second': 5.17, 'eval_steps_per_second': 1.349, 'epoch': 0.62}
                                                         62%|   | 4060/6500 [12:16:44<7:06:02, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4060/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3499, 'learning_rate': 3.091608992054365e-05, 'epoch': 0.62}
{'loss': 0.3361, 'learning_rate': 3.089374866735234e-05, 'epoch': 0.62}
{'loss': 0.3477, 'learning_rate': 3.0871411880131126e-05, 'epoch': 0.63}
{'loss': 0.3292, 'learning_rate': 3.084907956410107e-05, 'epoch': 0.63}
{'loss': 0.3256, 'learning_rate': 3.082675172448223e-05, 'epoch': 0.63}
 62%|   | 4061/6500 [12:16:55<8:05:19, 11.94s/it]                                                         62%|   | 4061/6500 [12:16:55<8:05:19, 11.94s/it] 62%|   | 4062/6500 [12:17:06<7:45:54, 11.47s/it]                                                         62%|   | 4062/6500 [12:17:06<7:45:54, 11.47s/it] 63%|   | 4063/6500 [12:17:16<7:32:14, 11.13s/it]                                                         63%|   | 4063/6500 [12:17:16<7:32:14, 11.13s/it] 63%|   | 4064/6500 [12:17:26<7:22:47, 10.91s/it]                                                         63%|   | 4064/6500 [12:17:26<7:22:47, 10.91s/it] 63%|   | 4065/6500 [12:17:37<7:16:01, 10.74s/it]                                                         63%|   | 4065/6500 [12:17:37<7:16:01, 10.74s/it] 63%|{'loss': 0.3217, 'learning_rate': 3.080442836649361e-05, 'epoch': 0.63}
{'loss': 0.4126, 'learning_rate': 3.078210949535314e-05, 'epoch': 0.63}
{'loss': 0.3499, 'learning_rate': 3.0759795116277725e-05, 'epoch': 0.63}
{'loss': 0.3355, 'learning_rate': 3.0737485234483223e-05, 'epoch': 0.63}
{'loss': 0.3489, 'learning_rate': 3.071517985518442e-05, 'epoch': 0.63}
   | 4066/6500 [12:17:47<7:11:34, 10.64s/it]                                                         63%|   | 4066/6500 [12:17:47<7:11:34, 10.64s/it] 63%|   | 4067/6500 [12:17:57<7:08:05, 10.56s/it]                                                         63%|   | 4067/6500 [12:17:57<7:08:05, 10.56s/it] 63%|   | 4068/6500 [12:18:08<7:05:40, 10.50s/it]                                                         63%|   | 4068/6500 [12:18:08<7:05:40, 10.50s/it] 63%|   | 4069/6500 [12:18:18<7:04:00, 10.47s/it]                                                         63%|   | 4069/6500 [12:18:18<7:04:00, 10.47s/it] 63%|   | 4070/6500 [12:18:29<7:04:32, 10.48s/it]                                                         63%|   | 4070/6500 [12:18:29<7:04:32, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8546911478042603, 'eval_runtime': 3.9961, 'eval_samples_per_second': 5.756, 'eval_steps_per_second': 1.501, 'epoch': 0.63}
                                                         63%|   | 4070/6500 [12:18:33<7:04:32, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8689, 'learning_rate': 3.069287898359509e-05, 'epoch': 0.63}
{'loss': 0.3556, 'learning_rate': 3.06705826249279e-05, 'epoch': 0.63}
{'loss': 0.338, 'learning_rate': 3.06482907843945e-05, 'epoch': 0.63}
{'loss': 0.3426, 'learning_rate': 3.062600346720545e-05, 'epoch': 0.63}
{'loss': 0.3188, 'learning_rate': 3.060372067857031e-05, 'epoch': 0.63}
 63%|   | 4071/6500 [12:18:44<7:57:00, 11.78s/it]                                                         63%|   | 4071/6500 [12:18:44<7:57:00, 11.78s/it] 63%|   | 4072/6500 [12:18:54<7:39:46, 11.36s/it]                                                         63%|   | 4072/6500 [12:18:54<7:39:46, 11.36s/it] 63%|   | 4073/6500 [12:19:05<7:33:17, 11.21s/it]                                                         63%|   | 4073/6500 [12:19:05<7:33:17, 11.21s/it] 63%|   | 4074/6500 [12:19:15<7:23:10, 10.96s/it]                                                         63%|   | 4074/6500 [12:19:15<7:23:10, 10.96s/it] 63%|   | 4075/6500 [12:19:26<7:15:47, 10.78s/it]                                                         63%|   | 4075/6500 [12:19:26<7:15:47, 10.78s/it] 63%|{'loss': 0.353, 'learning_rate': 3.058144242369753e-05, 'epoch': 0.63}
{'loss': 0.3286, 'learning_rate': 3.055916870779453e-05, 'epoch': 0.63}
{'loss': 0.3058, 'learning_rate': 3.053689953606762e-05, 'epoch': 0.63}
{'loss': 0.3383, 'learning_rate': 3.051463491372211e-05, 'epoch': 0.63}
{'loss': 0.3169, 'learning_rate': 3.0492374845962225e-05, 'epoch': 0.63}
   | 4076/6500 [12:19:36<7:10:30, 10.66s/it]                                                         63%|   | 4076/6500 [12:19:36<7:10:30, 10.66s/it] 63%|   | 4077/6500 [12:19:46<7:06:55, 10.57s/it]                                                         63%|   | 4077/6500 [12:19:46<7:06:55, 10.57s/it] 63%|   | 4078/6500 [12:19:57<7:04:18, 10.51s/it]                                                         63%|   | 4078/6500 [12:19:57<7:04:18, 10.51s/it] 63%|   | 4079/6500 [12:20:07<7:02:27, 10.47s/it]                                                         63%|   | 4079/6500 [12:20:07<7:02:27, 10.47s/it] 63%|   | 4080/6500 [12:20:17<7:00:56, 10.44s/it]                                                         63%|   | 4080/6500 [12:20:17<7:00:56, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8635196685791016, 'eval_runtime': 3.952, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.63}
                                                         63%|   | 4080/6500 [12:20:21<7:00:56, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4080
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4080/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4080/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.337, 'learning_rate': 3.04701193379911e-05, 'epoch': 0.63}
{'loss': 0.3284, 'learning_rate': 3.0447868395010836e-05, 'epoch': 0.63}
{'loss': 0.3421, 'learning_rate': 3.0425622022222478e-05, 'epoch': 0.63}
{'loss': 0.3265, 'learning_rate': 3.040338022482594e-05, 'epoch': 0.63}
{'loss': 0.3522, 'learning_rate': 3.038114300802012e-05, 'epoch': 0.63}
 63%|   | 4081/6500 [12:20:32<7:53:49, 11.75s/it]                                                         63%|   | 4081/6500 [12:20:32<7:53:49, 11.75s/it] 63%|   | 4082/6500 [12:20:43<7:36:55, 11.34s/it]                                                         63%|   | 4082/6500 [12:20:43<7:36:55, 11.34s/it] 63%|   | 4083/6500 [12:20:53<7:25:04, 11.05s/it]                                                         63%|   | 4083/6500 [12:20:53<7:25:04, 11.05s/it] 63%|   | 4084/6500 [12:21:03<7:16:27, 10.84s/it]                                                         63%|   | 4084/6500 [12:21:03<7:16:27, 10.84s/it] 63%|   | 4085/6500 [12:21:14<7:10:33, 10.70s/it]                                                         63%|   | 4085/6500 [12:21:14<7:10:33, 10.70s/it] 63%|{'loss': 0.3355, 'learning_rate': 3.0358910377002848e-05, 'epoch': 0.63}
{'loss': 0.3319, 'learning_rate': 3.0336682336970846e-05, 'epoch': 0.63}
{'loss': 0.3463, 'learning_rate': 3.0314458893119808e-05, 'epoch': 0.63}
{'loss': 0.3346, 'learning_rate': 3.0292240050644304e-05, 'epoch': 0.63}
{'loss': 0.3357, 'learning_rate': 3.0270025814737856e-05, 'epoch': 0.63}
   | 4086/6500 [12:21:24<7:06:20, 10.60s/it]                                                         63%|   | 4086/6500 [12:21:24<7:06:20, 10.60s/it] 63%|   | 4087/6500 [12:21:34<7:03:19, 10.53s/it]                                                         63%|   | 4087/6500 [12:21:34<7:03:19, 10.53s/it] 63%|   | 4088/6500 [12:21:45<7:01:14, 10.48s/it]                                                         63%|   | 4088/6500 [12:21:45<7:01:14, 10.48s/it] 63%|   | 4089/6500 [12:21:55<7:02:47, 10.52s/it]                                                         63%|   | 4089/6500 [12:21:55<7:02:47, 10.52s/it] 63%|   | 4090/6500 [12:22:06<7:00:52, 10.48s/it]                                                         63%|   | 4090/6500 [12:22:06<7:00:52, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8547579646110535, 'eval_runtime': 4.3606, 'eval_samples_per_second': 5.274, 'eval_steps_per_second': 1.376, 'epoch': 0.63}
                                                         63%|   | 4090/6500 [12:22:10<7:00:52, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4090/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3236, 'learning_rate': 3.024781619059292e-05, 'epoch': 0.63}
{'loss': 0.3635, 'learning_rate': 3.0225611183400855e-05, 'epoch': 0.63}
{'loss': 0.3269, 'learning_rate': 3.0203410798351945e-05, 'epoch': 0.63}
{'loss': 0.3354, 'learning_rate': 3.0181215040635402e-05, 'epoch': 0.63}
{'loss': 0.3254, 'learning_rate': 3.0159023915439338e-05, 'epoch': 0.63}
 63%|   | 4091/6500 [12:22:21<7:58:12, 11.91s/it]                                                         63%|   | 4091/6500 [12:22:21<7:58:12, 11.91s/it] 63%|   | 4092/6500 [12:22:31<7:39:20, 11.45s/it]                                                         63%|   | 4092/6500 [12:22:31<7:39:20, 11.45s/it] 63%|   | 4093/6500 [12:22:42<7:26:13, 11.12s/it]                                                         63%|   | 4093/6500 [12:22:42<7:26:13, 11.12s/it] 63%|   | 4094/6500 [12:22:52<7:16:49, 10.89s/it]                                                         63%|   | 4094/6500 [12:22:52<7:16:49, 10.89s/it] 63%|   | 4095/6500 [12:23:02<7:10:22, 10.74s/it]                                                         63%|   | 4095/6500 [12:23:02<7:10:22, 10.74s/it] 63%|{'loss': 0.347, 'learning_rate': 3.0136837427950793e-05, 'epoch': 0.63}
{'loss': 0.3941, 'learning_rate': 3.0114655583355733e-05, 'epoch': 0.63}
{'loss': 0.3333, 'learning_rate': 3.009247838683903e-05, 'epoch': 0.63}
{'loss': 0.3197, 'learning_rate': 3.007030584358447e-05, 'epoch': 0.63}
{'loss': 0.3394, 'learning_rate': 3.004813795877473e-05, 'epoch': 0.63}
   | 4096/6500 [12:23:13<7:05:49, 10.63s/it]                                                         63%|   | 4096/6500 [12:23:13<7:05:49, 10.63s/it] 63%|   | 4097/6500 [12:23:23<7:02:36, 10.55s/it]                                                         63%|   | 4097/6500 [12:23:23<7:02:36, 10.55s/it] 63%|   | 4098/6500 [12:23:34<7:00:10, 10.50s/it]                                                         63%|   | 4098/6500 [12:23:34<7:00:10, 10.50s/it] 63%|   | 4099/6500 [12:23:44<6:58:24, 10.46s/it]                                                         63%|   | 4099/6500 [12:23:44<6:58:24, 10.46s/it] 63%|   | 4100/6500 [12:23:54<6:57:02, 10.43s/it]                                                         63%|   | 4100/6500 [12:23:54<6:57:02, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8567953705787659, 'eval_runtime': 3.9474, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.63}
                                                         63%|   | 4100/6500 [12:23:58<6:57:02, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4100I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4100/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8524, 'learning_rate': 3.0025974737591434e-05, 'epoch': 0.63}
{'loss': 0.3496, 'learning_rate': 3.0003816185215107e-05, 'epoch': 0.63}
{'loss': 0.3381, 'learning_rate': 2.9981662306825163e-05, 'epoch': 0.63}
{'loss': 0.3232, 'learning_rate': 2.995951310759994e-05, 'epoch': 0.63}
{'loss': 0.327, 'learning_rate': 2.993736859271669e-05, 'epoch': 0.63}
 63%|   | 4101/6500 [12:24:09<7:49:06, 11.73s/it]                                                         63%|   | 4101/6500 [12:24:09<7:49:06, 11.73s/it] 63%|   | 4102/6500 [12:24:19<7:32:48, 11.33s/it]                                                         63%|   | 4102/6500 [12:24:19<7:32:48, 11.33s/it] 63%|   | 4103/6500 [12:24:30<7:21:07, 11.04s/it]                                                         63%|   | 4103/6500 [12:24:30<7:21:07, 11.04s/it] 63%|   | 4104/6500 [12:24:40<7:12:45, 10.84s/it]                                                         63%|   | 4104/6500 [12:24:40<7:12:45, 10.84s/it] 63%|   | 4105/6500 [12:24:51<7:11:48, 10.82s/it]                                                         63%|   | 4105/6500 [12:24:51<7:11:48, 10.82s/it] 63%|{'loss': 0.3528, 'learning_rate': 2.991522876735154e-05, 'epoch': 0.63}
{'loss': 0.3263, 'learning_rate': 2.9893093636679546e-05, 'epoch': 0.63}
{'loss': 0.3254, 'learning_rate': 2.987096320587467e-05, 'epoch': 0.63}
{'loss': 0.3225, 'learning_rate': 2.984883748010975e-05, 'epoch': 0.63}
{'loss': 0.337, 'learning_rate': 2.982671646455655e-05, 'epoch': 0.63}
   | 4106/6500 [12:25:01<7:06:07, 10.68s/it]                                                         63%|   | 4106/6500 [12:25:01<7:06:07, 10.68s/it] 63%|   | 4107/6500 [12:25:12<7:02:06, 10.58s/it]                                                         63%|   | 4107/6500 [12:25:12<7:02:06, 10.58s/it] 63%|   | 4108/6500 [12:25:22<6:59:23, 10.52s/it]                                                         63%|   | 4108/6500 [12:25:22<6:59:23, 10.52s/it] 63%|   | 4109/6500 [12:25:32<6:57:14, 10.47s/it]                                                         63%|   | 4109/6500 [12:25:32<6:57:14, 10.47s/it] 63%|   | 4110/6500 [12:25:43<6:55:46, 10.44s/it]                                                         63%|   | 4110/6500 [12:25:43<6:55:46, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8651890158653259, 'eval_runtime': 3.945, 'eval_samples_per_second': 5.83, 'eval_steps_per_second': 1.521, 'epoch': 0.63}
                                                         63%|   | 4110/6500 [12:25:47<6:55:46, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4110I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4110

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4110
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4110/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3271, 'learning_rate': 2.9804600164385733e-05, 'epoch': 0.63}
{'loss': 0.324, 'learning_rate': 2.9782488584766822e-05, 'epoch': 0.63}
{'loss': 0.3479, 'learning_rate': 2.976038173086828e-05, 'epoch': 0.63}
{'loss': 0.3356, 'learning_rate': 2.9738279607857455e-05, 'epoch': 0.63}
{'loss': 0.3464, 'learning_rate': 2.9716182220900578e-05, 'epoch': 0.63}
 63%|   | 4111/6500 [12:25:58<7:48:01, 11.75s/it]                                                         63%|   | 4111/6500 [12:25:58<7:48:01, 11.75s/it] 63%|   | 4112/6500 [12:26:08<7:31:07, 11.33s/it]                                                         63%|   | 4112/6500 [12:26:08<7:31:07, 11.33s/it] 63%|   | 4113/6500 [12:26:18<7:19:25, 11.05s/it]                                                         63%|   | 4113/6500 [12:26:18<7:19:25, 11.05s/it] 63%|   | 4114/6500 [12:26:29<7:11:01, 10.84s/it]                                                         63%|   | 4114/6500 [12:26:29<7:11:01, 10.84s/it] 63%|   | 4115/6500 [12:26:39<7:05:07, 10.69s/it]                                                         63%|   | 4115/6500 [12:26:39<7:05:07, 10.69s/it] 63%|{'loss': 0.3371, 'learning_rate': 2.9694089575162785e-05, 'epoch': 0.63}
{'loss': 0.3301, 'learning_rate': 2.9672001675808086e-05, 'epoch': 0.63}
{'loss': 0.3474, 'learning_rate': 2.96499185279994e-05, 'epoch': 0.63}
{'loss': 0.3364, 'learning_rate': 2.9627840136898523e-05, 'epoch': 0.63}
{'loss': 0.3391, 'learning_rate': 2.9605766507666145e-05, 'epoch': 0.63}
   | 4116/6500 [12:26:49<7:01:03, 10.60s/it]                                                         63%|   | 4116/6500 [12:26:49<7:01:03, 10.60s/it] 63%|   | 4117/6500 [12:27:00<6:58:13, 10.53s/it]                                                         63%|   | 4117/6500 [12:27:00<6:58:13, 10.53s/it] 63%|   | 4118/6500 [12:27:10<6:56:03, 10.48s/it]                                                         63%|   | 4118/6500 [12:27:10<6:56:03, 10.48s/it] 63%|   | 4119/6500 [12:27:20<6:54:25, 10.44s/it]                                                         63%|   | 4119/6500 [12:27:20<6:54:25, 10.44s/it] 63%|   | 4120/6500 [12:27:31<6:53:12, 10.42s/it]                                                         63%|   | 4120/6500 [12:27:31<6:53:12, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8544669151306152, 'eval_runtime': 3.9489, 'eval_samples_per_second': 5.824, 'eval_steps_per_second': 1.519, 'epoch': 0.63}
                                                         63%|   | 4120/6500 [12:27:35<6:53:12, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4120/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4120/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3345, 'learning_rate': 2.9583697645461848e-05, 'epoch': 0.63}
{'loss': 0.3548, 'learning_rate': 2.95616335554441e-05, 'epoch': 0.63}
{'loss': 0.3207, 'learning_rate': 2.953957424277024e-05, 'epoch': 0.63}
{'loss': 0.3219, 'learning_rate': 2.9517519712596498e-05, 'epoch': 0.63}
{'loss': 0.3092, 'learning_rate': 2.9495469970078e-05, 'epoch': 0.63}
 63%|   | 4121/6500 [12:27:46<7:48:56, 11.83s/it]                                                         63%|   | 4121/6500 [12:27:46<7:48:56, 11.83s/it] 63%|   | 4122/6500 [12:27:56<7:31:24, 11.39s/it]                                                         63%|   | 4122/6500 [12:27:56<7:31:24, 11.39s/it] 63%|   | 4123/6500 [12:28:07<7:19:08, 11.08s/it]                                                         63%|   | 4123/6500 [12:28:07<7:19:08, 11.08s/it] 63%|   | 4124/6500 [12:28:17<7:10:26, 10.87s/it]                                                         63%|   | 4124/6500 [12:28:17<7:10:26, 10.87s/it] 63%|   | 4125/6500 [12:28:28<7:06:51, 10.78s/it]                                                         63%|   | 4125/6500 [12:28:28<7:06:51, 10.78s/it] 63%|{'loss': 0.3656, 'learning_rate': 2.9473425020368716e-05, 'epoch': 0.63}
{'loss': 0.3814, 'learning_rate': 2.9451384868621523e-05, 'epoch': 0.63}
{'loss': 0.3246, 'learning_rate': 2.942934951998819e-05, 'epoch': 0.64}
{'loss': 0.3394, 'learning_rate': 2.940731897961933e-05, 'epoch': 0.64}
{'loss': 0.3368, 'learning_rate': 2.9385293252664452e-05, 'epoch': 0.64}
   | 4126/6500 [12:28:38<7:01:38, 10.66s/it]                                                         63%|   | 4126/6500 [12:28:38<7:01:38, 10.66s/it] 63%|   | 4127/6500 [12:28:48<6:58:04, 10.57s/it]                                                         63%|   | 4127/6500 [12:28:48<6:58:04, 10.57s/it] 64%|   | 4128/6500 [12:28:59<6:55:36, 10.51s/it]                                                         64%|   | 4128/6500 [12:28:59<6:55:36, 10.51s/it] 64%|   | 4129/6500 [12:29:09<6:53:44, 10.47s/it]                                                         64%|   | 4129/6500 [12:29:09<6:53:44, 10.47s/it] 64%|   | 4130/6500 [12:29:20<6:52:34, 10.45s/it]                                                         64%|   | 4130/6500 [12:29:20<6:52:34, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8571740984916687, 'eval_runtime': 3.9536, 'eval_samples_per_second': 5.818, 'eval_steps_per_second': 1.518, 'epoch': 0.64}
                                                         64%|   | 4130/6500 [12:29:23<6:52:34, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8627, 'learning_rate': 2.936327234427195e-05, 'epoch': 0.64}
{'loss': 0.3331, 'learning_rate': 2.9341256259589052e-05, 'epoch': 0.64}
{'loss': 0.3445, 'learning_rate': 2.9319245003761895e-05, 'epoch': 0.64}
{'loss': 0.3242, 'learning_rate': 2.9297238581935482e-05, 'epoch': 0.64}
{'loss': 0.3442, 'learning_rate': 2.9275236999253674e-05, 'epoch': 0.64}
 64%|   | 4131/6500 [12:29:34<7:45:31, 11.79s/it]                                                         64%|   | 4131/6500 [12:29:34<7:45:31, 11.79s/it] 64%|   | 4132/6500 [12:29:45<7:28:36, 11.37s/it]                                                         64%|   | 4132/6500 [12:29:45<7:28:36, 11.37s/it] 64%|   | 4133/6500 [12:29:55<7:16:36, 11.07s/it]                                                         64%|   | 4133/6500 [12:29:55<7:16:36, 11.07s/it] 64%|   | 4134/6500 [12:30:06<7:08:11, 10.86s/it]                                                         64%|   | 4134/6500 [12:30:06<7:08:11, 10.86s/it] 64%|   | 4135/6500 [12:30:16<7:02:13, 10.71s/it]                                                         64%|   | 4135/6500 [12:30:16<7:02:13, 10.71s/it] 64%|{'loss': 0.3393, 'learning_rate': 2.9253240260859215e-05, 'epoch': 0.64}
{'loss': 0.3035, 'learning_rate': 2.9231248371893695e-05, 'epoch': 0.64}
{'loss': 0.3396, 'learning_rate': 2.920926133749759e-05, 'epoch': 0.64}
{'loss': 0.3234, 'learning_rate': 2.9187279162810243e-05, 'epoch': 0.64}
{'loss': 0.3453, 'learning_rate': 2.916530185296984e-05, 'epoch': 0.64}
   | 4136/6500 [12:30:26<6:58:00, 10.61s/it]                                                         64%|   | 4136/6500 [12:30:26<6:58:00, 10.61s/it] 64%|   | 4137/6500 [12:30:37<6:54:56, 10.54s/it]                                                         64%|   | 4137/6500 [12:30:37<6:54:56, 10.54s/it] 64%|   | 4138/6500 [12:30:47<6:57:10, 10.60s/it]                                                         64%|   | 4138/6500 [12:30:47<6:57:10, 10.60s/it] 64%|   | 4139/6500 [12:30:58<6:54:27, 10.53s/it]                                                         64%|   | 4139/6500 [12:30:58<6:54:27, 10.53s/it] 64%|   | 4140/6500 [12:31:08<6:52:50, 10.50s/it]                                                         64%|   | 4140/6500 [12:31:08<6:52:50, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8623095750808716, 'eval_runtime': 4.5613, 'eval_samples_per_second': 5.042, 'eval_steps_per_second': 1.315, 'epoch': 0.64}
                                                         64%|   | 4140/6500 [12:31:13<6:52:50, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4140/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3406, 'learning_rate': 2.9143329413113464e-05, 'epoch': 0.64}
{'loss': 0.3482, 'learning_rate': 2.9121361848377014e-05, 'epoch': 0.64}
{'loss': 0.3324, 'learning_rate': 2.909939916389529e-05, 'epoch': 0.64}
{'loss': 0.3316, 'learning_rate': 2.9077441364801938e-05, 'epoch': 0.64}
{'loss': 0.3702, 'learning_rate': 2.9055488456229473e-05, 'epoch': 0.64}
 64%|   | 4141/6500 [12:31:24<7:51:24, 11.99s/it]                                                         64%|   | 4141/6500 [12:31:24<7:51:24, 11.99s/it] 64%|   | 4142/6500 [12:31:34<7:32:18, 11.51s/it]                                                         64%|   | 4142/6500 [12:31:34<7:32:18, 11.51s/it] 64%|   | 4143/6500 [12:31:44<7:18:35, 11.17s/it]                                                         64%|   | 4143/6500 [12:31:44<7:18:35, 11.17s/it] 64%|   | 4144/6500 [12:31:55<7:09:10, 10.93s/it]                                                         64%|   | 4144/6500 [12:31:55<7:09:10, 10.93s/it] 64%|   | 4145/6500 [12:32:05<7:02:13, 10.76s/it]                                                         64%|   | 4145/6500 [12:32:05<7:02:13, 10.76s/it] 64%|{'loss': 0.3264, 'learning_rate': 2.9033540443309227e-05, 'epoch': 0.64}
{'loss': 0.3499, 'learning_rate': 2.9011597331171414e-05, 'epoch': 0.64}
{'loss': 0.3548, 'learning_rate': 2.898965912494511e-05, 'epoch': 0.64}
{'loss': 0.3288, 'learning_rate': 2.8967725829758248e-05, 'epoch': 0.64}
{'loss': 0.3403, 'learning_rate': 2.8945797450737587e-05, 'epoch': 0.64}
   | 4146/6500 [12:32:16<6:57:24, 10.64s/it]                                                         64%|   | 4146/6500 [12:32:16<6:57:24, 10.64s/it] 64%|   | 4147/6500 [12:32:26<6:54:02, 10.56s/it]                                                         64%|   | 4147/6500 [12:32:26<6:54:02, 10.56s/it] 64%|   | 4148/6500 [12:32:36<6:51:43, 10.50s/it]                                                         64%|   | 4148/6500 [12:32:36<6:51:43, 10.50s/it] 64%|   | 4149/6500 [12:32:47<6:49:59, 10.46s/it]                                                         64%|   | 4149/6500 [12:32:47<6:49:59, 10.46s/it] 64%|   | 4150/6500 [12:32:57<6:48:50, 10.44s/it]                                                         64%|   | 4150/6500 [12:32:57<6:48:50, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8508399724960327, 'eval_runtime': 3.9456, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.64}
                                                         64%|   | 4150/6500 [12:33:01<6:48:50, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4150/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3471, 'learning_rate': 2.892387399300877e-05, 'epoch': 0.64}
{'loss': 0.3385, 'learning_rate': 2.8901955461696262e-05, 'epoch': 0.64}
{'loss': 0.3273, 'learning_rate': 2.888004186192338e-05, 'epoch': 0.64}
{'loss': 0.3253, 'learning_rate': 2.8858133198812305e-05, 'epoch': 0.64}
{'loss': 0.3169, 'learning_rate': 2.8836229477484046e-05, 'epoch': 0.64}
 64%|   | 4151/6500 [12:33:12<7:40:35, 11.76s/it]                                                         64%|   | 4151/6500 [12:33:12<7:40:35, 11.76s/it] 64%|   | 4152/6500 [12:33:22<7:24:06, 11.35s/it]                                                         64%|   | 4152/6500 [12:33:22<7:24:06, 11.35s/it] 64%|   | 4153/6500 [12:33:33<7:12:27, 11.06s/it]                                                         64%|   | 4153/6500 [12:33:33<7:12:27, 11.06s/it] 64%|   | 4154/6500 [12:33:43<7:07:20, 10.93s/it]                                                         64%|   | 4154/6500 [12:33:43<7:07:20, 10.93s/it] 64%|   | 4155/6500 [12:33:54<7:04:09, 10.85s/it]                                                         64%|   | 4155/6500 [12:33:54<7:04:09, 10.85s/it] 64%|{'loss': 0.4252, 'learning_rate': 2.881433070305849e-05, 'epoch': 0.64}
{'loss': 0.3186, 'learning_rate': 2.8792436880654305e-05, 'epoch': 0.64}
{'loss': 0.3267, 'learning_rate': 2.8770548015389054e-05, 'epoch': 0.64}
{'loss': 0.3443, 'learning_rate': 2.8748664112379127e-05, 'epoch': 0.64}
{'loss': 0.8578, 'learning_rate': 2.872678517673975e-05, 'epoch': 0.64}
   | 4156/6500 [12:34:04<6:58:18, 10.71s/it]                                                         64%|   | 4156/6500 [12:34:04<6:58:18, 10.71s/it] 64%|   | 4157/6500 [12:34:15<6:54:09, 10.61s/it]                                                         64%|   | 4157/6500 [12:34:15<6:54:09, 10.61s/it] 64%|   | 4158/6500 [12:34:25<6:51:24, 10.54s/it]                                                         64%|   | 4158/6500 [12:34:25<6:51:24, 10.54s/it] 64%|   | 4159/6500 [12:34:35<6:49:22, 10.49s/it]                                                         64%|   | 4159/6500 [12:34:35<6:49:22, 10.49s/it] 64%|   | 4160/6500 [12:34:46<6:47:41, 10.45s/it]                                                         64%|   | 4160/6500 [12:34:46<6:47:41, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8585836291313171, 'eval_runtime': 3.9546, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.64}
                                                         64%|   | 4160/6500 [12:34:50<6:47:41, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4160/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3462, 'learning_rate': 2.8704911213584996e-05, 'epoch': 0.64}
{'loss': 0.3369, 'learning_rate': 2.8683042228027772e-05, 'epoch': 0.64}
{'loss': 0.3404, 'learning_rate': 2.866117822517982e-05, 'epoch': 0.64}
{'loss': 0.3085, 'learning_rate': 2.863931921015171e-05, 'epoch': 0.64}
{'loss': 0.3486, 'learning_rate': 2.861746518805286e-05, 'epoch': 0.64}
 64%|   | 4161/6500 [12:35:01<7:39:10, 11.78s/it]                                                         64%|   | 4161/6500 [12:35:01<7:39:10, 11.78s/it] 64%|   | 4162/6500 [12:35:11<7:22:46, 11.36s/it]                                                         64%|   | 4162/6500 [12:35:11<7:22:46, 11.36s/it] 64%|   | 4163/6500 [12:35:21<7:11:05, 11.07s/it]                                                         64%|   | 4163/6500 [12:35:21<7:11:05, 11.07s/it] 64%|   | 4164/6500 [12:35:32<7:02:44, 10.86s/it]                                                         64%|   | 4164/6500 [12:35:32<7:02:44, 10.86s/it] 64%|   | 4165/6500 [12:35:42<6:56:51, 10.71s/it]                                                         64%|   | 4165/6500 [12:35:42<6:56:51, 10.71s/it] 64%|{'loss': 0.3194, 'learning_rate': 2.8595616163991523e-05, 'epoch': 0.64}
{'loss': 0.3109, 'learning_rate': 2.857377214307478e-05, 'epoch': 0.64}
{'loss': 0.3205, 'learning_rate': 2.8551933130408505e-05, 'epoch': 0.64}
{'loss': 0.326, 'learning_rate': 2.8530099131097455e-05, 'epoch': 0.64}
{'loss': 0.3389, 'learning_rate': 2.850827015024519e-05, 'epoch': 0.64}
   | 4166/6500 [12:35:53<6:52:32, 10.61s/it]                                                         64%|   | 4166/6500 [12:35:53<6:52:32, 10.61s/it] 64%|   | 4167/6500 [12:36:03<6:49:40, 10.54s/it]                                                         64%|   | 4167/6500 [12:36:03<6:49:40, 10.54s/it] 64%|   | 4168/6500 [12:36:13<6:47:32, 10.49s/it]                                                         64%|   | 4168/6500 [12:36:13<6:47:32, 10.49s/it] 64%|   | 4169/6500 [12:36:24<6:46:07, 10.45s/it]                                                         64%|   | 4169/6500 [12:36:24<6:46:07, 10.45s/it] 64%|   | 4170/6500 [12:36:34<6:49:25, 10.54s/it]                                                         64%|   | 4170/6500 [12:36:34<6:49:25, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8636817336082458, 'eval_runtime': 3.9585, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.64}
                                                         64%|   | 4170/6500 [12:36:38<6:49:25, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4170I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4170

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3225, 'learning_rate': 2.8486446192954107e-05, 'epoch': 0.64}
{'loss': 0.3579, 'learning_rate': 2.8464627264325426e-05, 'epoch': 0.64}
{'loss': 0.3315, 'learning_rate': 2.8442813369459187e-05, 'epoch': 0.64}
{'loss': 0.3495, 'learning_rate': 2.842100451345424e-05, 'epoch': 0.64}
{'loss': 0.3463, 'learning_rate': 2.8399200701408303e-05, 'epoch': 0.64}
 64%|   | 4171/6500 [12:36:49<7:39:28, 11.84s/it]                                                         64%|   | 4171/6500 [12:36:49<7:39:28, 11.84s/it] 64%|   | 4172/6500 [12:37:00<7:22:14, 11.40s/it]                                                         64%|   | 4172/6500 [12:37:00<7:22:14, 11.40s/it] 64%|   | 4173/6500 [12:37:10<7:10:09, 11.09s/it]                                                         64%|   | 4173/6500 [12:37:10<7:10:09, 11.09s/it] 64%|   | 4174/6500 [12:37:20<7:01:41, 10.88s/it]                                                         64%|   | 4174/6500 [12:37:20<7:01:41, 10.88s/it] 64%|   | 4175/6500 [12:37:31<6:55:39, 10.73s/it]                                                         64%|   | 4175/6500 [12:37:31<6:55:39, 10.73s/it] 64%|{'loss': 0.3414, 'learning_rate': 2.8377401938417858e-05, 'epoch': 0.64}
{'loss': 0.3498, 'learning_rate': 2.835560822957824e-05, 'epoch': 0.64}
{'loss': 0.3432, 'learning_rate': 2.8333819579983623e-05, 'epoch': 0.64}
{'loss': 0.3327, 'learning_rate': 2.8312035994726926e-05, 'epoch': 0.64}
{'loss': 0.3349, 'learning_rate': 2.8290257478899945e-05, 'epoch': 0.64}
   | 4176/6500 [12:37:41<6:51:22, 10.62s/it]                                                         64%|   | 4176/6500 [12:37:41<6:51:22, 10.62s/it] 64%|   | 4177/6500 [12:37:52<6:48:32, 10.55s/it]                                                         64%|   | 4177/6500 [12:37:52<6:48:32, 10.55s/it] 64%|   | 4178/6500 [12:38:02<6:46:22, 10.50s/it]                                                         64%|   | 4178/6500 [12:38:02<6:46:22, 10.50s/it] 64%|   | 4179/6500 [12:38:12<6:44:46, 10.46s/it]                                                         64%|   | 4179/6500 [12:38:12<6:44:46, 10.46s/it] 64%|   | 4180/6500 [12:38:23<6:43:42, 10.44s/it]                                                         64%|   | 4180/6500 [12:38:23<6:43:42, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8518005609512329, 'eval_runtime': 3.9616, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.515, 'epoch': 0.64}
                                                         64%|   | 4180/6500 [12:38:27<6:43:42, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4180
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.364, 'learning_rate': 2.8268484037593286e-05, 'epoch': 0.64}
{'loss': 0.3273, 'learning_rate': 2.824671567589635e-05, 'epoch': 0.64}
{'loss': 0.3354, 'learning_rate': 2.8224952398897363e-05, 'epoch': 0.64}
{'loss': 0.3171, 'learning_rate': 2.820319421168336e-05, 'epoch': 0.64}
{'loss': 0.359, 'learning_rate': 2.818144111934019e-05, 'epoch': 0.64}
 64%|   | 4181/6500 [12:38:38<7:34:38, 11.76s/it]                                                         64%|   | 4181/6500 [12:38:38<7:34:38, 11.76s/it] 64%|   | 4182/6500 [12:38:48<7:18:17, 11.35s/it]                                                         64%|   | 4182/6500 [12:38:48<7:18:17, 11.35s/it] 64%|   | 4183/6500 [12:38:58<7:06:46, 11.05s/it]                                                         64%|   | 4183/6500 [12:38:58<7:06:46, 11.05s/it] 64%|   | 4184/6500 [12:39:09<6:58:27, 10.84s/it]                                                         64%|   | 4184/6500 [12:39:09<6:58:27, 10.84s/it] 64%|   | 4185/6500 [12:39:19<6:52:49, 10.70s/it]                                                         64%|   | 4185/6500 [12:39:19<6:52:49, 10.70s/it] 64%|{'loss': 0.4015, 'learning_rate': 2.815969312695249e-05, 'epoch': 0.64}
{'loss': 0.3247, 'learning_rate': 2.8137950239603734e-05, 'epoch': 0.64}
{'loss': 0.3312, 'learning_rate': 2.8116212462376183e-05, 'epoch': 0.64}
{'loss': 0.3373, 'learning_rate': 2.8094479800350938e-05, 'epoch': 0.64}
{'loss': 0.851, 'learning_rate': 2.8072752258607828e-05, 'epoch': 0.64}
   | 4186/6500 [12:39:30<6:54:27, 10.75s/it]                                                         64%|   | 4186/6500 [12:39:30<6:54:27, 10.75s/it] 64%|   | 4187/6500 [12:39:40<6:49:53, 10.63s/it]                                                         64%|   | 4187/6500 [12:39:40<6:49:53, 10.63s/it] 64%|   | 4188/6500 [12:39:51<6:46:39, 10.55s/it]                                                         64%|   | 4188/6500 [12:39:51<6:46:39, 10.55s/it] 64%|   | 4189/6500 [12:40:01<6:44:16, 10.50s/it]                                                         64%|   | 4189/6500 [12:40:01<6:44:16, 10.50s/it] 64%|   | 4190/6500 [12:40:11<6:42:16, 10.45s/it]                                                         64%|   | 4190/6500 [12:40:11<6:42:16, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8577520847320557, 'eval_runtime': 3.9468, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.64}
                                                         64%|   | 4190/6500 [12:40:15<6:42:16, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4190/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4190/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3449, 'learning_rate': 2.8051029842225556e-05, 'epoch': 0.64}
{'loss': 0.3349, 'learning_rate': 2.8029312556281613e-05, 'epoch': 0.64}
{'loss': 0.3194, 'learning_rate': 2.8007600405852275e-05, 'epoch': 0.65}
{'loss': 0.328, 'learning_rate': 2.798589339601262e-05, 'epoch': 0.65}
{'loss': 0.3522, 'learning_rate': 2.7964191531836535e-05, 'epoch': 0.65}
 64%|   | 4191/6500 [12:40:26<7:33:03, 11.77s/it]                                                         64%|   | 4191/6500 [12:40:26<7:33:03, 11.77s/it] 64%|   | 4192/6500 [12:40:36<7:16:38, 11.35s/it]                                                         64%|   | 4192/6500 [12:40:37<7:16:38, 11.35s/it] 65%|   | 4193/6500 [12:40:47<7:04:47, 11.05s/it]                                                         65%|   | 4193/6500 [12:40:47<7:04:47, 11.05s/it] 65%|   | 4194/6500 [12:40:57<6:56:44, 10.84s/it]                                                         65%|   | 4194/6500 [12:40:57<6:56:44, 10.84s/it] 65%|   | 4195/6500 [12:41:08<6:50:53, 10.70s/it]                                                         65%|   | 4195/6500 [12:41:08<6:50:53, 10.70s/it] 65%|{'loss': 0.3183, 'learning_rate': 2.794249481839669e-05, 'epoch': 0.65}
{'loss': 0.3271, 'learning_rate': 2.7920803260764582e-05, 'epoch': 0.65}
{'loss': 0.3254, 'learning_rate': 2.789911686401049e-05, 'epoch': 0.65}
{'loss': 0.3309, 'learning_rate': 2.787743563320343e-05, 'epoch': 0.65}
{'loss': 0.3339, 'learning_rate': 2.785575957341129e-05, 'epoch': 0.65}
   | 4196/6500 [12:41:18<6:46:51, 10.60s/it]                                                         65%|   | 4196/6500 [12:41:18<6:46:51, 10.60s/it] 65%|   | 4197/6500 [12:41:28<6:43:52, 10.52s/it]                                                         65%|   | 4197/6500 [12:41:28<6:43:52, 10.52s/it] 65%|   | 4198/6500 [12:41:39<6:41:50, 10.47s/it]                                                         65%|   | 4198/6500 [12:41:39<6:41:50, 10.47s/it] 65%|   | 4199/6500 [12:41:49<6:40:14, 10.44s/it]                                                         65%|   | 4199/6500 [12:41:49<6:40:14, 10.44s/it] 65%|   | 4200/6500 [12:41:59<6:39:11, 10.41s/it]                                                         65%|   | 4200/6500 [12:41:59<6:39:11, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8639339804649353, 'eval_runtime': 3.9384, 'eval_samples_per_second': 5.84, 'eval_steps_per_second': 1.523, 'epoch': 0.65}
                                                         65%|   | 4200/6500 [12:42:03<6:39:11, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4200I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4200/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3287, 'learning_rate': 2.783408868970071e-05, 'epoch': 0.65}
{'loss': 0.3408, 'learning_rate': 2.7812422987137128e-05, 'epoch': 0.65}
{'loss': 0.3356, 'learning_rate': 2.779076247078477e-05, 'epoch': 0.65}
{'loss': 0.3446, 'learning_rate': 2.7769107145706645e-05, 'epoch': 0.65}
{'loss': 0.332, 'learning_rate': 2.7747457016964563e-05, 'epoch': 0.65}
 65%|   | 4201/6500 [12:42:14<7:29:37, 11.73s/it]                                                         65%|   | 4201/6500 [12:42:14<7:29:37, 11.73s/it] 65%|   | 4202/6500 [12:42:25<7:17:50, 11.43s/it]                                                         65%|   | 4202/6500 [12:42:25<7:17:50, 11.43s/it] 65%|   | 4203/6500 [12:42:35<7:05:32, 11.12s/it]                                                         65%|   | 4203/6500 [12:42:35<7:05:32, 11.12s/it] 65%|   | 4204/6500 [12:42:46<6:56:39, 10.89s/it]                                                         65%|   | 4204/6500 [12:42:46<6:56:39, 10.89s/it] 65%|   | 4205/6500 [12:42:56<6:50:26, 10.73s/it]                                                         65%|   | 4205/6500 [12:42:56<6:50:26, 10.73s/it] 65%|{'loss': 0.3256, 'learning_rate': 2.772581208961911e-05, 'epoch': 0.65}
{'loss': 0.3416, 'learning_rate': 2.7704172368729642e-05, 'epoch': 0.65}
{'loss': 0.3341, 'learning_rate': 2.7682537859354328e-05, 'epoch': 0.65}
{'loss': 0.3329, 'learning_rate': 2.7660908566550113e-05, 'epoch': 0.65}
{'loss': 0.331, 'learning_rate': 2.7639284495372682e-05, 'epoch': 0.65}
   | 4206/6500 [12:43:06<6:46:09, 10.62s/it]                                                         65%|   | 4206/6500 [12:43:06<6:46:09, 10.62s/it] 65%|   | 4207/6500 [12:43:17<6:43:01, 10.55s/it]                                                         65%|   | 4207/6500 [12:43:17<6:43:01, 10.55s/it] 65%|   | 4208/6500 [12:43:27<6:41:02, 10.50s/it]                                                         65%|   | 4208/6500 [12:43:27<6:41:02, 10.50s/it] 65%|   | 4209/6500 [12:43:37<6:39:19, 10.46s/it]                                                         65%|   | 4209/6500 [12:43:37<6:39:19, 10.46s/it] 65%|   | 4210/6500 [12:43:48<6:38:13, 10.43s/it]                                                         65%|   | 4210/6500 [12:43:48<6:38:13, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.855440616607666, 'eval_runtime': 3.9559, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.65}
                                                         65%|   | 4210/6500 [12:43:52<6:38:13, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3491, 'learning_rate': 2.7617665650876538e-05, 'epoch': 0.65}
{'loss': 0.3308, 'learning_rate': 2.759605203811497e-05, 'epoch': 0.65}
{'loss': 0.3277, 'learning_rate': 2.7574443662140016e-05, 'epoch': 0.65}
{'loss': 0.3106, 'learning_rate': 2.7552840528002498e-05, 'epoch': 0.65}
{'loss': 0.3643, 'learning_rate': 2.7531242640752035e-05, 'epoch': 0.65}
 65%|   | 4211/6500 [12:44:03<7:28:48, 11.76s/it]                                                         65%|   | 4211/6500 [12:44:03<7:28:48, 11.76s/it] 65%|   | 4212/6500 [12:44:13<7:12:40, 11.35s/it]                                                         65%|   | 4212/6500 [12:44:13<7:12:40, 11.35s/it] 65%|   | 4213/6500 [12:44:23<7:01:18, 11.05s/it]                                                         65%|   | 4213/6500 [12:44:23<7:01:18, 11.05s/it] 65%|   | 4214/6500 [12:44:34<6:53:14, 10.85s/it]                                                         65%|   | 4214/6500 [12:44:34<6:53:14, 10.85s/it] 65%|   | 4215/6500 [12:44:44<6:47:23, 10.70s/it]                                                         65%|   | 4215/6500 [12:44:44<6:47:23, 10.70s/it] 65%|{'loss': 0.3762, 'learning_rate': 2.7509650005436994e-05, 'epoch': 0.65}
{'loss': 0.318, 'learning_rate': 2.7488062627104517e-05, 'epoch': 0.65}
{'loss': 0.333, 'learning_rate': 2.7466480510800523e-05, 'epoch': 0.65}
{'loss': 0.4347, 'learning_rate': 2.744490366156971e-05, 'epoch': 0.65}
{'loss': 0.7618, 'learning_rate': 2.7423332084455544e-05, 'epoch': 0.65}
   | 4216/6500 [12:44:55<6:43:39, 10.60s/it]                                                         65%|   | 4216/6500 [12:44:55<6:43:39, 10.60s/it] 65%|   | 4217/6500 [12:45:05<6:40:50, 10.53s/it]                                                         65%|   | 4217/6500 [12:45:05<6:40:50, 10.53s/it] 65%|   | 4218/6500 [12:45:16<6:41:22, 10.55s/it]                                                         65%|   | 4218/6500 [12:45:16<6:41:22, 10.55s/it] 65%|   | 4219/6500 [12:45:26<6:39:07, 10.50s/it]                                                         65%|   | 4219/6500 [12:45:26<6:39:07, 10.50s/it] 65%|   | 4220/6500 [12:45:36<6:37:27, 10.46s/it]                                                         65%|   | 4220/6500 [12:45:36<6:37:27, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8573528528213501, 'eval_runtime': 4.1704, 'eval_samples_per_second': 5.515, 'eval_steps_per_second': 1.439, 'epoch': 0.65}
                                                         65%|   | 4220/6500 [12:45:40<6:37:27, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4220I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4220

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.353, 'learning_rate': 2.7401765784500215e-05, 'epoch': 0.65}
{'loss': 0.3444, 'learning_rate': 2.7380204766744737e-05, 'epoch': 0.65}
{'loss': 0.3128, 'learning_rate': 2.7358649036228866e-05, 'epoch': 0.65}
{'loss': 0.3428, 'learning_rate': 2.7337098597991116e-05, 'epoch': 0.65}
{'loss': 0.3411, 'learning_rate': 2.7315553457068778e-05, 'epoch': 0.65}
 65%|   | 4221/6500 [12:45:51<7:30:05, 11.85s/it]                                                         65%|   | 4221/6500 [12:45:51<7:30:05, 11.85s/it] 65%|   | 4222/6500 [12:46:02<7:13:00, 11.41s/it]                                                         65%|   | 4222/6500 [12:46:02<7:13:00, 11.41s/it] 65%|   | 4223/6500 [12:46:12<7:01:02, 11.09s/it]                                                         65%|   | 4223/6500 [12:46:12<7:01:02, 11.09s/it] 65%|   | 4224/6500 [12:46:22<6:52:42, 10.88s/it]                                                         65%|   | 4224/6500 [12:46:22<6:52:42, 10.88s/it] 65%|   | 4225/6500 [12:46:33<6:46:41, 10.73s/it]                                                         65%|   | 4225/6500 [12:46:33<6:46:41, 10.73s/it] 65%|{'loss': 0.3062, 'learning_rate': 2.7294013618497894e-05, 'epoch': 0.65}
{'loss': 0.3381, 'learning_rate': 2.7272479087313274e-05, 'epoch': 0.65}
{'loss': 0.3247, 'learning_rate': 2.725094986854848e-05, 'epoch': 0.65}
{'loss': 0.3285, 'learning_rate': 2.7229425967235846e-05, 'epoch': 0.65}
{'loss': 0.3311, 'learning_rate': 2.720790738840644e-05, 'epoch': 0.65}
   | 4226/6500 [12:46:43<6:42:31, 10.62s/it]                                                         65%|   | 4226/6500 [12:46:43<6:42:31, 10.62s/it] 65%|   | 4227/6500 [12:46:54<6:39:25, 10.54s/it]                                                         65%|   | 4227/6500 [12:46:54<6:39:25, 10.54s/it] 65%|   | 4228/6500 [12:47:04<6:37:13, 10.49s/it]                                                         65%|   | 4228/6500 [12:47:04<6:37:13, 10.49s/it] 65%|   | 4229/6500 [12:47:14<6:35:42, 10.45s/it]                                                         65%|   | 4229/6500 [12:47:14<6:35:42, 10.45s/it] 65%|   | 4230/6500 [12:47:25<6:34:35, 10.43s/it]                                                         65%|   | 4230/6500 [12:47:25<6:34:35, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8622186779975891, 'eval_runtime': 3.951, 'eval_samples_per_second': 5.821, 'eval_steps_per_second': 1.519, 'epoch': 0.65}
                                                         65%|   | 4230/6500 [12:47:29<6:34:35, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3331, 'learning_rate': 2.71863941370901e-05, 'epoch': 0.65}
{'loss': 0.3271, 'learning_rate': 2.7164886218315444e-05, 'epoch': 0.65}
{'loss': 0.3354, 'learning_rate': 2.7143383637109772e-05, 'epoch': 0.65}
{'loss': 0.3581, 'learning_rate': 2.7121886398499207e-05, 'epoch': 0.65}
{'loss': 0.3379, 'learning_rate': 2.71003945075086e-05, 'epoch': 0.65}
 65%|   | 4231/6500 [12:47:40<7:26:34, 11.81s/it]                                                         65%|   | 4231/6500 [12:47:40<7:26:34, 11.81s/it] 65%|   | 4232/6500 [12:47:50<7:10:02, 11.38s/it]                                                         65%|   | 4232/6500 [12:47:50<7:10:02, 11.38s/it] 65%|   | 4233/6500 [12:48:00<6:58:17, 11.07s/it]                                                         65%|   | 4233/6500 [12:48:00<6:58:17, 11.07s/it] 65%|   | 4234/6500 [12:48:11<6:49:50, 10.85s/it]                                                         65%|   | 4234/6500 [12:48:11<6:49:50, 10.85s/it] 65%|   | 4235/6500 [12:48:22<6:48:31, 10.82s/it]                                                         65%|   | 4235/6500 [12:48:22<6:48:31, 10.82s/it] 65%|{'loss': 0.3491, 'learning_rate': 2.707890796916153e-05, 'epoch': 0.65}
{'loss': 0.3416, 'learning_rate': 2.7057426788480372e-05, 'epoch': 0.65}
{'loss': 0.3306, 'learning_rate': 2.7035950970486207e-05, 'epoch': 0.65}
{'loss': 0.3386, 'learning_rate': 2.701448052019888e-05, 'epoch': 0.65}
{'loss': 0.3359, 'learning_rate': 2.699301544263697e-05, 'epoch': 0.65}
   | 4236/6500 [12:48:32<6:43:11, 10.69s/it]                                                         65%|   | 4236/6500 [12:48:32<6:43:11, 10.69s/it] 65%|   | 4237/6500 [12:48:42<6:39:26, 10.59s/it]                                                         65%|   | 4237/6500 [12:48:42<6:39:26, 10.59s/it] 65%|   | 4238/6500 [12:48:53<6:36:44, 10.52s/it]                                                         65%|   | 4238/6500 [12:48:53<6:36:44, 10.52s/it] 65%|   | 4239/6500 [12:49:03<6:34:45, 10.48s/it]                                                         65%|   | 4239/6500 [12:49:03<6:34:45, 10.48s/it] 65%|   | 4240/6500 [12:49:13<6:33:28, 10.45s/it]                                                         65%|   | 4240/6500 [12:49:13<6:33:28, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8530814051628113, 'eval_runtime': 3.9304, 'eval_samples_per_second': 5.852, 'eval_steps_per_second': 1.527, 'epoch': 0.65}
                                                         65%|   | 4240/6500 [12:49:17<6:33:28, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3361, 'learning_rate': 2.6971555742817823e-05, 'epoch': 0.65}
{'loss': 0.3306, 'learning_rate': 2.6950101425757507e-05, 'epoch': 0.65}
{'loss': 0.3161, 'learning_rate': 2.6928652496470853e-05, 'epoch': 0.65}
{'loss': 0.3346, 'learning_rate': 2.690720895997138e-05, 'epoch': 0.65}
{'loss': 0.4027, 'learning_rate': 2.688577082127141e-05, 'epoch': 0.65}
 65%|   | 4241/6500 [12:49:28<7:22:19, 11.75s/it]                                                         65%|   | 4241/6500 [12:49:28<7:22:19, 11.75s/it] 65%|   | 4242/6500 [12:49:39<7:06:39, 11.34s/it]                                                         65%|   | 4242/6500 [12:49:39<7:06:39, 11.34s/it] 65%|   | 4243/6500 [12:49:49<6:55:19, 11.04s/it]                                                         65%|   | 4243/6500 [12:49:49<6:55:19, 11.04s/it] 65%|   | 4244/6500 [12:49:59<6:47:19, 10.83s/it]                                                         65%|   | 4244/6500 [12:49:59<6:47:19, 10.83s/it] 65%|   | 4245/6500 [12:50:10<6:41:59, 10.70s/it]                                                         65%|   | 4245/6500 [12:50:10<6:41:59, 10.70s/it] 65%|{'loss': 0.342, 'learning_rate': 2.6864338085381975e-05, 'epoch': 0.65}
{'loss': 0.3181, 'learning_rate': 2.6842910757312843e-05, 'epoch': 0.65}
{'loss': 0.3483, 'learning_rate': 2.6821488842072524e-05, 'epoch': 0.65}
{'loss': 0.861, 'learning_rate': 2.6800072344668258e-05, 'epoch': 0.65}
{'loss': 0.3486, 'learning_rate': 2.6778661270106025e-05, 'epoch': 0.65}
   | 4246/6500 [12:50:20<6:38:01, 10.60s/it]                                                         65%|   | 4246/6500 [12:50:20<6:38:01, 10.60s/it] 65%|   | 4247/6500 [12:50:30<6:35:15, 10.53s/it]                                                         65%|   | 4247/6500 [12:50:30<6:35:15, 10.53s/it] 65%|   | 4248/6500 [12:50:41<6:33:18, 10.48s/it]                                                         65%|   | 4248/6500 [12:50:41<6:33:18, 10.48s/it] 65%|   | 4249/6500 [12:50:51<6:31:40, 10.44s/it]                                                         65%|   | 4249/6500 [12:50:51<6:31:40, 10.44s/it] 65%|   | 4250/6500 [12:51:01<6:30:47, 10.42s/it]                                                         65%|   | 4250/6500 [12:51:01<6:30:47, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8579965829849243, 'eval_runtime': 3.9336, 'eval_samples_per_second': 5.847, 'eval_steps_per_second': 1.525, 'epoch': 0.65}
                                                         65%|   | 4250/6500 [12:51:05<6:30:47, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3264, 'learning_rate': 2.6757255623390532e-05, 'epoch': 0.65}
{'loss': 0.3432, 'learning_rate': 2.6735855409525224e-05, 'epoch': 0.65}
{'loss': 0.3047, 'learning_rate': 2.6714460633512262e-05, 'epoch': 0.65}
{'loss': 0.35, 'learning_rate': 2.6693071300352568e-05, 'epoch': 0.65}
{'loss': 0.3142, 'learning_rate': 2.6671687415045733e-05, 'epoch': 0.65}
 65%|   | 4251/6500 [12:51:17<7:26:38, 11.92s/it]                                                         65%|   | 4251/6500 [12:51:17<7:26:38, 11.92s/it] 65%|   | 4252/6500 [12:51:27<7:08:57, 11.45s/it]                                                         65%|   | 4252/6500 [12:51:27<7:08:57, 11.45s/it] 65%|   | 4253/6500 [12:51:38<6:56:31, 11.12s/it]                                                         65%|   | 4253/6500 [12:51:38<6:56:31, 11.12s/it] 65%|   | 4254/6500 [12:51:48<6:47:48, 10.89s/it]                                                         65%|   | 4254/6500 [12:51:48<6:47:48, 10.89s/it] 65%|   | 4255/6500 [12:51:58<6:41:26, 10.73s/it]                                                         65%|   | 4255/6500 [12:51:58<6:41:26, 10.73s/it] 65%|{'loss': 0.3124, 'learning_rate': 2.665030898259012e-05, 'epoch': 0.65}
{'loss': 0.3232, 'learning_rate': 2.6628936007982815e-05, 'epoch': 0.65}
{'loss': 0.3251, 'learning_rate': 2.660756849621962e-05, 'epoch': 0.66}
{'loss': 0.3311, 'learning_rate': 2.6586206452295058e-05, 'epoch': 0.66}
{'loss': 0.3193, 'learning_rate': 2.6564849881202393e-05, 'epoch': 0.66}
   | 4256/6500 [12:52:09<6:37:24, 10.63s/it]                                                         65%|   | 4256/6500 [12:52:09<6:37:24, 10.63s/it] 65%|   | 4257/6500 [12:52:19<6:34:04, 10.54s/it]                                                         65%|   | 4257/6500 [12:52:19<6:34:04, 10.54s/it] 66%|   | 4258/6500 [12:52:29<6:31:46, 10.48s/it]                                                         66%|   | 4258/6500 [12:52:29<6:31:46, 10.48s/it] 66%|   | 4259/6500 [12:52:40<6:30:15, 10.45s/it]                                                         66%|   | 4259/6500 [12:52:40<6:30:15, 10.45s/it] 66%|   | 4260/6500 [12:52:50<6:29:12, 10.43s/it]                                                         66%|   | 4260/6500 [12:52:50<6:29:12, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8627527356147766, 'eval_runtime': 3.94, 'eval_samples_per_second': 5.838, 'eval_steps_per_second': 1.523, 'epoch': 0.66}
                                                         66%|   | 4260/6500 [12:52:54<6:29:12, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4260
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3431, 'learning_rate': 2.6543498787933563e-05, 'epoch': 0.66}
{'loss': 0.3227, 'learning_rate': 2.652215317747928e-05, 'epoch': 0.66}
{'loss': 0.3439, 'learning_rate': 2.650081305482895e-05, 'epoch': 0.66}
{'loss': 0.332, 'learning_rate': 2.6479478424970683e-05, 'epoch': 0.66}
{'loss': 0.3194, 'learning_rate': 2.6458149292891353e-05, 'epoch': 0.66}
 66%|   | 4261/6500 [12:53:05<7:18:21, 11.75s/it]                                                         66%|   | 4261/6500 [12:53:05<7:18:21, 11.75s/it] 66%|   | 4262/6500 [12:53:15<7:02:34, 11.33s/it]                                                         66%|   | 4262/6500 [12:53:15<7:02:34, 11.33s/it] 66%|   | 4263/6500 [12:53:26<6:51:22, 11.03s/it]                                                         66%|   | 4263/6500 [12:53:26<6:51:22, 11.03s/it] 66%|   | 4264/6500 [12:53:36<6:43:30, 10.83s/it]                                                         66%|   | 4264/6500 [12:53:36<6:43:30, 10.83s/it] 66%|   | 4265/6500 [12:53:46<6:38:04, 10.69s/it]                                                         66%|   | 4265/6500 [12:53:46<6:38:04, 10.69s/it] 66%|{'loss': 0.3382, 'learning_rate': 2.6436825663576466e-05, 'epoch': 0.66}
{'loss': 0.3308, 'learning_rate': 2.6415507542010315e-05, 'epoch': 0.66}
{'loss': 0.3313, 'learning_rate': 2.6394194933175875e-05, 'epoch': 0.66}
{'loss': 0.3337, 'learning_rate': 2.637288784205485e-05, 'epoch': 0.66}
{'loss': 0.3554, 'learning_rate': 2.6351586273627636e-05, 'epoch': 0.66}
   | 4266/6500 [12:53:57<6:34:06, 10.58s/it]                                                         66%|   | 4266/6500 [12:53:57<6:34:06, 10.58s/it] 66%|   | 4267/6500 [12:54:07<6:34:19, 10.60s/it]                                                         66%|   | 4267/6500 [12:54:07<6:34:19, 10.60s/it] 66%|   | 4268/6500 [12:54:18<6:31:45, 10.53s/it]                                                         66%|   | 4268/6500 [12:54:18<6:31:45, 10.53s/it] 66%|   | 4269/6500 [12:54:28<6:29:56, 10.49s/it]                                                         66%|   | 4269/6500 [12:54:28<6:29:56, 10.49s/it] 66%|   | 4270/6500 [12:54:38<6:28:34, 10.45s/it]                                                         66%|   | 4270/6500 [12:54:38<6:28:34, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8553134202957153, 'eval_runtime': 3.9488, 'eval_samples_per_second': 5.825, 'eval_steps_per_second': 1.519, 'epoch': 0.66}
                                                         66%|   | 4270/6500 [12:54:42<6:28:34, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4270
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3108, 'learning_rate': 2.6330290232873345e-05, 'epoch': 0.66}
{'loss': 0.3326, 'learning_rate': 2.6308999724769778e-05, 'epoch': 0.66}
{'loss': 0.3139, 'learning_rate': 2.6287714754293503e-05, 'epoch': 0.66}
{'loss': 0.353, 'learning_rate': 2.6266435326419747e-05, 'epoch': 0.66}
{'loss': 0.3853, 'learning_rate': 2.624516144612241e-05, 'epoch': 0.66}
 66%|   | 4271/6500 [12:54:53<7:18:16, 11.80s/it]                                                         66%|   | 4271/6500 [12:54:53<7:18:16, 11.80s/it] 66%|   | 4272/6500 [12:55:04<7:02:40, 11.38s/it]                                                         66%|   | 4272/6500 [12:55:04<7:02:40, 11.38s/it] 66%|   | 4273/6500 [12:55:14<6:51:26, 11.08s/it]                                                         66%|   | 4273/6500 [12:55:14<6:51:26, 11.08s/it] 66%|   | 4274/6500 [12:55:25<6:43:32, 10.88s/it]                                                         66%|   | 4274/6500 [12:55:25<6:43:32, 10.88s/it] 66%|   | 4275/6500 [12:55:35<6:37:58, 10.73s/it]                                                         66%|   | 4275/6500 [12:55:35<6:37:58, 10.73s/it] 66%|{'loss': 0.3252, 'learning_rate': 2.6223893118374154e-05, 'epoch': 0.66}
{'loss': 0.3384, 'learning_rate': 2.6202630348146324e-05, 'epoch': 0.66}
{'loss': 0.3146, 'learning_rate': 2.618137314040896e-05, 'epoch': 0.66}
{'loss': 0.8502, 'learning_rate': 2.61601215001308e-05, 'epoch': 0.66}
{'loss': 0.3446, 'learning_rate': 2.6138875432279297e-05, 'epoch': 0.66}
   | 4276/6500 [12:55:45<6:34:04, 10.63s/it]                                                         66%|   | 4276/6500 [12:55:45<6:34:04, 10.63s/it] 66%|   | 4277/6500 [12:55:56<6:31:57, 10.58s/it]                                                         66%|   | 4277/6500 [12:55:56<6:31:57, 10.58s/it] 66%|   | 4278/6500 [12:56:06<6:29:45, 10.52s/it]                                                         66%|   | 4278/6500 [12:56:06<6:29:45, 10.52s/it] 66%|   | 4279/6500 [12:56:17<6:28:14, 10.49s/it]                                                         66%|   | 4279/6500 [12:56:17<6:28:14, 10.49s/it] 66%|   | 4280/6500 [12:56:27<6:27:05, 10.46s/it]                                                         66%|   | 4280/6500 [12:56:27<6:27:05, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.861299991607666, 'eval_runtime': 3.9472, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.66}
                                                         66%|   | 4280/6500 [12:56:31<6:27:05, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4280
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4280/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4280/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.336, 'learning_rate': 2.6117634941820578e-05, 'epoch': 0.66}
{'loss': 0.3173, 'learning_rate': 2.6096400033719487e-05, 'epoch': 0.66}
{'loss': 0.3226, 'learning_rate': 2.607517071293955e-05, 'epoch': 0.66}
{'loss': 0.3435, 'learning_rate': 2.6053946984443e-05, 'epoch': 0.66}
{'loss': 0.314, 'learning_rate': 2.6032728853190757e-05, 'epoch': 0.66}
 66%|   | 4281/6500 [12:56:42<7:15:17, 11.77s/it]                                                         66%|   | 4281/6500 [12:56:42<7:15:17, 11.77s/it] 66%|   | 4282/6500 [12:56:52<7:00:19, 11.37s/it]                                                         66%|   | 4282/6500 [12:56:52<7:00:19, 11.37s/it] 66%|   | 4283/6500 [12:57:03<6:56:05, 11.26s/it]                                                         66%|   | 4283/6500 [12:57:03<6:56:05, 11.26s/it] 66%|   | 4284/6500 [12:57:14<6:46:16, 11.00s/it]                                                         66%|   | 4284/6500 [12:57:14<6:46:16, 11.00s/it] 66%|   | 4285/6500 [12:57:24<6:39:16, 10.82s/it]                                                         66%|   | 4285/6500 [12:57:24<6:39:16, 10.82s/it] 66%|{'loss': 0.3257, 'learning_rate': 2.601151632414241e-05, 'epoch': 0.66}
{'loss': 0.3211, 'learning_rate': 2.5990309402256264e-05, 'epoch': 0.66}
{'loss': 0.3265, 'learning_rate': 2.596910809248932e-05, 'epoch': 0.66}
{'loss': 0.3349, 'learning_rate': 2.5947912399797246e-05, 'epoch': 0.66}
{'loss': 0.3177, 'learning_rate': 2.5926722329134412e-05, 'epoch': 0.66}
   | 4286/6500 [12:57:34<6:34:15, 10.68s/it]                                                         66%|   | 4286/6500 [12:57:34<6:34:15, 10.68s/it] 66%|   | 4287/6500 [12:57:45<6:30:46, 10.59s/it]                                                         66%|   | 4287/6500 [12:57:45<6:30:46, 10.59s/it] 66%|   | 4288/6500 [12:57:55<6:28:46, 10.55s/it]                                                         66%|   | 4288/6500 [12:57:55<6:28:46, 10.55s/it] 66%|   | 4289/6500 [12:58:06<6:26:57, 10.50s/it]                                                         66%|   | 4289/6500 [12:58:06<6:26:57, 10.50s/it] 66%|   | 4290/6500 [12:58:16<6:25:40, 10.47s/it]                                                         66%|   | 4290/6500 [12:58:16<6:25:40, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8629361987113953, 'eval_runtime': 3.946, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.66}
                                                         66%|   | 4290/6500 [12:58:20<6:25:40, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4290
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.34, 'learning_rate': 2.5905537885453856e-05, 'epoch': 0.66}
{'loss': 0.3216, 'learning_rate': 2.588435907370733e-05, 'epoch': 0.66}
{'loss': 0.3559, 'learning_rate': 2.5863185898845243e-05, 'epoch': 0.66}
{'loss': 0.3198, 'learning_rate': 2.58420183658167e-05, 'epoch': 0.66}
{'loss': 0.3354, 'learning_rate': 2.5820856479569487e-05, 'epoch': 0.66}
 66%|   | 4291/6500 [12:58:31<7:13:32, 11.78s/it]                                                         66%|   | 4291/6500 [12:58:31<7:13:32, 11.78s/it] 66%|   | 4292/6500 [12:58:41<6:58:05, 11.36s/it]                                                         66%|   | 4292/6500 [12:58:41<6:58:05, 11.36s/it] 66%|   | 4293/6500 [12:58:52<6:47:08, 11.07s/it]                                                         66%|   | 4293/6500 [12:58:52<6:47:08, 11.07s/it] 66%|   | 4294/6500 [12:59:02<6:39:27, 10.86s/it]                                                         66%|   | 4294/6500 [12:59:02<6:39:27, 10.86s/it] 66%|   | 4295/6500 [12:59:12<6:33:57, 10.72s/it]                                                         66%|   | 4295/6500 [12:59:12<6:33:57, 10.72s/it] 66%|{'loss': 0.3532, 'learning_rate': 2.5799700245050074e-05, 'epoch': 0.66}
{'loss': 0.3243, 'learning_rate': 2.5778549667203568e-05, 'epoch': 0.66}
{'loss': 0.344, 'learning_rate': 2.5757404750973806e-05, 'epoch': 0.66}
{'loss': 0.3292, 'learning_rate': 2.573626550130329e-05, 'epoch': 0.66}
{'loss': 0.3363, 'learning_rate': 2.5715131923133184e-05, 'epoch': 0.66}
   | 4296/6500 [12:59:23<6:30:20, 10.63s/it]                                                         66%|   | 4296/6500 [12:59:23<6:30:20, 10.63s/it] 66%|   | 4297/6500 [12:59:33<6:27:36, 10.56s/it]                                                         66%|   | 4297/6500 [12:59:33<6:27:36, 10.56s/it] 66%|   | 4298/6500 [12:59:44<6:25:35, 10.51s/it]                                                         66%|   | 4298/6500 [12:59:44<6:25:35, 10.51s/it] 66%|   | 4299/6500 [12:59:54<6:26:54, 10.55s/it]                                                         66%|   | 4299/6500 [12:59:54<6:26:54, 10.55s/it] 66%|   | 4300/6500 [13:00:05<6:25:10, 10.50s/it]                                                         66%|   | 4300/6500 [13:00:05<6:25:10, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8568576574325562, 'eval_runtime': 3.9456, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.66}
                                                         66%|   | 4300/6500 [13:00:09<6:25:10, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3167, 'learning_rate': 2.569400402140334e-05, 'epoch': 0.66}
{'loss': 0.3162, 'learning_rate': 2.5672881801052273e-05, 'epoch': 0.66}
{'loss': 0.3107, 'learning_rate': 2.565176526701717e-05, 'epoch': 0.66}
{'loss': 0.4044, 'learning_rate': 2.5630654424233903e-05, 'epoch': 0.66}
{'loss': 0.3319, 'learning_rate': 2.560954927763699e-05, 'epoch': 0.66}
 66%|   | 4301/6500 [13:00:20<7:13:34, 11.83s/it]                                                         66%|   | 4301/6500 [13:00:20<7:13:34, 11.83s/it] 66%|   | 4302/6500 [13:00:30<6:57:34, 11.40s/it]                                                         66%|   | 4302/6500 [13:00:30<6:57:34, 11.40s/it] 66%|   | 4303/6500 [13:00:40<6:46:20, 11.10s/it]                                                         66%|   | 4303/6500 [13:00:40<6:46:20, 11.10s/it] 66%|   | 4304/6500 [13:00:51<6:38:15, 10.88s/it]                                                         66%|   | 4304/6500 [13:00:51<6:38:15, 10.88s/it] 66%|   | 4305/6500 [13:01:01<6:32:45, 10.74s/it]                                                         66%|   | 4305/6500 [13:01:01<6:32:45, 10.74s/it] 66%|{'loss': 0.3115, 'learning_rate': 2.5588449832159633e-05, 'epoch': 0.66}
{'loss': 0.3416, 'learning_rate': 2.556735609273373e-05, 'epoch': 0.66}
{'loss': 0.8495, 'learning_rate': 2.554626806428977e-05, 'epoch': 0.66}
{'loss': 0.3412, 'learning_rate': 2.5525185751756963e-05, 'epoch': 0.66}
{'loss': 0.3283, 'learning_rate': 2.5504109160063182e-05, 'epoch': 0.66}
   | 4306/6500 [13:01:12<6:28:48, 10.63s/it]                                                         66%|   | 4306/6500 [13:01:12<6:28:48, 10.63s/it] 66%|   | 4307/6500 [13:01:22<6:25:54, 10.56s/it]                                                         66%|   | 4307/6500 [13:01:22<6:25:54, 10.56s/it] 66%|   | 4308/6500 [13:01:32<6:23:43, 10.50s/it]                                                         66%|   | 4308/6500 [13:01:32<6:23:43, 10.50s/it] 66%|   | 4309/6500 [13:01:43<6:22:25, 10.47s/it]                                                         66%|   | 4309/6500 [13:01:43<6:22:25, 10.47s/it] 66%|   | 4310/6500 [13:01:53<6:21:21, 10.45s/it]                                                         66%|   | 4310/6500 [13:01:53<6:21:21, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8624433279037476, 'eval_runtime': 3.9664, 'eval_samples_per_second': 5.799, 'eval_steps_per_second': 1.513, 'epoch': 0.66}
                                                         66%|   | 4310/6500 [13:01:57<6:21:21, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3434, 'learning_rate': 2.5483038294134943e-05, 'epoch': 0.66}
{'loss': 0.3156, 'learning_rate': 2.5461973158897435e-05, 'epoch': 0.66}
{'loss': 0.3403, 'learning_rate': 2.5440913759274514e-05, 'epoch': 0.66}
{'loss': 0.3249, 'learning_rate': 2.5419860100188674e-05, 'epoch': 0.66}
{'loss': 0.2998, 'learning_rate': 2.5398812186561095e-05, 'epoch': 0.66}
 66%|   | 4311/6500 [13:02:08<7:09:41, 11.78s/it]                                                         66%|   | 4311/6500 [13:02:08<7:09:41, 11.78s/it] 66%|   | 4312/6500 [13:02:18<6:54:01, 11.35s/it]                                                         66%|   | 4312/6500 [13:02:18<6:54:01, 11.35s/it] 66%|   | 4313/6500 [13:02:29<6:42:57, 11.05s/it]                                                         66%|   | 4313/6500 [13:02:29<6:42:57, 11.05s/it] 66%|   | 4314/6500 [13:02:39<6:35:08, 10.85s/it]                                                         66%|   | 4314/6500 [13:02:39<6:35:08, 10.85s/it] 66%|   | 4315/6500 [13:02:50<6:36:07, 10.88s/it]                                                         66%|   | 4315/6500 [13:02:50<6:36:07, 10.88s/it] 66%|{'loss': 0.3378, 'learning_rate': 2.537777002331158e-05, 'epoch': 0.66}
{'loss': 0.3211, 'learning_rate': 2.535673361535862e-05, 'epoch': 0.66}
{'loss': 0.3409, 'learning_rate': 2.5335702967619347e-05, 'epoch': 0.66}
{'loss': 0.3351, 'learning_rate': 2.5314678085009558e-05, 'epoch': 0.66}
{'loss': 0.3398, 'learning_rate': 2.5293658972443663e-05, 'epoch': 0.66}
   | 4316/6500 [13:03:00<6:30:21, 10.72s/it]                                                         66%|   | 4316/6500 [13:03:00<6:30:21, 10.72s/it] 66%|   | 4317/6500 [13:03:11<6:26:11, 10.61s/it]                                                         66%|   | 4317/6500 [13:03:11<6:26:11, 10.61s/it] 66%|   | 4318/6500 [13:03:21<6:23:13, 10.54s/it]                                                         66%|   | 4318/6500 [13:03:21<6:23:13, 10.54s/it] 66%|   | 4319/6500 [13:03:31<6:21:05, 10.48s/it]                                                         66%|   | 4319/6500 [13:03:31<6:21:05, 10.48s/it] 66%|   | 4320/6500 [13:03:42<6:19:24, 10.44s/it]                                                         66%|   | 4320/6500 [13:03:42<6:19:24, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8623929023742676, 'eval_runtime': 3.9597, 'eval_samples_per_second': 5.809, 'eval_steps_per_second': 1.515, 'epoch': 0.66}
                                                         66%|   | 4320/6500 [13:03:46<6:19:24, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3248, 'learning_rate': 2.5272645634834764e-05, 'epoch': 0.66}
{'loss': 0.3476, 'learning_rate': 2.5251638077094602e-05, 'epoch': 0.66}
{'loss': 0.3444, 'learning_rate': 2.523063630413357e-05, 'epoch': 0.67}
{'loss': 0.3309, 'learning_rate': 2.5209640320860694e-05, 'epoch': 0.67}
{'loss': 0.3445, 'learning_rate': 2.5188650132183677e-05, 'epoch': 0.67}
 66%|   | 4321/6500 [13:03:57<7:06:40, 11.75s/it]                                                         66%|   | 4321/6500 [13:03:57<7:06:40, 11.75s/it] 66%|   | 4322/6500 [13:04:07<6:51:11, 11.33s/it]                                                         66%|   | 4322/6500 [13:04:07<6:51:11, 11.33s/it] 67%|   | 4323/6500 [13:04:17<6:40:06, 11.03s/it]                                                         67%|   | 4323/6500 [13:04:17<6:40:06, 11.03s/it] 67%|   | 4324/6500 [13:04:29<6:48:02, 11.25s/it]                                                         67%|   | 4324/6500 [13:04:29<6:48:02, 11.25s/it] 67%|   | 4325/6500 [13:04:39<6:38:46, 11.00s/it]                                                         67%|   | 4325/6500 [13:04:39<6:38:46, 11.00s/it] 67%|{'loss': 0.3385, 'learning_rate': 2.5167665743008828e-05, 'epoch': 0.67}
{'loss': 0.3353, 'learning_rate': 2.5146687158241132e-05, 'epoch': 0.67}
{'loss': 0.326, 'learning_rate': 2.5125714382784198e-05, 'epoch': 0.67}
{'loss': 0.3506, 'learning_rate': 2.5104747421540294e-05, 'epoch': 0.67}
{'loss': 0.3297, 'learning_rate': 2.5083786279410325e-05, 'epoch': 0.67}
   | 4326/6500 [13:04:50<6:31:22, 10.80s/it]                                                         67%|   | 4326/6500 [13:04:50<6:31:22, 10.80s/it] 67%|   | 4327/6500 [13:05:00<6:26:05, 10.66s/it]                                                         67%|   | 4327/6500 [13:05:00<6:26:05, 10.66s/it] 67%|   | 4328/6500 [13:05:10<6:22:44, 10.57s/it]                                                         67%|   | 4328/6500 [13:05:10<6:22:44, 10.57s/it] 67%|   | 4329/6500 [13:05:21<6:20:06, 10.51s/it]                                                         67%|   | 4329/6500 [13:05:21<6:20:06, 10.51s/it] 67%|   | 4330/6500 [13:05:31<6:18:08, 10.46s/it]                                                         67%|   | 4330/6500 [13:05:31<6:18:08, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8535552024841309, 'eval_runtime': 3.9593, 'eval_samples_per_second': 5.809, 'eval_steps_per_second': 1.515, 'epoch': 0.67}
                                                         67%|   | 4330/6500 [13:05:35<6:18:08, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4330the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4330

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3244, 'learning_rate': 2.5062830961293815e-05, 'epoch': 0.67}
{'loss': 0.3176, 'learning_rate': 2.5041881472088934e-05, 'epoch': 0.67}
{'loss': 0.3393, 'learning_rate': 2.502093781669252e-05, 'epoch': 0.67}
{'loss': 0.3883, 'learning_rate': 2.500000000000001e-05, 'epoch': 0.67}
{'loss': 0.3232, 'learning_rate': 2.49790680269055e-05, 'epoch': 0.67}
 67%|   | 4331/6500 [13:05:46<7:08:16, 11.85s/it]                                                         67%|   | 4331/6500 [13:05:46<7:08:16, 11.85s/it] 67%|   | 4332/6500 [13:05:57<6:51:53, 11.40s/it]                                                         67%|   | 4332/6500 [13:05:57<6:51:53, 11.40s/it] 67%|   | 4333/6500 [13:06:07<6:40:17, 11.08s/it]                                                         67%|   | 4333/6500 [13:06:07<6:40:17, 11.08s/it] 67%|   | 4334/6500 [13:06:17<6:32:08, 10.86s/it]                                                         67%|   | 4334/6500 [13:06:17<6:32:08, 10.86s/it] 67%|   | 4335/6500 [13:06:28<6:26:20, 10.71s/it]                                                         67%|   | 4335/6500 [13:06:28<6:26:20, 10.71s/it] 67%|{'loss': 0.3215, 'learning_rate': 2.495814190230171e-05, 'epoch': 0.67}
{'loss': 0.3378, 'learning_rate': 2.4937221631079993e-05, 'epoch': 0.67}
{'loss': 0.8519, 'learning_rate': 2.4916307218130337e-05, 'epoch': 0.67}
{'loss': 0.3437, 'learning_rate': 2.4895398668341352e-05, 'epoch': 0.67}
{'loss': 0.3284, 'learning_rate': 2.4874495986600294e-05, 'epoch': 0.67}
   | 4336/6500 [13:06:38<6:22:23, 10.60s/it]                                                         67%|   | 4336/6500 [13:06:38<6:22:23, 10.60s/it] 67%|   | 4337/6500 [13:06:48<6:19:26, 10.53s/it]                                                         67%|   | 4337/6500 [13:06:48<6:19:26, 10.53s/it] 67%|   | 4338/6500 [13:06:59<6:17:15, 10.47s/it]                                                         67%|   | 4338/6500 [13:06:59<6:17:15, 10.47s/it] 67%|   | 4339/6500 [13:07:09<6:15:55, 10.44s/it]                                                         67%|   | 4339/6500 [13:07:09<6:15:55, 10.44s/it] 67%|   | 4340/6500 [13:07:19<6:14:52, 10.41s/it]                                                         67%|   | 4340/6500 [13:07:19<6:14:52, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8630316257476807, 'eval_runtime': 4.1702, 'eval_samples_per_second': 5.515, 'eval_steps_per_second': 1.439, 'epoch': 0.67}
                                                         67%|   | 4340/6500 [13:07:24<6:14:52, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4340/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4340/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3223, 'learning_rate': 2.4853599177793052e-05, 'epoch': 0.67}
{'loss': 0.3149, 'learning_rate': 2.483270824680409e-05, 'epoch': 0.67}
{'loss': 0.3487, 'learning_rate': 2.4811823198516555e-05, 'epoch': 0.67}
{'loss': 0.3081, 'learning_rate': 2.4790944037812202e-05, 'epoch': 0.67}
{'loss': 0.3195, 'learning_rate': 2.4770070769571408e-05, 'epoch': 0.67}
 67%|   | 4341/6500 [13:07:35<7:05:34, 11.83s/it]                                                         67%|   | 4341/6500 [13:07:35<7:05:34, 11.83s/it] 67%|   | 4342/6500 [13:07:45<6:49:30, 11.39s/it]                                                         67%|   | 4342/6500 [13:07:45<6:49:30, 11.39s/it] 67%|   | 4343/6500 [13:07:55<6:38:19, 11.08s/it]                                                         67%|   | 4343/6500 [13:07:55<6:38:19, 11.08s/it] 67%|   | 4344/6500 [13:08:06<6:30:08, 10.86s/it]                                                         67%|   | 4344/6500 [13:08:06<6:30:08, 10.86s/it] 67%|   | 4345/6500 [13:08:16<6:24:25, 10.70s/it]                                                         67%|   | 4345/6500 [13:08:16<6:24:25, 10.70s/it] 67%|{'loss': 0.3149, 'learning_rate': 2.4749203398673172e-05, 'epoch': 0.67}
{'loss': 0.3244, 'learning_rate': 2.4728341929995092e-05, 'epoch': 0.67}
{'loss': 0.3264, 'learning_rate': 2.4707486368413445e-05, 'epoch': 0.67}
{'loss': 0.3151, 'learning_rate': 2.4686636718803086e-05, 'epoch': 0.67}
{'loss': 0.3542, 'learning_rate': 2.4665792986037507e-05, 'epoch': 0.67}
   | 4346/6500 [13:08:26<6:20:20, 10.59s/it]                                                         67%|   | 4346/6500 [13:08:26<6:20:20, 10.59s/it] 67%|   | 4347/6500 [13:08:37<6:17:28, 10.52s/it]                                                         67%|   | 4347/6500 [13:08:37<6:17:28, 10.52s/it] 67%|   | 4348/6500 [13:08:47<6:19:51, 10.59s/it]                                                         67%|   | 4348/6500 [13:08:47<6:19:51, 10.59s/it] 67%|   | 4349/6500 [13:08:58<6:17:07, 10.52s/it]                                                         67%|   | 4349/6500 [13:08:58<6:17:07, 10.52s/it] 67%|   | 4350/6500 [13:09:08<6:15:09, 10.47s/it]                                                         67%|   | 4350/6500 [13:09:08<6:15:09, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8659295439720154, 'eval_runtime': 4.4803, 'eval_samples_per_second': 5.134, 'eval_steps_per_second': 1.339, 'epoch': 0.67}
                                                         67%|   | 4350/6500 [13:09:13<6:15:09, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4350
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3318, 'learning_rate': 2.464495517498876e-05, 'epoch': 0.67}
{'loss': 0.3528, 'learning_rate': 2.4624123290527578e-05, 'epoch': 0.67}
{'loss': 0.3451, 'learning_rate': 2.4603297337523294e-05, 'epoch': 0.67}
{'loss': 0.3356, 'learning_rate': 2.458247732084384e-05, 'epoch': 0.67}
{'loss': 0.343, 'learning_rate': 2.456166324535577e-05, 'epoch': 0.67}
 67%|   | 4351/6500 [13:09:23<7:07:28, 11.94s/it]                                                         67%|   | 4351/6500 [13:09:23<7:07:28, 11.94s/it] 67%|   | 4352/6500 [13:09:34<6:50:13, 11.46s/it]                                                         67%|   | 4352/6500 [13:09:34<6:50:13, 11.46s/it] 67%|   | 4353/6500 [13:09:44<6:38:01, 11.12s/it]                                                         67%|   | 4353/6500 [13:09:44<6:38:01, 11.12s/it] 67%|   | 4354/6500 [13:09:54<6:29:30, 10.89s/it]                                                         67%|   | 4354/6500 [13:09:54<6:29:30, 10.89s/it] 67%|   | 4355/6500 [13:10:05<6:23:27, 10.73s/it]                                                         67%|   | 4355/6500 [13:10:05<6:23:27, 10.73s/it] 67%|{'loss': 0.3338, 'learning_rate': 2.454085511592425e-05, 'epoch': 0.67}
{'loss': 0.3303, 'learning_rate': 2.4520052937413058e-05, 'epoch': 0.67}
{'loss': 0.3395, 'learning_rate': 2.4499256714684565e-05, 'epoch': 0.67}
{'loss': 0.3551, 'learning_rate': 2.447846645259977e-05, 'epoch': 0.67}
{'loss': 0.3106, 'learning_rate': 2.4457682156018263e-05, 'epoch': 0.67}
   | 4356/6500 [13:10:15<6:19:16, 10.61s/it]                                                         67%|   | 4356/6500 [13:10:15<6:19:16, 10.61s/it] 67%|   | 4357/6500 [13:10:26<6:16:06, 10.53s/it]                                                         67%|   | 4357/6500 [13:10:26<6:16:06, 10.53s/it] 67%|   | 4358/6500 [13:10:36<6:14:00, 10.48s/it]                                                         67%|   | 4358/6500 [13:10:36<6:14:00, 10.48s/it] 67%|   | 4359/6500 [13:10:46<6:12:17, 10.43s/it]                                                         67%|   | 4359/6500 [13:10:46<6:12:17, 10.43s/it] 67%|   | 4360/6500 [13:10:57<6:11:07, 10.41s/it]                                                         67%|   | 4360/6500 [13:10:57<6:11:07, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.854509174823761, 'eval_runtime': 3.9407, 'eval_samples_per_second': 5.836, 'eval_steps_per_second': 1.523, 'epoch': 0.67}
                                                         67%|   | 4360/6500 [13:11:00<6:11:07, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3269, 'learning_rate': 2.4436903829798274e-05, 'epoch': 0.67}
{'loss': 0.3178, 'learning_rate': 2.441613147879657e-05, 'epoch': 0.67}
{'loss': 0.3675, 'learning_rate': 2.4395365107868583e-05, 'epoch': 0.67}
{'loss': 0.3795, 'learning_rate': 2.437460472186832e-05, 'epoch': 0.67}
{'loss': 0.3242, 'learning_rate': 2.4353850325648404e-05, 'epoch': 0.67}
 67%|   | 4361/6500 [13:11:11<6:57:57, 11.72s/it]                                                         67%|   | 4361/6500 [13:11:11<6:57:57, 11.72s/it] 67%|   | 4362/6500 [13:11:22<6:43:13, 11.32s/it]                                                         67%|   | 4362/6500 [13:11:22<6:43:13, 11.32s/it] 67%|   | 4363/6500 [13:11:32<6:32:48, 11.03s/it]                                                         67%|   | 4363/6500 [13:11:32<6:32:48, 11.03s/it] 67%|   | 4364/6500 [13:11:43<6:27:49, 10.89s/it]                                                         67%|   | 4364/6500 [13:11:43<6:27:49, 10.89s/it] 67%|   | 4365/6500 [13:11:53<6:21:37, 10.72s/it]                                                         67%|   | 4365/6500 [13:11:53<6:21:37, 10.72s/it] 67%|{'loss': 0.3406, 'learning_rate': 2.4333101924060035e-05, 'epoch': 0.67}
{'loss': 0.3169, 'learning_rate': 2.4312359521953045e-05, 'epoch': 0.67}
{'loss': 0.8502, 'learning_rate': 2.4291623124175822e-05, 'epoch': 0.67}
{'loss': 0.3284, 'learning_rate': 2.427089273557539e-05, 'epoch': 0.67}
{'loss': 0.344, 'learning_rate': 2.4250168360997344e-05, 'epoch': 0.67}
   | 4366/6500 [13:12:03<6:17:18, 10.61s/it]                                                         67%|   | 4366/6500 [13:12:03<6:17:18, 10.61s/it] 67%|   | 4367/6500 [13:12:14<6:14:23, 10.53s/it]                                                         67%|   | 4367/6500 [13:12:14<6:14:23, 10.53s/it] 67%|   | 4368/6500 [13:12:24<6:12:05, 10.47s/it]                                                         67%|   | 4368/6500 [13:12:24<6:12:05, 10.47s/it] 67%|   | 4369/6500 [13:12:34<6:10:32, 10.43s/it]                                                         67%|   | 4369/6500 [13:12:34<6:10:32, 10.43s/it] 67%|   | 4370/6500 [13:12:45<6:09:41, 10.41s/it]                                                         67%|   | 4370/6500 [13:12:45<6:09:41, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8623281717300415, 'eval_runtime': 3.9384, 'eval_samples_per_second': 5.84, 'eval_steps_per_second': 1.523, 'epoch': 0.67}
                                                         67%|   | 4370/6500 [13:12:49<6:09:41, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3134, 'learning_rate': 2.422945000528588e-05, 'epoch': 0.67}
{'loss': 0.3426, 'learning_rate': 2.4208737673283815e-05, 'epoch': 0.67}
{'loss': 0.3306, 'learning_rate': 2.4188031369832482e-05, 'epoch': 0.67}
{'loss': 0.3044, 'learning_rate': 2.416733109977188e-05, 'epoch': 0.67}
{'loss': 0.3246, 'learning_rate': 2.4146636867940565e-05, 'epoch': 0.67}
 67%|   | 4371/6500 [13:12:59<6:55:39, 11.71s/it]                                                         67%|   | 4371/6500 [13:12:59<6:55:39, 11.71s/it] 67%|   | 4372/6500 [13:13:10<6:40:52, 11.30s/it]                                                         67%|   | 4372/6500 [13:13:10<6:40:52, 11.30s/it] 67%|   | 4373/6500 [13:13:20<6:30:30, 11.02s/it]                                                         67%|   | 4373/6500 [13:13:20<6:30:30, 11.02s/it] 67%|   | 4374/6500 [13:13:30<6:23:08, 10.81s/it]                                                         67%|   | 4374/6500 [13:13:30<6:23:08, 10.81s/it] 67%|   | 4375/6500 [13:13:41<6:18:14, 10.68s/it]                                                         67%|   | 4375/6500 [13:13:41<6:18:14, 10.68s/it] 67%|{'loss': 0.3148, 'learning_rate': 2.4125948679175686e-05, 'epoch': 0.67}
{'loss': 0.3327, 'learning_rate': 2.4105266538312994e-05, 'epoch': 0.67}
{'loss': 0.3332, 'learning_rate': 2.4084590450186806e-05, 'epoch': 0.67}
{'loss': 0.3267, 'learning_rate': 2.4063920419630025e-05, 'epoch': 0.67}
{'loss': 0.3257, 'learning_rate': 2.4043256451474162e-05, 'epoch': 0.67}
   | 4376/6500 [13:13:51<6:14:38, 10.58s/it]                                                         67%|   | 4376/6500 [13:13:51<6:14:38, 10.58s/it] 67%|   | 4377/6500 [13:14:02<6:11:58, 10.51s/it]                                                         67%|   | 4377/6500 [13:14:02<6:11:58, 10.51s/it] 67%|   | 4378/6500 [13:14:12<6:09:59, 10.46s/it]                                                         67%|   | 4378/6500 [13:14:12<6:09:59, 10.46s/it] 67%|   | 4379/6500 [13:14:22<6:08:33, 10.43s/it]                                                         67%|   | 4379/6500 [13:14:22<6:08:33, 10.43s/it] 67%|   | 4380/6500 [13:14:33<6:11:51, 10.52s/it]                                                         67%|   | 4380/6500 [13:14:33<6:11:51, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8649044632911682, 'eval_runtime': 4.419, 'eval_samples_per_second': 5.205, 'eval_steps_per_second': 1.358, 'epoch': 0.67}
                                                         67%|   | 4380/6500 [13:14:37<6:11:51, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4380
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4380

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3282, 'learning_rate': 2.402259855054928e-05, 'epoch': 0.67}
{'loss': 0.3485, 'learning_rate': 2.400194672168404e-05, 'epoch': 0.67}
{'loss': 0.3144, 'learning_rate': 2.39813009697057e-05, 'epoch': 0.67}
{'loss': 0.338, 'learning_rate': 2.3960661299440047e-05, 'epoch': 0.67}
{'loss': 0.3408, 'learning_rate': 2.3940027715711495e-05, 'epoch': 0.67}
 67%|   | 4381/6500 [13:14:48<7:02:57, 11.98s/it]                                                         67%|   | 4381/6500 [13:14:48<7:02:57, 11.98s/it] 67%|   | 4382/6500 [13:14:59<6:45:55, 11.50s/it]                                                         67%|   | 4382/6500 [13:14:59<6:45:55, 11.50s/it] 67%|   | 4383/6500 [13:15:09<6:33:39, 11.16s/it]                                                         67%|   | 4383/6500 [13:15:09<6:33:39, 11.16s/it] 67%|   | 4384/6500 [13:15:19<6:24:51, 10.91s/it]                                                         67%|   | 4384/6500 [13:15:19<6:24:51, 10.91s/it] 67%|   | 4385/6500 [13:15:30<6:18:52, 10.75s/it]                                                         67%|   | 4385/6500 [13:15:30<6:18:52, 10.75s/it] 67%|{'loss': 0.313, 'learning_rate': 2.3919400223343015e-05, 'epoch': 0.67}
{'loss': 0.3297, 'learning_rate': 2.3898778827156156e-05, 'epoch': 0.67}
{'loss': 0.3428, 'learning_rate': 2.3878163531971053e-05, 'epoch': 0.68}
{'loss': 0.3349, 'learning_rate': 2.3857554342606397e-05, 'epoch': 0.68}
{'loss': 0.3229, 'learning_rate': 2.383695126387947e-05, 'epoch': 0.68}
   | 4386/6500 [13:15:40<6:14:35, 10.63s/it]                                                         67%|   | 4386/6500 [13:15:40<6:14:35, 10.63s/it] 67%|   | 4387/6500 [13:15:51<6:11:34, 10.55s/it]                                                         67%|   | 4387/6500 [13:15:51<6:11:34, 10.55s/it] 68%|   | 4388/6500 [13:16:01<6:09:31, 10.50s/it]                                                         68%|   | 4388/6500 [13:16:01<6:09:31, 10.50s/it] 68%|   | 4389/6500 [13:16:11<6:07:55, 10.46s/it]                                                         68%|   | 4389/6500 [13:16:11<6:07:55, 10.46s/it] 68%|   | 4390/6500 [13:16:22<6:13:07, 10.61s/it]                                                         68%|   | 4390/6500 [13:16:22<6:13:07, 10.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8582383990287781, 'eval_runtime': 3.9934, 'eval_samples_per_second': 5.759, 'eval_steps_per_second': 1.502, 'epoch': 0.68}
                                                         68%|   | 4390/6500 [13:16:26<6:13:07, 10.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4390I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4390

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4390/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3189, 'learning_rate': 2.381635430060611e-05, 'epoch': 0.68}
{'loss': 0.3147, 'learning_rate': 2.379576345760073e-05, 'epoch': 0.68}
{'loss': 0.4063, 'learning_rate': 2.3775178739676318e-05, 'epoch': 0.68}
{'loss': 0.324, 'learning_rate': 2.3754600151644445e-05, 'epoch': 0.68}
{'loss': 0.3113, 'learning_rate': 2.37340276983152e-05, 'epoch': 0.68}
 68%|   | 4391/6500 [13:16:37<6:58:31, 11.91s/it]                                                         68%|   | 4391/6500 [13:16:37<6:58:31, 11.91s/it] 68%|   | 4392/6500 [13:16:48<6:42:10, 11.45s/it]                                                         68%|   | 4392/6500 [13:16:48<6:42:10, 11.45s/it] 68%|   | 4393/6500 [13:16:58<6:30:34, 11.12s/it]                                                         68%|   | 4393/6500 [13:16:58<6:30:34, 11.12s/it] 68%|   | 4394/6500 [13:17:08<6:22:35, 10.90s/it]                                                         68%|   | 4394/6500 [13:17:08<6:22:35, 10.90s/it] 68%|   | 4395/6500 [13:17:19<6:16:57, 10.74s/it]                                                         68%|   | 4395/6500 [13:17:19<6:16:57, 10.74s/it] 68%|{'loss': 0.3399, 'learning_rate': 2.371346138449727e-05, 'epoch': 0.68}
{'loss': 0.8537, 'learning_rate': 2.369290121499792e-05, 'epoch': 0.68}
{'loss': 0.3372, 'learning_rate': 2.367234719462297e-05, 'epoch': 0.68}
{'loss': 0.344, 'learning_rate': 2.3651799328176776e-05, 'epoch': 0.68}
{'loss': 0.3336, 'learning_rate': 2.3631257620462294e-05, 'epoch': 0.68}
   | 4396/6500 [13:17:29<6:15:15, 10.70s/it]                                                         68%|   | 4396/6500 [13:17:29<6:15:15, 10.70s/it] 68%|   | 4397/6500 [13:17:40<6:11:17, 10.59s/it]                                                         68%|   | 4397/6500 [13:17:40<6:11:17, 10.59s/it] 68%|   | 4398/6500 [13:17:50<6:08:57, 10.53s/it]                                                         68%|   | 4398/6500 [13:17:50<6:08:57, 10.53s/it] 68%|   | 4399/6500 [13:18:00<6:07:15, 10.49s/it]                                                         68%|   | 4399/6500 [13:18:00<6:07:15, 10.49s/it] 68%|   | 4400/6500 [13:18:13<6:31:54, 11.20s/it]                                                         68%|   | 4400/6500 [13:18:13<6:31:54, 11.20s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8641708493232727, 'eval_runtime': 4.0121, 'eval_samples_per_second': 5.733, 'eval_steps_per_second': 1.495, 'epoch': 0.68}
                                                         68%|   | 4400/6500 [13:18:17<6:31:54, 11.20s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4400/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3094, 'learning_rate': 2.3610722076281023e-05, 'epoch': 0.68}
{'loss': 0.3514, 'learning_rate': 2.3590192700433013e-05, 'epoch': 0.68}
{'loss': 0.3183, 'learning_rate': 2.3569669497716883e-05, 'epoch': 0.68}
{'loss': 0.3103, 'learning_rate': 2.3549152472929808e-05, 'epoch': 0.68}
{'loss': 0.3289, 'learning_rate': 2.3528641630867526e-05, 'epoch': 0.68}
 68%|   | 4401/6500 [13:18:28<7:12:20, 12.36s/it]                                                         68%|   | 4401/6500 [13:18:28<7:12:20, 12.36s/it] 68%|   | 4402/6500 [13:18:39<6:51:20, 11.76s/it]                                                         68%|   | 4402/6500 [13:18:39<6:51:20, 11.76s/it] 68%|   | 4403/6500 [13:18:49<6:36:16, 11.34s/it]                                                         68%|   | 4403/6500 [13:18:49<6:36:16, 11.34s/it] 68%|   | 4404/6500 [13:18:59<6:25:50, 11.04s/it]                                                         68%|   | 4404/6500 [13:18:59<6:25:50, 11.04s/it] 68%|   | 4405/6500 [13:19:10<6:18:25, 10.84s/it]                                                         68%|   | 4405/6500 [13:19:10<6:18:25, 10.84s/it] 68%|{'loss': 0.3257, 'learning_rate': 2.350813697632433e-05, 'epoch': 0.68}
{'loss': 0.3306, 'learning_rate': 2.348763851409302e-05, 'epoch': 0.68}
{'loss': 0.3178, 'learning_rate': 2.346714624896501e-05, 'epoch': 0.68}
{'loss': 0.3389, 'learning_rate': 2.3446660185730247e-05, 'epoch': 0.68}
{'loss': 0.319, 'learning_rate': 2.3426180329177215e-05, 'epoch': 0.68}
   | 4406/6500 [13:19:20<6:13:16, 10.70s/it]                                                         68%|   | 4406/6500 [13:19:20<6:13:16, 10.70s/it] 68%|   | 4407/6500 [13:19:30<6:09:44, 10.60s/it]                                                         68%|   | 4407/6500 [13:19:30<6:09:44, 10.60s/it] 68%|   | 4408/6500 [13:19:41<6:07:04, 10.53s/it]                                                         68%|   | 4408/6500 [13:19:41<6:07:04, 10.53s/it] 68%|   | 4409/6500 [13:19:51<6:05:12, 10.48s/it]                                                         68%|   | 4409/6500 [13:19:51<6:05:12, 10.48s/it] 68%|   | 4410/6500 [13:20:02<6:03:49, 10.44s/it]                                                         68%|   | 4410/6500 [13:20:02<6:03:49, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8630115389823914, 'eval_runtime': 3.9521, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.68}
                                                         68%|   | 4410/6500 [13:20:06<6:03:49, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3432, 'learning_rate': 2.3405706684092958e-05, 'epoch': 0.68}
{'loss': 0.3363, 'learning_rate': 2.3385239255263077e-05, 'epoch': 0.68}
{'loss': 0.3319, 'learning_rate': 2.336477804747169e-05, 'epoch': 0.68}
{'loss': 0.3409, 'learning_rate': 2.3344323065501494e-05, 'epoch': 0.68}
{'loss': 0.3377, 'learning_rate': 2.332387431413371e-05, 'epoch': 0.68}
 68%|   | 4411/6500 [13:20:16<6:50:28, 11.79s/it]                                                         68%|   | 4411/6500 [13:20:16<6:50:28, 11.79s/it] 68%|   | 4412/6500 [13:20:28<6:47:55, 11.72s/it]                                                         68%|   | 4412/6500 [13:20:28<6:47:55, 11.72s/it] 68%|   | 4413/6500 [13:20:38<6:34:12, 11.33s/it]                                                         68%|   | 4413/6500 [13:20:38<6:34:12, 11.33s/it] 68%|   | 4414/6500 [13:20:49<6:24:48, 11.07s/it]                                                         68%|   | 4414/6500 [13:20:49<6:24:48, 11.07s/it] 68%|   | 4415/6500 [13:20:59<6:17:58, 10.88s/it]                                                         68%|   | 4415/6500 [13:20:59<6:17:58, 10.88s/it] 68%|{'loss': 0.3385, 'learning_rate': 2.330343179814811e-05, 'epoch': 0.68}
{'loss': 0.3186, 'learning_rate': 2.328299552232303e-05, 'epoch': 0.68}
{'loss': 0.3507, 'learning_rate': 2.326256549143529e-05, 'epoch': 0.68}
{'loss': 0.3254, 'learning_rate': 2.3242141710260295e-05, 'epoch': 0.68}
{'loss': 0.3233, 'learning_rate': 2.3221724183571986e-05, 'epoch': 0.68}
   | 4416/6500 [13:21:10<6:13:08, 10.74s/it]                                                         68%|   | 4416/6500 [13:21:10<6:13:08, 10.74s/it] 68%|   | 4417/6500 [13:21:20<6:09:45, 10.65s/it]                                                         68%|   | 4417/6500 [13:21:20<6:09:45, 10.65s/it] 68%|   | 4418/6500 [13:21:31<6:07:17, 10.58s/it]                                                         68%|   | 4418/6500 [13:21:31<6:07:17, 10.58s/it] 68%|   | 4419/6500 [13:21:41<6:05:23, 10.53s/it]                                                         68%|   | 4419/6500 [13:21:41<6:05:23, 10.53s/it] 68%|   | 4420/6500 [13:21:52<6:04:11, 10.51s/it]                                                         68%|   | 4420/6500 [13:21:52<6:04:11, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8570143580436707, 'eval_runtime': 4.2215, 'eval_samples_per_second': 5.448, 'eval_steps_per_second': 1.421, 'epoch': 0.68}
                                                         68%|   | 4420/6500 [13:21:56<6:04:11, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.31, 'learning_rate': 2.320131291614283e-05, 'epoch': 0.68}
{'loss': 0.3414, 'learning_rate': 2.318090791274385e-05, 'epoch': 0.68}
{'loss': 0.3941, 'learning_rate': 2.316050917814456e-05, 'epoch': 0.68}
{'loss': 0.3356, 'learning_rate': 2.314011671711308e-05, 'epoch': 0.68}
{'loss': 0.3228, 'learning_rate': 2.3119730534416028e-05, 'epoch': 0.68}
 68%|   | 4421/6500 [13:22:07<6:52:26, 11.90s/it]                                                         68%|   | 4421/6500 [13:22:07<6:52:26, 11.90s/it] 68%|   | 4422/6500 [13:22:17<6:36:57, 11.46s/it]                                                         68%|   | 4422/6500 [13:22:17<6:36:57, 11.46s/it] 68%|   | 4423/6500 [13:22:28<6:25:55, 11.15s/it]                                                         68%|   | 4423/6500 [13:22:28<6:25:55, 11.15s/it] 68%|   | 4424/6500 [13:22:38<6:18:07, 10.93s/it]                                                         68%|   | 4424/6500 [13:22:38<6:18:07, 10.93s/it] 68%|   | 4425/6500 [13:22:48<6:12:35, 10.77s/it]                                                         68%|   | 4425/6500 [13:22:48<6:12:35, 10.77s/it] 68%|{'loss': 0.3419, 'learning_rate': 2.3099350634818506e-05, 'epoch': 0.68}
{'loss': 0.8545, 'learning_rate': 2.307897702308422e-05, 'epoch': 0.68}
{'loss': 0.3495, 'learning_rate': 2.305860970397537e-05, 'epoch': 0.68}
{'loss': 0.3239, 'learning_rate': 2.3038248682252693e-05, 'epoch': 0.68}
{'loss': 0.3212, 'learning_rate': 2.3017893962675458e-05, 'epoch': 0.68}
   | 4426/6500 [13:22:59<6:08:40, 10.67s/it]                                                         68%|   | 4426/6500 [13:22:59<6:08:40, 10.67s/it] 68%|   | 4427/6500 [13:23:09<6:05:52, 10.59s/it]                                                         68%|   | 4427/6500 [13:23:09<6:05:52, 10.59s/it] 68%|   | 4428/6500 [13:23:20<6:07:36, 10.65s/it]                                                         68%|   | 4428/6500 [13:23:20<6:07:36, 10.65s/it] 68%|   | 4429/6500 [13:23:30<6:05:03, 10.58s/it]                                                         68%|   | 4429/6500 [13:23:30<6:05:03, 10.58s/it] 68%|   | 4430/6500 [13:23:41<6:10:22, 10.74s/it]                                                         68%|   | 4430/6500 [13:23:42<6:10:22, 10.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.860815703868866, 'eval_runtime': 4.2338, 'eval_samples_per_second': 5.432, 'eval_steps_per_second': 1.417, 'epoch': 0.68}
                                                         68%|   | 4430/6500 [13:23:46<6:10:22, 10.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4430the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4430

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4430/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4430/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4430/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.312, 'learning_rate': 2.2997545550001455e-05, 'epoch': 0.68}
{'loss': 0.3452, 'learning_rate': 2.2977203448987007e-05, 'epoch': 0.68}
{'loss': 0.3111, 'learning_rate': 2.2956867664386945e-05, 'epoch': 0.68}
{'loss': 0.3164, 'learning_rate': 2.2936538200954644e-05, 'epoch': 0.68}
{'loss': 0.3148, 'learning_rate': 2.2916215063441985e-05, 'epoch': 0.68}
 68%|   | 4431/6500 [13:23:57<6:56:43, 12.08s/it]                                                         68%|   | 4431/6500 [13:23:57<6:56:43, 12.08s/it] 68%|   | 4432/6500 [13:24:07<6:39:12, 11.58s/it]                                                         68%|   | 4432/6500 [13:24:07<6:39:12, 11.58s/it] 68%|   | 4433/6500 [13:24:18<6:27:07, 11.24s/it]                                                         68%|   | 4433/6500 [13:24:18<6:27:07, 11.24s/it] 68%|   | 4434/6500 [13:24:28<6:18:12, 10.98s/it]                                                         68%|   | 4434/6500 [13:24:28<6:18:12, 10.98s/it] 68%|   | 4435/6500 [13:24:38<6:11:29, 10.79s/it]                                                         68%|   | 4435/6500 [13:24:38<6:11:29, 10.79s/it] 68%|{'loss': 0.3232, 'learning_rate': 2.2895898256599392e-05, 'epoch': 0.68}
{'loss': 0.3326, 'learning_rate': 2.28755877851758e-05, 'epoch': 0.68}
{'loss': 0.3131, 'learning_rate': 2.2855283653918625e-05, 'epoch': 0.68}
{'loss': 0.3364, 'learning_rate': 2.2834985867573855e-05, 'epoch': 0.68}
{'loss': 0.3208, 'learning_rate': 2.281469443088597e-05, 'epoch': 0.68}
   | 4436/6500 [13:24:49<6:07:10, 10.67s/it]                                                         68%|   | 4436/6500 [13:24:49<6:07:10, 10.67s/it] 68%|   | 4437/6500 [13:24:59<6:03:51, 10.58s/it]                                                         68%|   | 4437/6500 [13:24:59<6:03:51, 10.58s/it] 68%|   | 4438/6500 [13:25:09<6:01:25, 10.52s/it]                                                         68%|   | 4438/6500 [13:25:09<6:01:25, 10.52s/it] 68%|   | 4439/6500 [13:25:20<5:59:37, 10.47s/it]                                                         68%|   | 4439/6500 [13:25:20<5:59:37, 10.47s/it] 68%|   | 4440/6500 [13:25:30<5:58:26, 10.44s/it]                                                         68%|   | 4440/6500 [13:25:30<5:58:26, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.867468535900116, 'eval_runtime': 3.951, 'eval_samples_per_second': 5.821, 'eval_steps_per_second': 1.519, 'epoch': 0.68}
                                                         68%|   | 4440/6500 [13:25:34<5:58:26, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4440
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3422, 'learning_rate': 2.2794409348597972e-05, 'epoch': 0.68}
{'loss': 0.3245, 'learning_rate': 2.277413062545138e-05, 'epoch': 0.68}
{'loss': 0.3227, 'learning_rate': 2.2753858266186213e-05, 'epoch': 0.68}
{'loss': 0.3299, 'learning_rate': 2.273359227554102e-05, 'epoch': 0.68}
{'loss': 0.3273, 'learning_rate': 2.271333265825285e-05, 'epoch': 0.68}
 68%|   | 4441/6500 [13:25:45<6:43:34, 11.76s/it]                                                         68%|   | 4441/6500 [13:25:45<6:43:34, 11.76s/it] 68%|   | 4442/6500 [13:25:55<6:29:02, 11.34s/it]                                                         68%|   | 4442/6500 [13:25:55<6:29:02, 11.34s/it] 68%|   | 4443/6500 [13:26:06<6:18:56, 11.05s/it]                                                         68%|   | 4443/6500 [13:26:06<6:18:56, 11.05s/it] 68%|   | 4444/6500 [13:26:16<6:11:37, 10.85s/it]                                                         68%|   | 4444/6500 [13:26:16<6:11:37, 10.85s/it] 68%|   | 4445/6500 [13:26:27<6:15:30, 10.96s/it]                                                         68%|   | 4445/6500 [13:26:27<6:15:30, 10.96s/it] 68%|{'loss': 0.3289, 'learning_rate': 2.2693079419057266e-05, 'epoch': 0.68}
{'loss': 0.3281, 'learning_rate': 2.267283256268834e-05, 'epoch': 0.68}
{'loss': 0.3408, 'learning_rate': 2.2652592093878666e-05, 'epoch': 0.68}
{'loss': 0.313, 'learning_rate': 2.2632358017359302e-05, 'epoch': 0.68}
{'loss': 0.328, 'learning_rate': 2.261213033785985e-05, 'epoch': 0.68}
   | 4446/6500 [13:26:38<6:09:19, 10.79s/it]                                                         68%|   | 4446/6500 [13:26:38<6:09:19, 10.79s/it] 68%|   | 4447/6500 [13:26:48<6:05:04, 10.67s/it]                                                         68%|   | 4447/6500 [13:26:48<6:05:04, 10.67s/it] 68%|   | 4448/6500 [13:26:58<6:01:51, 10.58s/it]                                                         68%|   | 4448/6500 [13:26:58<6:01:51, 10.58s/it] 68%|   | 4449/6500 [13:27:09<5:59:37, 10.52s/it]                                                         68%|   | 4449/6500 [13:27:09<5:59:37, 10.52s/it] 68%|   | 4450/6500 [13:27:19<5:58:04, 10.48s/it]                                                         68%|   | 4450/6500 [13:27:19<5:58:04, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8565696477890015, 'eval_runtime': 3.9284, 'eval_samples_per_second': 5.855, 'eval_steps_per_second': 1.527, 'epoch': 0.68}
                                                         68%|   | 4450/6500 [13:27:23<5:58:04, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4450I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4450/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3093, 'learning_rate': 2.2591909060108407e-05, 'epoch': 0.68}
{'loss': 0.3569, 'learning_rate': 2.2571694188831582e-05, 'epoch': 0.68}
{'loss': 0.371, 'learning_rate': 2.2551485728754467e-05, 'epoch': 0.69}
{'loss': 0.3173, 'learning_rate': 2.253128368460068e-05, 'epoch': 0.69}
{'loss': 0.3351, 'learning_rate': 2.2511088061092318e-05, 'epoch': 0.69}
 68%|   | 4451/6500 [13:27:34<6:42:33, 11.79s/it]                                                         68%|   | 4451/6500 [13:27:34<6:42:33, 11.79s/it] 68%|   | 4452/6500 [13:27:44<6:28:00, 11.37s/it]                                                         68%|   | 4452/6500 [13:27:44<6:28:00, 11.37s/it] 69%|   | 4453/6500 [13:27:55<6:17:42, 11.07s/it]                                                         69%|   | 4453/6500 [13:27:55<6:17:42, 11.07s/it] 69%|   | 4454/6500 [13:28:05<6:10:29, 10.86s/it]                                                         69%|   | 4454/6500 [13:28:05<6:10:29, 10.86s/it] 69%|   | 4455/6500 [13:28:16<6:05:18, 10.72s/it]                                                         69%|   | 4455/6500 [13:28:16<6:05:18, 10.72s/it] 69%|{'loss': 0.3521, 'learning_rate': 2.2490898862949987e-05, 'epoch': 0.69}
{'loss': 0.8172, 'learning_rate': 2.2470716094892785e-05, 'epoch': 0.69}
{'loss': 0.3342, 'learning_rate': 2.2450539761638316e-05, 'epoch': 0.69}
{'loss': 0.3317, 'learning_rate': 2.2430369867902694e-05, 'epoch': 0.69}
{'loss': 0.3151, 'learning_rate': 2.2410206418400477e-05, 'epoch': 0.69}
   | 4456/6500 [13:28:26<6:01:31, 10.61s/it]                                                         69%|   | 4456/6500 [13:28:26<6:01:31, 10.61s/it] 69%|   | 4457/6500 [13:28:36<5:58:49, 10.54s/it]                                                         69%|   | 4457/6500 [13:28:36<5:58:49, 10.54s/it] 69%|   | 4458/6500 [13:28:47<5:57:14, 10.50s/it]                                                         69%|   | 4458/6500 [13:28:47<5:57:14, 10.50s/it] 69%|   | 4459/6500 [13:28:57<5:56:00, 10.47s/it]                                                         69%|   | 4459/6500 [13:28:57<5:56:00, 10.47s/it] 69%|   | 4460/6500 [13:29:08<5:55:07, 10.44s/it]                                                         69%|   | 4460/6500 [13:29:08<5:55:07, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8663368821144104, 'eval_runtime': 3.9467, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.69}
                                                         69%|   | 4460/6500 [13:29:11<5:55:07, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3328, 'learning_rate': 2.2390049417844756e-05, 'epoch': 0.69}
{'loss': 0.3314, 'learning_rate': 2.236989887094711e-05, 'epoch': 0.69}
{'loss': 0.2984, 'learning_rate': 2.234975478241761e-05, 'epoch': 0.69}
{'loss': 0.3269, 'learning_rate': 2.232961715696481e-05, 'epoch': 0.69}
{'loss': 0.3141, 'learning_rate': 2.2309485999295765e-05, 'epoch': 0.69}
 69%|   | 4461/6500 [13:29:23<6:44:35, 11.91s/it]                                                         69%|   | 4461/6500 [13:29:23<6:44:35, 11.91s/it] 69%|   | 4462/6500 [13:29:33<6:28:54, 11.45s/it]                                                         69%|   | 4462/6500 [13:29:33<6:28:54, 11.45s/it] 69%|   | 4463/6500 [13:29:44<6:17:50, 11.13s/it]                                                         69%|   | 4463/6500 [13:29:44<6:17:50, 11.13s/it] 69%|   | 4464/6500 [13:29:54<6:10:14, 10.91s/it]                                                         69%|   | 4464/6500 [13:29:54<6:10:14, 10.91s/it] 69%|   | 4465/6500 [13:30:04<6:04:58, 10.76s/it]                                                         69%|   | 4465/6500 [13:30:04<6:04:58, 10.76s/it] 69%|{'loss': 0.3221, 'learning_rate': 2.228936131411601e-05, 'epoch': 0.69}
{'loss': 0.3252, 'learning_rate': 2.226924310612956e-05, 'epoch': 0.69}
{'loss': 0.3298, 'learning_rate': 2.2249131380038928e-05, 'epoch': 0.69}
{'loss': 0.3191, 'learning_rate': 2.2229026140545112e-05, 'epoch': 0.69}
{'loss': 0.3277, 'learning_rate': 2.2208927392347596e-05, 'epoch': 0.69}
   | 4466/6500 [13:30:15<6:00:57, 10.65s/it]                                                         69%|   | 4466/6500 [13:30:15<6:00:57, 10.65s/it] 69%|   | 4467/6500 [13:30:25<5:58:06, 10.57s/it]                                                         69%|   | 4467/6500 [13:30:25<5:58:06, 10.57s/it] 69%|   | 4468/6500 [13:30:36<5:55:59, 10.51s/it]                                                         69%|   | 4468/6500 [13:30:36<5:55:59, 10.51s/it] 69%|   | 4469/6500 [13:30:46<5:54:37, 10.48s/it]                                                         69%|   | 4469/6500 [13:30:46<5:54:37, 10.48s/it] 69%|   | 4470/6500 [13:30:56<5:53:29, 10.45s/it]                                                         69%|   | 4470/6500 [13:30:56<5:53:29, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8659981489181519, 'eval_runtime': 3.9508, 'eval_samples_per_second': 5.822, 'eval_steps_per_second': 1.519, 'epoch': 0.69}
                                                         69%|   | 4470/6500 [13:31:00<5:53:29, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4470
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4470
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3428, 'learning_rate': 2.2188835140144314e-05, 'epoch': 0.69}
{'loss': 0.3178, 'learning_rate': 2.2168749388631727e-05, 'epoch': 0.69}
{'loss': 0.3411, 'learning_rate': 2.214867014250475e-05, 'epoch': 0.69}
{'loss': 0.3374, 'learning_rate': 2.212859740645679e-05, 'epoch': 0.69}
{'loss': 0.3237, 'learning_rate': 2.2108531185179727e-05, 'epoch': 0.69}
 69%|   | 4471/6500 [13:31:11<6:38:00, 11.77s/it]                                                         69%|   | 4471/6500 [13:31:11<6:38:00, 11.77s/it] 69%|   | 4472/6500 [13:31:22<6:23:57, 11.36s/it]                                                         69%|   | 4472/6500 [13:31:22<6:23:57, 11.36s/it] 69%|   | 4473/6500 [13:31:32<6:14:06, 11.07s/it]                                                         69%|   | 4473/6500 [13:31:32<6:14:06, 11.07s/it] 69%|   | 4474/6500 [13:31:42<6:06:53, 10.87s/it]                                                         69%|   | 4474/6500 [13:31:42<6:06:53, 10.87s/it] 69%|   | 4475/6500 [13:31:53<6:01:57, 10.72s/it]                                                         69%|   | 4475/6500 [13:31:53<6:01:57, 10.72s/it] 69%|{'loss': 0.3283, 'learning_rate': 2.2088471483363916e-05, 'epoch': 0.69}
{'loss': 0.3362, 'learning_rate': 2.206841830569819e-05, 'epoch': 0.69}
{'loss': 0.3296, 'learning_rate': 2.204837165686986e-05, 'epoch': 0.69}
{'loss': 0.3154, 'learning_rate': 2.20283315415647e-05, 'epoch': 0.69}
{'loss': 0.3094, 'learning_rate': 2.200829796446698e-05, 'epoch': 0.69}
   | 4476/6500 [13:32:03<5:58:25, 10.63s/it]                                                         69%|   | 4476/6500 [13:32:03<5:58:25, 10.63s/it] 69%|   | 4477/6500 [13:32:14<5:58:28, 10.63s/it]                                                         69%|   | 4477/6500 [13:32:14<5:58:28, 10.63s/it] 69%|   | 4478/6500 [13:32:24<5:55:54, 10.56s/it]                                                         69%|   | 4478/6500 [13:32:24<5:55:54, 10.56s/it] 69%|   | 4479/6500 [13:32:35<5:54:05, 10.51s/it]                                                         69%|   | 4479/6500 [13:32:35<5:54:05, 10.51s/it] 69%|   | 4480/6500 [13:32:45<5:52:47, 10.48s/it]                                                         69%|   | 4480/6500 [13:32:45<5:52:47, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8590885400772095, 'eval_runtime': 3.974, 'eval_samples_per_second': 5.788, 'eval_steps_per_second': 1.51, 'epoch': 0.69}
                                                         69%|   | 4480/6500 [13:32:49<5:52:47, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3227, 'learning_rate': 2.198827093025943e-05, 'epoch': 0.69}
{'loss': 0.3943, 'learning_rate': 2.1968250443623224e-05, 'epoch': 0.69}
{'loss': 0.3222, 'learning_rate': 2.1948236509238034e-05, 'epoch': 0.69}
{'loss': 0.3043, 'learning_rate': 2.1928229131782007e-05, 'epoch': 0.69}
{'loss': 0.3419, 'learning_rate': 2.190822831593174e-05, 'epoch': 0.69}
 69%|   | 4481/6500 [13:33:00<6:37:32, 11.81s/it]                                                         69%|   | 4481/6500 [13:33:00<6:37:32, 11.81s/it] 69%|   | 4482/6500 [13:33:10<6:23:06, 11.39s/it]                                                         69%|   | 4482/6500 [13:33:10<6:23:06, 11.39s/it] 69%|   | 4483/6500 [13:33:21<6:12:54, 11.09s/it]                                                         69%|   | 4483/6500 [13:33:21<6:12:54, 11.09s/it] 69%|   | 4484/6500 [13:33:31<6:05:42, 10.88s/it]                                                         69%|   | 4484/6500 [13:33:31<6:05:42, 10.88s/it] 69%|   | 4485/6500 [13:33:42<6:00:34, 10.74s/it]                                                         69%|   | 4485/6500 [13:33:42<6:00:34, 10.74s/it] 69%|{'loss': 0.8414, 'learning_rate': 2.1888234066362302e-05, 'epoch': 0.69}
{'loss': 0.3368, 'learning_rate': 2.1868246387747232e-05, 'epoch': 0.69}
{'loss': 0.325, 'learning_rate': 2.1848265284758524e-05, 'epoch': 0.69}
{'loss': 0.3444, 'learning_rate': 2.182829076206664e-05, 'epoch': 0.69}
{'loss': 0.304, 'learning_rate': 2.18083228243405e-05, 'epoch': 0.69}
   | 4486/6500 [13:33:52<5:56:47, 10.63s/it]                                                         69%|   | 4486/6500 [13:33:52<5:56:47, 10.63s/it] 69%|   | 4487/6500 [13:34:02<5:54:20, 10.56s/it]                                                         69%|   | 4487/6500 [13:34:02<5:54:20, 10.56s/it] 69%|   | 4488/6500 [13:34:13<5:52:32, 10.51s/it]                                                         69%|   | 4488/6500 [13:34:13<5:52:32, 10.51s/it] 69%|   | 4489/6500 [13:34:23<5:51:17, 10.48s/it]                                                         69%|   | 4489/6500 [13:34:23<5:51:17, 10.48s/it] 69%|   | 4490/6500 [13:34:34<5:50:27, 10.46s/it]                                                         69%|   | 4490/6500 [13:34:34<5:50:27, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.867300808429718, 'eval_runtime': 4.1725, 'eval_samples_per_second': 5.512, 'eval_steps_per_second': 1.438, 'epoch': 0.69}
                                                         69%|   | 4490/6500 [13:34:38<5:50:27, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4490
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4490

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4490
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4490/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3455, 'learning_rate': 2.17883614762475e-05, 'epoch': 0.69}
{'loss': 0.3097, 'learning_rate': 2.1768406722453465e-05, 'epoch': 0.69}
{'loss': 0.3091, 'learning_rate': 2.1748458567622733e-05, 'epoch': 0.69}
{'loss': 0.3302, 'learning_rate': 2.1728517016418016e-05, 'epoch': 0.69}
{'loss': 0.3239, 'learning_rate': 2.1708582073500554e-05, 'epoch': 0.69}
 69%|   | 4491/6500 [13:34:49<6:37:17, 11.87s/it]                                                         69%|   | 4491/6500 [13:34:49<6:37:17, 11.87s/it] 69%|   | 4492/6500 [13:34:59<6:22:14, 11.42s/it]                                                         69%|   | 4492/6500 [13:34:59<6:22:14, 11.42s/it] 69%|   | 4493/6500 [13:35:10<6:15:18, 11.22s/it]                                                         69%|   | 4493/6500 [13:35:10<6:15:18, 11.22s/it] 69%|   | 4494/6500 [13:35:20<6:06:35, 10.96s/it]                                                         69%|   | 4494/6500 [13:35:20<6:06:35, 10.96s/it] 69%|   | 4495/6500 [13:35:31<6:00:36, 10.79s/it]                                                         69%|   | 4495/6500 [13:35:31<6:00:36, 10.79s/it] 69%|{'loss': 0.3325, 'learning_rate': 2.1688653743530023e-05, 'epoch': 0.69}
{'loss': 0.3284, 'learning_rate': 2.166873203116454e-05, 'epoch': 0.69}
{'loss': 0.3395, 'learning_rate': 2.1648816941060668e-05, 'epoch': 0.69}
{'loss': 0.3206, 'learning_rate': 2.162890847787348e-05, 'epoch': 0.69}
{'loss': 0.3515, 'learning_rate': 2.160900664625643e-05, 'epoch': 0.69}
   | 4496/6500 [13:35:41<5:56:30, 10.67s/it]                                                         69%|   | 4496/6500 [13:35:41<5:56:30, 10.67s/it] 69%|   | 4497/6500 [13:35:51<5:53:20, 10.58s/it]                                                         69%|   | 4497/6500 [13:35:51<5:53:20, 10.58s/it] 69%|   | 4498/6500 [13:36:02<5:51:03, 10.52s/it]                                                         69%|   | 4498/6500 [13:36:02<5:51:03, 10.52s/it] 69%|   | 4499/6500 [13:36:12<5:49:33, 10.48s/it]                                                         69%|   | 4499/6500 [13:36:12<5:49:33, 10.48s/it] 69%|   | 4500/6500 [13:36:22<5:48:14, 10.45s/it]                                                         69%|   | 4500/6500 [13:36:22<5:48:14, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.863676905632019, 'eval_runtime': 3.9651, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.69}
                                                         69%|   | 4500/6500 [13:36:26<5:48:14, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3264, 'learning_rate': 2.1589111450861475e-05, 'epoch': 0.69}
{'loss': 0.3267, 'learning_rate': 2.1569222896338966e-05, 'epoch': 0.69}
{'loss': 0.337, 'learning_rate': 2.154934098733774e-05, 'epoch': 0.69}
{'loss': 0.3351, 'learning_rate': 2.1529465728505078e-05, 'epoch': 0.69}
{'loss': 0.3272, 'learning_rate': 2.150959712448669e-05, 'epoch': 0.69}
 69%|   | 4501/6500 [13:36:37<6:32:10, 11.77s/it]                                                         69%|   | 4501/6500 [13:36:37<6:32:10, 11.77s/it] 69%|   | 4502/6500 [13:36:48<6:18:05, 11.35s/it]                                                         69%|   | 4502/6500 [13:36:48<6:18:05, 11.35s/it] 69%|   | 4503/6500 [13:36:58<6:08:17, 11.07s/it]                                                         69%|   | 4503/6500 [13:36:58<6:08:17, 11.07s/it] 69%|   | 4504/6500 [13:37:09<6:01:18, 10.86s/it]                                                         69%|   | 4504/6500 [13:37:09<6:01:18, 10.86s/it] 69%|   | 4505/6500 [13:37:19<5:56:21, 10.72s/it]                                                         69%|   | 4505/6500 [13:37:19<5:56:21, 10.72s/it] 69%|{'loss': 0.3328, 'learning_rate': 2.1489735179926757e-05, 'epoch': 0.69}
{'loss': 0.3461, 'learning_rate': 2.1469879899467877e-05, 'epoch': 0.69}
{'loss': 0.3118, 'learning_rate': 2.1450031287751103e-05, 'epoch': 0.69}
{'loss': 0.3228, 'learning_rate': 2.1430189349415934e-05, 'epoch': 0.69}
{'loss': 0.3065, 'learning_rate': 2.1410354089100292e-05, 'epoch': 0.69}
   | 4506/6500 [13:37:29<5:52:59, 10.62s/it]                                                         69%|   | 4506/6500 [13:37:29<5:52:59, 10.62s/it] 69%|   | 4507/6500 [13:37:40<5:50:19, 10.55s/it]                                                         69%|   | 4507/6500 [13:37:40<5:50:19, 10.55s/it] 69%|   | 4508/6500 [13:37:50<5:48:40, 10.50s/it]                                                         69%|   | 4508/6500 [13:37:50<5:48:40, 10.50s/it] 69%|   | 4509/6500 [13:38:01<5:49:38, 10.54s/it]                                                         69%|   | 4509/6500 [13:38:01<5:49:38, 10.54s/it] 69%|   | 4510/6500 [13:38:11<5:48:02, 10.49s/it]                                                         69%|   | 4510/6500 [13:38:11<5:48:02, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8609734177589417, 'eval_runtime': 5.1578, 'eval_samples_per_second': 4.459, 'eval_steps_per_second': 1.163, 'epoch': 0.69}
                                                         69%|   | 4510/6500 [13:38:16<5:48:02, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3501, 'learning_rate': 2.1390525511440563e-05, 'epoch': 0.69}
{'loss': 0.3751, 'learning_rate': 2.1370703621071557e-05, 'epoch': 0.69}
{'loss': 0.3173, 'learning_rate': 2.1350888422626492e-05, 'epoch': 0.69}
{'loss': 0.3347, 'learning_rate': 2.1331079920737074e-05, 'epoch': 0.69}
{'loss': 0.3176, 'learning_rate': 2.1311278120033412e-05, 'epoch': 0.69}
 69%|   | 4511/6500 [13:38:27<6:43:46, 12.18s/it]                                                         69%|   | 4511/6500 [13:38:27<6:43:46, 12.18s/it] 69%|   | 4512/6500 [13:38:38<6:25:52, 11.65s/it]                                                         69%|   | 4512/6500 [13:38:38<6:25:52, 11.65s/it] 69%|   | 4513/6500 [13:38:48<6:13:17, 11.27s/it]                                                         69%|   | 4513/6500 [13:38:48<6:13:17, 11.27s/it] 69%|   | 4514/6500 [13:38:58<6:04:18, 11.01s/it]                                                         69%|   | 4514/6500 [13:38:58<6:04:18, 11.01s/it] 69%|   | 4515/6500 [13:39:09<5:57:55, 10.82s/it]                                                         69%|   | 4515/6500 [13:39:09<5:57:55, 10.82s/it] 69%|{'loss': 0.8416, 'learning_rate': 2.129148302514406e-05, 'epoch': 0.69}
{'loss': 0.3435, 'learning_rate': 2.1271694640696e-05, 'epoch': 0.69}
{'loss': 0.3326, 'learning_rate': 2.125191297131464e-05, 'epoch': 0.7}
{'loss': 0.3073, 'learning_rate': 2.1232138021623837e-05, 'epoch': 0.7}
{'loss': 0.3092, 'learning_rate': 2.1212369796245864e-05, 'epoch': 0.7}
   | 4516/6500 [13:39:19<5:53:22, 10.69s/it]                                                         69%|   | 4516/6500 [13:39:19<5:53:22, 10.69s/it] 69%|   | 4517/6500 [13:39:30<5:50:11, 10.60s/it]                                                         69%|   | 4517/6500 [13:39:30<5:50:11, 10.60s/it] 70%|   | 4518/6500 [13:39:40<5:48:02, 10.54s/it]                                                         70%|   | 4518/6500 [13:39:40<5:48:02, 10.54s/it] 70%|   | 4519/6500 [13:39:50<5:46:28, 10.49s/it]                                                         70%|   | 4519/6500 [13:39:50<5:46:28, 10.49s/it] 70%|   | 4520/6500 [13:40:01<5:45:20, 10.46s/it]                                                         70%|   | 4520/6500 [13:40:01<5:45:20, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8665909171104431, 'eval_runtime': 3.9496, 'eval_samples_per_second': 5.823, 'eval_steps_per_second': 1.519, 'epoch': 0.7}
                                                         70%|   | 4520/6500 [13:40:05<5:45:20, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4520/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4520/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4520/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3392, 'learning_rate': 2.1192608299801412e-05, 'epoch': 0.7}
{'loss': 0.3024, 'learning_rate': 2.1172853536909625e-05, 'epoch': 0.7}
{'loss': 0.3192, 'learning_rate': 2.115310551218805e-05, 'epoch': 0.7}
{'loss': 0.3103, 'learning_rate': 2.1133364230252688e-05, 'epoch': 0.7}
{'loss': 0.3225, 'learning_rate': 2.1113629695717907e-05, 'epoch': 0.7}
 70%|   | 4521/6500 [13:40:16<6:28:58, 11.79s/it]                                                         70%|   | 4521/6500 [13:40:16<6:28:58, 11.79s/it] 70%|   | 4522/6500 [13:40:26<6:14:50, 11.37s/it]                                                         70%|   | 4522/6500 [13:40:26<6:14:50, 11.37s/it] 70%|   | 4523/6500 [13:40:36<6:04:51, 11.07s/it]                                                         70%|   | 4523/6500 [13:40:36<6:04:51, 11.07s/it] 70%|   | 4524/6500 [13:40:47<5:58:01, 10.87s/it]                                                         70%|   | 4524/6500 [13:40:47<5:58:01, 10.87s/it] 70%|   | 4525/6500 [13:40:58<5:56:44, 10.84s/it]                                                         70%|   | 4525/6500 [13:40:58<5:56:44, 10.84s/it] 70%|{'loss': 0.3275, 'learning_rate': 2.109390191319655e-05, 'epoch': 0.7}
{'loss': 0.3114, 'learning_rate': 2.107418088729987e-05, 'epoch': 0.7}
{'loss': 0.341, 'learning_rate': 2.1054466622637538e-05, 'epoch': 0.7}
{'loss': 0.3309, 'learning_rate': 2.1034759123817638e-05, 'epoch': 0.7}
{'loss': 0.3624, 'learning_rate': 2.101505839544668e-05, 'epoch': 0.7}
   | 4526/6500 [13:41:08<5:52:05, 10.70s/it]                                                         70%|   | 4526/6500 [13:41:08<5:52:05, 10.70s/it] 70%|   | 4527/6500 [13:41:18<5:48:59, 10.61s/it]                                                         70%|   | 4527/6500 [13:41:18<5:48:59, 10.61s/it] 70%|   | 4528/6500 [13:41:29<5:46:36, 10.55s/it]                                                         70%|   | 4528/6500 [13:41:29<5:46:36, 10.55s/it] 70%|   | 4529/6500 [13:41:39<5:45:09, 10.51s/it]                                                         70%|   | 4529/6500 [13:41:39<5:45:09, 10.51s/it] 70%|   | 4530/6500 [13:41:50<5:44:03, 10.48s/it]                                                         70%|   | 4530/6500 [13:41:50<5:44:03, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8677715063095093, 'eval_runtime': 3.9559, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.7}
                                                         70%|   | 4530/6500 [13:41:53<5:44:03, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4530/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3374, 'learning_rate': 2.0995364442129596e-05, 'epoch': 0.7}
{'loss': 0.3352, 'learning_rate': 2.097567726846972e-05, 'epoch': 0.7}
{'loss': 0.3427, 'learning_rate': 2.0955996879068807e-05, 'epoch': 0.7}
{'loss': 0.3262, 'learning_rate': 2.0936323278527036e-05, 'epoch': 0.7}
{'loss': 0.3357, 'learning_rate': 2.0916656471443013e-05, 'epoch': 0.7}
 70%|   | 4531/6500 [13:42:04<6:27:54, 11.82s/it]                                                         70%|   | 4531/6500 [13:42:04<6:27:54, 11.82s/it] 70%|   | 4532/6500 [13:42:15<6:13:58, 11.40s/it]                                                         70%|   | 4532/6500 [13:42:15<6:13:58, 11.40s/it] 70%|   | 4533/6500 [13:42:25<6:04:07, 11.11s/it]                                                         70%|   | 4533/6500 [13:42:25<6:04:07, 11.11s/it] 70%|   | 4534/6500 [13:42:36<5:56:58, 10.89s/it]                                                         70%|   | 4534/6500 [13:42:36<5:56:58, 10.89s/it] 70%|   | 4535/6500 [13:42:46<5:52:02, 10.75s/it]                                                         70%|   | 4535/6500 [13:42:46<5:52:02, 10.75s/it] 70%|{'loss': 0.3331, 'learning_rate': 2.0896996462413686e-05, 'epoch': 0.7}
{'loss': 0.3392, 'learning_rate': 2.0877343256034487e-05, 'epoch': 0.7}
{'loss': 0.3101, 'learning_rate': 2.0857696856899232e-05, 'epoch': 0.7}
{'loss': 0.3257, 'learning_rate': 2.0838057269600154e-05, 'epoch': 0.7}
{'loss': 0.3236, 'learning_rate': 2.081842449872788e-05, 'epoch': 0.7}
   | 4536/6500 [13:42:57<5:48:52, 10.66s/it]                                                         70%|   | 4536/6500 [13:42:57<5:48:52, 10.66s/it] 70%|   | 4537/6500 [13:43:07<5:46:07, 10.58s/it]                                                         70%|   | 4537/6500 [13:43:07<5:46:07, 10.58s/it] 70%|   | 4538/6500 [13:43:17<5:44:30, 10.54s/it]                                                         70%|   | 4538/6500 [13:43:17<5:44:30, 10.54s/it] 70%|   | 4539/6500 [13:43:28<5:43:11, 10.50s/it]                                                         70%|   | 4539/6500 [13:43:28<5:43:11, 10.50s/it] 70%|   | 4540/6500 [13:43:38<5:41:58, 10.47s/it]                                                         70%|   | 4540/6500 [13:43:38<5:41:58, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8552194833755493, 'eval_runtime': 3.9523, 'eval_samples_per_second': 5.819, 'eval_steps_per_second': 1.518, 'epoch': 0.7}
                                                         70%|   | 4540/6500 [13:43:42<5:41:58, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4540/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4540/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3972, 'learning_rate': 2.079879854887145e-05, 'epoch': 0.7}
{'loss': 0.3281, 'learning_rate': 2.0779179424618313e-05, 'epoch': 0.7}
{'loss': 0.3148, 'learning_rate': 2.075956713055432e-05, 'epoch': 0.7}
{'loss': 0.3353, 'learning_rate': 2.0739961671263725e-05, 'epoch': 0.7}
{'loss': 0.8345, 'learning_rate': 2.0720363051329188e-05, 'epoch': 0.7}
 70%|   | 4541/6500 [13:43:53<6:25:12, 11.80s/it]                                                         70%|   | 4541/6500 [13:43:53<6:25:12, 11.80s/it] 70%|   | 4542/6500 [13:44:04<6:15:43, 11.51s/it]                                                         70%|   | 4542/6500 [13:44:04<6:15:43, 11.51s/it] 70%|   | 4543/6500 [13:44:14<6:04:11, 11.17s/it]                                                         70%|   | 4543/6500 [13:44:14<6:04:11, 11.17s/it] 70%|   | 4544/6500 [13:44:25<5:56:02, 10.92s/it]                                                         70%|   | 4544/6500 [13:44:25<5:56:02, 10.92s/it] 70%|   | 4545/6500 [13:44:35<5:50:01, 10.74s/it]                                                         70%|   | 4545/6500 [13:44:35<5:50:01, 10.74s/it] 70%|{'loss': 0.3302, 'learning_rate': 2.070077127533178e-05, 'epoch': 0.7}
{'loss': 0.3352, 'learning_rate': 2.068118634785093e-05, 'epoch': 0.7}
{'loss': 0.3351, 'learning_rate': 2.0661608273464506e-05, 'epoch': 0.7}
{'loss': 0.3109, 'learning_rate': 2.064203705674877e-05, 'epoch': 0.7}
{'loss': 0.3357, 'learning_rate': 2.0622472702278372e-05, 'epoch': 0.7}
   | 4546/6500 [13:44:45<5:46:12, 10.63s/it]                                                         70%|   | 4546/6500 [13:44:45<5:46:12, 10.63s/it] 70%|   | 4547/6500 [13:44:56<5:43:44, 10.56s/it]                                                         70%|   | 4547/6500 [13:44:56<5:43:44, 10.56s/it] 70%|   | 4548/6500 [13:45:06<5:41:37, 10.50s/it]                                                         70%|   | 4548/6500 [13:45:06<5:41:37, 10.50s/it] 70%|   | 4549/6500 [13:45:16<5:40:06, 10.46s/it]                                                         70%|   | 4549/6500 [13:45:16<5:40:06, 10.46s/it] 70%|   | 4550/6500 [13:45:27<5:38:52, 10.43s/it]                                                         70%|   | 4550/6500 [13:45:27<5:38:52, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8690766096115112, 'eval_runtime': 3.9399, 'eval_samples_per_second': 5.838, 'eval_steps_per_second': 1.523, 'epoch': 0.7}
                                                         70%|   | 4550/6500 [13:45:31<5:38:52, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4550
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3309, 'learning_rate': 2.060291521462636e-05, 'epoch': 0.7}
{'loss': 0.2954, 'learning_rate': 2.0583364598364184e-05, 'epoch': 0.7}
{'loss': 0.3252, 'learning_rate': 2.056382085806167e-05, 'epoch': 0.7}
{'loss': 0.3088, 'learning_rate': 2.054428399828706e-05, 'epoch': 0.7}
{'loss': 0.3327, 'learning_rate': 2.0524754023606972e-05, 'epoch': 0.7}
 70%|   | 4551/6500 [13:45:42<6:22:09, 11.76s/it]                                                         70%|   | 4551/6500 [13:45:42<6:22:09, 11.76s/it] 70%|   | 4552/6500 [13:45:52<6:08:07, 11.34s/it]                                                         70%|   | 4552/6500 [13:45:52<6:08:07, 11.34s/it] 70%|   | 4553/6500 [13:46:02<5:58:13, 11.04s/it]                                                         70%|   | 4553/6500 [13:46:02<5:58:13, 11.04s/it] 70%|   | 4554/6500 [13:46:13<5:51:21, 10.83s/it]                                                         70%|   | 4554/6500 [13:46:13<5:51:21, 10.83s/it] 70%|   | 4555/6500 [13:46:23<5:46:28, 10.69s/it]                                                         70%|   | 4555/6500 [13:46:23<5:46:28, 10.69s/it] 70%|{'loss': 0.3196, 'learning_rate': 2.0505230938586418e-05, 'epoch': 0.7}
{'loss': 0.3266, 'learning_rate': 2.048571474778882e-05, 'epoch': 0.7}
{'loss': 0.3165, 'learning_rate': 2.0466205455775934e-05, 'epoch': 0.7}
{'loss': 0.3257, 'learning_rate': 2.0446703067107947e-05, 'epoch': 0.7}
{'loss': 0.3404, 'learning_rate': 2.0427207586343432e-05, 'epoch': 0.7}
   | 4556/6500 [13:46:33<5:42:57, 10.59s/it]                                                         70%|   | 4556/6500 [13:46:33<5:42:57, 10.59s/it] 70%|   | 4557/6500 [13:46:44<5:40:16, 10.51s/it]                                                         70%|   | 4557/6500 [13:46:44<5:40:16, 10.51s/it] 70%|   | 4558/6500 [13:46:55<5:42:08, 10.57s/it]                                                         70%|   | 4558/6500 [13:46:55<5:42:08, 10.57s/it] 70%|   | 4559/6500 [13:47:05<5:39:39, 10.50s/it]                                                         70%|   | 4559/6500 [13:47:05<5:39:39, 10.50s/it] 70%|   | 4560/6500 [13:47:15<5:37:56, 10.45s/it]                                                         70%|   | 4560/6500 [13:47:15<5:37:56, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8656131029129028, 'eval_runtime': 3.9448, 'eval_samples_per_second': 5.83, 'eval_steps_per_second': 1.521, 'epoch': 0.7}
                                                         70%|   | 4560/6500 [13:47:19<5:37:56, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4560
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4560/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3185, 'learning_rate': 2.0407719018039345e-05, 'epoch': 0.7}
{'loss': 0.3333, 'learning_rate': 2.0388237366751006e-05, 'epoch': 0.7}
{'loss': 0.3194, 'learning_rate': 2.0368762637032144e-05, 'epoch': 0.7}
{'loss': 0.3221, 'learning_rate': 2.0349294833434855e-05, 'epoch': 0.7}
{'loss': 0.3174, 'learning_rate': 2.032983396050962e-05, 'epoch': 0.7}
 70%|   | 4561/6500 [13:47:30<6:20:50, 11.78s/it]                                                         70%|   | 4561/6500 [13:47:30<6:20:50, 11.78s/it] 70%|   | 4562/6500 [13:47:40<6:06:41, 11.35s/it]                                                         70%|   | 4562/6500 [13:47:40<6:06:41, 11.35s/it] 70%|   | 4563/6500 [13:47:51<5:57:06, 11.06s/it]                                                         70%|   | 4563/6500 [13:47:51<5:57:06, 11.06s/it] 70%|   | 4564/6500 [13:48:01<5:50:21, 10.86s/it]                                                         70%|   | 4564/6500 [13:48:01<5:50:21, 10.86s/it] 70%|   | 4565/6500 [13:48:12<5:45:29, 10.71s/it]                                                         70%|   | 4565/6500 [13:48:12<5:45:29, 10.71s/it] 70%|{'loss': 0.3435, 'learning_rate': 2.0310380022805298e-05, 'epoch': 0.7}
{'loss': 0.325, 'learning_rate': 2.029093302486913e-05, 'epoch': 0.7}
{'loss': 0.3228, 'learning_rate': 2.0271492971246753e-05, 'epoch': 0.7}
{'loss': 0.3119, 'learning_rate': 2.025205986648212e-05, 'epoch': 0.7}
{'loss': 0.3335, 'learning_rate': 2.0232633715117625e-05, 'epoch': 0.7}
   | 4566/6500 [13:48:22<5:42:06, 10.61s/it]                                                         70%|   | 4566/6500 [13:48:22<5:42:06, 10.61s/it] 70%|   | 4567/6500 [13:48:32<5:39:32, 10.54s/it]                                                         70%|   | 4567/6500 [13:48:32<5:39:32, 10.54s/it] 70%|   | 4568/6500 [13:48:43<5:37:46, 10.49s/it]                                                         70%|   | 4568/6500 [13:48:43<5:37:46, 10.49s/it] 70%|   | 4569/6500 [13:48:53<5:36:21, 10.45s/it]                                                         70%|   | 4569/6500 [13:48:53<5:36:21, 10.45s/it] 70%|   | 4570/6500 [13:49:03<5:35:16, 10.42s/it]                                                         70%|   | 4570/6500 [13:49:03<5:35:16, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8589733242988586, 'eval_runtime': 3.9598, 'eval_samples_per_second': 5.808, 'eval_steps_per_second': 1.515, 'epoch': 0.7}
                                                         70%|   | 4570/6500 [13:49:07<5:35:16, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.382, 'learning_rate': 2.0213214521694006e-05, 'epoch': 0.7}
{'loss': 0.328, 'learning_rate': 2.019380229075039e-05, 'epoch': 0.7}
{'loss': 0.3065, 'learning_rate': 2.0174397026824266e-05, 'epoch': 0.7}
{'loss': 0.3339, 'learning_rate': 2.0154998734451474e-05, 'epoch': 0.7}
{'loss': 0.8394, 'learning_rate': 2.0135607418166285e-05, 'epoch': 0.7}
 70%|   | 4571/6500 [13:49:18<6:17:54, 11.75s/it]                                                         70%|   | 4571/6500 [13:49:18<6:17:54, 11.75s/it] 70%|   | 4572/6500 [13:49:29<6:04:17, 11.34s/it]                                                         70%|   | 4572/6500 [13:49:29<6:04:17, 11.34s/it] 70%|   | 4573/6500 [13:49:39<5:54:42, 11.04s/it]                                                         70%|   | 4573/6500 [13:49:39<5:54:42, 11.04s/it] 70%|   | 4574/6500 [13:49:50<5:50:08, 10.91s/it]                                                         70%|   | 4574/6500 [13:49:50<5:50:08, 10.91s/it] 70%|   | 4575/6500 [13:50:00<5:44:40, 10.74s/it]                                                         70%|   | 4575/6500 [13:50:00<5:44:40, 10.74s/it] 70%|{'loss': 0.3382, 'learning_rate': 2.0116223082501285e-05, 'epoch': 0.7}
{'loss': 0.3398, 'learning_rate': 2.0096845731987462e-05, 'epoch': 0.7}
{'loss': 0.3182, 'learning_rate': 2.0077475371154114e-05, 'epoch': 0.7}
{'loss': 0.3143, 'learning_rate': 2.0058112004528966e-05, 'epoch': 0.7}
{'loss': 0.3477, 'learning_rate': 2.003875563663809e-05, 'epoch': 0.7}
   | 4576/6500 [13:50:10<5:40:51, 10.63s/it]                                                         70%|   | 4576/6500 [13:50:10<5:40:51, 10.63s/it] 70%|   | 4577/6500 [13:50:21<5:38:05, 10.55s/it]                                                         70%|   | 4577/6500 [13:50:21<5:38:05, 10.55s/it] 70%|   | 4578/6500 [13:50:31<5:36:03, 10.49s/it]                                                         70%|   | 4578/6500 [13:50:31<5:36:03, 10.49s/it] 70%|   | 4579/6500 [13:50:41<5:34:36, 10.45s/it]                                                         70%|   | 4579/6500 [13:50:41<5:34:36, 10.45s/it] 70%|   | 4580/6500 [13:50:52<5:33:37, 10.43s/it]                                                         70%|   | 4580/6500 [13:50:52<5:33:37, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8648234009742737, 'eval_runtime': 4.1932, 'eval_samples_per_second': 5.485, 'eval_steps_per_second': 1.431, 'epoch': 0.7}
                                                         70%|   | 4580/6500 [13:50:56<5:33:37, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4580
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3115, 'learning_rate': 2.0019406272005915e-05, 'epoch': 0.7}
{'loss': 0.3173, 'learning_rate': 2.0000063915155237e-05, 'epoch': 0.7}
{'loss': 0.3213, 'learning_rate': 1.998072857060722e-05, 'epoch': 0.71}
{'loss': 0.3252, 'learning_rate': 1.996140024288138e-05, 'epoch': 0.71}
{'loss': 0.3124, 'learning_rate': 1.994207893649559e-05, 'epoch': 0.71}
 70%|   | 4581/6500 [13:51:07<6:17:24, 11.80s/it]                                                         70%|   | 4581/6500 [13:51:07<6:17:24, 11.80s/it] 70%|   | 4582/6500 [13:51:17<6:03:17, 11.36s/it]                                                         70%|   | 4582/6500 [13:51:17<6:03:17, 11.36s/it] 71%|   | 4583/6500 [13:51:27<5:53:16, 11.06s/it]                                                         71%|   | 4583/6500 [13:51:27<5:53:16, 11.06s/it] 71%|   | 4584/6500 [13:51:38<5:46:21, 10.85s/it]                                                         71%|   | 4584/6500 [13:51:38<5:46:21, 10.85s/it] 71%|   | 4585/6500 [13:51:48<5:41:33, 10.70s/it]                                                         71%|   | 4585/6500 [13:51:48<5:41:33, 10.70s/it] 71%|{'loss': 0.3207, 'learning_rate': 1.99227646559661e-05, 'epoch': 0.71}
{'loss': 0.3416, 'learning_rate': 1.990345740580749e-05, 'epoch': 0.71}
{'loss': 0.3172, 'learning_rate': 1.9884157190532747e-05, 'epoch': 0.71}
{'loss': 0.3368, 'learning_rate': 1.9864864014653133e-05, 'epoch': 0.71}
{'loss': 0.3369, 'learning_rate': 1.9845577882678336e-05, 'epoch': 0.71}
   | 4586/6500 [13:51:59<5:38:04, 10.60s/it]                                                         71%|   | 4586/6500 [13:51:59<5:38:04, 10.60s/it] 71%|   | 4587/6500 [13:52:09<5:35:33, 10.52s/it]                                                         71%|   | 4587/6500 [13:52:09<5:35:33, 10.52s/it] 71%|   | 4588/6500 [13:52:19<5:33:47, 10.47s/it]                                                         71%|   | 4588/6500 [13:52:19<5:33:47, 10.47s/it] 71%|   | 4589/6500 [13:52:30<5:32:29, 10.44s/it]                                                         71%|   | 4589/6500 [13:52:30<5:32:29, 10.44s/it] 71%|   | 4590/6500 [13:52:40<5:35:23, 10.54s/it]                                                         71%|   | 4590/6500 [13:52:40<5:35:23, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8654009699821472, 'eval_runtime': 3.9652, 'eval_samples_per_second': 5.8, 'eval_steps_per_second': 1.513, 'epoch': 0.71}
                                                         71%|   | 4590/6500 [13:52:44<5:35:23, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4590
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4590
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4590/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4590/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4590/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4590/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3271, 'learning_rate': 1.982629879911636e-05, 'epoch': 0.71}
{'loss': 0.3346, 'learning_rate': 1.980702676847358e-05, 'epoch': 0.71}
{'loss': 0.3312, 'learning_rate': 1.978776179525472e-05, 'epoch': 0.71}
{'loss': 0.3288, 'learning_rate': 1.9768503883962846e-05, 'epoch': 0.71}
{'loss': 0.3288, 'learning_rate': 1.974925303909938e-05, 'epoch': 0.71}
 71%|   | 4591/6500 [13:52:55<6:16:24, 11.83s/it]                                                         71%|   | 4591/6500 [13:52:55<6:16:24, 11.83s/it] 71%|   | 4592/6500 [13:53:06<6:02:06, 11.39s/it]                                                         71%|   | 4592/6500 [13:53:06<6:02:06, 11.39s/it] 71%|   | 4593/6500 [13:53:16<5:52:04, 11.08s/it]                                                         71%|   | 4593/6500 [13:53:16<5:52:04, 11.08s/it] 71%|   | 4594/6500 [13:53:26<5:44:56, 10.86s/it]                                                         71%|   | 4594/6500 [13:53:26<5:44:56, 10.86s/it] 71%|   | 4595/6500 [13:53:37<5:40:00, 10.71s/it]                                                         71%|   | 4595/6500 [13:53:37<5:40:00, 10.71s/it] 71%|{'loss': 0.34, 'learning_rate': 1.973000926516409e-05, 'epoch': 0.71}
{'loss': 0.3073, 'learning_rate': 1.971077256665509e-05, 'epoch': 0.71}
{'loss': 0.3203, 'learning_rate': 1.9691542948068837e-05, 'epoch': 0.71}
{'loss': 0.3003, 'learning_rate': 1.967232041390016e-05, 'epoch': 0.71}
{'loss': 0.3537, 'learning_rate': 1.9653104968642173e-05, 'epoch': 0.71}
   | 4596/6500 [13:53:47<5:36:27, 10.60s/it]                                                         71%|   | 4596/6500 [13:53:47<5:36:27, 10.60s/it] 71%|   | 4597/6500 [13:53:57<5:33:52, 10.53s/it]                                                         71%|   | 4597/6500 [13:53:57<5:33:52, 10.53s/it] 71%|   | 4598/6500 [13:54:08<5:32:12, 10.48s/it]                                                         71%|   | 4598/6500 [13:54:08<5:32:12, 10.48s/it] 71%|   | 4599/6500 [13:54:18<5:30:47, 10.44s/it]                                                         71%|   | 4599/6500 [13:54:18<5:30:47, 10.44s/it] 71%|   | 4600/6500 [13:54:28<5:29:51, 10.42s/it]                                                         71%|   | 4600/6500 [13:54:28<5:29:51, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8592895865440369, 'eval_runtime': 3.9589, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.71}
                                                         71%|   | 4600/6500 [13:54:32<5:29:51, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4600/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4600/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4600/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3748, 'learning_rate': 1.963389661678639e-05, 'epoch': 0.71}
{'loss': 0.3304, 'learning_rate': 1.961469536282264e-05, 'epoch': 0.71}
{'loss': 0.3353, 'learning_rate': 1.9595501211239102e-05, 'epoch': 0.71}
{'loss': 0.3286, 'learning_rate': 1.9576314166522303e-05, 'epoch': 0.71}
{'loss': 0.846, 'learning_rate': 1.955713423315708e-05, 'epoch': 0.71}
 71%|   | 4601/6500 [13:54:43<6:11:56, 11.75s/it]                                                         71%|   | 4601/6500 [13:54:43<6:11:56, 11.75s/it] 71%|   | 4602/6500 [13:54:54<5:58:50, 11.34s/it]                                                         71%|   | 4602/6500 [13:54:54<5:58:50, 11.34s/it] 71%|   | 4603/6500 [13:55:04<5:49:07, 11.04s/it]                                                         71%|   | 4603/6500 [13:55:04<5:49:07, 11.04s/it] 71%|   | 4604/6500 [13:55:14<5:42:15, 10.83s/it]                                                         71%|   | 4604/6500 [13:55:14<5:42:15, 10.83s/it] 71%|   | 4605/6500 [13:55:25<5:37:25, 10.68s/it]                                                         71%|   | 4605/6500 [13:55:25<5:37:25, 10.68s/it] 71%|{'loss': 0.3378, 'learning_rate': 1.9537961415626638e-05, 'epoch': 0.71}
{'loss': 0.331, 'learning_rate': 1.9518795718412502e-05, 'epoch': 0.71}
{'loss': 0.3032, 'learning_rate': 1.9499637145994538e-05, 'epoch': 0.71}
{'loss': 0.3314, 'learning_rate': 1.948048570285094e-05, 'epoch': 0.71}
{'loss': 0.3221, 'learning_rate': 1.9461341393458254e-05, 'epoch': 0.71}
   | 4606/6500 [13:55:36<5:40:15, 10.78s/it]                                                         71%|   | 4606/6500 [13:55:36<5:40:15, 10.78s/it] 71%|   | 4607/6500 [13:55:46<5:35:58, 10.65s/it]                                                         71%|   | 4607/6500 [13:55:46<5:35:58, 10.65s/it] 71%|   | 4608/6500 [13:55:56<5:32:49, 10.55s/it]                                                         71%|   | 4608/6500 [13:55:56<5:32:49, 10.55s/it] 71%|   | 4609/6500 [13:56:07<5:30:31, 10.49s/it]                                                         71%|   | 4609/6500 [13:56:07<5:30:31, 10.49s/it] 71%|   | 4610/6500 [13:56:17<5:28:42, 10.43s/it]                                                         71%|   | 4610/6500 [13:56:17<5:28:42, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8648437857627869, 'eval_runtime': 3.9371, 'eval_samples_per_second': 5.842, 'eval_steps_per_second': 1.524, 'epoch': 0.71}
                                                         71%|   | 4610/6500 [13:56:21<5:28:42, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4610
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4610/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2963, 'learning_rate': 1.9442204222291362e-05, 'epoch': 0.71}
{'loss': 0.3127, 'learning_rate': 1.942307419382341e-05, 'epoch': 0.71}
{'loss': 0.3136, 'learning_rate': 1.9403951312525957e-05, 'epoch': 0.71}
{'loss': 0.3225, 'learning_rate': 1.938483558286886e-05, 'epoch': 0.71}
{'loss': 0.3253, 'learning_rate': 1.9365727009320307e-05, 'epoch': 0.71}
 71%|   | 4611/6500 [13:56:32<6:10:43, 11.78s/it]                                                         71%|   | 4611/6500 [13:56:32<6:10:43, 11.78s/it] 71%|   | 4612/6500 [13:56:42<5:56:52, 11.34s/it]                                                         71%|   | 4612/6500 [13:56:42<5:56:52, 11.34s/it] 71%|   | 4613/6500 [13:56:53<5:47:16, 11.04s/it]                                                         71%|   | 4613/6500 [13:56:53<5:47:16, 11.04s/it] 71%|   | 4614/6500 [13:57:03<5:40:22, 10.83s/it]                                                         71%|   | 4614/6500 [13:57:03<5:40:22, 10.83s/it] 71%|   | 4615/6500 [13:57:13<5:35:35, 10.68s/it]                                                         71%|   | 4615/6500 [13:57:13<5:35:35, 10.68s/it] 71%|{'loss': 0.3168, 'learning_rate': 1.9346625596346794e-05, 'epoch': 0.71}
{'loss': 0.3258, 'learning_rate': 1.9327531348413185e-05, 'epoch': 0.71}
{'loss': 0.3222, 'learning_rate': 1.9308444269982624e-05, 'epoch': 0.71}
{'loss': 0.3392, 'learning_rate': 1.928936436551661e-05, 'epoch': 0.71}
{'loss': 0.3153, 'learning_rate': 1.9270291639474953e-05, 'epoch': 0.71}
   | 4616/6500 [13:57:24<5:32:11, 10.58s/it]                                                         71%|   | 4616/6500 [13:57:24<5:32:11, 10.58s/it] 71%|   | 4617/6500 [13:57:34<5:29:45, 10.51s/it]                                                         71%|   | 4617/6500 [13:57:34<5:29:45, 10.51s/it] 71%|   | 4618/6500 [13:57:44<5:27:53, 10.45s/it]                                                         71%|   | 4618/6500 [13:57:44<5:27:53, 10.45s/it] 71%|   | 4619/6500 [13:57:55<5:26:35, 10.42s/it]                                                         71%|   | 4619/6500 [13:57:55<5:26:35, 10.42s/it] 71%|   | 4620/6500 [13:58:05<5:25:35, 10.39s/it]                                                         71%|   | 4620/6500 [13:58:05<5:25:35, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8655244708061218, 'eval_runtime': 3.9401, 'eval_samples_per_second': 5.837, 'eval_steps_per_second': 1.523, 'epoch': 0.71}
                                                         71%|   | 4620/6500 [13:58:09<5:25:35, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3281, 'learning_rate': 1.9251226096315782e-05, 'epoch': 0.71}
{'loss': 0.3339, 'learning_rate': 1.923216774049557e-05, 'epoch': 0.71}
{'loss': 0.3137, 'learning_rate': 1.921311657646907e-05, 'epoch': 0.71}
{'loss': 0.3284, 'learning_rate': 1.919407260868937e-05, 'epoch': 0.71}
{'loss': 0.3385, 'learning_rate': 1.917503584160791e-05, 'epoch': 0.71}
 71%|   | 4621/6500 [13:58:20<6:07:02, 11.72s/it]                                                         71%|   | 4621/6500 [13:58:20<6:07:02, 11.72s/it] 71%|   | 4622/6500 [13:58:30<5:57:00, 11.41s/it]                                                         71%|   | 4622/6500 [13:58:30<5:57:00, 11.41s/it] 71%|   | 4623/6500 [13:58:41<5:47:36, 11.11s/it]                                                         71%|   | 4623/6500 [13:58:41<5:47:36, 11.11s/it] 71%|   | 4624/6500 [13:58:51<5:40:47, 10.90s/it]                                                         71%|   | 4624/6500 [13:58:51<5:40:47, 10.90s/it] 71%|   | 4625/6500 [13:59:02<5:35:55, 10.75s/it]                                                         71%|   | 4625/6500 [13:59:02<5:35:55, 10.75s/it] 71%|{'loss': 0.3227, 'learning_rate': 1.9156006279674393e-05, 'epoch': 0.71}
{'loss': 0.3152, 'learning_rate': 1.913698392733687e-05, 'epoch': 0.71}
{'loss': 0.3186, 'learning_rate': 1.9117968789041712e-05, 'epoch': 0.71}
{'loss': 0.3094, 'learning_rate': 1.9098960869233584e-05, 'epoch': 0.71}
{'loss': 0.4039, 'learning_rate': 1.9079960172355464e-05, 'epoch': 0.71}
   | 4626/6500 [13:59:12<5:32:35, 10.65s/it]                                                         71%|   | 4626/6500 [13:59:12<5:32:35, 10.65s/it] 71%|   | 4627/6500 [13:59:22<5:30:13, 10.58s/it]                                                         71%|   | 4627/6500 [13:59:22<5:30:13, 10.58s/it] 71%|   | 4628/6500 [13:59:33<5:28:29, 10.53s/it]                                                         71%|   | 4628/6500 [13:59:33<5:28:29, 10.53s/it] 71%|   | 4629/6500 [13:59:43<5:27:28, 10.50s/it]                                                         71%|   | 4629/6500 [13:59:43<5:27:28, 10.50s/it] 71%|   | 4630/6500 [13:59:54<5:26:26, 10.47s/it]                                                         71%|   | 4630/6500 [13:59:54<5:26:26, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8594408631324768, 'eval_runtime': 3.9734, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.71}
                                                         71%|   | 4630/6500 [13:59:58<5:26:26, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3166, 'learning_rate': 1.9060966702848664e-05, 'epoch': 0.71}
{'loss': 0.3114, 'learning_rate': 1.904198046515278e-05, 'epoch': 0.71}
{'loss': 0.3354, 'learning_rate': 1.9023001463705754e-05, 'epoch': 0.71}
{'loss': 0.8387, 'learning_rate': 1.9004029702943775e-05, 'epoch': 0.71}
{'loss': 0.3276, 'learning_rate': 1.8985065187301394e-05, 'epoch': 0.71}
 71%|   | 4631/6500 [14:00:09<6:08:07, 11.82s/it]                                                         71%|   | 4631/6500 [14:00:09<6:08:07, 11.82s/it] 71%|  | 4632/6500 [14:00:19<5:54:42, 11.39s/it]                                                         71%|  | 4632/6500 [14:00:19<5:54:42, 11.39s/it] 71%|  | 4633/6500 [14:00:29<5:45:09, 11.09s/it]                                                         71%|  | 4633/6500 [14:00:29<5:45:09, 11.09s/it] 71%|  | 4634/6500 [14:00:40<5:38:26, 10.88s/it]                                                         71%|  | 4634/6500 [14:00:40<5:38:26, 10.88s/it] 71%|  | 4635/6500 [14:00:50<5:33:46, 10.74s/it]                                                         71%|  | 4635/6500 [14:00:50<5:33:46, 10.74s/it] 71%|{'loss': 0.3266, 'learning_rate': 1.896610792121145e-05, 'epoch': 0.71}
{'loss': 0.3295, 'learning_rate': 1.8947157909105097e-05, 'epoch': 0.71}
{'loss': 0.3092, 'learning_rate': 1.8928215155411773e-05, 'epoch': 0.71}
{'loss': 0.3384, 'learning_rate': 1.8909279664559236e-05, 'epoch': 0.71}
{'loss': 0.3095, 'learning_rate': 1.8890351440973542e-05, 'epoch': 0.71}
  | 4636/6500 [14:01:01<5:30:27, 10.64s/it]                                                         71%|  | 4636/6500 [14:01:01<5:30:27, 10.64s/it] 71%|  | 4637/6500 [14:01:11<5:28:10, 10.57s/it]                                                         71%|  | 4637/6500 [14:01:11<5:28:10, 10.57s/it] 71%|  | 4638/6500 [14:01:21<5:26:25, 10.52s/it]                                                         71%|  | 4638/6500 [14:01:21<5:26:25, 10.52s/it] 71%|  | 4639/6500 [14:01:32<5:28:43, 10.60s/it]                                                         71%|  | 4639/6500 [14:01:32<5:28:43, 10.60s/it] 71%|  | 4640/6500 [14:01:43<5:26:39, 10.54s/it]                                                         71%|  | 4640/6500 [14:01:43<5:26:39, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8717530965805054, 'eval_runtime': 4.1869, 'eval_samples_per_second': 5.493, 'eval_steps_per_second': 1.433, 'epoch': 0.71}
                                                         71%|  | 4640/6500 [14:01:47<5:26:39, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4640I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3053, 'learning_rate': 1.8871430489079052e-05, 'epoch': 0.71}
{'loss': 0.3166, 'learning_rate': 1.885251681329842e-05, 'epoch': 0.71}
{'loss': 0.3168, 'learning_rate': 1.8833610418052606e-05, 'epoch': 0.71}
{'loss': 0.3245, 'learning_rate': 1.8814711307760875e-05, 'epoch': 0.71}
{'loss': 0.3166, 'learning_rate': 1.8795819486840747e-05, 'epoch': 0.71}
 71%|  | 4641/6500 [14:01:58<6:08:41, 11.90s/it]                                                         71%|  | 4641/6500 [14:01:58<6:08:41, 11.90s/it] 71%|  | 4642/6500 [14:02:08<5:54:22, 11.44s/it]                                                         71%|  | 4642/6500 [14:02:08<5:54:22, 11.44s/it] 71%|  | 4643/6500 [14:02:18<5:44:13, 11.12s/it]                                                         71%|  | 4643/6500 [14:02:18<5:44:13, 11.12s/it] 71%|  | 4644/6500 [14:02:29<5:37:09, 10.90s/it]                                                         71%|  | 4644/6500 [14:02:29<5:37:09, 10.90s/it] 71%|  | 4645/6500 [14:02:39<5:32:17, 10.75s/it]                                                         71%|  | 4645/6500 [14:02:39<5:32:17, 10.75s/it] 71{'loss': 0.3314, 'learning_rate': 1.877693495970809e-05, 'epoch': 0.71}
{'loss': 0.3163, 'learning_rate': 1.875805773077705e-05, 'epoch': 0.71}
{'loss': 0.3313, 'learning_rate': 1.873918780446006e-05, 'epoch': 0.72}
{'loss': 0.3238, 'learning_rate': 1.8720325185167836e-05, 'epoch': 0.72}
{'loss': 0.3187, 'learning_rate': 1.870146987730943e-05, 'epoch': 0.72}
%|  | 4646/6500 [14:02:50<5:28:39, 10.64s/it]                                                         71%|  | 4646/6500 [14:02:50<5:28:39, 10.64s/it] 71%|  | 4647/6500 [14:03:00<5:26:13, 10.56s/it]                                                         71%|  | 4647/6500 [14:03:00<5:26:13, 10.56s/it] 72%|  | 4648/6500 [14:03:10<5:24:23, 10.51s/it]                                                         72%|  | 4648/6500 [14:03:10<5:24:23, 10.51s/it] 72%|  | 4649/6500 [14:03:21<5:22:57, 10.47s/it]                                                         72%|  | 4649/6500 [14:03:21<5:22:57, 10.47s/it] 72%|  | 4650/6500 [14:03:31<5:22:00, 10.44s/it]                                                         72%|  | 4650/6500 [14:03:31<5:22:00, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8649644255638123, 'eval_runtime': 3.9475, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.72}
                                                         72%|  | 4650/6500 [14:03:35<5:22:00, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4650
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4650/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3387, 'learning_rate': 1.8682621885292145e-05, 'epoch': 0.72}
{'loss': 0.3284, 'learning_rate': 1.866378121352158e-05, 'epoch': 0.72}
{'loss': 0.3346, 'learning_rate': 1.8644947866401653e-05, 'epoch': 0.72}
{'loss': 0.3119, 'learning_rate': 1.8626121848334493e-05, 'epoch': 0.72}
{'loss': 0.3471, 'learning_rate': 1.8607303163720602e-05, 'epoch': 0.72}
 72%|  | 4651/6500 [14:03:46<6:02:30, 11.76s/it]                                                         72%|  | 4651/6500 [14:03:46<6:02:30, 11.76s/it] 72%|  | 4652/6500 [14:03:56<5:49:42, 11.35s/it]                                                         72%|  | 4652/6500 [14:03:56<5:49:42, 11.35s/it] 72%|  | 4653/6500 [14:04:07<5:40:38, 11.07s/it]                                                         72%|  | 4653/6500 [14:04:07<5:40:38, 11.07s/it] 72%|  | 4654/6500 [14:04:17<5:34:12, 10.86s/it]                                                         72%|  | 4654/6500 [14:04:17<5:34:12, 10.86s/it] 72%|  | 4655/6500 [14:04:28<5:32:00, 10.80s/it]                                                         72%|  | 4655/6500 [14:04:28<5:32:00, 10.80s/it] 72{'loss': 0.3197, 'learning_rate': 1.858849181695872e-05, 'epoch': 0.72}
{'loss': 0.3068, 'learning_rate': 1.8569687812445896e-05, 'epoch': 0.72}
{'loss': 0.3036, 'learning_rate': 1.8550891154577442e-05, 'epoch': 0.72}
{'loss': 0.3377, 'learning_rate': 1.853210184774697e-05, 'epoch': 0.72}
{'loss': 0.384, 'learning_rate': 1.8513319896346358e-05, 'epoch': 0.72}
%|  | 4656/6500 [14:04:38<5:27:50, 10.67s/it]                                                         72%|  | 4656/6500 [14:04:38<5:27:50, 10.67s/it] 72%|  | 4657/6500 [14:04:49<5:25:08, 10.59s/it]                                                         72%|  | 4657/6500 [14:04:49<5:25:08, 10.59s/it] 72%|  | 4658/6500 [14:04:59<5:23:06, 10.52s/it]                                                         72%|  | 4658/6500 [14:04:59<5:23:06, 10.52s/it] 72%|  | 4659/6500 [14:05:09<5:21:31, 10.48s/it]                                                         72%|  | 4659/6500 [14:05:09<5:21:31, 10.48s/it] 72%|  | 4660/6500 [14:05:20<5:20:38, 10.46s/it]                                                         72%|  | 4660/6500 [14:05:20<5:20:38, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.859749436378479, 'eval_runtime': 3.9483, 'eval_samples_per_second': 5.825, 'eval_steps_per_second': 1.52, 'epoch': 0.72}
                                                         72%|  | 4660/6500 [14:05:24<5:20:38, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4660/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4660/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3138, 'learning_rate': 1.8494545304765785e-05, 'epoch': 0.72}
{'loss': 0.3139, 'learning_rate': 1.8475778077393685e-05, 'epoch': 0.72}
{'loss': 0.3299, 'learning_rate': 1.8457018218616794e-05, 'epoch': 0.72}
{'loss': 0.8386, 'learning_rate': 1.8438265732820126e-05, 'epoch': 0.72}
{'loss': 0.3377, 'learning_rate': 1.8419520624386925e-05, 'epoch': 0.72}
 72%|  | 4661/6500 [14:05:35<6:01:07, 11.78s/it]                                                         72%|  | 4661/6500 [14:05:35<6:01:07, 11.78s/it] 72%|  | 4662/6500 [14:05:45<5:47:58, 11.36s/it]                                                         72%|  | 4662/6500 [14:05:45<5:47:58, 11.36s/it] 72%|  | 4663/6500 [14:05:55<5:38:44, 11.06s/it]                                                         72%|  | 4663/6500 [14:05:55<5:38:44, 11.06s/it] 72%|  | 4664/6500 [14:06:06<5:32:05, 10.85s/it]                                                         72%|  | 4664/6500 [14:06:06<5:32:05, 10.85s/it] 72%|  | 4665/6500 [14:06:16<5:27:38, 10.71s/it]                                                         72%|  | 4665/6500 [14:06:16<5:27:38, 10.71s/it] 72{'loss': 0.3192, 'learning_rate': 1.8400782897698764e-05, 'epoch': 0.72}
{'loss': 0.3159, 'learning_rate': 1.838205255713548e-05, 'epoch': 0.72}
{'loss': 0.3122, 'learning_rate': 1.8363329607075168e-05, 'epoch': 0.72}
{'loss': 0.3394, 'learning_rate': 1.8344614051894204e-05, 'epoch': 0.72}
{'loss': 0.3079, 'learning_rate': 1.8325905895967237e-05, 'epoch': 0.72}
%|  | 4666/6500 [14:06:26<5:24:22, 10.61s/it]                                                         72%|  | 4666/6500 [14:06:26<5:24:22, 10.61s/it] 72%|  | 4667/6500 [14:06:37<5:22:00, 10.54s/it]                                                         72%|  | 4667/6500 [14:06:37<5:22:00, 10.54s/it] 72%|  | 4668/6500 [14:06:47<5:20:24, 10.49s/it]                                                         72%|  | 4668/6500 [14:06:47<5:20:24, 10.49s/it] 72%|  | 4669/6500 [14:06:58<5:19:23, 10.47s/it]                                                         72%|  | 4669/6500 [14:06:58<5:19:23, 10.47s/it] 72%|  | 4670/6500 [14:07:08<5:18:28, 10.44s/it]                                                         72%|  | 4670/6500 [14:07:08<5:18:28, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8703429102897644, 'eval_runtime': 3.9345, 'eval_samples_per_second': 5.846, 'eval_steps_per_second': 1.525, 'epoch': 0.72}
                                                         72%|  | 4670/6500 [14:07:12<5:18:28, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4670
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4670/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3185, 'learning_rate': 1.830720514366719e-05, 'epoch': 0.72}
{'loss': 0.3208, 'learning_rate': 1.8288511799365243e-05, 'epoch': 0.72}
{'loss': 0.3236, 'learning_rate': 1.826982586743085e-05, 'epoch': 0.72}
{'loss': 0.3321, 'learning_rate': 1.8251147352231747e-05, 'epoch': 0.72}
{'loss': 0.3297, 'learning_rate': 1.8232476258133924e-05, 'epoch': 0.72}
 72%|  | 4671/6500 [14:07:23<6:04:03, 11.94s/it]                                                         72%|  | 4671/6500 [14:07:23<6:04:03, 11.94s/it] 72%|  | 4672/6500 [14:07:34<5:49:35, 11.47s/it]                                                         72%|  | 4672/6500 [14:07:34<5:49:35, 11.47s/it] 72%|  | 4673/6500 [14:07:44<5:39:25, 11.15s/it]                                                         72%|  | 4673/6500 [14:07:44<5:39:25, 11.15s/it] 72%|  | 4674/6500 [14:07:55<5:32:15, 10.92s/it]                                                         72%|  | 4674/6500 [14:07:55<5:32:15, 10.92s/it] 72%|  | 4675/6500 [14:08:05<5:27:05, 10.75s/it]                                                         72%|  | 4675/6500 [14:08:05<5:27:05, 10.75s/it] 72{'loss': 0.3352, 'learning_rate': 1.821381258950161e-05, 'epoch': 0.72}
{'loss': 0.3235, 'learning_rate': 1.819515635069734e-05, 'epoch': 0.72}
{'loss': 0.3415, 'learning_rate': 1.8176507546081894e-05, 'epoch': 0.72}
{'loss': 0.3293, 'learning_rate': 1.8157866180014327e-05, 'epoch': 0.72}
{'loss': 0.3227, 'learning_rate': 1.8139232256851928e-05, 'epoch': 0.72}
%|  | 4676/6500 [14:08:15<5:23:24, 10.64s/it]                                                         72%|  | 4676/6500 [14:08:15<5:23:24, 10.64s/it] 72%|  | 4677/6500 [14:08:26<5:20:47, 10.56s/it]                                                         72%|  | 4677/6500 [14:08:26<5:20:47, 10.56s/it] 72%|  | 4678/6500 [14:08:36<5:18:57, 10.50s/it]                                                         72%|  | 4678/6500 [14:08:36<5:18:57, 10.50s/it] 72%|  | 4679/6500 [14:08:47<5:17:42, 10.47s/it]                                                         72%|  | 4679/6500 [14:08:47<5:17:42, 10.47s/it] 72%|  | 4680/6500 [14:08:57<5:16:59, 10.45s/it]                                                         72%|  | 4680/6500 [14:08:57<5:16:59, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.863594114780426, 'eval_runtime': 3.9512, 'eval_samples_per_second': 5.821, 'eval_steps_per_second': 1.519, 'epoch': 0.72}
                                                         72%|  | 4680/6500 [14:09:01<5:16:59, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4680/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4680/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3378, 'learning_rate': 1.8120605780950277e-05, 'epoch': 0.72}
{'loss': 0.329, 'learning_rate': 1.8101986756663197e-05, 'epoch': 0.72}
{'loss': 0.3308, 'learning_rate': 1.8083375188342765e-05, 'epoch': 0.72}
{'loss': 0.3274, 'learning_rate': 1.8064771080339328e-05, 'epoch': 0.72}
{'loss': 0.34, 'learning_rate': 1.8046174437001484e-05, 'epoch': 0.72}
 72%|  | 4681/6500 [14:09:12<5:56:17, 11.75s/it]                                                         72%|  | 4681/6500 [14:09:12<5:56:17, 11.75s/it] 72%|  | 4682/6500 [14:09:22<5:43:32, 11.34s/it]                                                         72%|  | 4682/6500 [14:09:22<5:43:32, 11.34s/it] 72%|  | 4683/6500 [14:09:32<5:34:32, 11.05s/it]                                                         72%|  | 4683/6500 [14:09:32<5:34:32, 11.05s/it] 72%|  | 4684/6500 [14:09:43<5:28:05, 10.84s/it]                                                         72%|  | 4684/6500 [14:09:43<5:28:05, 10.84s/it] 72%|  | 4685/6500 [14:09:53<5:24:30, 10.73s/it]                                                         72%|  | 4685/6500 [14:09:53<5:24:30, 10.73s/it] 72{'loss': 0.3115, 'learning_rate': 1.8027585262676093e-05, 'epoch': 0.72}
{'loss': 0.3245, 'learning_rate': 1.8009003561708244e-05, 'epoch': 0.72}
{'loss': 0.2942, 'learning_rate': 1.7990429338441293e-05, 'epoch': 0.72}
{'loss': 0.3541, 'learning_rate': 1.7971862597216872e-05, 'epoch': 0.72}
{'loss': 0.3661, 'learning_rate': 1.7953303342374832e-05, 'epoch': 0.72}
%|  | 4686/6500 [14:10:04<5:21:04, 10.62s/it]                                                         72%|  | 4686/6500 [14:10:04<5:21:04, 10.62s/it] 72%|  | 4687/6500 [14:10:14<5:20:50, 10.62s/it]                                                         72%|  | 4687/6500 [14:10:14<5:20:50, 10.62s/it] 72%|  | 4688/6500 [14:10:25<5:18:25, 10.54s/it]                                                         72%|  | 4688/6500 [14:10:25<5:18:25, 10.54s/it] 72%|  | 4689/6500 [14:10:35<5:16:42, 10.49s/it]                                                         72%|  | 4689/6500 [14:10:35<5:16:42, 10.49s/it] 72%|  | 4690/6500 [14:10:45<5:15:32, 10.46s/it]                                                         72%|  | 4690/6500 [14:10:45<5:15:32, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8594657778739929, 'eval_runtime': 3.9473, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.72}
                                                         72%|  | 4690/6500 [14:10:49<5:15:32, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3141, 'learning_rate': 1.793475157825329e-05, 'epoch': 0.72}
{'loss': 0.3331, 'learning_rate': 1.7916207309188604e-05, 'epoch': 0.72}
{'loss': 0.3236, 'learning_rate': 1.7897670539515387e-05, 'epoch': 0.72}
{'loss': 0.8504, 'learning_rate': 1.7879141273566497e-05, 'epoch': 0.72}
{'loss': 0.3254, 'learning_rate': 1.7860619515673033e-05, 'epoch': 0.72}
 72%|  | 4691/6500 [14:11:00<5:55:24, 11.79s/it]                                                         72%|  | 4691/6500 [14:11:00<5:55:24, 11.79s/it] 72%|  | 4692/6500 [14:11:11<5:42:46, 11.38s/it]                                                         72%|  | 4692/6500 [14:11:11<5:42:46, 11.38s/it] 72%|  | 4693/6500 [14:11:21<5:33:50, 11.09s/it]                                                         72%|  | 4693/6500 [14:11:21<5:33:50, 11.09s/it] 72%|  | 4694/6500 [14:11:31<5:27:25, 10.88s/it]                                                         72%|  | 4694/6500 [14:11:31<5:27:25, 10.88s/it] 72%|  | 4695/6500 [14:11:42<5:22:51, 10.73s/it]                                                         72%|  | 4695/6500 [14:11:42<5:22:51, 10.73s/it] 72{'loss': 0.3302, 'learning_rate': 1.784210527016435e-05, 'epoch': 0.72}
{'loss': 0.3016, 'learning_rate': 1.7823598541368035e-05, 'epoch': 0.72}
{'loss': 0.3278, 'learning_rate': 1.7805099333609944e-05, 'epoch': 0.72}
{'loss': 0.326, 'learning_rate': 1.778660765121412e-05, 'epoch': 0.72}
{'loss': 0.284, 'learning_rate': 1.776812349850289e-05, 'epoch': 0.72}
%|  | 4696/6500 [14:11:52<5:19:44, 10.63s/it]                                                         72%|  | 4696/6500 [14:11:52<5:19:44, 10.63s/it] 72%|  | 4697/6500 [14:12:03<5:17:27, 10.56s/it]                                                         72%|  | 4697/6500 [14:12:03<5:17:27, 10.56s/it] 72%|  | 4698/6500 [14:12:13<5:15:57, 10.52s/it]                                                         72%|  | 4698/6500 [14:12:13<5:15:57, 10.52s/it] 72%|  | 4699/6500 [14:12:24<5:14:53, 10.49s/it]                                                         72%|  | 4699/6500 [14:12:24<5:14:53, 10.49s/it] 72%|  | 4700/6500 [14:12:34<5:13:54, 10.46s/it]                                                         72%|  | 4700/6500 [14:12:34<5:13:54, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8701381087303162, 'eval_runtime': 3.953, 'eval_samples_per_second': 5.818, 'eval_steps_per_second': 1.518, 'epoch': 0.72}
                                                         72%|  | 4700/6500 [14:12:38<5:13:54, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4700/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3267, 'learning_rate': 1.7749646879796828e-05, 'epoch': 0.72}
{'loss': 0.3046, 'learning_rate': 1.7731177799414718e-05, 'epoch': 0.72}
{'loss': 0.3213, 'learning_rate': 1.77127162616736e-05, 'epoch': 0.72}
{'loss': 0.3212, 'learning_rate': 1.7694262270888747e-05, 'epoch': 0.72}
{'loss': 0.3281, 'learning_rate': 1.7675815831373665e-05, 'epoch': 0.72}
 72%|  | 4701/6500 [14:12:49<5:53:34, 11.79s/it]                                                         72%|  | 4701/6500 [14:12:49<5:53:34, 11.79s/it] 72%|  | 4702/6500 [14:12:59<5:40:43, 11.37s/it]                                                         72%|  | 4702/6500 [14:12:59<5:40:43, 11.37s/it] 72%|  | 4703/6500 [14:13:10<5:34:47, 11.18s/it]                                                         72%|  | 4703/6500 [14:13:10<5:34:47, 11.18s/it] 72%|  | 4704/6500 [14:13:20<5:27:45, 10.95s/it]                                                         72%|  | 4704/6500 [14:13:20<5:27:45, 10.95s/it] 72%|  | 4705/6500 [14:13:31<5:22:42, 10.79s/it]                                                         72%|  | 4705/6500 [14:13:31<5:22:42, 10.79s/it] 72{'loss': 0.3233, 'learning_rate': 1.7657376947440103e-05, 'epoch': 0.72}
{'loss': 0.333, 'learning_rate': 1.7638945623398022e-05, 'epoch': 0.72}
{'loss': 0.3551, 'learning_rate': 1.7620521863555656e-05, 'epoch': 0.72}
{'loss': 0.3253, 'learning_rate': 1.7602105672219444e-05, 'epoch': 0.72}
{'loss': 0.3415, 'learning_rate': 1.7583697053694033e-05, 'epoch': 0.72}
%|  | 4706/6500 [14:13:41<5:19:14, 10.68s/it]                                                         72%|  | 4706/6500 [14:13:41<5:19:14, 10.68s/it] 72%|  | 4707/6500 [14:13:52<5:16:37, 10.60s/it]                                                         72%|  | 4707/6500 [14:13:52<5:16:37, 10.60s/it] 72%|  | 4708/6500 [14:14:02<5:14:43, 10.54s/it]                                                         72%|  | 4708/6500 [14:14:02<5:14:43, 10.54s/it] 72%|  | 4709/6500 [14:14:12<5:13:53, 10.52s/it]                                                         72%|  | 4709/6500 [14:14:12<5:13:53, 10.52s/it] 72%|  | 4710/6500 [14:14:23<5:12:52, 10.49s/it]                                                         72%|  | 4710/6500 [14:14:23<5:12:52, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8663173317909241, 'eval_runtime': 4.2651, 'eval_samples_per_second': 5.393, 'eval_steps_per_second': 1.407, 'epoch': 0.72}
                                                         72%|  | 4710/6500 [14:14:27<5:12:52, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4710/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3333, 'learning_rate': 1.756529601228234e-05, 'epoch': 0.72}
{'loss': 0.3187, 'learning_rate': 1.75469025522855e-05, 'epoch': 0.72}
{'loss': 0.3234, 'learning_rate': 1.7528516678002864e-05, 'epoch': 0.73}
{'loss': 0.3404, 'learning_rate': 1.7510138393732028e-05, 'epoch': 0.73}
{'loss': 0.3262, 'learning_rate': 1.7491767703768807e-05, 'epoch': 0.73}
 72%|  | 4711/6500 [14:14:38<5:55:11, 11.91s/it]                                                         72%|  | 4711/6500 [14:14:38<5:55:11, 11.91s/it] 72%|  | 4712/6500 [14:14:49<5:41:34, 11.46s/it]                                                         72%|  | 4712/6500 [14:14:49<5:41:34, 11.46s/it] 73%|  | 4713/6500 [14:14:59<5:32:20, 11.16s/it]                                                         73%|  | 4713/6500 [14:14:59<5:32:20, 11.16s/it] 73%|  | 4714/6500 [14:15:09<5:25:20, 10.93s/it]                                                         73%|  | 4714/6500 [14:15:09<5:25:20, 10.93s/it] 73%|  | 4715/6500 [14:15:20<5:20:28, 10.77s/it]                                                         73%|  | 4715/6500 [14:15:20<5:20:28, 10.77s/it] 73{'loss': 0.3116, 'learning_rate': 1.7473404612407225e-05, 'epoch': 0.73}
{'loss': 0.3163, 'learning_rate': 1.7455049123939554e-05, 'epoch': 0.73}
{'loss': 0.3369, 'learning_rate': 1.7436701242656272e-05, 'epoch': 0.73}
{'loss': 0.3881, 'learning_rate': 1.7418360972846088e-05, 'epoch': 0.73}
{'loss': 0.3155, 'learning_rate': 1.740002831879594e-05, 'epoch': 0.73}
%|  | 4716/6500 [14:15:30<5:16:58, 10.66s/it]                                                         73%|  | 4716/6500 [14:15:30<5:16:58, 10.66s/it] 73%|  | 4717/6500 [14:15:41<5:14:30, 10.58s/it]                                                         73%|  | 4717/6500 [14:15:41<5:14:30, 10.58s/it] 73%|  | 4718/6500 [14:15:51<5:12:47, 10.53s/it]                                                         73%|  | 4718/6500 [14:15:51<5:12:47, 10.53s/it] 73%|  | 4719/6500 [14:16:02<5:18:21, 10.73s/it]                                                         73%|  | 4719/6500 [14:16:02<5:18:21, 10.73s/it] 73%|  | 4720/6500 [14:16:13<5:15:18, 10.63s/it]                                                         73%|  | 4720/6500 [14:16:13<5:15:18, 10.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8576517701148987, 'eval_runtime': 3.9654, 'eval_samples_per_second': 5.8, 'eval_steps_per_second': 1.513, 'epoch': 0.73}
                                                         73%|  | 4720/6500 [14:16:17<5:15:18, 10.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4720
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.32, 'learning_rate': 1.7381703284790947e-05, 'epoch': 0.73}
{'loss': 0.3327, 'learning_rate': 1.736338587511449e-05, 'epoch': 0.73}
{'loss': 0.828, 'learning_rate': 1.7345076094048147e-05, 'epoch': 0.73}
{'loss': 0.3317, 'learning_rate': 1.732677394587173e-05, 'epoch': 0.73}
{'loss': 0.3265, 'learning_rate': 1.7308479434863216e-05, 'epoch': 0.73}
 73%|  | 4721/6500 [14:16:27<5:53:04, 11.91s/it]                                                         73%|  | 4721/6500 [14:16:27<5:53:04, 11.91s/it] 73%|  | 4722/6500 [14:16:38<5:39:17, 11.45s/it]                                                         73%|  | 4722/6500 [14:16:38<5:39:17, 11.45s/it] 73%|  | 4723/6500 [14:16:48<5:29:22, 11.12s/it]                                                         73%|  | 4723/6500 [14:16:48<5:29:22, 11.12s/it] 73%|  | 4724/6500 [14:16:59<5:22:32, 10.90s/it]                                                         73%|  | 4724/6500 [14:16:59<5:22:32, 10.90s/it] 73%|  | 4725/6500 [14:17:09<5:17:49, 10.74s/it]                                                         73%|  | 4725/6500 [14:17:09<5:17:49, 10.74s/it] 73{'loss': 0.3316, 'learning_rate': 1.7290192565298897e-05, 'epoch': 0.73}
{'loss': 0.3064, 'learning_rate': 1.7271913341453176e-05, 'epoch': 0.73}
{'loss': 0.3411, 'learning_rate': 1.725364176759873e-05, 'epoch': 0.73}
{'loss': 0.3088, 'learning_rate': 1.7235377848006434e-05, 'epoch': 0.73}
{'loss': 0.3068, 'learning_rate': 1.7217121586945334e-05, 'epoch': 0.73}
%|  | 4726/6500 [14:17:19<5:14:25, 10.63s/it]                                                         73%|  | 4726/6500 [14:17:19<5:14:25, 10.63s/it] 73%|  | 4727/6500 [14:17:30<5:11:55, 10.56s/it]                                                         73%|  | 4727/6500 [14:17:30<5:11:55, 10.56s/it] 73%|  | 4728/6500 [14:17:40<5:10:15, 10.51s/it]                                                         73%|  | 4728/6500 [14:17:40<5:10:15, 10.51s/it] 73%|  | 4729/6500 [14:17:50<5:08:50, 10.46s/it]                                                         73%|  | 4729/6500 [14:17:50<5:08:50, 10.46s/it] 73%|  | 4730/6500 [14:18:01<5:08:04, 10.44s/it]                                                         73%|  | 4730/6500 [14:18:01<5:08:04, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8687577247619629, 'eval_runtime': 3.9543, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.73}
                                                         73%|  | 4730/6500 [14:18:05<5:08:04, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4730
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4730/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3172, 'learning_rate': 1.719887298868274e-05, 'epoch': 0.73}
{'loss': 0.3065, 'learning_rate': 1.718063205748416e-05, 'epoch': 0.73}
{'loss': 0.3277, 'learning_rate': 1.7162398797613282e-05, 'epoch': 0.73}
{'loss': 0.3171, 'learning_rate': 1.714417321333203e-05, 'epoch': 0.73}
{'loss': 0.3311, 'learning_rate': 1.7125955308900527e-05, 'epoch': 0.73}
 73%|  | 4731/6500 [14:18:16<5:46:54, 11.77s/it]                                                         73%|  | 4731/6500 [14:18:16<5:46:54, 11.77s/it] 73%|  | 4732/6500 [14:18:26<5:34:22, 11.35s/it]                                                         73%|  | 4732/6500 [14:18:26<5:34:22, 11.35s/it] 73%|  | 4733/6500 [14:18:36<5:25:43, 11.06s/it]                                                         73%|  | 4733/6500 [14:18:36<5:25:43, 11.06s/it] 73%|  | 4734/6500 [14:18:47<5:19:25, 10.85s/it]                                                         73%|  | 4734/6500 [14:18:47<5:19:25, 10.85s/it] 73%|  | 4735/6500 [14:18:58<5:18:08, 10.82s/it]                                                         73%|  | 4735/6500 [14:18:58<5:18:08, 10.82s/it] 73{'loss': 0.3145, 'learning_rate': 1.7107745088577087e-05, 'epoch': 0.73}
{'loss': 0.3381, 'learning_rate': 1.708954255661825e-05, 'epoch': 0.73}
{'loss': 0.3186, 'learning_rate': 1.7071347717278734e-05, 'epoch': 0.73}
{'loss': 0.3113, 'learning_rate': 1.7053160574811488e-05, 'epoch': 0.73}
{'loss': 0.33, 'learning_rate': 1.7034981133467652e-05, 'epoch': 0.73}
%|  | 4736/6500 [14:19:08<5:14:23, 10.69s/it]                                                         73%|  | 4736/6500 [14:19:08<5:14:23, 10.69s/it] 73%|  | 4737/6500 [14:19:18<5:11:45, 10.61s/it]                                                         73%|  | 4737/6500 [14:19:18<5:11:45, 10.61s/it] 73%|  | 4738/6500 [14:19:29<5:09:46, 10.55s/it]                                                         73%|  | 4738/6500 [14:19:29<5:09:46, 10.55s/it] 73%|  | 4739/6500 [14:19:39<5:08:22, 10.51s/it]                                                         73%|  | 4739/6500 [14:19:39<5:08:22, 10.51s/it] 73%|  | 4740/6500 [14:19:50<5:07:09, 10.47s/it]                                                         73%|  | 4740/6500 [14:19:50<5:07:09, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8665082454681396, 'eval_runtime': 3.9538, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 1.518, 'epoch': 0.73}
                                                         73%|  | 4740/6500 [14:19:54<5:07:09, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4740
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3217, 'learning_rate': 1.7016809397496523e-05, 'epoch': 0.73}
{'loss': 0.3137, 'learning_rate': 1.6998645371145666e-05, 'epoch': 0.73}
{'loss': 0.3291, 'learning_rate': 1.6980489058660797e-05, 'epoch': 0.73}
{'loss': 0.3364, 'learning_rate': 1.6962340464285848e-05, 'epoch': 0.73}
{'loss': 0.3223, 'learning_rate': 1.6944199592262943e-05, 'epoch': 0.73}
 73%|  | 4741/6500 [14:20:04<5:46:02, 11.80s/it]                                                         73%|  | 4741/6500 [14:20:04<5:46:02, 11.80s/it] 73%|  | 4742/6500 [14:20:15<5:33:15, 11.37s/it]                                                         73%|  | 4742/6500 [14:20:15<5:33:15, 11.37s/it] 73%|  | 4743/6500 [14:20:25<5:24:19, 11.08s/it]                                                         73%|  | 4743/6500 [14:20:25<5:24:19, 11.08s/it] 73%|  | 4744/6500 [14:20:36<5:17:53, 10.86s/it]                                                         73%|  | 4744/6500 [14:20:36<5:17:53, 10.86s/it] 73%|  | 4745/6500 [14:20:46<5:13:42, 10.73s/it]                                                         73%|  | 4745/6500 [14:20:46<5:13:42, 10.73s/it] 73{'loss': 0.3156, 'learning_rate': 1.69260664468324e-05, 'epoch': 0.73}
{'loss': 0.3038, 'learning_rate': 1.6907941032232738e-05, 'epoch': 0.73}
{'loss': 0.3406, 'learning_rate': 1.6889823352700652e-05, 'epoch': 0.73}
{'loss': 0.3732, 'learning_rate': 1.687171341247104e-05, 'epoch': 0.73}
{'loss': 0.3134, 'learning_rate': 1.6853611215777006e-05, 'epoch': 0.73}
%|  | 4746/6500 [14:20:56<5:10:30, 10.62s/it]                                                         73%|  | 4746/6500 [14:20:56<5:10:30, 10.62s/it] 73%|  | 4747/6500 [14:21:07<5:08:10, 10.55s/it]                                                         73%|  | 4747/6500 [14:21:07<5:08:10, 10.55s/it] 73%|  | 4748/6500 [14:21:17<5:08:49, 10.58s/it]                                                         73%|  | 4748/6500 [14:21:17<5:08:49, 10.58s/it] 73%|  | 4749/6500 [14:21:28<5:08:03, 10.56s/it]                                                         73%|  | 4749/6500 [14:21:28<5:08:03, 10.56s/it] 73%|  | 4750/6500 [14:21:38<5:06:16, 10.50s/it]                                                         73%|  | 4750/6500 [14:21:38<5:06:16, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8625116944313049, 'eval_runtime': 4.8082, 'eval_samples_per_second': 4.784, 'eval_steps_per_second': 1.248, 'epoch': 0.73}
                                                         73%|  | 4750/6500 [14:21:43<5:06:16, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3023, 'learning_rate': 1.6835516766849822e-05, 'epoch': 0.73}
{'loss': 0.3405, 'learning_rate': 1.681743006991894e-05, 'epoch': 0.73}
{'loss': 0.8332, 'learning_rate': 1.679935112921202e-05, 'epoch': 0.73}
{'loss': 0.3394, 'learning_rate': 1.678127994895492e-05, 'epoch': 0.73}
{'loss': 0.3411, 'learning_rate': 1.6763216533371652e-05, 'epoch': 0.73}
 73%|  | 4751/6500 [14:21:54<5:52:46, 12.10s/it]                                                         73%|  | 4751/6500 [14:21:54<5:52:46, 12.10s/it] 73%|  | 4752/6500 [14:22:05<5:39:29, 11.65s/it]                                                         73%|  | 4752/6500 [14:22:05<5:39:29, 11.65s/it] 73%|  | 4753/6500 [14:22:15<5:27:50, 11.26s/it]                                                         73%|  | 4753/6500 [14:22:15<5:27:50, 11.26s/it] 73%|  | 4754/6500 [14:22:25<5:19:51, 10.99s/it]                                                         73%|  | 4754/6500 [14:22:25<5:19:51, 10.99s/it] 73%|  | 4755/6500 [14:22:36<5:14:11, 10.80s/it]                                                         73%|  | 4755/6500 [14:22:36<5:14:11, 10.80s/it] 73{'loss': 0.2957, 'learning_rate': 1.6745160886684434e-05, 'epoch': 0.73}
{'loss': 0.3175, 'learning_rate': 1.6727113013113673e-05, 'epoch': 0.73}
{'loss': 0.3368, 'learning_rate': 1.6709072916877938e-05, 'epoch': 0.73}
{'loss': 0.3062, 'learning_rate': 1.6691040602194004e-05, 'epoch': 0.73}
{'loss': 0.3211, 'learning_rate': 1.6673016073276798e-05, 'epoch': 0.73}
%|  | 4756/6500 [14:22:46<5:10:09, 10.67s/it]                                                         73%|  | 4756/6500 [14:22:46<5:10:09, 10.67s/it] 73%|  | 4757/6500 [14:22:57<5:07:54, 10.60s/it]                                                         73%|  | 4757/6500 [14:22:57<5:07:54, 10.60s/it] 73%|  | 4758/6500 [14:23:07<5:05:47, 10.53s/it]                                                         73%|  | 4758/6500 [14:23:07<5:05:47, 10.53s/it] 73%|  | 4759/6500 [14:23:17<5:04:09, 10.48s/it]                                                         73%|  | 4759/6500 [14:23:17<5:04:09, 10.48s/it] 73%|  | 4760/6500 [14:23:28<5:03:34, 10.47s/it]                                                         73%|  | 4760/6500 [14:23:28<5:03:34, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8673942685127258, 'eval_runtime': 3.9732, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.73}
                                                         73%|  | 4760/6500 [14:23:32<5:03:34, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4760/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.32, 'learning_rate': 1.6654999334339455e-05, 'epoch': 0.73}
{'loss': 0.3191, 'learning_rate': 1.6636990389593298e-05, 'epoch': 0.73}
{'loss': 0.3103, 'learning_rate': 1.661898924324777e-05, 'epoch': 0.73}
{'loss': 0.3206, 'learning_rate': 1.660099589951054e-05, 'epoch': 0.73}
{'loss': 0.33, 'learning_rate': 1.6583010362587453e-05, 'epoch': 0.73}
 73%|  | 4761/6500 [14:23:43<5:42:15, 11.81s/it]                                                         73%|  | 4761/6500 [14:23:43<5:42:15, 11.81s/it] 73%|  | 4762/6500 [14:23:53<5:30:04, 11.39s/it]                                                         73%|  | 4762/6500 [14:23:53<5:30:04, 11.39s/it] 73%|  | 4763/6500 [14:24:04<5:20:54, 11.09s/it]                                                         73%|  | 4763/6500 [14:24:04<5:20:54, 11.09s/it] 73%|  | 4764/6500 [14:24:14<5:14:27, 10.87s/it]                                                         73%|  | 4764/6500 [14:24:14<5:14:27, 10.87s/it] 73%|  | 4765/6500 [14:24:24<5:09:52, 10.72s/it]                                                         73%|  | 4765/6500 [14:24:24<5:09:52, 10.72s/it] 73{'loss': 0.3173, 'learning_rate': 1.6565032636682515e-05, 'epoch': 0.73}
{'loss': 0.3479, 'learning_rate': 1.6547062725997915e-05, 'epoch': 0.73}
{'loss': 0.3238, 'learning_rate': 1.6529100634733996e-05, 'epoch': 0.73}
{'loss': 0.3204, 'learning_rate': 1.65111463670893e-05, 'epoch': 0.73}
{'loss': 0.337, 'learning_rate': 1.6493199927260534e-05, 'epoch': 0.73}
%|  | 4766/6500 [14:24:35<5:06:34, 10.61s/it]                                                         73%|  | 4766/6500 [14:24:35<5:06:34, 10.61s/it] 73%|  | 4767/6500 [14:24:45<5:04:13, 10.53s/it]                                                         73%|  | 4767/6500 [14:24:45<5:04:13, 10.53s/it] 73%|  | 4768/6500 [14:24:56<5:05:58, 10.60s/it]                                                         73%|  | 4768/6500 [14:24:56<5:05:58, 10.60s/it] 73%|  | 4769/6500 [14:25:06<5:03:45, 10.53s/it]                                                         73%|  | 4769/6500 [14:25:06<5:03:45, 10.53s/it] 73%|  | 4770/6500 [14:25:16<5:02:11, 10.48s/it]                                                         73%|  | 4770/6500 [14:25:16<5:02:11, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8659707903862, 'eval_runtime': 3.9485, 'eval_samples_per_second': 5.825, 'eval_steps_per_second': 1.52, 'epoch': 0.73}
                                                         73%|  | 4770/6500 [14:25:20<5:02:11, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4770/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3222, 'learning_rate': 1.6475261319442553e-05, 'epoch': 0.73}
{'loss': 0.3363, 'learning_rate': 1.6457330547828402e-05, 'epoch': 0.73}
{'loss': 0.3174, 'learning_rate': 1.6439407616609316e-05, 'epoch': 0.73}
{'loss': 0.3326, 'learning_rate': 1.642149252997463e-05, 'epoch': 0.73}
{'loss': 0.3126, 'learning_rate': 1.64035852921119e-05, 'epoch': 0.73}
 73%|  | 4771/6500 [14:25:31<5:39:44, 11.79s/it]                                                         73%|  | 4771/6500 [14:25:31<5:39:44, 11.79s/it] 73%|  | 4772/6500 [14:25:42<5:27:04, 11.36s/it]                                                         73%|  | 4772/6500 [14:25:42<5:27:04, 11.36s/it] 73%|  | 4773/6500 [14:25:52<5:18:29, 11.07s/it]                                                         73%|  | 4773/6500 [14:25:52<5:18:29, 11.07s/it] 73%|  | 4774/6500 [14:26:02<5:12:11, 10.85s/it]                                                         73%|  | 4774/6500 [14:26:02<5:12:11, 10.85s/it] 73%|  | 4775/6500 [14:26:13<5:07:41, 10.70s/it]                                                         73%|  | 4775/6500 [14:26:13<5:07:41, 10.70s/it] 73{'loss': 0.3126, 'learning_rate': 1.6385685907206842e-05, 'epoch': 0.73}
{'loss': 0.3071, 'learning_rate': 1.6367794379443323e-05, 'epoch': 0.73}
{'loss': 0.3907, 'learning_rate': 1.6349910713003385e-05, 'epoch': 0.74}
{'loss': 0.3362, 'learning_rate': 1.6332034912067217e-05, 'epoch': 0.74}
{'loss': 0.3184, 'learning_rate': 1.6314166980813183e-05, 'epoch': 0.74}
%|  | 4776/6500 [14:26:23<5:04:32, 10.60s/it]                                                         73%|  | 4776/6500 [14:26:23<5:04:32, 10.60s/it] 73%|  | 4777/6500 [14:26:33<5:02:21, 10.53s/it]                                                         73%|  | 4777/6500 [14:26:33<5:02:21, 10.53s/it] 74%|  | 4778/6500 [14:26:44<5:00:44, 10.48s/it]                                                         74%|  | 4778/6500 [14:26:44<5:00:44, 10.48s/it] 74%|  | 4779/6500 [14:26:54<4:59:52, 10.45s/it]                                                         74%|  | 4779/6500 [14:26:54<4:59:52, 10.45s/it] 74%|  | 4780/6500 [14:27:05<4:58:57, 10.43s/it]                                                         74%|  | 4780/6500 [14:27:05<4:58:57, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.861360490322113, 'eval_runtime': 4.8031, 'eval_samples_per_second': 4.789, 'eval_steps_per_second': 1.249, 'epoch': 0.74}
                                                         74%|  | 4780/6500 [14:27:09<4:58:57, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4780/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3305, 'learning_rate': 1.6296306923417807e-05, 'epoch': 0.74}
{'loss': 0.7765, 'learning_rate': 1.6278454744055765e-05, 'epoch': 0.74}
{'loss': 0.4022, 'learning_rate': 1.6260610446899898e-05, 'epoch': 0.74}
{'loss': 0.3267, 'learning_rate': 1.62427740361212e-05, 'epoch': 0.74}
{'loss': 0.3304, 'learning_rate': 1.622494551588884e-05, 'epoch': 0.74}
 74%|  | 4781/6500 [14:27:20<5:44:15, 12.02s/it]                                                         74%|  | 4781/6500 [14:27:20<5:44:15, 12.02s/it] 74%|  | 4782/6500 [14:27:31<5:29:42, 11.52s/it]                                                         74%|  | 4782/6500 [14:27:31<5:29:42, 11.52s/it] 74%|  | 4783/6500 [14:27:41<5:19:33, 11.17s/it]                                                         74%|  | 4783/6500 [14:27:41<5:19:33, 11.17s/it] 74%|  | 4784/6500 [14:27:52<5:16:13, 11.06s/it]                                                         74%|  | 4784/6500 [14:27:52<5:16:13, 11.06s/it] 74%|  | 4785/6500 [14:28:02<5:10:04, 10.85s/it]                                                         74%|  | 4785/6500 [14:28:02<5:10:04, 10.85s/it] 74{'loss': 0.2983, 'learning_rate': 1.620712489037009e-05, 'epoch': 0.74}
{'loss': 0.3362, 'learning_rate': 1.6189312163730436e-05, 'epoch': 0.74}
{'loss': 0.3175, 'learning_rate': 1.617150734013349e-05, 'epoch': 0.74}
{'loss': 0.2901, 'learning_rate': 1.615371042374102e-05, 'epoch': 0.74}
{'loss': 0.3218, 'learning_rate': 1.6135921418712956e-05, 'epoch': 0.74}
%|  | 4786/6500 [14:28:12<5:05:39, 10.70s/it]                                                         74%|  | 4786/6500 [14:28:13<5:05:39, 10.70s/it] 74%|  | 4787/6500 [14:28:23<5:02:42, 10.60s/it]                                                         74%|  | 4787/6500 [14:28:23<5:02:42, 10.60s/it] 74%|  | 4788/6500 [14:28:33<5:00:39, 10.54s/it]                                                         74%|  | 4788/6500 [14:28:33<5:00:39, 10.54s/it] 74%|  | 4789/6500 [14:28:44<4:58:57, 10.48s/it]                                                         74%|  | 4789/6500 [14:28:44<4:58:57, 10.48s/it] 74%|  | 4790/6500 [14:28:55<5:08:06, 10.81s/it]                                                         74%|  | 4790/6500 [14:28:55<5:08:06, 10.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8695994019508362, 'eval_runtime': 3.9512, 'eval_samples_per_second': 5.821, 'eval_steps_per_second': 1.519, 'epoch': 0.74}
                                                         74%|  | 4790/6500 [14:28:59<5:08:06, 10.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4790
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4790/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3071, 'learning_rate': 1.611814032920736e-05, 'epoch': 0.74}
{'loss': 0.3214, 'learning_rate': 1.610036715938046e-05, 'epoch': 0.74}
{'loss': 0.3138, 'learning_rate': 1.608260191338662e-05, 'epoch': 0.74}
{'loss': 0.321, 'learning_rate': 1.606484459537836e-05, 'epoch': 0.74}
{'loss': 0.3136, 'learning_rate': 1.6047095209506346e-05, 'epoch': 0.74}
 74%|  | 4791/6500 [14:29:10<5:43:05, 12.05s/it]                                                         74%|  | 4791/6500 [14:29:10<5:43:05, 12.05s/it] 74%|  | 4792/6500 [14:29:20<5:28:33, 11.54s/it]                                                         74%|  | 4792/6500 [14:29:20<5:28:33, 11.54s/it] 74%|  | 4793/6500 [14:29:31<5:18:15, 11.19s/it]                                                         74%|  | 4793/6500 [14:29:31<5:18:15, 11.19s/it] 74%|  | 4794/6500 [14:29:41<5:10:50, 10.93s/it]                                                         74%|  | 4794/6500 [14:29:41<5:10:50, 10.93s/it] 74%|  | 4795/6500 [14:29:52<5:06:01, 10.77s/it]                                                         74%|  | 4795/6500 [14:29:52<5:06:01, 10.77s/it] 74{'loss': 0.3208, 'learning_rate': 1.6029353759919408e-05, 'epoch': 0.74}
{'loss': 0.3378, 'learning_rate': 1.601162025076447e-05, 'epoch': 0.74}
{'loss': 0.3216, 'learning_rate': 1.5993894686186645e-05, 'epoch': 0.74}
{'loss': 0.3299, 'learning_rate': 1.5976177070329174e-05, 'epoch': 0.74}
{'loss': 0.3145, 'learning_rate': 1.5958467407333428e-05, 'epoch': 0.74}
%|  | 4796/6500 [14:30:02<5:02:43, 10.66s/it]                                                         74%|  | 4796/6500 [14:30:02<5:02:43, 10.66s/it] 74%|  | 4797/6500 [14:30:12<5:00:01, 10.57s/it]                                                         74%|  | 4797/6500 [14:30:12<5:00:01, 10.57s/it] 74%|  | 4798/6500 [14:30:23<4:57:56, 10.50s/it]                                                         74%|  | 4798/6500 [14:30:23<4:57:56, 10.50s/it] 74%|  | 4799/6500 [14:30:33<4:56:23, 10.45s/it]                                                         74%|  | 4799/6500 [14:30:33<4:56:23, 10.45s/it] 74%|  | 4800/6500 [14:30:44<4:57:39, 10.51s/it]                                                         74%|  | 4800/6500 [14:30:44<4:57:39, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8686956167221069, 'eval_runtime': 3.9986, 'eval_samples_per_second': 5.752, 'eval_steps_per_second': 1.501, 'epoch': 0.74}
                                                         74%|  | 4800/6500 [14:30:48<4:57:39, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4800
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4800/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4800/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4800/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3185, 'learning_rate': 1.5940765701338966e-05, 'epoch': 0.74}
{'loss': 0.3158, 'learning_rate': 1.5923071956483442e-05, 'epoch': 0.74}
{'loss': 0.3407, 'learning_rate': 1.5905386176902655e-05, 'epoch': 0.74}
{'loss': 0.3233, 'learning_rate': 1.5887708366730554e-05, 'epoch': 0.74}
{'loss': 0.3164, 'learning_rate': 1.5870038530099225e-05, 'epoch': 0.74}
 74%|  | 4801/6500 [14:30:59<5:35:19, 11.84s/it]                                                         74%|  | 4801/6500 [14:30:59<5:35:19, 11.84s/it] 74%|  | 4802/6500 [14:31:09<5:22:28, 11.39s/it]                                                         74%|  | 4802/6500 [14:31:09<5:22:28, 11.39s/it] 74%|  | 4803/6500 [14:31:19<5:13:43, 11.09s/it]                                                         74%|  | 4803/6500 [14:31:19<5:13:43, 11.09s/it] 74%|  | 4804/6500 [14:31:30<5:07:24, 10.88s/it]                                                         74%|  | 4804/6500 [14:31:30<5:07:24, 10.88s/it] 74%|  | 4805/6500 [14:31:40<5:02:56, 10.72s/it]                                                         74%|  | 4805/6500 [14:31:40<5:02:56, 10.72s/it] 74{'loss': 0.3087, 'learning_rate': 1.5852376671138863e-05, 'epoch': 0.74}
{'loss': 0.335, 'learning_rate': 1.5834722793977835e-05, 'epoch': 0.74}
{'loss': 0.3814, 'learning_rate': 1.5817076902742622e-05, 'epoch': 0.74}
{'loss': 0.3181, 'learning_rate': 1.5799439001557846e-05, 'epoch': 0.74}
{'loss': 0.3083, 'learning_rate': 1.5781809094546268e-05, 'epoch': 0.74}
%|  | 4806/6500 [14:31:51<5:00:14, 10.63s/it]                                                         74%|  | 4806/6500 [14:31:51<5:00:14, 10.63s/it] 74%|  | 4807/6500 [14:32:01<4:57:44, 10.55s/it]                                                         74%|  | 4807/6500 [14:32:01<4:57:44, 10.55s/it] 74%|  | 4808/6500 [14:32:11<4:55:54, 10.49s/it]                                                         74%|  | 4808/6500 [14:32:11<4:55:54, 10.49s/it] 74%|  | 4809/6500 [14:32:22<4:54:35, 10.45s/it]                                                         74%|  | 4809/6500 [14:32:22<4:54:35, 10.45s/it] 74%|  | 4810/6500 [14:32:32<4:53:36, 10.42s/it]                                                         74%|  | 4810/6500 [14:32:32<4:53:36, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8619711399078369, 'eval_runtime': 3.9394, 'eval_samples_per_second': 5.838, 'eval_steps_per_second': 1.523, 'epoch': 0.74}
                                                         74%|  | 4810/6500 [14:32:36<4:53:36, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4810I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4810

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.326, 'learning_rate': 1.576418718582876e-05, 'epoch': 0.74}
{'loss': 0.8293, 'learning_rate': 1.574657327952434e-05, 'epoch': 0.74}
{'loss': 0.3326, 'learning_rate': 1.5728967379750147e-05, 'epoch': 0.74}
{'loss': 0.3158, 'learning_rate': 1.5711369490621465e-05, 'epoch': 0.74}
{'loss': 0.3197, 'learning_rate': 1.5693779616251687e-05, 'epoch': 0.74}
 74%|  | 4811/6500 [14:32:47<5:30:52, 11.75s/it]                                                         74%|  | 4811/6500 [14:32:47<5:30:52, 11.75s/it] 74%|  | 4812/6500 [14:32:57<5:18:39, 11.33s/it]                                                         74%|  | 4812/6500 [14:32:57<5:18:39, 11.33s/it] 74%|  | 4813/6500 [14:33:08<5:10:31, 11.04s/it]                                                         74%|  | 4813/6500 [14:33:08<5:10:31, 11.04s/it] 74%|  | 4814/6500 [14:33:18<5:04:39, 10.84s/it]                                                         74%|  | 4814/6500 [14:33:18<5:04:39, 10.84s/it] 74%|  | 4815/6500 [14:33:28<5:00:32, 10.70s/it]                                                         74%|  | 4815/6500 [14:33:28<5:00:32, 10.70s/it] 74{'loss': 0.3133, 'learning_rate': 1.5676197760752348e-05, 'epoch': 0.74}
{'loss': 0.3406, 'learning_rate': 1.565862392823308e-05, 'epoch': 0.74}
{'loss': 0.3009, 'learning_rate': 1.5641058122801665e-05, 'epoch': 0.74}
{'loss': 0.3129, 'learning_rate': 1.5623500348564013e-05, 'epoch': 0.74}
{'loss': 0.3058, 'learning_rate': 1.5605950609624136e-05, 'epoch': 0.74}
%|  | 4816/6500 [14:33:40<5:05:53, 10.90s/it]                                                         74%|  | 4816/6500 [14:33:40<5:05:53, 10.90s/it] 74%|  | 4817/6500 [14:33:50<5:01:28, 10.75s/it]                                                         74%|  | 4817/6500 [14:33:50<5:01:28, 10.75s/it] 74%|  | 4818/6500 [14:34:00<4:58:04, 10.63s/it]                                                         74%|  | 4818/6500 [14:34:00<4:58:04, 10.63s/it] 74%|  | 4819/6500 [14:34:11<4:55:50, 10.56s/it]                                                         74%|  | 4819/6500 [14:34:11<4:55:50, 10.56s/it] 74%|  | 4820/6500 [14:34:21<4:54:03, 10.50s/it]                                                         74%|  | 4820/6500 [14:34:21<4:54:03, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8724462389945984, 'eval_runtime': 3.9437, 'eval_samples_per_second': 5.832, 'eval_steps_per_second': 1.521, 'epoch': 0.74}
                                                         74%|  | 4820/6500 [14:34:25<4:54:03, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4820
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3176, 'learning_rate': 1.558840891008419e-05, 'epoch': 0.74}
{'loss': 0.3191, 'learning_rate': 1.557087525404443e-05, 'epoch': 0.74}
{'loss': 0.3097, 'learning_rate': 1.5553349645603254e-05, 'epoch': 0.74}
{'loss': 0.3321, 'learning_rate': 1.5535832088857157e-05, 'epoch': 0.74}
{'loss': 0.3209, 'learning_rate': 1.5518322587900758e-05, 'epoch': 0.74}
 74%|  | 4821/6500 [14:34:36<5:30:18, 11.80s/it]                                                         74%|  | 4821/6500 [14:34:36<5:30:18, 11.80s/it] 74%|  | 4822/6500 [14:34:46<5:17:57, 11.37s/it]                                                         74%|  | 4822/6500 [14:34:46<5:17:57, 11.37s/it] 74%|  | 4823/6500 [14:34:57<5:09:18, 11.07s/it]                                                         74%|  | 4823/6500 [14:34:57<5:09:18, 11.07s/it] 74%|  | 4824/6500 [14:35:07<5:03:13, 10.86s/it]                                                         74%|  | 4824/6500 [14:35:07<5:03:13, 10.86s/it] 74%|  | 4825/6500 [14:35:17<4:58:56, 10.71s/it]                                                         74%|  | 4825/6500 [14:35:17<4:58:56, 10.71s/it] 74{'loss': 0.3255, 'learning_rate': 1.5500821146826805e-05, 'epoch': 0.74}
{'loss': 0.3232, 'learning_rate': 1.548332776972617e-05, 'epoch': 0.74}
{'loss': 0.3157, 'learning_rate': 1.5465842460687784e-05, 'epoch': 0.74}
{'loss': 0.3327, 'learning_rate': 1.5448365223798756e-05, 'epoch': 0.74}
{'loss': 0.3223, 'learning_rate': 1.5430896063144274e-05, 'epoch': 0.74}
%|  | 4826/6500 [14:35:28<4:55:52, 10.60s/it]                                                         74%|  | 4826/6500 [14:35:28<4:55:52, 10.60s/it] 74%|  | 4827/6500 [14:35:38<4:53:40, 10.53s/it]                                                         74%|  | 4827/6500 [14:35:38<4:53:40, 10.53s/it] 74%|  | 4828/6500 [14:35:49<4:52:14, 10.49s/it]                                                         74%|  | 4828/6500 [14:35:49<4:52:14, 10.49s/it] 74%|  | 4829/6500 [14:35:59<4:51:02, 10.45s/it]                                                         74%|  | 4829/6500 [14:35:59<4:51:02, 10.45s/it] 74%|  | 4830/6500 [14:36:09<4:50:14, 10.43s/it]                                                         74%|  | 4830/6500 [14:36:09<4:50:14, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8679118752479553, 'eval_runtime': 3.9447, 'eval_samples_per_second': 5.831, 'eval_steps_per_second': 1.521, 'epoch': 0.74}
                                                         74%|  | 4830/6500 [14:36:13<4:50:14, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4830
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4830/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4830/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3192, 'learning_rate': 1.5413434982807657e-05, 'epoch': 0.74}
{'loss': 0.3261, 'learning_rate': 1.5395981986870322e-05, 'epoch': 0.74}
{'loss': 0.3397, 'learning_rate': 1.5378537079411802e-05, 'epoch': 0.74}
{'loss': 0.3045, 'learning_rate': 1.5361100264509735e-05, 'epoch': 0.74}
{'loss': 0.3087, 'learning_rate': 1.534367154623988e-05, 'epoch': 0.74}
 74%|  | 4831/6500 [14:36:24<5:27:22, 11.77s/it]                                                         74%|  | 4831/6500 [14:36:24<5:27:22, 11.77s/it] 74%|  | 4832/6500 [14:36:35<5:17:07, 11.41s/it]                                                         74%|  | 4832/6500 [14:36:35<5:17:07, 11.41s/it] 74%|  | 4833/6500 [14:36:45<5:08:07, 11.09s/it]                                                         74%|  | 4833/6500 [14:36:45<5:08:07, 11.09s/it] 74%|  | 4834/6500 [14:36:55<5:02:04, 10.88s/it]                                                         74%|  | 4834/6500 [14:36:55<5:02:04, 10.88s/it] 74%|  | 4835/6500 [14:37:06<4:57:31, 10.72s/it]                                                         74%|  | 4835/6500 [14:37:06<4:57:31, 10.72s/it] 74{'loss': 0.2966, 'learning_rate': 1.5326250928676084e-05, 'epoch': 0.74}
{'loss': 0.3485, 'learning_rate': 1.5308838415890313e-05, 'epoch': 0.74}
{'loss': 0.3642, 'learning_rate': 1.5291434011952655e-05, 'epoch': 0.74}
{'loss': 0.3114, 'learning_rate': 1.5274037720931244e-05, 'epoch': 0.74}
{'loss': 0.3295, 'learning_rate': 1.5256649546892382e-05, 'epoch': 0.74}
%|  | 4836/6500 [14:37:16<4:54:27, 10.62s/it]                                                         74%|  | 4836/6500 [14:37:16<4:54:27, 10.62s/it] 74%|  | 4837/6500 [14:37:27<4:52:08, 10.54s/it]                                                         74%|  | 4837/6500 [14:37:27<4:52:08, 10.54s/it] 74%|  | 4838/6500 [14:37:37<4:50:17, 10.48s/it]                                                         74%|  | 4838/6500 [14:37:37<4:50:17, 10.48s/it] 74%|  | 4839/6500 [14:37:47<4:49:10, 10.45s/it]                                                         74%|  | 4839/6500 [14:37:47<4:49:10, 10.45s/it] 74%|  | 4840/6500 [14:37:58<4:48:25, 10.43s/it]                                                         74%|  | 4840/6500 [14:37:58<4:48:25, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8633487820625305, 'eval_runtime': 3.9669, 'eval_samples_per_second': 5.798, 'eval_steps_per_second': 1.513, 'epoch': 0.74}
                                                         74%|  | 4840/6500 [14:38:02<4:48:25, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4840
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4840/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4840/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3124, 'learning_rate': 1.5239269493900443e-05, 'epoch': 0.74}
{'loss': 0.8371, 'learning_rate': 1.5221897566017911e-05, 'epoch': 0.74}
{'loss': 0.3305, 'learning_rate': 1.5204533767305373e-05, 'epoch': 0.75}
{'loss': 0.3298, 'learning_rate': 1.5187178101821503e-05, 'epoch': 0.75}
{'loss': 0.3065, 'learning_rate': 1.5169830573623089e-05, 'epoch': 0.75}
 74%|  | 4841/6500 [14:38:13<5:25:17, 11.76s/it]                                                         74%|  | 4841/6500 [14:38:13<5:25:17, 11.76s/it] 74%|  | 4842/6500 [14:38:23<5:13:25, 11.34s/it]                                                         74%|  | 4842/6500 [14:38:23<5:13:25, 11.34s/it] 75%|  | 4843/6500 [14:38:33<5:05:14, 11.05s/it]                                                         75%|  | 4843/6500 [14:38:33<5:05:14, 11.05s/it] 75%|  | 4844/6500 [14:38:44<4:59:35, 10.85s/it]                                                         75%|  | 4844/6500 [14:38:44<4:59:35, 10.85s/it] 75%|  | 4845/6500 [14:38:54<4:55:27, 10.71s/it]                                                         75%|  | 4845/6500 [14:38:54<4:55:27, 10.71s/it] 75{'loss': 0.3314, 'learning_rate': 1.5152491186765005e-05, 'epoch': 0.75}
{'loss': 0.3229, 'learning_rate': 1.5135159945300231e-05, 'epoch': 0.75}
{'loss': 0.289, 'learning_rate': 1.5117836853279837e-05, 'epoch': 0.75}
{'loss': 0.3206, 'learning_rate': 1.5100521914753007e-05, 'epoch': 0.75}
{'loss': 0.3147, 'learning_rate': 1.5083215133766971e-05, 'epoch': 0.75}
%|  | 4846/6500 [14:39:04<4:52:25, 10.61s/it]                                                         75%|  | 4846/6500 [14:39:04<4:52:25, 10.61s/it] 75%|  | 4847/6500 [14:39:15<4:50:08, 10.53s/it]                                                         75%|  | 4847/6500 [14:39:15<4:50:08, 10.53s/it] 75%|  | 4848/6500 [14:39:25<4:48:25, 10.48s/it]                                                         75%|  | 4848/6500 [14:39:25<4:48:25, 10.48s/it] 75%|  | 4849/6500 [14:39:36<4:52:20, 10.62s/it]                                                         75%|  | 4849/6500 [14:39:36<4:52:20, 10.62s/it] 75%|  | 4850/6500 [14:39:46<4:49:53, 10.54s/it]                                                         75%|  | 4850/6500 [14:39:46<4:49:53, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8712788224220276, 'eval_runtime': 3.9667, 'eval_samples_per_second': 5.798, 'eval_steps_per_second': 1.513, 'epoch': 0.75}
                                                         75%|  | 4850/6500 [14:39:50<4:49:53, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4850
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4850

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4850
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4850/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4850/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4850/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3276, 'learning_rate': 1.5065916514367101e-05, 'epoch': 0.75}
{'loss': 0.3289, 'learning_rate': 1.5048626060596843e-05, 'epoch': 0.75}
{'loss': 0.3283, 'learning_rate': 1.5031343776497736e-05, 'epoch': 0.75}
{'loss': 0.3222, 'learning_rate': 1.501406966610941e-05, 'epoch': 0.75}
{'loss': 0.3226, 'learning_rate': 1.4996803733469578e-05, 'epoch': 0.75}
 75%|  | 4851/6500 [14:40:01<5:25:14, 11.83s/it]                                                         75%|  | 4851/6500 [14:40:01<5:25:14, 11.83s/it] 75%|  | 4852/6500 [14:40:12<5:12:52, 11.39s/it]                                                         75%|  | 4852/6500 [14:40:12<5:12:52, 11.39s/it] 75%|  | 4853/6500 [14:40:22<5:04:08, 11.08s/it]                                                         75%|  | 4853/6500 [14:40:22<5:04:08, 11.08s/it] 75%|  | 4854/6500 [14:40:32<4:57:53, 10.86s/it]                                                         75%|  | 4854/6500 [14:40:32<4:57:53, 10.86s/it] 75%|  | 4855/6500 [14:40:43<4:53:36, 10.71s/it]                                                         75%|  | 4855/6500 [14:40:43<4:53:36, 10.71s/it] 75{'loss': 0.3446, 'learning_rate': 1.4979545982614051e-05, 'epoch': 0.75}
{'loss': 0.3157, 'learning_rate': 1.4962296417576726e-05, 'epoch': 0.75}
{'loss': 0.3292, 'learning_rate': 1.4945055042389578e-05, 'epoch': 0.75}
{'loss': 0.3451, 'learning_rate': 1.492782186108268e-05, 'epoch': 0.75}
{'loss': 0.3162, 'learning_rate': 1.4910596877684191e-05, 'epoch': 0.75}
%|  | 4856/6500 [14:40:53<4:50:25, 10.60s/it]                                                         75%|  | 4856/6500 [14:40:53<4:50:25, 10.60s/it] 75%|  | 4857/6500 [14:41:03<4:49:10, 10.56s/it]                                                         75%|  | 4857/6500 [14:41:04<4:49:10, 10.56s/it] 75%|  | 4858/6500 [14:41:14<4:47:29, 10.51s/it]                                                         75%|  | 4858/6500 [14:41:14<4:47:29, 10.51s/it] 75%|  | 4859/6500 [14:41:24<4:46:16, 10.47s/it]                                                         75%|  | 4859/6500 [14:41:24<4:46:16, 10.47s/it] 75%|  | 4860/6500 [14:41:35<4:45:13, 10.43s/it]                                                         75%|  | 4860/6500 [14:41:35<4:45:13, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8667150735855103, 'eval_runtime': 4.0256, 'eval_samples_per_second': 5.713, 'eval_steps_per_second': 1.49, 'epoch': 0.75}
                                                         75%|  | 4860/6500 [14:41:39<4:45:13, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4860
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3271, 'learning_rate': 1.4893380096220321e-05, 'epoch': 0.75}
{'loss': 0.3276, 'learning_rate': 1.4876171520715399e-05, 'epoch': 0.75}
{'loss': 0.3309, 'learning_rate': 1.485897115519183e-05, 'epoch': 0.75}
{'loss': 0.3092, 'learning_rate': 1.4841779003670092e-05, 'epoch': 0.75}
{'loss': 0.3136, 'learning_rate': 1.4824595070168745e-05, 'epoch': 0.75}
 75%|  | 4861/6500 [14:41:50<5:21:55, 11.79s/it]                                                         75%|  | 4861/6500 [14:41:50<5:21:55, 11.79s/it] 75%|  | 4862/6500 [14:42:00<5:10:18, 11.37s/it]                                                         75%|  | 4862/6500 [14:42:00<5:10:18, 11.37s/it] 75%|  | 4863/6500 [14:42:10<5:01:57, 11.07s/it]                                                         75%|  | 4863/6500 [14:42:10<5:01:57, 11.07s/it] 75%|  | 4864/6500 [14:42:21<4:56:14, 10.86s/it]                                                         75%|  | 4864/6500 [14:42:21<4:56:14, 10.86s/it] 75%|  | 4865/6500 [14:42:31<4:54:15, 10.80s/it]                                                         75%|  | 4865/6500 [14:42:31<4:54:15, 10.80s/it] 75{'loss': 0.3006, 'learning_rate': 1.4807419358704433e-05, 'epoch': 0.75}
{'loss': 0.4005, 'learning_rate': 1.4790251873291872e-05, 'epoch': 0.75}
{'loss': 0.3139, 'learning_rate': 1.4773092617943851e-05, 'epoch': 0.75}
{'loss': 0.3078, 'learning_rate': 1.4755941596671253e-05, 'epoch': 0.75}
{'loss': 0.3319, 'learning_rate': 1.4738798813483017e-05, 'epoch': 0.75}
%|  | 4866/6500 [14:42:42<4:50:40, 10.67s/it]                                                         75%|  | 4866/6500 [14:42:42<4:50:40, 10.67s/it] 75%|  | 4867/6500 [14:42:52<4:48:06, 10.59s/it]                                                         75%|  | 4867/6500 [14:42:52<4:48:06, 10.59s/it] 75%|  | 4868/6500 [14:43:02<4:46:12, 10.52s/it]                                                         75%|  | 4868/6500 [14:43:02<4:46:12, 10.52s/it] 75%|  | 4869/6500 [14:43:13<4:44:53, 10.48s/it]                                                         75%|  | 4869/6500 [14:43:13<4:44:53, 10.48s/it] 75%|  | 4870/6500 [14:43:24<4:53:05, 10.79s/it]                                                         75%|  | 4870/6500 [14:43:24<4:53:05, 10.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8630439043045044, 'eval_runtime': 4.6962, 'eval_samples_per_second': 4.898, 'eval_steps_per_second': 1.278, 'epoch': 0.75}
                                                         75%|  | 4870/6500 [14:43:29<4:53:05, 10.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.83, 'learning_rate': 1.4721664272386172e-05, 'epoch': 0.75}
{'loss': 0.3375, 'learning_rate': 1.4704537977385818e-05, 'epoch': 0.75}
{'loss': 0.3218, 'learning_rate': 1.4687419932485097e-05, 'epoch': 0.75}
{'loss': 0.3241, 'learning_rate': 1.4670310141685263e-05, 'epoch': 0.75}
{'loss': 0.299, 'learning_rate': 1.4653208608985624e-05, 'epoch': 0.75}
 75%|  | 4871/6500 [14:43:40<5:31:32, 12.21s/it]                                                         75%|  | 4871/6500 [14:43:40<5:31:32, 12.21s/it] 75%|  | 4872/6500 [14:43:50<5:16:22, 11.66s/it]                                                         75%|  | 4872/6500 [14:43:50<5:16:22, 11.66s/it] 75%|  | 4873/6500 [14:44:01<5:05:31, 11.27s/it]                                                         75%|  | 4873/6500 [14:44:01<5:05:31, 11.27s/it] 75%|  | 4874/6500 [14:44:11<4:57:56, 10.99s/it]                                                         75%|  | 4874/6500 [14:44:11<4:57:56, 10.99s/it] 75%|  | 4875/6500 [14:44:21<4:52:23, 10.80s/it]                                                         75%|  | 4875/6500 [14:44:21<4:52:23, 10.80s/it] 75{'loss': 0.3345, 'learning_rate': 1.463611533838355e-05, 'epoch': 0.75}
{'loss': 0.3079, 'learning_rate': 1.4619030333874506e-05, 'epoch': 0.75}
{'loss': 0.2974, 'learning_rate': 1.4601953599452012e-05, 'epoch': 0.75}
{'loss': 0.3129, 'learning_rate': 1.4584885139107635e-05, 'epoch': 0.75}
{'loss': 0.3065, 'learning_rate': 1.4567824956831043e-05, 'epoch': 0.75}
%|  | 4876/6500 [14:44:32<4:48:23, 10.66s/it]                                                         75%|  | 4876/6500 [14:44:32<4:48:23, 10.66s/it] 75%|  | 4877/6500 [14:44:42<4:45:24, 10.55s/it]                                                         75%|  | 4877/6500 [14:44:42<4:45:24, 10.55s/it] 75%|  | 4878/6500 [14:44:52<4:44:10, 10.51s/it]                                                         75%|  | 4878/6500 [14:44:52<4:44:10, 10.51s/it] 75%|  | 4879/6500 [14:45:03<4:42:34, 10.46s/it]                                                         75%|  | 4879/6500 [14:45:03<4:42:34, 10.46s/it] 75%|  | 4880/6500 [14:45:13<4:41:21, 10.42s/it]                                                         75%|  | 4880/6500 [14:45:13<4:41:21, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711079359054565, 'eval_runtime': 4.2974, 'eval_samples_per_second': 5.352, 'eval_steps_per_second': 1.396, 'epoch': 0.75}
                                                         75%|  | 4880/6500 [14:45:17<4:41:21, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4880/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3266, 'learning_rate': 1.4550773056609923e-05, 'epoch': 0.75}
{'loss': 0.3097, 'learning_rate': 1.4533729442430066e-05, 'epoch': 0.75}
{'loss': 0.34, 'learning_rate': 1.4516694118275315e-05, 'epoch': 0.75}
{'loss': 0.3011, 'learning_rate': 1.4499667088127572e-05, 'epoch': 0.75}
{'loss': 0.3497, 'learning_rate': 1.44826483559668e-05, 'epoch': 0.75}
 75%|  | 4881/6500 [14:45:29<5:22:31, 11.95s/it]                                                         75%|  | 4881/6500 [14:45:29<5:22:31, 11.95s/it] 75%|  | 4882/6500 [14:45:39<5:09:10, 11.46s/it]                                                         75%|  | 4882/6500 [14:45:39<5:09:10, 11.46s/it] 75%|  | 4883/6500 [14:45:49<5:00:13, 11.14s/it]                                                         75%|  | 4883/6500 [14:45:49<5:00:13, 11.14s/it] 75%|  | 4884/6500 [14:46:00<4:53:27, 10.90s/it]                                                         75%|  | 4884/6500 [14:46:00<4:53:27, 10.90s/it] 75%|  | 4885/6500 [14:46:10<4:48:42, 10.73s/it]                                                         75%|  | 4885/6500 [14:46:10<4:48:42, 10.73s/it] 75{'loss': 0.3309, 'learning_rate': 1.4465637925771025e-05, 'epoch': 0.75}
{'loss': 0.3326, 'learning_rate': 1.444863580151633e-05, 'epoch': 0.75}
{'loss': 0.3285, 'learning_rate': 1.4431641987176869e-05, 'epoch': 0.75}
{'loss': 0.323, 'learning_rate': 1.4414656486724826e-05, 'epoch': 0.75}
{'loss': 0.3311, 'learning_rate': 1.4397679304130468e-05, 'epoch': 0.75}
%|  | 4886/6500 [14:46:20<4:45:20, 10.61s/it]                                                         75%|  | 4886/6500 [14:46:20<4:45:20, 10.61s/it] 75%|  | 4887/6500 [14:46:31<4:43:00, 10.53s/it]                                                         75%|  | 4887/6500 [14:46:31<4:43:00, 10.53s/it] 75%|  | 4888/6500 [14:46:41<4:41:19, 10.47s/it]                                                         75%|  | 4888/6500 [14:46:41<4:41:19, 10.47s/it] 75%|  | 4889/6500 [14:46:51<4:40:19, 10.44s/it]                                                         75%|  | 4889/6500 [14:46:51<4:40:19, 10.44s/it] 75%|  | 4890/6500 [14:47:02<4:39:23, 10.41s/it]                                                         75%|  | 4890/6500 [14:47:02<4:39:23, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8682409524917603, 'eval_runtime': 3.9354, 'eval_samples_per_second': 5.844, 'eval_steps_per_second': 1.525, 'epoch': 0.75}
                                                         75%|  | 4890/6500 [14:47:06<4:39:23, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4890
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4890/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3139, 'learning_rate': 1.4380710443362112e-05, 'epoch': 0.75}
{'loss': 0.3434, 'learning_rate': 1.4363749908386132e-05, 'epoch': 0.75}
{'loss': 0.3179, 'learning_rate': 1.4346797703166925e-05, 'epoch': 0.75}
{'loss': 0.3178, 'learning_rate': 1.4329853831666978e-05, 'epoch': 0.75}
{'loss': 0.3107, 'learning_rate': 1.4312918297846822e-05, 'epoch': 0.75}
 75%|  | 4891/6500 [14:47:17<5:15:09, 11.75s/it]                                                         75%|  | 4891/6500 [14:47:17<5:15:09, 11.75s/it] 75%|  | 4892/6500 [14:47:27<5:03:34, 11.33s/it]                                                         75%|  | 4892/6500 [14:47:27<5:03:34, 11.33s/it] 75%|  | 4893/6500 [14:47:37<4:55:33, 11.03s/it]                                                         75%|  | 4893/6500 [14:47:37<4:55:33, 11.03s/it] 75%|  | 4894/6500 [14:47:48<4:50:02, 10.84s/it]                                                         75%|  | 4894/6500 [14:47:48<4:50:02, 10.84s/it] 75%|  | 4895/6500 [14:47:58<4:46:05, 10.69s/it]                                                         75%|  | 4895/6500 [14:47:58<4:46:05, 10.69s/it] 75{'loss': 0.3391, 'learning_rate': 1.4295991105665035e-05, 'epoch': 0.75}
{'loss': 0.3847, 'learning_rate': 1.4279072259078241e-05, 'epoch': 0.75}
{'loss': 0.3141, 'learning_rate': 1.4262161762041121e-05, 'epoch': 0.75}
{'loss': 0.3158, 'learning_rate': 1.4245259618506396e-05, 'epoch': 0.75}
{'loss': 0.3247, 'learning_rate': 1.4228365832424844e-05, 'epoch': 0.75}
%|  | 4896/6500 [14:48:08<4:43:24, 10.60s/it]                                                         75%|  | 4896/6500 [14:48:08<4:43:24, 10.60s/it] 75%|  | 4897/6500 [14:48:19<4:43:09, 10.60s/it]                                                         75%|  | 4897/6500 [14:48:19<4:43:09, 10.60s/it] 75%|  | 4898/6500 [14:48:29<4:41:11, 10.53s/it]                                                         75%|  | 4898/6500 [14:48:29<4:41:11, 10.53s/it] 75%|  | 4899/6500 [14:48:40<4:39:45, 10.48s/it]                                                         75%|  | 4899/6500 [14:48:40<4:39:45, 10.48s/it] 75%|  | 4900/6500 [14:48:50<4:38:32, 10.45s/it]                                                         75%|  | 4900/6500 [14:48:50<4:38:32, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8615931868553162, 'eval_runtime': 3.9399, 'eval_samples_per_second': 5.838, 'eval_steps_per_second': 1.523, 'epoch': 0.75}
                                                         75%|  | 4900/6500 [14:48:54<4:38:32, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4900I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4900
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8221, 'learning_rate': 1.421148040774528e-05, 'epoch': 0.75}
{'loss': 0.3336, 'learning_rate': 1.4194603348414581e-05, 'epoch': 0.75}
{'loss': 0.3229, 'learning_rate': 1.4177734658377662e-05, 'epoch': 0.75}
{'loss': 0.3134, 'learning_rate': 1.4160874341577446e-05, 'epoch': 0.75}
{'loss': 0.314, 'learning_rate': 1.4144022401954949e-05, 'epoch': 0.75}
 75%|  | 4901/6500 [14:49:05<5:13:33, 11.77s/it]                                                         75%|  | 4901/6500 [14:49:05<5:13:33, 11.77s/it] 75%|  | 4902/6500 [14:49:15<5:02:06, 11.34s/it]                                                         75%|  | 4902/6500 [14:49:15<5:02:06, 11.34s/it] 75%|  | 4903/6500 [14:49:26<4:54:02, 11.05s/it]                                                         75%|  | 4903/6500 [14:49:26<4:54:02, 11.05s/it] 75%|  | 4904/6500 [14:49:36<4:48:18, 10.84s/it]                                                         75%|  | 4904/6500 [14:49:36<4:48:18, 10.84s/it] 75%|  | 4905/6500 [14:49:46<4:44:22, 10.70s/it]                                                         75%|  | 4905/6500 [14:49:46<4:44:22, 10.70s/it] 75{'loss': 0.3346, 'learning_rate': 1.41271788434492e-05, 'epoch': 0.75}
{'loss': 0.3103, 'learning_rate': 1.4110343669997295e-05, 'epoch': 0.75}
{'loss': 0.3169, 'learning_rate': 1.409351688553434e-05, 'epoch': 0.76}
{'loss': 0.3111, 'learning_rate': 1.40766984939935e-05, 'epoch': 0.76}
{'loss': 0.3172, 'learning_rate': 1.4059888499305973e-05, 'epoch': 0.76}
%|  | 4906/6500 [14:49:57<4:41:28, 10.59s/it]                                                         75%|  | 4906/6500 [14:49:57<4:41:28, 10.59s/it] 75%|  | 4907/6500 [14:50:07<4:39:17, 10.52s/it]                                                         75%|  | 4907/6500 [14:50:07<4:39:17, 10.52s/it] 76%|  | 4908/6500 [14:50:17<4:37:52, 10.47s/it]                                                         76%|  | 4908/6500 [14:50:17<4:37:52, 10.47s/it] 76%|  | 4909/6500 [14:50:28<4:36:44, 10.44s/it]                                                         76%|  | 4909/6500 [14:50:28<4:36:44, 10.44s/it] 76%|  | 4910/6500 [14:50:38<4:35:58, 10.41s/it]                                                         76%|  | 4910/6500 [14:50:38<4:35:58, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8696452379226685, 'eval_runtime': 3.978, 'eval_samples_per_second': 5.782, 'eval_steps_per_second': 1.508, 'epoch': 0.76}
                                                         76%|  | 4910/6500 [14:50:42<4:35:58, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4910
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3193, 'learning_rate': 1.4043086905400988e-05, 'epoch': 0.76}
{'loss': 0.3136, 'learning_rate': 1.4026293716205812e-05, 'epoch': 0.76}
{'loss': 0.3267, 'learning_rate': 1.400950893564576e-05, 'epoch': 0.76}
{'loss': 0.318, 'learning_rate': 1.3992732567644185e-05, 'epoch': 0.76}
{'loss': 0.3287, 'learning_rate': 1.3975964616122428e-05, 'epoch': 0.76}
 76%|  | 4911/6500 [14:50:53<5:12:52, 11.81s/it]                                                         76%|  | 4911/6500 [14:50:53<5:12:52, 11.81s/it] 76%|  | 4912/6500 [14:51:04<5:01:11, 11.38s/it]                                                         76%|  | 4912/6500 [14:51:04<5:01:11, 11.38s/it] 76%|  | 4913/6500 [14:51:14<4:55:56, 11.19s/it]                                                         76%|  | 4913/6500 [14:51:14<4:55:56, 11.19s/it] 76%|  | 4914/6500 [14:51:25<4:49:31, 10.95s/it]                                                         76%|  | 4914/6500 [14:51:25<4:49:31, 10.95s/it] 76%|  | 4915/6500 [14:51:35<4:44:43, 10.78s/it]                                                         76%|  | 4915/6500 [14:51:35<4:44:43, 10.78s/it] 76{'loss': 0.3142, 'learning_rate': 1.3959205084999911e-05, 'epoch': 0.76}
{'loss': 0.309, 'learning_rate': 1.3942453978194075e-05, 'epoch': 0.76}
{'loss': 0.326, 'learning_rate': 1.3925711299620386e-05, 'epoch': 0.76}
{'loss': 0.3134, 'learning_rate': 1.3908977053192352e-05, 'epoch': 0.76}
{'loss': 0.3204, 'learning_rate': 1.3892251242821491e-05, 'epoch': 0.76}
%|  | 4916/6500 [14:51:45<4:41:24, 10.66s/it]                                                         76%|  | 4916/6500 [14:51:45<4:41:24, 10.66s/it] 76%|  | 4917/6500 [14:51:56<4:39:00, 10.57s/it]                                                         76%|  | 4917/6500 [14:51:56<4:39:00, 10.57s/it] 76%|  | 4918/6500 [14:52:06<4:37:17, 10.52s/it]                                                         76%|  | 4918/6500 [14:52:06<4:37:17, 10.52s/it] 76%|  | 4919/6500 [14:52:17<4:36:02, 10.48s/it]                                                         76%|  | 4919/6500 [14:52:17<4:36:02, 10.48s/it] 76%|  | 4920/6500 [14:52:27<4:35:03, 10.45s/it]                                                         76%|  | 4920/6500 [14:52:27<4:35:03, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8680755496025085, 'eval_runtime': 3.9597, 'eval_samples_per_second': 5.809, 'eval_steps_per_second': 1.515, 'epoch': 0.76}
                                                         76%|  | 4920/6500 [14:52:31<4:35:03, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4920
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.314, 'learning_rate': 1.3875533872417362e-05, 'epoch': 0.76}
{'loss': 0.337, 'learning_rate': 1.385882494588755e-05, 'epoch': 0.76}
{'loss': 0.3071, 'learning_rate': 1.3842124467137662e-05, 'epoch': 0.76}
{'loss': 0.3189, 'learning_rate': 1.382543244007134e-05, 'epoch': 0.76}
{'loss': 0.2978, 'learning_rate': 1.3808748868590254e-05, 'epoch': 0.76}
 76%|  | 4921/6500 [14:52:42<5:10:27, 11.80s/it]                                                         76%|  | 4921/6500 [14:52:42<5:10:27, 11.80s/it] 76%|  | 4922/6500 [14:52:52<4:58:52, 11.36s/it]                                                         76%|  | 4922/6500 [14:52:52<4:58:52, 11.36s/it] 76%|  | 4923/6500 [14:53:03<4:50:49, 11.06s/it]                                                         76%|  | 4923/6500 [14:53:03<4:50:49, 11.06s/it] 76%|  | 4924/6500 [14:53:13<4:45:09, 10.86s/it]                                                         76%|  | 4924/6500 [14:53:13<4:45:09, 10.86s/it] 76%|  | 4925/6500 [14:53:23<4:41:08, 10.71s/it]                                                         76%|  | 4925/6500 [14:53:23<4:41:08, 10.71s/it] 76{'loss': 0.3503, 'learning_rate': 1.3792073756594065e-05, 'epoch': 0.76}
{'loss': 0.3651, 'learning_rate': 1.3775407107980481e-05, 'epoch': 0.76}
{'loss': 0.3082, 'learning_rate': 1.3758748926645237e-05, 'epoch': 0.76}
{'loss': 0.3181, 'learning_rate': 1.374209921648208e-05, 'epoch': 0.76}
{'loss': 0.3259, 'learning_rate': 1.3725457981382783e-05, 'epoch': 0.76}
%|  | 4926/6500 [14:53:34<4:38:12, 10.61s/it]                                                         76%|  | 4926/6500 [14:53:34<4:38:12, 10.61s/it] 76%|  | 4927/6500 [14:53:44<4:36:13, 10.54s/it]                                                         76%|  | 4927/6500 [14:53:44<4:36:13, 10.54s/it] 76%|  | 4928/6500 [14:53:54<4:34:47, 10.49s/it]                                                         76%|  | 4928/6500 [14:53:54<4:34:47, 10.49s/it] 76%|  | 4929/6500 [14:54:05<4:35:39, 10.53s/it]                                                         76%|  | 4929/6500 [14:54:05<4:35:39, 10.53s/it] 76%|  | 4930/6500 [14:54:15<4:34:19, 10.48s/it]                                                         76%|  | 4930/6500 [14:54:15<4:34:19, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8640810251235962, 'eval_runtime': 3.964, 'eval_samples_per_second': 5.802, 'eval_steps_per_second': 1.514, 'epoch': 0.76}
                                                         76%|  | 4930/6500 [14:54:19<4:34:19, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4930/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8356, 'learning_rate': 1.3708825225237126e-05, 'epoch': 0.76}
{'loss': 0.328, 'learning_rate': 1.369220095193292e-05, 'epoch': 0.76}
{'loss': 0.3408, 'learning_rate': 1.3675585165355987e-05, 'epoch': 0.76}
{'loss': 0.2956, 'learning_rate': 1.3658977869390166e-05, 'epoch': 0.76}
{'loss': 0.3282, 'learning_rate': 1.364237906791731e-05, 'epoch': 0.76}
 76%|  | 4931/6500 [14:54:30<5:08:26, 11.79s/it]                                                         76%|  | 4931/6500 [14:54:30<5:08:26, 11.79s/it] 76%|  | 4932/6500 [14:54:41<4:57:01, 11.37s/it]                                                         76%|  | 4932/6500 [14:54:41<4:57:01, 11.37s/it] 76%|  | 4933/6500 [14:54:51<4:49:06, 11.07s/it]                                                         76%|  | 4933/6500 [14:54:51<4:49:06, 11.07s/it] 76%|  | 4934/6500 [14:55:01<4:43:24, 10.86s/it]                                                         76%|  | 4934/6500 [14:55:01<4:43:24, 10.86s/it] 76%|  | 4935/6500 [14:55:12<4:39:27, 10.71s/it]                                                         76%|  | 4935/6500 [14:55:12<4:39:27, 10.71s/it] 76{'loss': 0.326, 'learning_rate': 1.3625788764817305e-05, 'epoch': 0.76}
{'loss': 0.2911, 'learning_rate': 1.3609206963968002e-05, 'epoch': 0.76}
{'loss': 0.3245, 'learning_rate': 1.3592633669245309e-05, 'epoch': 0.76}
{'loss': 0.3106, 'learning_rate': 1.3576068884523142e-05, 'epoch': 0.76}
{'loss': 0.3189, 'learning_rate': 1.3559512613673402e-05, 'epoch': 0.76}
%|  | 4936/6500 [14:55:22<4:36:26, 10.61s/it]                                                         76%|  | 4936/6500 [14:55:22<4:36:26, 10.61s/it] 76%|  | 4937/6500 [14:55:33<4:34:21, 10.53s/it]                                                         76%|  | 4937/6500 [14:55:33<4:34:21, 10.53s/it] 76%|  | 4938/6500 [14:55:43<4:33:02, 10.49s/it]                                                         76%|  | 4938/6500 [14:55:43<4:33:02, 10.49s/it] 76%|  | 4939/6500 [14:55:53<4:31:57, 10.45s/it]                                                         76%|  | 4939/6500 [14:55:53<4:31:57, 10.45s/it] 76%|  | 4940/6500 [14:56:04<4:31:16, 10.43s/it]                                                         76%|  | 4940/6500 [14:56:04<4:31:16, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8691074252128601, 'eval_runtime': 4.1725, 'eval_samples_per_second': 5.512, 'eval_steps_per_second': 1.438, 'epoch': 0.76}
                                                         76%|  | 4940/6500 [14:56:08<4:31:16, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3174, 'learning_rate': 1.3542964860566037e-05, 'epoch': 0.76}
{'loss': 0.3201, 'learning_rate': 1.3526425629068967e-05, 'epoch': 0.76}
{'loss': 0.3095, 'learning_rate': 1.3509894923048145e-05, 'epoch': 0.76}
{'loss': 0.314, 'learning_rate': 1.3493372746367522e-05, 'epoch': 0.76}
{'loss': 0.3494, 'learning_rate': 1.3476859102889056e-05, 'epoch': 0.76}
 76%|  | 4941/6500 [14:56:19<5:07:17, 11.83s/it]                                                         76%|  | 4941/6500 [14:56:19<5:07:17, 11.83s/it] 76%|  | 4942/6500 [14:56:29<4:55:37, 11.38s/it]                                                         76%|  | 4942/6500 [14:56:29<4:55:37, 11.38s/it] 76%|  | 4943/6500 [14:56:39<4:47:28, 11.08s/it]                                                         76%|  | 4943/6500 [14:56:39<4:47:28, 11.08s/it] 76%|  | 4944/6500 [14:56:50<4:41:51, 10.87s/it]                                                         76%|  | 4944/6500 [14:56:50<4:41:51, 10.87s/it] 76%|  | 4945/6500 [14:57:00<4:37:37, 10.71s/it]                                                         76%|  | 4945/6500 [14:57:00<4:37:37, 10.71s/it] 76{'loss': 0.3181, 'learning_rate': 1.3460353996472707e-05, 'epoch': 0.76}
{'loss': 0.3336, 'learning_rate': 1.3443857430976465e-05, 'epoch': 0.76}
{'loss': 0.3326, 'learning_rate': 1.3427369410256269e-05, 'epoch': 0.76}
{'loss': 0.3167, 'learning_rate': 1.3410889938166105e-05, 'epoch': 0.76}
{'loss': 0.3226, 'learning_rate': 1.3394419018557957e-05, 'epoch': 0.76}
%|  | 4946/6500 [14:57:11<4:37:46, 10.72s/it]                                                         76%|  | 4946/6500 [14:57:11<4:37:46, 10.72s/it] 76%|  | 4947/6500 [14:57:21<4:34:39, 10.61s/it]                                                         76%|  | 4947/6500 [14:57:21<4:34:39, 10.61s/it] 76%|  | 4948/6500 [14:57:32<4:32:41, 10.54s/it]                                                         76%|  | 4948/6500 [14:57:32<4:32:41, 10.54s/it] 76%|  | 4949/6500 [14:57:42<4:31:10, 10.49s/it]                                                         76%|  | 4949/6500 [14:57:42<4:31:10, 10.49s/it] 76%|  | 4950/6500 [14:57:52<4:30:03, 10.45s/it]                                                         76%|  | 4950/6500 [14:57:52<4:30:03, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8672629594802856, 'eval_runtime': 3.9459, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.76}
                                                         76%|  | 4950/6500 [14:57:56<4:30:03, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4950/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3288, 'learning_rate': 1.3377956655281792e-05, 'epoch': 0.76}
{'loss': 0.3211, 'learning_rate': 1.336150285218558e-05, 'epoch': 0.76}
{'loss': 0.3142, 'learning_rate': 1.334505761311533e-05, 'epoch': 0.76}
{'loss': 0.3021, 'learning_rate': 1.3328620941914994e-05, 'epoch': 0.76}
{'loss': 0.3304, 'learning_rate': 1.3312192842426546e-05, 'epoch': 0.76}
 76%|  | 4951/6500 [14:58:07<5:04:03, 11.78s/it]                                                         76%|  | 4951/6500 [14:58:07<5:04:03, 11.78s/it] 76%|  | 4952/6500 [14:58:18<4:53:04, 11.36s/it]                                                         76%|  | 4952/6500 [14:58:18<4:53:04, 11.36s/it] 76%|  | 4953/6500 [14:58:28<4:45:09, 11.06s/it]                                                         76%|  | 4953/6500 [14:58:28<4:45:09, 11.06s/it] 76%|  | 4954/6500 [14:58:38<4:39:32, 10.85s/it]                                                         76%|  | 4954/6500 [14:58:38<4:39:32, 10.85s/it] 76%|  | 4955/6500 [14:58:49<4:35:42, 10.71s/it]                                                         76%|  | 4955/6500 [14:58:49<4:35:42, 10.71s/it] 76{'loss': 0.3793, 'learning_rate': 1.3295773318489974e-05, 'epoch': 0.76}
{'loss': 0.3248, 'learning_rate': 1.3279362373943204e-05, 'epoch': 0.76}
{'loss': 0.3117, 'learning_rate': 1.3262960012622216e-05, 'epoch': 0.76}
{'loss': 0.3389, 'learning_rate': 1.3246566238360963e-05, 'epoch': 0.76}
{'loss': 0.8398, 'learning_rate': 1.323018105499138e-05, 'epoch': 0.76}
%|  | 4956/6500 [14:58:59<4:32:52, 10.60s/it]                                                         76%|  | 4956/6500 [14:58:59<4:32:52, 10.60s/it] 76%|  | 4957/6500 [14:59:09<4:30:55, 10.53s/it]                                                         76%|  | 4957/6500 [14:59:09<4:30:55, 10.53s/it] 76%|  | 4958/6500 [14:59:20<4:29:39, 10.49s/it]                                                         76%|  | 4958/6500 [14:59:20<4:29:39, 10.49s/it] 76%|  | 4959/6500 [14:59:30<4:28:41, 10.46s/it]                                                         76%|  | 4959/6500 [14:59:30<4:28:41, 10.46s/it] 76%|  | 4960/6500 [14:59:41<4:27:45, 10.43s/it]                                                         76%|  | 4960/6500 [14:59:41<4:27:45, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8632422685623169, 'eval_runtime': 3.9474, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.76}
                                                         76%|  | 4960/6500 [14:59:45<4:27:45, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4960
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4960/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3342, 'learning_rate': 1.3213804466343421e-05, 'epoch': 0.76}
{'loss': 0.3173, 'learning_rate': 1.3197436476244995e-05, 'epoch': 0.76}
{'loss': 0.3258, 'learning_rate': 1.3181077088522036e-05, 'epoch': 0.76}
{'loss': 0.2967, 'learning_rate': 1.3164726306998442e-05, 'epoch': 0.76}
{'loss': 0.3312, 'learning_rate': 1.314838413549611e-05, 'epoch': 0.76}
 76%|  | 4961/6500 [14:59:56<5:01:40, 11.76s/it]                                                         76%|  | 4961/6500 [14:59:56<5:01:40, 11.76s/it] 76%|  | 4962/6500 [15:00:07<4:55:53, 11.54s/it]                                                         76%|  | 4962/6500 [15:00:07<4:55:53, 11.54s/it] 76%|  | 4963/6500 [15:00:17<4:46:55, 11.20s/it]                                                         76%|  | 4963/6500 [15:00:17<4:46:55, 11.20s/it] 76%|  | 4964/6500 [15:00:27<4:40:35, 10.96s/it]                                                         76%|  | 4964/6500 [15:00:27<4:40:35, 10.96s/it] 76%|  | 4965/6500 [15:00:38<4:36:06, 10.79s/it]                                                         76%|  | 4965/6500 [15:00:38<4:36:06, 10.79s/it] 76{'loss': 0.3057, 'learning_rate': 1.3132050577834925e-05, 'epoch': 0.76}
{'loss': 0.2968, 'learning_rate': 1.3115725637832776e-05, 'epoch': 0.76}
{'loss': 0.3137, 'learning_rate': 1.3099409319305483e-05, 'epoch': 0.76}
{'loss': 0.3036, 'learning_rate': 1.3083101626066901e-05, 'epoch': 0.76}
{'loss': 0.3187, 'learning_rate': 1.3066802561928854e-05, 'epoch': 0.76}
%|  | 4966/6500 [15:00:48<4:32:51, 10.67s/it]                                                         76%|  | 4966/6500 [15:00:48<4:32:51, 10.67s/it] 76%|  | 4967/6500 [15:00:59<4:30:31, 10.59s/it]                                                         76%|  | 4967/6500 [15:00:59<4:30:31, 10.59s/it] 76%|  | 4968/6500 [15:01:09<4:28:47, 10.53s/it]                                                         76%|  | 4968/6500 [15:01:09<4:28:47, 10.53s/it] 76%|  | 4969/6500 [15:01:19<4:27:34, 10.49s/it]                                                         76%|  | 4969/6500 [15:01:19<4:27:34, 10.49s/it] 76%|  | 4970/6500 [15:01:30<4:26:47, 10.46s/it]                                                         76%|  | 4970/6500 [15:01:30<4:26:47, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8707479238510132, 'eval_runtime': 3.9571, 'eval_samples_per_second': 5.812, 'eval_steps_per_second': 1.516, 'epoch': 0.76}
                                                         76%|  | 4970/6500 [15:01:34<4:26:47, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4970
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3059, 'learning_rate': 1.3050512130701154e-05, 'epoch': 0.76}
{'loss': 0.3304, 'learning_rate': 1.3034230336191583e-05, 'epoch': 0.76}
{'loss': 0.3107, 'learning_rate': 1.3017957182205915e-05, 'epoch': 0.77}
{'loss': 0.3329, 'learning_rate': 1.3001692672547905e-05, 'epoch': 0.77}
{'loss': 0.313, 'learning_rate': 1.2985436811019274e-05, 'epoch': 0.77}
 76%|  | 4971/6500 [15:01:45<5:01:13, 11.82s/it]                                                         76%|  | 4971/6500 [15:01:45<5:01:13, 11.82s/it] 76%|  | 4972/6500 [15:01:55<4:50:11, 11.39s/it]                                                         76%|  | 4972/6500 [15:01:55<4:50:11, 11.39s/it] 77%|  | 4973/6500 [15:02:05<4:42:19, 11.09s/it]                                                         77%|  | 4973/6500 [15:02:05<4:42:19, 11.09s/it] 77%|  | 4974/6500 [15:02:16<4:36:40, 10.88s/it]                                                         77%|  | 4974/6500 [15:02:16<4:36:40, 10.88s/it] 77%|  | 4975/6500 [15:02:26<4:32:36, 10.73s/it]                                                         77%|  | 4975/6500 [15:02:26<4:32:36, 10.73s/it] 77{'loss': 0.3174, 'learning_rate': 1.2969189601419745e-05, 'epoch': 0.77}
{'loss': 0.319, 'learning_rate': 1.295295104754699e-05, 'epoch': 0.77}
{'loss': 0.3174, 'learning_rate': 1.293672115319668e-05, 'epoch': 0.77}
{'loss': 0.3175, 'learning_rate': 1.292049992216246e-05, 'epoch': 0.77}
{'loss': 0.3253, 'learning_rate': 1.2904287358235928e-05, 'epoch': 0.77}
%|  | 4976/6500 [15:02:37<4:29:40, 10.62s/it]                                                         77%|  | 4976/6500 [15:02:37<4:29:40, 10.62s/it] 77%|  | 4977/6500 [15:02:47<4:27:47, 10.55s/it]                                                         77%|  | 4977/6500 [15:02:47<4:27:47, 10.55s/it] 77%|  | 4978/6500 [15:02:58<4:28:15, 10.58s/it]                                                         77%|  | 4978/6500 [15:02:58<4:28:15, 10.58s/it] 77%|  | 4979/6500 [15:03:08<4:26:35, 10.52s/it]                                                         77%|  | 4979/6500 [15:03:08<4:26:35, 10.52s/it] 77%|  | 4980/6500 [15:03:18<4:25:18, 10.47s/it]                                                         77%|  | 4980/6500 [15:03:18<4:25:18, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8705636858940125, 'eval_runtime': 3.9549, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.77}
                                                         77%|  | 4980/6500 [15:03:22<4:25:18, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3383, 'learning_rate': 1.288808346520668e-05, 'epoch': 0.77}
{'loss': 0.3055, 'learning_rate': 1.2871888246862274e-05, 'epoch': 0.77}
{'loss': 0.3135, 'learning_rate': 1.2855701706988254e-05, 'epoch': 0.77}
{'loss': 0.3005, 'learning_rate': 1.2839523849368113e-05, 'epoch': 0.77}
{'loss': 0.3346, 'learning_rate': 1.2823354677783334e-05, 'epoch': 0.77}
 77%|  | 4981/6500 [15:03:33<4:58:45, 11.80s/it]                                                         77%|  | 4981/6500 [15:03:33<4:58:45, 11.80s/it] 77%|  | 4982/6500 [15:03:44<4:47:42, 11.37s/it]                                                         77%|  | 4982/6500 [15:03:44<4:47:42, 11.37s/it] 77%|  | 4983/6500 [15:03:54<4:39:52, 11.07s/it]                                                         77%|  | 4983/6500 [15:03:54<4:39:52, 11.07s/it] 77%|  | 4984/6500 [15:04:04<4:34:24, 10.86s/it]                                                         77%|  | 4984/6500 [15:04:04<4:34:24, 10.86s/it] 77%|  | 4985/6500 [15:04:15<4:30:32, 10.71s/it]                                                         77%|  | 4985/6500 [15:04:15<4:30:32, 10.71s/it] 77{'loss': 0.3747, 'learning_rate': 1.2807194196013367e-05, 'epoch': 0.77}
{'loss': 0.3123, 'learning_rate': 1.2791042407835613e-05, 'epoch': 0.77}
{'loss': 0.3073, 'learning_rate': 1.2774899317025468e-05, 'epoch': 0.77}
{'loss': 0.3215, 'learning_rate': 1.275876492735627e-05, 'epoch': 0.77}
{'loss': 0.8273, 'learning_rate': 1.2742639242599358e-05, 'epoch': 0.77}
%|  | 4986/6500 [15:04:25<4:27:36, 10.61s/it]                                                         77%|  | 4986/6500 [15:04:25<4:27:36, 10.61s/it] 77%|  | 4987/6500 [15:04:35<4:25:46, 10.54s/it]                                                         77%|  | 4987/6500 [15:04:35<4:25:46, 10.54s/it] 77%|  | 4988/6500 [15:04:46<4:24:21, 10.49s/it]                                                         77%|  | 4988/6500 [15:04:46<4:24:21, 10.49s/it] 77%|  | 4989/6500 [15:04:56<4:23:23, 10.46s/it]                                                         77%|  | 4989/6500 [15:04:56<4:23:23, 10.46s/it] 77%|  | 4990/6500 [15:05:07<4:22:35, 10.43s/it]                                                         77%|  | 4990/6500 [15:05:07<4:22:35, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8648850917816162, 'eval_runtime': 3.9602, 'eval_samples_per_second': 5.808, 'eval_steps_per_second': 1.515, 'epoch': 0.77}
                                                         77%|  | 4990/6500 [15:05:11<4:22:35, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-4990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4990
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4990
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4990/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-4990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3375, 'learning_rate': 1.2726522266523976e-05, 'epoch': 0.77}
{'loss': 0.3263, 'learning_rate': 1.2710414002897393e-05, 'epoch': 0.77}
{'loss': 0.2945, 'learning_rate': 1.2694314455484813e-05, 'epoch': 0.77}
{'loss': 0.3185, 'learning_rate': 1.2678223628049402e-05, 'epoch': 0.77}
{'loss': 0.3357, 'learning_rate': 1.26621415243523e-05, 'epoch': 0.77}
 77%|  | 4991/6500 [15:05:22<4:55:55, 11.77s/it]                                                         77%|  | 4991/6500 [15:05:22<4:55:55, 11.77s/it] 77%|  | 4992/6500 [15:05:32<4:45:18, 11.35s/it]                                                         77%|  | 4992/6500 [15:05:32<4:45:18, 11.35s/it] 77%|  | 4993/6500 [15:05:42<4:37:42, 11.06s/it]                                                         77%|  | 4993/6500 [15:05:42<4:37:42, 11.06s/it] 77%|  | 4994/6500 [15:05:53<4:35:05, 10.96s/it]                                                         77%|  | 4994/6500 [15:05:53<4:35:05, 10.96s/it] 77%|  | 4995/6500 [15:06:03<4:30:22, 10.78s/it]                                                         77%|  | 4995/6500 [15:06:03<4:30:22, 10.78s/it] 77{'loss': 0.3034, 'learning_rate': 1.2646068148152596e-05, 'epoch': 0.77}
{'loss': 0.3098, 'learning_rate': 1.263000350320735e-05, 'epoch': 0.77}
{'loss': 0.3035, 'learning_rate': 1.261394759327157e-05, 'epoch': 0.77}
{'loss': 0.315, 'learning_rate': 1.259790042209823e-05, 'epoch': 0.77}
{'loss': 0.312, 'learning_rate': 1.258186199343826e-05, 'epoch': 0.77}
%|  | 4996/6500 [15:06:14<4:27:02, 10.65s/it]                                                         77%|  | 4996/6500 [15:06:14<4:27:02, 10.65s/it] 77%|  | 4997/6500 [15:06:24<4:24:51, 10.57s/it]                                                         77%|  | 4997/6500 [15:06:24<4:24:51, 10.57s/it] 77%|  | 4998/6500 [15:06:34<4:23:06, 10.51s/it]                                                         77%|  | 4998/6500 [15:06:34<4:23:06, 10.51s/it] 77%|  | 4999/6500 [15:06:45<4:21:49, 10.47s/it]                                                         77%|  | 4999/6500 [15:06:45<4:21:49, 10.47s/it] 77%|  | 5000/6500 [15:06:55<4:20:55, 10.44s/it]                                                         77%|  | 5000/6500 [15:06:55<4:20:55, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.87158203125, 'eval_runtime': 3.9681, 'eval_samples_per_second': 5.796, 'eval_steps_per_second': 1.512, 'epoch': 0.77}
                                                         77%|  | 5000/6500 [15:06:59<4:20:55, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5000
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5000/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5000/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3116, 'learning_rate': 1.2565832311040554e-05, 'epoch': 0.77}
{'loss': 0.33, 'learning_rate': 1.2549811378651932e-05, 'epoch': 0.77}
{'loss': 0.3093, 'learning_rate': 1.2533799200017194e-05, 'epoch': 0.77}
{'loss': 0.3378, 'learning_rate': 1.25177957788791e-05, 'epoch': 0.77}
{'loss': 0.3152, 'learning_rate': 1.250180111897834e-05, 'epoch': 0.77}
 77%|  | 5001/6500 [15:07:10<4:54:09, 11.77s/it]                                                         77%|  | 5001/6500 [15:07:10<4:54:09, 11.77s/it] 77%|  | 5002/6500 [15:07:20<4:43:19, 11.35s/it]                                                         77%|  | 5002/6500 [15:07:20<4:43:19, 11.35s/it] 77%|  | 5003/6500 [15:07:31<4:35:43, 11.05s/it]                                                         77%|  | 5003/6500 [15:07:31<4:35:43, 11.05s/it] 77%|  | 5004/6500 [15:07:41<4:30:12, 10.84s/it]                                                         77%|  | 5004/6500 [15:07:41<4:30:12, 10.84s/it] 77%|  | 5005/6500 [15:07:52<4:26:49, 10.71s/it]                                                         77%|  | 5005/6500 [15:07:52<4:26:49, 10.71s/it] 77{'loss': 0.3147, 'learning_rate': 1.2485815224053582e-05, 'epoch': 0.77}
{'loss': 0.3319, 'learning_rate': 1.2469838097841424e-05, 'epoch': 0.77}
{'loss': 0.3168, 'learning_rate': 1.2453869744076419e-05, 'epoch': 0.77}
{'loss': 0.3292, 'learning_rate': 1.2437910166491084e-05, 'epoch': 0.77}
{'loss': 0.3135, 'learning_rate': 1.242195936881586e-05, 'epoch': 0.77}
%|  | 5006/6500 [15:08:02<4:23:58, 10.60s/it]                                                         77%|  | 5006/6500 [15:08:02<4:23:58, 10.60s/it] 77%|  | 5007/6500 [15:08:12<4:21:55, 10.53s/it]                                                         77%|  | 5007/6500 [15:08:12<4:21:55, 10.53s/it] 77%|  | 5008/6500 [15:08:23<4:20:30, 10.48s/it]                                                         77%|  | 5008/6500 [15:08:23<4:20:30, 10.48s/it] 77%|  | 5009/6500 [15:08:33<4:19:23, 10.44s/it]                                                         77%|  | 5009/6500 [15:08:33<4:19:23, 10.44s/it] 77%|  | 5010/6500 [15:08:44<4:22:28, 10.57s/it]                                                         77%|  | 5010/6500 [15:08:44<4:22:28, 10.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8689339756965637, 'eval_runtime': 4.0847, 'eval_samples_per_second': 5.631, 'eval_steps_per_second': 1.469, 'epoch': 0.77}
                                                         77%|  | 5010/6500 [15:08:48<4:22:28, 10.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3344, 'learning_rate': 1.240601735477916e-05, 'epoch': 0.77}
{'loss': 0.3034, 'learning_rate': 1.2390084128107343e-05, 'epoch': 0.77}
{'loss': 0.3018, 'learning_rate': 1.2374159692524673e-05, 'epoch': 0.77}
{'loss': 0.3004, 'learning_rate': 1.2358244051753403e-05, 'epoch': 0.77}
{'loss': 0.3854, 'learning_rate': 1.2342337209513721e-05, 'epoch': 0.77}
 77%|  | 5011/6500 [15:08:59<4:55:45, 11.92s/it]                                                         77%|  | 5011/6500 [15:08:59<4:55:45, 11.92s/it] 77%|  | 5012/6500 [15:09:09<4:44:02, 11.45s/it]                                                         77%|  | 5012/6500 [15:09:09<4:44:02, 11.45s/it] 77%|  | 5013/6500 [15:09:20<4:35:40, 11.12s/it]                                                         77%|  | 5013/6500 [15:09:20<4:35:40, 11.12s/it] 77%|  | 5014/6500 [15:09:30<4:29:46, 10.89s/it]                                                         77%|  | 5014/6500 [15:09:30<4:29:46, 10.89s/it] 77%|  | 5015/6500 [15:09:40<4:25:34, 10.73s/it]                                                         77%|  | 5015/6500 [15:09:40<4:25:34, 10.73s/it] 77{'loss': 0.32, 'learning_rate': 1.2326439169523757e-05, 'epoch': 0.77}
{'loss': 0.3025, 'learning_rate': 1.2310549935499576e-05, 'epoch': 0.77}
{'loss': 0.3269, 'learning_rate': 1.2294669511155193e-05, 'epoch': 0.77}
{'loss': 0.668, 'learning_rate': 1.2278797900202559e-05, 'epoch': 0.77}
{'loss': 0.4925, 'learning_rate': 1.226293510635157e-05, 'epoch': 0.77}
%|  | 5016/6500 [15:09:51<4:22:37, 10.62s/it]                                                         77%|  | 5016/6500 [15:09:51<4:22:37, 10.62s/it] 77%|  | 5017/6500 [15:10:01<4:20:33, 10.54s/it]                                                         77%|  | 5017/6500 [15:10:01<4:20:33, 10.54s/it] 77%|  | 5018/6500 [15:10:11<4:19:02, 10.49s/it]                                                         77%|  | 5018/6500 [15:10:11<4:19:02, 10.49s/it] 77%|  | 5019/6500 [15:10:22<4:17:46, 10.44s/it]                                                         77%|  | 5019/6500 [15:10:22<4:17:46, 10.44s/it] 77%|  | 5020/6500 [15:10:32<4:17:06, 10.42s/it]                                                         77%|  | 5020/6500 [15:10:32<4:17:06, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8641321063041687, 'eval_runtime': 3.9689, 'eval_samples_per_second': 5.795, 'eval_steps_per_second': 1.512, 'epoch': 0.77}
                                                         77%|  | 5020/6500 [15:10:36<4:17:06, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5020
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3188, 'learning_rate': 1.2247081133310051e-05, 'epoch': 0.77}
{'loss': 0.3335, 'learning_rate': 1.2231235984783779e-05, 'epoch': 0.77}
{'loss': 0.3038, 'learning_rate': 1.2215399664476468e-05, 'epoch': 0.77}
{'loss': 0.3261, 'learning_rate': 1.2199572176089741e-05, 'epoch': 0.77}
{'loss': 0.3207, 'learning_rate': 1.2183753523323182e-05, 'epoch': 0.77}
 77%|  | 5021/6500 [15:10:47<4:50:29, 11.78s/it]                                                         77%|  | 5021/6500 [15:10:47<4:50:29, 11.78s/it] 77%|  | 5022/6500 [15:10:57<4:39:47, 11.36s/it]                                                         77%|  | 5022/6500 [15:10:57<4:39:47, 11.36s/it] 77%|  | 5023/6500 [15:11:08<4:32:17, 11.06s/it]                                                         77%|  | 5023/6500 [15:11:08<4:32:17, 11.06s/it] 77%|  | 5024/6500 [15:11:18<4:26:53, 10.85s/it]                                                         77%|  | 5024/6500 [15:11:18<4:26:53, 10.85s/it] 77%|  | 5025/6500 [15:11:29<4:23:11, 10.71s/it]                                                         77%|  | 5025/6500 [15:11:29<4:23:11, 10.71s/it] 77{'loss': 0.2877, 'learning_rate': 1.2167943709874313e-05, 'epoch': 0.77}
{'loss': 0.3296, 'learning_rate': 1.2152142739438565e-05, 'epoch': 0.77}
{'loss': 0.3054, 'learning_rate': 1.2136350615709352e-05, 'epoch': 0.77}
{'loss': 0.3311, 'learning_rate': 1.2120567342377965e-05, 'epoch': 0.77}
{'loss': 0.3205, 'learning_rate': 1.2104792923133646e-05, 'epoch': 0.77}
%|  | 5026/6500 [15:11:39<4:22:57, 10.70s/it]                                                         77%|  | 5026/6500 [15:11:39<4:22:57, 10.70s/it] 77%|  | 5027/6500 [15:11:50<4:20:23, 10.61s/it]                                                         77%|  | 5027/6500 [15:11:50<4:20:23, 10.61s/it] 77%|  | 5028/6500 [15:12:00<4:18:28, 10.54s/it]                                                         77%|  | 5028/6500 [15:12:00<4:18:28, 10.54s/it] 77%|  | 5029/6500 [15:12:10<4:17:01, 10.48s/it]                                                         77%|  | 5029/6500 [15:12:10<4:17:01, 10.48s/it] 77%|  | 5030/6500 [15:12:21<4:16:02, 10.45s/it]                                                         77%|  | 5030/6500 [15:12:21<4:16:02, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8699012398719788, 'eval_runtime': 3.9606, 'eval_samples_per_second': 5.807, 'eval_steps_per_second': 1.515, 'epoch': 0.77}
                                                         77%|  | 5030/6500 [15:12:25<4:16:02, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5030I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.322, 'learning_rate': 1.208902736166358e-05, 'epoch': 0.77}
{'loss': 0.3195, 'learning_rate': 1.2073270661652875e-05, 'epoch': 0.77}
{'loss': 0.3211, 'learning_rate': 1.2057522826784545e-05, 'epoch': 0.77}
{'loss': 0.3437, 'learning_rate': 1.2041783860739559e-05, 'epoch': 0.77}
{'loss': 0.3194, 'learning_rate': 1.2026053767196803e-05, 'epoch': 0.77}
 77%|  | 5031/6500 [15:12:36<4:48:06, 11.77s/it]                                                         77%|  | 5031/6500 [15:12:36<4:48:06, 11.77s/it] 77%|  | 5032/6500 [15:12:46<4:37:39, 11.35s/it]                                                         77%|  | 5032/6500 [15:12:46<4:37:39, 11.35s/it] 77%|  | 5033/6500 [15:12:56<4:30:21, 11.06s/it]                                                         77%|  | 5033/6500 [15:12:56<4:30:21, 11.06s/it] 77%|  | 5034/6500 [15:13:07<4:25:01, 10.85s/it]                                                         77%|  | 5034/6500 [15:13:07<4:25:01, 10.85s/it] 77%|  | 5035/6500 [15:13:17<4:21:16, 10.70s/it]                                                         77%|  | 5035/6500 [15:13:17<4:21:16, 10.70s/it] 77{'loss': 0.3332, 'learning_rate': 1.2010332549833098e-05, 'epoch': 0.77}
{'loss': 0.3296, 'learning_rate': 1.1994620212323177e-05, 'epoch': 0.77}
{'loss': 0.322, 'learning_rate': 1.1978916758339704e-05, 'epoch': 0.78}
{'loss': 0.3196, 'learning_rate': 1.196322219155327e-05, 'epoch': 0.78}
{'loss': 0.3374, 'learning_rate': 1.1947536515632374e-05, 'epoch': 0.78}
%|  | 5036/6500 [15:13:27<4:18:44, 10.60s/it]                                                         77%|  | 5036/6500 [15:13:27<4:18:44, 10.60s/it] 77%|  | 5037/6500 [15:13:38<4:16:50, 10.53s/it]                                                         77%|  | 5037/6500 [15:13:38<4:16:50, 10.53s/it] 78%|  | 5038/6500 [15:13:48<4:15:31, 10.49s/it]                                                         78%|  | 5038/6500 [15:13:48<4:15:31, 10.49s/it] 78%|  | 5039/6500 [15:13:59<4:14:32, 10.45s/it]                                                         78%|  | 5039/6500 [15:13:59<4:14:32, 10.45s/it] 78%|  | 5040/6500 [15:14:09<4:13:48, 10.43s/it]                                                         78%|  | 5040/6500 [15:14:09<4:13:48, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8674741983413696, 'eval_runtime': 4.1973, 'eval_samples_per_second': 5.48, 'eval_steps_per_second': 1.429, 'epoch': 0.78}
                                                         78%|  | 5040/6500 [15:14:13<4:13:48, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5040
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3177, 'learning_rate': 1.1931859734243461e-05, 'epoch': 0.78}
{'loss': 0.3154, 'learning_rate': 1.1916191851050873e-05, 'epoch': 0.78}
{'loss': 0.3045, 'learning_rate': 1.1900532869716908e-05, 'epoch': 0.78}
{'loss': 0.3211, 'learning_rate': 1.1884882793901714e-05, 'epoch': 0.78}
{'loss': 0.3758, 'learning_rate': 1.1869241627263427e-05, 'epoch': 0.78}
 78%|  | 5041/6500 [15:14:24<4:47:45, 11.83s/it]                                                         78%|  | 5041/6500 [15:14:24<4:47:45, 11.83s/it] 78%|  | 5042/6500 [15:14:34<4:36:55, 11.40s/it]                                                         78%|  | 5042/6500 [15:14:34<4:36:55, 11.40s/it] 78%|  | 5043/6500 [15:14:45<4:31:00, 11.16s/it]                                                         78%|  | 5043/6500 [15:14:45<4:31:00, 11.16s/it] 78%|  | 5044/6500 [15:14:55<4:24:56, 10.92s/it]                                                         78%|  | 5044/6500 [15:14:55<4:24:56, 10.92s/it] 78%|  | 5045/6500 [15:15:06<4:20:43, 10.75s/it]                                                         78%|  | 5045/6500 [15:15:06<4:20:43, 10.75s/it] 78{'loss': 0.3083, 'learning_rate': 1.1853609373458069e-05, 'epoch': 0.78}
{'loss': 0.3045, 'learning_rate': 1.1837986036139587e-05, 'epoch': 0.78}
{'loss': 0.332, 'learning_rate': 1.1822371618959837e-05, 'epoch': 0.78}
{'loss': 0.8307, 'learning_rate': 1.1806766125568603e-05, 'epoch': 0.78}
{'loss': 0.334, 'learning_rate': 1.1791169559613564e-05, 'epoch': 0.78}
%|  | 5046/6500 [15:15:16<4:17:46, 10.64s/it]                                                         78%|  | 5046/6500 [15:15:16<4:17:46, 10.64s/it] 78%|  | 5047/6500 [15:15:26<4:15:37, 10.56s/it]                                                         78%|  | 5047/6500 [15:15:26<4:15:37, 10.56s/it] 78%|  | 5048/6500 [15:15:37<4:14:05, 10.50s/it]                                                         78%|  | 5048/6500 [15:15:37<4:14:05, 10.50s/it] 78%|  | 5049/6500 [15:15:47<4:12:52, 10.46s/it]                                                         78%|  | 5049/6500 [15:15:47<4:12:52, 10.46s/it] 78%|  | 5050/6500 [15:15:58<4:12:16, 10.44s/it]                                                         78%|  | 5050/6500 [15:15:58<4:12:16, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8648681044578552, 'eval_runtime': 3.9863, 'eval_samples_per_second': 5.77, 'eval_steps_per_second': 1.505, 'epoch': 0.78}
                                                         78%|  | 5050/6500 [15:16:02<4:12:16, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.317, 'learning_rate': 1.177558192474033e-05, 'epoch': 0.78}
{'loss': 0.3218, 'learning_rate': 1.1760003224592415e-05, 'epoch': 0.78}
{'loss': 0.2965, 'learning_rate': 1.174443346281124e-05, 'epoch': 0.78}
{'loss': 0.333, 'learning_rate': 1.1728872643036154e-05, 'epoch': 0.78}
{'loss': 0.3015, 'learning_rate': 1.171332076890439e-05, 'epoch': 0.78}
 78%|  | 5051/6500 [15:16:13<4:44:51, 11.80s/it]                                                         78%|  | 5051/6500 [15:16:13<4:44:51, 11.80s/it] 78%|  | 5052/6500 [15:16:23<4:34:19, 11.37s/it]                                                         78%|  | 5052/6500 [15:16:23<4:34:19, 11.37s/it] 78%|  | 5053/6500 [15:16:33<4:26:55, 11.07s/it]                                                         78%|  | 5053/6500 [15:16:33<4:26:55, 11.07s/it] 78%|  | 5054/6500 [15:16:44<4:21:37, 10.86s/it]                                                         78%|  | 5054/6500 [15:16:44<4:21:37, 10.86s/it] 78%|  | 5055/6500 [15:16:54<4:17:57, 10.71s/it]                                                         78%|  | 5055/6500 [15:16:54<4:17:57, 10.71s/it] 78{'loss': 0.3004, 'learning_rate': 1.1697777844051105e-05, 'epoch': 0.78}
{'loss': 0.3077, 'learning_rate': 1.1682243872109367e-05, 'epoch': 0.78}
{'loss': 0.3087, 'learning_rate': 1.1666718856710152e-05, 'epoch': 0.78}
{'loss': 0.3192, 'learning_rate': 1.1651202801482331e-05, 'epoch': 0.78}
{'loss': 0.3022, 'learning_rate': 1.163569571005269e-05, 'epoch': 0.78}
%|  | 5056/6500 [15:17:04<4:15:24, 10.61s/it]                                                         78%|  | 5056/6500 [15:17:04<4:15:24, 10.61s/it] 78%|  | 5057/6500 [15:17:15<4:13:30, 10.54s/it]                                                         78%|  | 5057/6500 [15:17:15<4:13:30, 10.54s/it] 78%|  | 5058/6500 [15:17:25<4:12:02, 10.49s/it]                                                         78%|  | 5058/6500 [15:17:25<4:12:02, 10.49s/it] 78%|  | 5059/6500 [15:17:36<4:13:48, 10.57s/it]                                                         78%|  | 5059/6500 [15:17:36<4:13:48, 10.57s/it] 78%|  | 5060/6500 [15:17:46<4:12:18, 10.51s/it]                                                         78%|  | 5060/6500 [15:17:46<4:12:18, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8720164895057678, 'eval_runtime': 3.9757, 'eval_samples_per_second': 5.785, 'eval_steps_per_second': 1.509, 'epoch': 0.78}
                                                         78%|  | 5060/6500 [15:17:50<4:12:18, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3393, 'learning_rate': 1.1620197586045922e-05, 'epoch': 0.78}
{'loss': 0.3202, 'learning_rate': 1.160470843308461e-05, 'epoch': 0.78}
{'loss': 0.3397, 'learning_rate': 1.1589228254789258e-05, 'epoch': 0.78}
{'loss': 0.331, 'learning_rate': 1.1573757054778261e-05, 'epoch': 0.78}
{'loss': 0.3268, 'learning_rate': 1.1558294836667916e-05, 'epoch': 0.78}
 78%|  | 5061/6500 [15:18:01<4:44:07, 11.85s/it]                                                         78%|  | 5061/6500 [15:18:01<4:44:07, 11.85s/it] 78%|  | 5062/6500 [15:18:12<4:33:21, 11.41s/it]                                                         78%|  | 5062/6500 [15:18:12<4:33:21, 11.41s/it] 78%|  | 5063/6500 [15:18:22<4:25:46, 11.10s/it]                                                         78%|  | 5063/6500 [15:18:22<4:25:46, 11.10s/it] 78%|  | 5064/6500 [15:18:32<4:20:19, 10.88s/it]                                                         78%|  | 5064/6500 [15:18:32<4:20:19, 10.88s/it] 78%|  | 5065/6500 [15:18:43<4:16:27, 10.72s/it]                                                         78%|  | 5065/6500 [15:18:43<4:16:27, 10.72s/it] 78{'loss': 0.3286, 'learning_rate': 1.1542841604072435e-05, 'epoch': 0.78}
{'loss': 0.3241, 'learning_rate': 1.1527397360603897e-05, 'epoch': 0.78}
{'loss': 0.3192, 'learning_rate': 1.1511962109872305e-05, 'epoch': 0.78}
{'loss': 0.3284, 'learning_rate': 1.1496535855485557e-05, 'epoch': 0.78}
{'loss': 0.3448, 'learning_rate': 1.1481118601049452e-05, 'epoch': 0.78}
%|  | 5066/6500 [15:18:53<4:13:47, 10.62s/it]                                                         78%|  | 5066/6500 [15:18:53<4:13:47, 10.62s/it] 78%|  | 5067/6500 [15:19:03<4:12:00, 10.55s/it]                                                         78%|  | 5067/6500 [15:19:03<4:12:00, 10.55s/it] 78%|  | 5068/6500 [15:19:14<4:10:28, 10.49s/it]                                                         78%|  | 5068/6500 [15:19:14<4:10:28, 10.49s/it] 78%|  | 5069/6500 [15:19:24<4:09:28, 10.46s/it]                                                         78%|  | 5069/6500 [15:19:24<4:09:28, 10.46s/it] 78%|  | 5070/6500 [15:19:35<4:08:36, 10.43s/it]                                                         78%|  | 5070/6500 [15:19:35<4:08:36, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8677067756652832, 'eval_runtime': 3.9662, 'eval_samples_per_second': 5.799, 'eval_steps_per_second': 1.513, 'epoch': 0.78}
                                                         78%|  | 5070/6500 [15:19:39<4:08:36, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5070I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5070

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5070/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2979, 'learning_rate': 1.1465710350167674e-05, 'epoch': 0.78}
{'loss': 0.3213, 'learning_rate': 1.1450311106441807e-05, 'epoch': 0.78}
{'loss': 0.3082, 'learning_rate': 1.1434920873471333e-05, 'epoch': 0.78}
{'loss': 0.3423, 'learning_rate': 1.1419539654853628e-05, 'epoch': 0.78}
{'loss': 0.3726, 'learning_rate': 1.1404167454183957e-05, 'epoch': 0.78}
 78%|  | 5071/6500 [15:19:49<4:39:59, 11.76s/it]                                                         78%|  | 5071/6500 [15:19:49<4:39:59, 11.76s/it] 78%|  | 5072/6500 [15:20:00<4:30:05, 11.35s/it]                                                         78%|  | 5072/6500 [15:20:00<4:30:05, 11.35s/it] 78%|  | 5073/6500 [15:20:10<4:22:56, 11.06s/it]                                                         78%|  | 5073/6500 [15:20:10<4:22:56, 11.06s/it] 78%|  | 5074/6500 [15:20:21<4:17:52, 10.85s/it]                                                         78%|  | 5074/6500 [15:20:21<4:17:52, 10.85s/it] 78%|  | 5075/6500 [15:20:31<4:17:40, 10.85s/it]                                                         78%|  | 5075/6500 [15:20:31<4:17:40, 10.85s/it] 78{'loss': 0.3098, 'learning_rate': 1.138880427505547e-05, 'epoch': 0.78}
{'loss': 0.3319, 'learning_rate': 1.1373450121059242e-05, 'epoch': 0.78}
{'loss': 0.3065, 'learning_rate': 1.1358104995784186e-05, 'epoch': 0.78}
{'loss': 0.8272, 'learning_rate': 1.1342768902817136e-05, 'epoch': 0.78}
{'loss': 0.3307, 'learning_rate': 1.1327441845742814e-05, 'epoch': 0.78}
%|  | 5076/6500 [15:20:42<4:14:04, 10.71s/it]                                                         78%|  | 5076/6500 [15:20:42<4:14:04, 10.71s/it] 78%|  | 5077/6500 [15:20:52<4:11:47, 10.62s/it]                                                         78%|  | 5077/6500 [15:20:52<4:11:47, 10.62s/it] 78%|  | 5078/6500 [15:21:03<4:09:50, 10.54s/it]                                                         78%|  | 5078/6500 [15:21:03<4:09:50, 10.54s/it] 78%|  | 5079/6500 [15:21:14<4:14:25, 10.74s/it]                                                         78%|  | 5079/6500 [15:21:14<4:14:25, 10.74s/it] 78%|  | 5080/6500 [15:21:24<4:12:19, 10.66s/it]                                                         78%|  | 5080/6500 [15:21:24<4:12:19, 10.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8628273606300354, 'eval_runtime': 4.4979, 'eval_samples_per_second': 5.113, 'eval_steps_per_second': 1.334, 'epoch': 0.78}
                                                         78%|  | 5080/6500 [15:21:29<4:12:19, 10.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5080/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.331, 'learning_rate': 1.1312123828143827e-05, 'epoch': 0.78}
{'loss': 0.3009, 'learning_rate': 1.1296814853600673e-05, 'epoch': 0.78}
{'loss': 0.3369, 'learning_rate': 1.1281514925691722e-05, 'epoch': 0.78}
{'loss': 0.3153, 'learning_rate': 1.126622404799325e-05, 'epoch': 0.78}
{'loss': 0.2977, 'learning_rate': 1.1250942224079403e-05, 'epoch': 0.78}
 78%|  | 5081/6500 [15:21:40<4:46:46, 12.13s/it]                                                         78%|  | 5081/6500 [15:21:40<4:46:46, 12.13s/it] 78%|  | 5082/6500 [15:21:50<4:34:36, 11.62s/it]                                                         78%|  | 5082/6500 [15:21:50<4:34:36, 11.62s/it] 78%|  | 5083/6500 [15:22:01<4:25:30, 11.24s/it]                                                         78%|  | 5083/6500 [15:22:01<4:25:30, 11.24s/it] 78%|  | 5084/6500 [15:22:11<4:19:09, 10.98s/it]                                                         78%|  | 5084/6500 [15:22:11<4:19:09, 10.98s/it] 78%|  | 5085/6500 [15:22:21<4:14:39, 10.80s/it]                                                         78%|  | 5085/6500 [15:22:21<4:14:39, 10.80s/it] 78{'loss': 0.3123, 'learning_rate': 1.1235669457522207e-05, 'epoch': 0.78}
{'loss': 0.3086, 'learning_rate': 1.1220405751891588e-05, 'epoch': 0.78}
{'loss': 0.3177, 'learning_rate': 1.1205151110755352e-05, 'epoch': 0.78}
{'loss': 0.3182, 'learning_rate': 1.1189905537679157e-05, 'epoch': 0.78}
{'loss': 0.3147, 'learning_rate': 1.1174669036226571e-05, 'epoch': 0.78}
%|  | 5086/6500 [15:22:32<4:11:24, 10.67s/it]                                                         78%|  | 5086/6500 [15:22:32<4:11:24, 10.67s/it] 78%|  | 5087/6500 [15:22:42<4:09:07, 10.58s/it]                                                         78%|  | 5087/6500 [15:22:42<4:09:07, 10.58s/it] 78%|  | 5088/6500 [15:22:52<4:07:39, 10.52s/it]                                                         78%|  | 5088/6500 [15:22:52<4:07:39, 10.52s/it] 78%|  | 5089/6500 [15:23:03<4:06:31, 10.48s/it]                                                         78%|  | 5089/6500 [15:23:03<4:06:31, 10.48s/it] 78%|  | 5090/6500 [15:23:13<4:05:39, 10.45s/it]                                                         78%|  | 5090/6500 [15:23:13<4:05:39, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8713815808296204, 'eval_runtime': 3.9612, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.515, 'epoch': 0.78}
                                                         78%|  | 5090/6500 [15:23:17<4:05:39, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5090/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3159, 'learning_rate': 1.1159441609959038e-05, 'epoch': 0.78}
{'loss': 0.3189, 'learning_rate': 1.1144223262435871e-05, 'epoch': 0.78}
{'loss': 0.3341, 'learning_rate': 1.1129013997214271e-05, 'epoch': 0.78}
{'loss': 0.2999, 'learning_rate': 1.111381381784931e-05, 'epoch': 0.78}
{'loss': 0.3232, 'learning_rate': 1.1098622727893937e-05, 'epoch': 0.78}
 78%|  | 5091/6500 [15:23:28<4:39:16, 11.89s/it]                                                         78%|  | 5091/6500 [15:23:28<4:39:16, 11.89s/it] 78%|  | 5092/6500 [15:23:39<4:28:32, 11.44s/it]                                                         78%|  | 5092/6500 [15:23:39<4:28:32, 11.44s/it] 78%|  | 5093/6500 [15:23:49<4:20:57, 11.13s/it]                                                         78%|  | 5093/6500 [15:23:49<4:20:57, 11.13s/it] 78%|  | 5094/6500 [15:24:00<4:15:42, 10.91s/it]                                                         78%|  | 5094/6500 [15:24:00<4:15:42, 10.91s/it] 78%|  | 5095/6500 [15:24:10<4:11:55, 10.76s/it]                                                         78%|  | 5095/6500 [15:24:10<4:11:55, 10.76s/it] 78{'loss': 0.3283, 'learning_rate': 1.1083440730898974e-05, 'epoch': 0.78}
{'loss': 0.302, 'learning_rate': 1.1068267830413126e-05, 'epoch': 0.78}
{'loss': 0.3282, 'learning_rate': 1.105310402998297e-05, 'epoch': 0.78}
{'loss': 0.3238, 'learning_rate': 1.1037949333152953e-05, 'epoch': 0.78}
{'loss': 0.326, 'learning_rate': 1.102280374346537e-05, 'epoch': 0.78}
%|  | 5096/6500 [15:24:20<4:09:10, 10.65s/it]                                                         78%|  | 5096/6500 [15:24:20<4:09:10, 10.65s/it] 78%|  | 5097/6500 [15:24:31<4:07:22, 10.58s/it]                                                         78%|  | 5097/6500 [15:24:31<4:07:22, 10.58s/it] 78%|  | 5098/6500 [15:24:41<4:05:54, 10.52s/it]                                                         78%|  | 5098/6500 [15:24:41<4:05:54, 10.52s/it] 78%|  | 5099/6500 [15:24:52<4:05:05, 10.50s/it]                                                         78%|  | 5099/6500 [15:24:52<4:05:05, 10.50s/it] 78%|  | 5100/6500 [15:25:02<4:04:11, 10.47s/it]                                                         78%|  | 5100/6500 [15:25:02<4:04:11, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711079359054565, 'eval_runtime': 3.9612, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.515, 'epoch': 0.78}
                                                         78%|  | 5100/6500 [15:25:06<4:04:11, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3101, 'learning_rate': 1.1007667264460431e-05, 'epoch': 0.78}
{'loss': 0.3109, 'learning_rate': 1.099253989967619e-05, 'epoch': 0.78}
{'loss': 0.2972, 'learning_rate': 1.0977421652648568e-05, 'epoch': 0.79}
{'loss': 0.392, 'learning_rate': 1.0962312526911383e-05, 'epoch': 0.79}
{'loss': 0.3143, 'learning_rate': 1.0947212525996292e-05, 'epoch': 0.79}
 78%|  | 5101/6500 [15:25:17<4:35:17, 11.81s/it]                                                         78%|  | 5101/6500 [15:25:17<4:35:17, 11.81s/it] 78%|  | 5102/6500 [15:25:27<4:25:13, 11.38s/it]                                                         78%|  | 5102/6500 [15:25:27<4:25:13, 11.38s/it] 79%|  | 5103/6500 [15:25:38<4:18:06, 11.09s/it]                                                         79%|  | 5103/6500 [15:25:38<4:18:06, 11.09s/it] 79%|  | 5104/6500 [15:25:48<4:13:08, 10.88s/it]                                                         79%|  | 5104/6500 [15:25:48<4:13:08, 10.88s/it] 79%|  | 5105/6500 [15:25:59<4:09:27, 10.73s/it]                                                         79%|  | 5105/6500 [15:25:59<4:09:27, 10.73s/it] 79{'loss': 0.3031, 'learning_rate': 1.0932121653432831e-05, 'epoch': 0.79}
{'loss': 0.3279, 'learning_rate': 1.0917039912748395e-05, 'epoch': 0.79}
{'loss': 0.8292, 'learning_rate': 1.0901967307468269e-05, 'epoch': 0.79}
{'loss': 0.3314, 'learning_rate': 1.0886903841115547e-05, 'epoch': 0.79}
{'loss': 0.3316, 'learning_rate': 1.087184951721124e-05, 'epoch': 0.79}
%|  | 5106/6500 [15:26:09<4:06:47, 10.62s/it]                                                         79%|  | 5106/6500 [15:26:09<4:06:47, 10.62s/it] 79%|  | 5107/6500 [15:26:20<4:10:20, 10.78s/it]                                                         79%|  | 5107/6500 [15:26:20<4:10:20, 10.78s/it] 79%|  | 5108/6500 [15:26:31<4:07:28, 10.67s/it]                                                         79%|  | 5108/6500 [15:26:31<4:07:28, 10.67s/it] 79%|  | 5109/6500 [15:26:41<4:05:16, 10.58s/it]                                                         79%|  | 5109/6500 [15:26:41<4:05:16, 10.58s/it] 79%|  | 5110/6500 [15:26:52<4:06:13, 10.63s/it]                                                         79%|  | 5110/6500 [15:26:52<4:06:13, 10.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8656061291694641, 'eval_runtime': 4.0617, 'eval_samples_per_second': 5.663, 'eval_steps_per_second': 1.477, 'epoch': 0.79}
                                                         79%|  | 5110/6500 [15:26:56<4:06:13, 10.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5110/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3267, 'learning_rate': 1.0856804339274207e-05, 'epoch': 0.79}
{'loss': 0.2999, 'learning_rate': 1.0841768310821165e-05, 'epoch': 0.79}
{'loss': 0.3311, 'learning_rate': 1.0826741435366695e-05, 'epoch': 0.79}
{'loss': 0.3156, 'learning_rate': 1.0811723716423233e-05, 'epoch': 0.79}
{'loss': 0.2961, 'learning_rate': 1.0796715157501086e-05, 'epoch': 0.79}
 79%|  | 5111/6500 [15:27:07<4:36:37, 11.95s/it]                                                         79%|  | 5111/6500 [15:27:07<4:36:37, 11.95s/it] 79%|  | 5112/6500 [15:27:17<4:25:18, 11.47s/it]                                                         79%|  | 5112/6500 [15:27:17<4:25:18, 11.47s/it] 79%|  | 5113/6500 [15:27:27<4:17:16, 11.13s/it]                                                         79%|  | 5113/6500 [15:27:27<4:17:16, 11.13s/it] 79%|  | 5114/6500 [15:27:38<4:11:38, 10.89s/it]                                                         79%|  | 5114/6500 [15:27:38<4:11:38, 10.89s/it] 79%|  | 5115/6500 [15:27:48<4:07:51, 10.74s/it]                                                         79%|  | 5115/6500 [15:27:48<4:07:51, 10.74s/it] 79{'loss': 0.3206, 'learning_rate': 1.0781715762108412e-05, 'epoch': 0.79}
{'loss': 0.3084, 'learning_rate': 1.0766725533751232e-05, 'epoch': 0.79}
{'loss': 0.3193, 'learning_rate': 1.0751744475933411e-05, 'epoch': 0.79}
{'loss': 0.3098, 'learning_rate': 1.0736772592156697e-05, 'epoch': 0.79}
{'loss': 0.3215, 'learning_rate': 1.0721809885920653e-05, 'epoch': 0.79}
%|  | 5116/6500 [15:27:58<4:04:57, 10.62s/it]                                                         79%|  | 5116/6500 [15:27:58<4:04:57, 10.62s/it] 79%|  | 5117/6500 [15:28:09<4:02:55, 10.54s/it]                                                         79%|  | 5117/6500 [15:28:09<4:02:55, 10.54s/it] 79%|  | 5118/6500 [15:28:19<4:01:30, 10.48s/it]                                                         79%|  | 5118/6500 [15:28:19<4:01:30, 10.48s/it] 79%|  | 5119/6500 [15:28:29<4:00:16, 10.44s/it]                                                         79%|  | 5119/6500 [15:28:29<4:00:16, 10.44s/it] 79%|  | 5120/6500 [15:28:40<3:59:22, 10.41s/it]                                                         79%|  | 5120/6500 [15:28:40<3:59:22, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8702948093414307, 'eval_runtime': 3.957, 'eval_samples_per_second': 5.812, 'eval_steps_per_second': 1.516, 'epoch': 0.79}
                                                         79%|  | 5120/6500 [15:28:44<3:59:22, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3031, 'learning_rate': 1.0706856360722739e-05, 'epoch': 0.79}
{'loss': 0.3378, 'learning_rate': 1.0691912020058236e-05, 'epoch': 0.79}
{'loss': 0.3246, 'learning_rate': 1.0676976867420308e-05, 'epoch': 0.79}
{'loss': 0.323, 'learning_rate': 1.0662050906299942e-05, 'epoch': 0.79}
{'loss': 0.3278, 'learning_rate': 1.0647134140186004e-05, 'epoch': 0.79}
 79%|  | 5121/6500 [15:28:55<4:29:43, 11.74s/it]                                                         79%|  | 5121/6500 [15:28:55<4:29:43, 11.74s/it] 79%|  | 5122/6500 [15:29:05<4:19:56, 11.32s/it]                                                         79%|  | 5122/6500 [15:29:05<4:19:56, 11.32s/it] 79%|  | 5123/6500 [15:29:16<4:17:57, 11.24s/it]                                                         79%|  | 5123/6500 [15:29:16<4:17:57, 11.24s/it] 79%|  | 5124/6500 [15:29:26<4:11:46, 10.98s/it]                                                         79%|  | 5124/6500 [15:29:26<4:11:46, 10.98s/it] 79%|  | 5125/6500 [15:29:37<4:07:18, 10.79s/it]                                                         79%|  | 5125/6500 [15:29:37<4:07:18, 10.79s/it] 79{'loss': 0.3266, 'learning_rate': 1.0632226572565191e-05, 'epoch': 0.79}
{'loss': 0.3251, 'learning_rate': 1.0617328206922056e-05, 'epoch': 0.79}
{'loss': 0.311, 'learning_rate': 1.0602439046738999e-05, 'epoch': 0.79}
{'loss': 0.3356, 'learning_rate': 1.058755909549628e-05, 'epoch': 0.79}
{'loss': 0.3161, 'learning_rate': 1.0572688356672e-05, 'epoch': 0.79}
%|  | 5126/6500 [15:29:47<4:04:14, 10.67s/it]                                                         79%|  | 5126/6500 [15:29:47<4:04:14, 10.67s/it] 79%|  | 5127/6500 [15:29:58<4:01:58, 10.57s/it]                                                         79%|  | 5127/6500 [15:29:58<4:01:58, 10.57s/it] 79%|  | 5128/6500 [15:30:08<4:00:16, 10.51s/it]                                                         79%|  | 5128/6500 [15:30:08<4:00:16, 10.51s/it] 79%|  | 5129/6500 [15:30:18<3:58:59, 10.46s/it]                                                         79%|  | 5129/6500 [15:30:18<3:58:59, 10.46s/it] 79%|  | 5130/6500 [15:30:29<3:58:10, 10.43s/it]                                                         79%|  | 5130/6500 [15:30:29<3:58:10, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.869802713394165, 'eval_runtime': 3.9649, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.79}
                                                         79%|  | 5130/6500 [15:30:33<3:58:10, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5130/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5130/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3115, 'learning_rate': 1.0557826833742079e-05, 'epoch': 0.79}
{'loss': 0.304, 'learning_rate': 1.0542974530180327e-05, 'epoch': 0.79}
{'loss': 0.3241, 'learning_rate': 1.0528131449458372e-05, 'epoch': 0.79}
{'loss': 0.3766, 'learning_rate': 1.0513297595045702e-05, 'epoch': 0.79}
{'loss': 0.3261, 'learning_rate': 1.0498472970409635e-05, 'epoch': 0.79}
 79%|  | 5131/6500 [15:30:44<4:30:39, 11.86s/it]                                                         79%|  | 5131/6500 [15:30:44<4:30:39, 11.86s/it] 79%|  | 5132/6500 [15:30:54<4:20:09, 11.41s/it]                                                         79%|  | 5132/6500 [15:30:54<4:20:09, 11.41s/it] 79%|  | 5133/6500 [15:31:04<4:12:47, 11.10s/it]                                                         79%|  | 5133/6500 [15:31:04<4:12:47, 11.10s/it] 79%|  | 5134/6500 [15:31:15<4:07:30, 10.87s/it]                                                         79%|  | 5134/6500 [15:31:15<4:07:30, 10.87s/it] 79%|  | 5135/6500 [15:31:25<4:03:37, 10.71s/it]                                                         79%|  | 5135/6500 [15:31:25<4:03:37, 10.71s/it] 79{'loss': 0.3071, 'learning_rate': 1.0483657579015338e-05, 'epoch': 0.79}
{'loss': 0.327, 'learning_rate': 1.046885142432582e-05, 'epoch': 0.79}
{'loss': 0.8411, 'learning_rate': 1.0454054509801926e-05, 'epoch': 0.79}
{'loss': 0.3332, 'learning_rate': 1.0439266838902345e-05, 'epoch': 0.79}
{'loss': 0.3184, 'learning_rate': 1.042448841508361e-05, 'epoch': 0.79}
%|  | 5136/6500 [15:31:36<4:01:01, 10.60s/it]                                                         79%|  | 5136/6500 [15:31:36<4:01:01, 10.60s/it] 79%|  | 5137/6500 [15:31:46<3:59:01, 10.52s/it]                                                         79%|  | 5137/6500 [15:31:46<3:59:01, 10.52s/it] 79%|  | 5138/6500 [15:31:56<3:57:37, 10.47s/it]                                                         79%|  | 5138/6500 [15:31:56<3:57:37, 10.47s/it] 79%|  | 5139/6500 [15:32:08<4:08:00, 10.93s/it]                                                         79%|  | 5139/6500 [15:32:08<4:08:00, 10.93s/it] 79%|  | 5140/6500 [15:32:19<4:04:06, 10.77s/it]                                                         79%|  | 5140/6500 [15:32:19<4:04:06, 10.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8653706312179565, 'eval_runtime': 3.9527, 'eval_samples_per_second': 5.819, 'eval_steps_per_second': 1.518, 'epoch': 0.79}
                                                         79%|  | 5140/6500 [15:32:23<4:04:06, 10.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5140
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5140/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5140/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3084, 'learning_rate': 1.0409719241800098e-05, 'epoch': 0.79}
{'loss': 0.3053, 'learning_rate': 1.0394959322503989e-05, 'epoch': 0.79}
{'loss': 0.3248, 'learning_rate': 1.038020866064533e-05, 'epoch': 0.79}
{'loss': 0.3065, 'learning_rate': 1.0365467259672013e-05, 'epoch': 0.79}
{'loss': 0.3058, 'learning_rate': 1.0350735123029736e-05, 'epoch': 0.79}
 79%|  | 5141/6500 [15:32:33<4:31:36, 11.99s/it]                                                         79%|  | 5141/6500 [15:32:33<4:31:36, 11.99s/it] 79%|  | 5142/6500 [15:32:44<4:20:14, 11.50s/it]                                                         79%|  | 5142/6500 [15:32:44<4:20:14, 11.50s/it] 79%|  | 5143/6500 [15:32:54<4:12:14, 11.15s/it]                                                         79%|  | 5143/6500 [15:32:54<4:12:14, 11.15s/it] 79%|  | 5144/6500 [15:33:04<4:06:33, 10.91s/it]                                                         79%|  | 5144/6500 [15:33:04<4:06:33, 10.91s/it] 79%|  | 5145/6500 [15:33:15<4:02:40, 10.75s/it]                                                         79%|  | 5145/6500 [15:33:15<4:02:40, 10.75s/it] 79{'loss': 0.3043, 'learning_rate': 1.0336012254162053e-05, 'epoch': 0.79}
{'loss': 0.313, 'learning_rate': 1.0321298656510342e-05, 'epoch': 0.79}
{'loss': 0.3181, 'learning_rate': 1.0306594333513825e-05, 'epoch': 0.79}
{'loss': 0.2989, 'learning_rate': 1.029189928860954e-05, 'epoch': 0.79}
{'loss': 0.3279, 'learning_rate': 1.027721352523237e-05, 'epoch': 0.79}
%|  | 5146/6500 [15:33:25<3:59:52, 10.63s/it]                                                         79%|  | 5146/6500 [15:33:25<3:59:52, 10.63s/it] 79%|  | 5147/6500 [15:33:36<3:57:55, 10.55s/it]                                                         79%|  | 5147/6500 [15:33:36<3:57:55, 10.55s/it] 79%|  | 5148/6500 [15:33:46<3:56:22, 10.49s/it]                                                         79%|  | 5148/6500 [15:33:46<3:56:22, 10.49s/it] 79%|  | 5149/6500 [15:33:56<3:55:20, 10.45s/it]                                                         79%|  | 5149/6500 [15:33:56<3:55:20, 10.45s/it] 79%|  | 5150/6500 [15:34:07<3:54:30, 10.42s/it]                                                         79%|  | 5150/6500 [15:34:07<3:54:30, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8712230920791626, 'eval_runtime': 3.9503, 'eval_samples_per_second': 5.822, 'eval_steps_per_second': 1.519, 'epoch': 0.79}
                                                         79%|  | 5150/6500 [15:34:11<3:54:30, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5150
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3144, 'learning_rate': 1.0262537046815018e-05, 'epoch': 0.79}
{'loss': 0.3216, 'learning_rate': 1.0247869856788023e-05, 'epoch': 0.79}
{'loss': 0.3168, 'learning_rate': 1.0233211958579764e-05, 'epoch': 0.79}
{'loss': 0.3093, 'learning_rate': 1.0218563355616411e-05, 'epoch': 0.79}
{'loss': 0.3257, 'learning_rate': 1.0203924051322e-05, 'epoch': 0.79}
 79%|  | 5151/6500 [15:34:21<4:24:15, 11.75s/it]                                                         79%|  | 5151/6500 [15:34:21<4:24:15, 11.75s/it] 79%|  | 5152/6500 [15:34:32<4:14:38, 11.33s/it]                                                         79%|  | 5152/6500 [15:34:32<4:14:38, 11.33s/it] 79%|  | 5153/6500 [15:34:42<4:07:52, 11.04s/it]                                                         79%|  | 5153/6500 [15:34:42<4:07:52, 11.04s/it] 79%|  | 5154/6500 [15:34:53<4:03:23, 10.85s/it]                                                         79%|  | 5154/6500 [15:34:53<4:03:23, 10.85s/it] 79%|  | 5155/6500 [15:35:03<4:00:03, 10.71s/it]                                                         79%|  | 5155/6500 [15:35:03<4:00:03, 10.71s/it] 79{'loss': 0.3153, 'learning_rate': 1.0189294049118374e-05, 'epoch': 0.79}
{'loss': 0.3193, 'learning_rate': 1.0174673352425217e-05, 'epoch': 0.79}
{'loss': 0.3226, 'learning_rate': 1.0160061964660017e-05, 'epoch': 0.79}
{'loss': 0.3379, 'learning_rate': 1.0145459889238106e-05, 'epoch': 0.79}
{'loss': 0.3002, 'learning_rate': 1.0130867129572625e-05, 'epoch': 0.79}
%|  | 5156/6500 [15:35:14<3:59:18, 10.68s/it]                                                         79%|  | 5156/6500 [15:35:14<3:59:18, 10.68s/it] 79%|  | 5157/6500 [15:35:24<3:56:58, 10.59s/it]                                                         79%|  | 5157/6500 [15:35:24<3:56:58, 10.59s/it] 79%|  | 5158/6500 [15:35:34<3:55:28, 10.53s/it]                                                         79%|  | 5158/6500 [15:35:34<3:55:28, 10.53s/it] 79%|  | 5159/6500 [15:35:45<3:54:18, 10.48s/it]                                                         79%|  | 5159/6500 [15:35:45<3:54:18, 10.48s/it] 79%|  | 5160/6500 [15:35:55<3:53:28, 10.45s/it]                                                         79%|  | 5160/6500 [15:35:55<3:53:28, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8709299564361572, 'eval_runtime': 4.1938, 'eval_samples_per_second': 5.484, 'eval_steps_per_second': 1.431, 'epoch': 0.79}
                                                         79%|  | 5160/6500 [15:35:59<3:53:28, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3194, 'learning_rate': 1.011628368907454e-05, 'epoch': 0.79}
{'loss': 0.2956, 'learning_rate': 1.0101709571152651e-05, 'epoch': 0.79}
{'loss': 0.3463, 'learning_rate': 1.0087144779213564e-05, 'epoch': 0.79}
{'loss': 0.3613, 'learning_rate': 1.0072589316661723e-05, 'epoch': 0.79}
{'loss': 0.3063, 'learning_rate': 1.0058043186899351e-05, 'epoch': 0.79}
 79%|  | 5161/6500 [15:36:10<4:24:26, 11.85s/it]                                                         79%|  | 5161/6500 [15:36:10<4:24:26, 11.85s/it] 79%|  | 5162/6500 [15:36:21<4:14:23, 11.41s/it]                                                         79%|  | 5162/6500 [15:36:21<4:14:23, 11.41s/it] 79%|  | 5163/6500 [15:36:31<4:07:14, 11.10s/it]                                                         79%|  | 5163/6500 [15:36:31<4:07:14, 11.10s/it] 79%|  | 5164/6500 [15:36:41<4:02:08, 10.87s/it]                                                         79%|  | 5164/6500 [15:36:41<4:02:08, 10.87s/it] 79%|  | 5165/6500 [15:36:52<3:58:33, 10.72s/it]                                                         79%|  | 5165/6500 [15:36:52<3:58:33, 10.72s/it] 79{'loss': 0.3198, 'learning_rate': 1.0043506393326535e-05, 'epoch': 0.79}
{'loss': 0.3144, 'learning_rate': 1.0028978939341161e-05, 'epoch': 0.79}
{'loss': 0.8261, 'learning_rate': 1.0014460828338928e-05, 'epoch': 0.8}
{'loss': 0.3226, 'learning_rate': 9.999952063713364e-06, 'epoch': 0.8}
{'loss': 0.3237, 'learning_rate': 9.985452648855803e-06, 'epoch': 0.8}
%|  | 5166/6500 [15:37:02<3:56:05, 10.62s/it]                                                         79%|  | 5166/6500 [15:37:02<3:56:05, 10.62s/it] 79%|  | 5167/6500 [15:37:12<3:54:16, 10.54s/it]                                                         79%|  | 5167/6500 [15:37:12<3:54:16, 10.54s/it] 80%|  | 5168/6500 [15:37:23<3:52:53, 10.49s/it]                                                         80%|  | 5168/6500 [15:37:23<3:52:53, 10.49s/it] 80%|  | 5169/6500 [15:37:33<3:52:05, 10.46s/it]                                                         80%|  | 5169/6500 [15:37:33<3:52:05, 10.46s/it] 80%|  | 5170/6500 [15:37:44<3:51:17, 10.43s/it]                                                         80%|  | 5170/6500 [15:37:44<3:51:17, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8675674796104431, 'eval_runtime': 3.9463, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.8}
                                                         80%|  | 5170/6500 [15:37:48<3:51:17, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3034, 'learning_rate': 9.970962587155386e-06, 'epoch': 0.8}
{'loss': 0.3254, 'learning_rate': 9.95648188199909e-06, 'epoch': 0.8}
{'loss': 0.3174, 'learning_rate': 9.942010536771685e-06, 'epoch': 0.8}
{'loss': 0.2916, 'learning_rate': 9.927548554855758e-06, 'epoch': 0.8}
{'loss': 0.3096, 'learning_rate': 9.913095939631722e-06, 'epoch': 0.8}
 80%|  | 5171/6500 [15:37:58<4:20:16, 11.75s/it]                                                         80%|  | 5171/6500 [15:37:58<4:20:16, 11.75s/it] 80%|  | 5172/6500 [15:38:09<4:13:10, 11.44s/it]                                                         80%|  | 5172/6500 [15:38:09<4:13:10, 11.44s/it] 80%|  | 5173/6500 [15:38:19<4:05:53, 11.12s/it]                                                         80%|  | 5173/6500 [15:38:19<4:05:53, 11.12s/it] 80%|  | 5174/6500 [15:38:30<4:00:40, 10.89s/it]                                                         80%|  | 5174/6500 [15:38:30<4:00:40, 10.89s/it] 80%|  | 5175/6500 [15:38:40<3:57:00, 10.73s/it]                                                         80%|  | 5175/6500 [15:38:40<3:57:00, 10.73s/it] 80{'loss': 0.3055, 'learning_rate': 9.898652694477773e-06, 'epoch': 0.8}
{'loss': 0.3135, 'learning_rate': 9.884218822769931e-06, 'epoch': 0.8}
{'loss': 0.3175, 'learning_rate': 9.869794327882016e-06, 'epoch': 0.8}
{'loss': 0.32, 'learning_rate': 9.8553792131857e-06, 'epoch': 0.8}
{'loss': 0.313, 'learning_rate': 9.840973482050403e-06, 'epoch': 0.8}
%|  | 5176/6500 [15:38:51<3:54:44, 10.64s/it]                                                         80%|  | 5176/6500 [15:38:51<3:54:44, 10.64s/it] 80%|  | 5177/6500 [15:39:01<3:52:54, 10.56s/it]                                                         80%|  | 5177/6500 [15:39:01<3:52:54, 10.56s/it] 80%|  | 5178/6500 [15:39:11<3:51:25, 10.50s/it]                                                         80%|  | 5178/6500 [15:39:11<3:51:25, 10.50s/it] 80%|  | 5179/6500 [15:39:22<3:50:24, 10.47s/it]                                                         80%|  | 5179/6500 [15:39:22<3:50:24, 10.47s/it] 80%|  | 5180/6500 [15:39:32<3:49:38, 10.44s/it]                                                         80%|  | 5180/6500 [15:39:32<3:49:38, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8725002408027649, 'eval_runtime': 4.2333, 'eval_samples_per_second': 5.433, 'eval_steps_per_second': 1.417, 'epoch': 0.8}
                                                         80%|  | 5180/6500 [15:39:36<3:49:38, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3059, 'learning_rate': 9.82657713784339e-06, 'epoch': 0.8}
{'loss': 0.3407, 'learning_rate': 9.81219018392971e-06, 'epoch': 0.8}
{'loss': 0.3043, 'learning_rate': 9.797812623672225e-06, 'epoch': 0.8}
{'loss': 0.3192, 'learning_rate': 9.783444460431624e-06, 'epoch': 0.8}
{'loss': 0.3433, 'learning_rate': 9.769085697566343e-06, 'epoch': 0.8}
 80%|  | 5181/6500 [15:39:47<4:20:49, 11.86s/it]                                                         80%|  | 5181/6500 [15:39:47<4:20:49, 11.86s/it] 80%|  | 5182/6500 [15:39:58<4:10:40, 11.41s/it]                                                         80%|  | 5182/6500 [15:39:58<4:10:40, 11.41s/it] 80%|  | 5183/6500 [15:40:08<4:03:28, 11.09s/it]                                                         80%|  | 5183/6500 [15:40:08<4:03:28, 11.09s/it] 80%|  | 5184/6500 [15:40:18<3:58:27, 10.87s/it]                                                         80%|  | 5184/6500 [15:40:18<3:58:27, 10.87s/it] 80%|  | 5185/6500 [15:40:29<3:54:48, 10.71s/it]                                                         80%|  | 5185/6500 [15:40:29<3:54:48, 10.71s/it] 80{'loss': 0.3085, 'learning_rate': 9.75473633843268e-06, 'epoch': 0.8}
{'loss': 0.3184, 'learning_rate': 9.740396386384692e-06, 'epoch': 0.8}
{'loss': 0.331, 'learning_rate': 9.726065844774274e-06, 'epoch': 0.8}
{'loss': 0.3148, 'learning_rate': 9.711744716951093e-06, 'epoch': 0.8}
{'loss': 0.3077, 'learning_rate': 9.697433006262624e-06, 'epoch': 0.8}
%|  | 5186/6500 [15:40:39<3:52:13, 10.60s/it]                                                         80%|  | 5186/6500 [15:40:39<3:52:13, 10.60s/it] 80%|  | 5187/6500 [15:40:49<3:50:21, 10.53s/it]                                                         80%|  | 5187/6500 [15:40:49<3:50:21, 10.53s/it] 80%|  | 5188/6500 [15:41:00<3:51:04, 10.57s/it]                                                         80%|  | 5188/6500 [15:41:00<3:51:04, 10.57s/it] 80%|  | 5189/6500 [15:41:10<3:49:29, 10.50s/it]                                                         80%|  | 5189/6500 [15:41:10<3:49:29, 10.50s/it] 80%|  | 5190/6500 [15:41:21<3:48:22, 10.46s/it]                                                         80%|  | 5190/6500 [15:41:21<3:48:22, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8719781041145325, 'eval_runtime': 3.9351, 'eval_samples_per_second': 5.845, 'eval_steps_per_second': 1.525, 'epoch': 0.8}
                                                         80%|  | 5190/6500 [15:41:25<3:48:22, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5190I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5190
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2957, 'learning_rate': 9.683130716054151e-06, 'epoch': 0.8}
{'loss': 0.3181, 'learning_rate': 9.668837849668744e-06, 'epoch': 0.8}
{'loss': 0.378, 'learning_rate': 9.65455441044727e-06, 'epoch': 0.8}
{'loss': 0.3018, 'learning_rate': 9.640280401728396e-06, 'epoch': 0.8}
{'loss': 0.3004, 'learning_rate': 9.6260158268486e-06, 'epoch': 0.8}
 80%|  | 5191/6500 [15:41:36<4:21:53, 12.00s/it]                                                         80%|  | 5191/6500 [15:41:36<4:21:53, 12.00s/it] 80%|  | 5192/6500 [15:41:47<4:10:48, 11.51s/it]                                                         80%|  | 5192/6500 [15:41:47<4:10:48, 11.51s/it] 80%|  | 5193/6500 [15:41:57<4:03:12, 11.17s/it]                                                         80%|  | 5193/6500 [15:41:57<4:03:12, 11.17s/it] 80%|  | 5194/6500 [15:42:07<3:57:51, 10.93s/it]                                                         80%|  | 5194/6500 [15:42:07<3:57:51, 10.93s/it] 80%|  | 5195/6500 [15:42:18<3:53:55, 10.76s/it]                                                         80%|  | 5195/6500 [15:42:18<3:53:55, 10.76s/it] 80{'loss': 0.3311, 'learning_rate': 9.611760689142114e-06, 'epoch': 0.8}
{'loss': 0.8277, 'learning_rate': 9.597514991941003e-06, 'epoch': 0.8}
{'loss': 0.3265, 'learning_rate': 9.583278738575113e-06, 'epoch': 0.8}
{'loss': 0.3189, 'learning_rate': 9.569051932372081e-06, 'epoch': 0.8}
{'loss': 0.3289, 'learning_rate': 9.554834576657334e-06, 'epoch': 0.8}
%|  | 5196/6500 [15:42:28<3:51:09, 10.64s/it]                                                         80%|  | 5196/6500 [15:42:28<3:51:09, 10.64s/it] 80%|  | 5197/6500 [15:42:39<3:48:59, 10.54s/it]                                                         80%|  | 5197/6500 [15:42:39<3:48:59, 10.54s/it] 80%|  | 5198/6500 [15:42:49<3:47:39, 10.49s/it]                                                         80%|  | 5198/6500 [15:42:49<3:47:39, 10.49s/it] 80%|  | 5199/6500 [15:42:59<3:46:33, 10.45s/it]                                                         80%|  | 5199/6500 [15:42:59<3:46:33, 10.45s/it] 80%|  | 5200/6500 [15:43:10<3:45:50, 10.42s/it]                                                         80%|  | 5200/6500 [15:43:10<3:45:50, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8673409223556519, 'eval_runtime': 3.9392, 'eval_samples_per_second': 5.839, 'eval_steps_per_second': 1.523, 'epoch': 0.8}
                                                         80%|  | 5200/6500 [15:43:14<3:45:50, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5200
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.301, 'learning_rate': 9.540626674754094e-06, 'epoch': 0.8}
{'loss': 0.3329, 'learning_rate': 9.52642822998337e-06, 'epoch': 0.8}
{'loss': 0.3027, 'learning_rate': 9.512239245663968e-06, 'epoch': 0.8}
{'loss': 0.2996, 'learning_rate': 9.498059725112467e-06, 'epoch': 0.8}
{'loss': 0.3222, 'learning_rate': 9.483889671643253e-06, 'epoch': 0.8}
 80%|  | 5201/6500 [15:43:24<4:14:23, 11.75s/it]                                                         80%|  | 5201/6500 [15:43:24<4:14:23, 11.75s/it] 80%|  | 5202/6500 [15:43:35<4:05:10, 11.33s/it]                                                         80%|  | 5202/6500 [15:43:35<4:05:10, 11.33s/it] 80%|  | 5203/6500 [15:43:45<3:58:23, 11.03s/it]                                                         80%|  | 5203/6500 [15:43:45<3:58:23, 11.03s/it] 80%|  | 5204/6500 [15:43:56<3:56:19, 10.94s/it]                                                         80%|  | 5204/6500 [15:43:56<3:56:19, 10.94s/it] 80%|  | 5205/6500 [15:44:06<3:52:12, 10.76s/it]                                                         80%|  | 5205/6500 [15:44:06<3:52:12, 10.76s/it] 80{'loss': 0.3151, 'learning_rate': 9.469729088568497e-06, 'epoch': 0.8}
{'loss': 0.3243, 'learning_rate': 9.455577979198127e-06, 'epoch': 0.8}
{'loss': 0.3176, 'learning_rate': 9.441436346839894e-06, 'epoch': 0.8}
{'loss': 0.3303, 'learning_rate': 9.427304194799309e-06, 'epoch': 0.8}
{'loss': 0.312, 'learning_rate': 9.413181526379683e-06, 'epoch': 0.8}
%|  | 5206/6500 [15:44:17<3:49:17, 10.63s/it]                                                         80%|  | 5206/6500 [15:44:17<3:49:17, 10.63s/it] 80%|  | 5207/6500 [15:44:27<3:47:17, 10.55s/it]                                                         80%|  | 5207/6500 [15:44:27<3:47:17, 10.55s/it] 80%|  | 5208/6500 [15:44:37<3:45:45, 10.48s/it]                                                         80%|  | 5208/6500 [15:44:37<3:45:45, 10.48s/it] 80%|  | 5209/6500 [15:44:48<3:44:34, 10.44s/it]                                                         80%|  | 5209/6500 [15:44:48<3:44:34, 10.44s/it] 80%|  | 5210/6500 [15:44:58<3:43:48, 10.41s/it]                                                         80%|  | 5210/6500 [15:44:58<3:43:48, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8698863983154297, 'eval_runtime': 3.9397, 'eval_samples_per_second': 5.838, 'eval_steps_per_second': 1.523, 'epoch': 0.8}
                                                         80%|  | 5210/6500 [15:45:02<3:43:48, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5210/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3298, 'learning_rate': 9.399068344882106e-06, 'epoch': 0.8}
{'loss': 0.3166, 'learning_rate': 9.384964653605444e-06, 'epoch': 0.8}
{'loss': 0.3255, 'learning_rate': 9.370870455846354e-06, 'epoch': 0.8}
{'loss': 0.324, 'learning_rate': 9.356785754899262e-06, 'epoch': 0.8}
{'loss': 0.3258, 'learning_rate': 9.342710554056389e-06, 'epoch': 0.8}
 80%|  | 5211/6500 [15:45:13<4:12:02, 11.73s/it]                                                         80%|  | 5211/6500 [15:45:13<4:12:02, 11.73s/it] 80%|  | 5212/6500 [15:45:23<4:02:53, 11.31s/it]                                                         80%|  | 5212/6500 [15:45:23<4:02:53, 11.31s/it] 80%|  | 5213/6500 [15:45:33<3:56:28, 11.02s/it]                                                         80%|  | 5213/6500 [15:45:33<3:56:28, 11.02s/it] 80%|  | 5214/6500 [15:45:44<3:51:53, 10.82s/it]                                                         80%|  | 5214/6500 [15:45:44<3:51:53, 10.82s/it] 80%|  | 5215/6500 [15:45:54<3:48:41, 10.68s/it]                                                         80%|  | 5215/6500 [15:45:54<3:48:41, 10.68s/it] 80{'loss': 0.3202, 'learning_rate': 9.32864485660772e-06, 'epoch': 0.8}
{'loss': 0.3222, 'learning_rate': 9.314588665841039e-06, 'epoch': 0.8}
{'loss': 0.3345, 'learning_rate': 9.300541985041883e-06, 'epoch': 0.8}
{'loss': 0.3139, 'learning_rate': 9.286504817493574e-06, 'epoch': 0.8}
{'loss': 0.3083, 'learning_rate': 9.272477166477223e-06, 'epoch': 0.8}
%|  | 5216/6500 [15:46:04<3:46:23, 10.58s/it]                                                         80%|  | 5216/6500 [15:46:04<3:46:23, 10.58s/it] 80%|  | 5217/6500 [15:46:15<3:44:46, 10.51s/it]                                                         80%|  | 5217/6500 [15:46:15<3:44:46, 10.51s/it] 80%|  | 5218/6500 [15:46:25<3:43:28, 10.46s/it]                                                         80%|  | 5218/6500 [15:46:25<3:43:28, 10.46s/it] 80%|  | 5219/6500 [15:46:35<3:42:38, 10.43s/it]                                                         80%|  | 5219/6500 [15:46:36<3:42:38, 10.43s/it] 80%|  | 5220/6500 [15:46:46<3:43:28, 10.48s/it]                                                         80%|  | 5220/6500 [15:46:46<3:43:28, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8690677881240845, 'eval_runtime': 4.1989, 'eval_samples_per_second': 5.478, 'eval_steps_per_second': 1.429, 'epoch': 0.8}
                                                         80%|  | 5220/6500 [15:46:50<3:43:28, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5220
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3015, 'learning_rate': 9.25845903527171e-06, 'epoch': 0.8}
{'loss': 0.3287, 'learning_rate': 9.244450427153683e-06, 'epoch': 0.8}
{'loss': 0.3762, 'learning_rate': 9.230451345397568e-06, 'epoch': 0.8}
{'loss': 0.3048, 'learning_rate': 9.216461793275572e-06, 'epoch': 0.8}
{'loss': 0.3097, 'learning_rate': 9.202481774057659e-06, 'epoch': 0.8}
 80%|  | 5221/6500 [15:47:01<4:12:45, 11.86s/it]                                                         80%|  | 5221/6500 [15:47:01<4:12:45, 11.86s/it] 80%|  | 5222/6500 [15:47:12<4:02:57, 11.41s/it]                                                         80%|  | 5222/6500 [15:47:12<4:02:57, 11.41s/it] 80%|  | 5223/6500 [15:47:22<3:56:04, 11.09s/it]                                                         80%|  | 5223/6500 [15:47:22<3:56:04, 11.09s/it] 80%|  | 5224/6500 [15:47:32<3:51:16, 10.87s/it]                                                         80%|  | 5224/6500 [15:47:32<3:51:16, 10.87s/it] 80%|  | 5225/6500 [15:47:43<3:47:53, 10.72s/it]                                                         80%|  | 5225/6500 [15:47:43<3:47:53, 10.72s/it] 80{'loss': 0.3243, 'learning_rate': 9.188511291011581e-06, 'epoch': 0.8}
{'loss': 0.8269, 'learning_rate': 9.174550347402855e-06, 'epoch': 0.8}
{'loss': 0.3339, 'learning_rate': 9.160598946494769e-06, 'epoch': 0.8}
{'loss': 0.3189, 'learning_rate': 9.14665709154836e-06, 'epoch': 0.8}
{'loss': 0.2968, 'learning_rate': 9.132724785822466e-06, 'epoch': 0.8}
%|  | 5226/6500 [15:47:53<3:45:24, 10.62s/it]                                                         80%|  | 5226/6500 [15:47:53<3:45:24, 10.62s/it] 80%|  | 5227/6500 [15:48:03<3:43:36, 10.54s/it]                                                         80%|  | 5227/6500 [15:48:03<3:43:36, 10.54s/it] 80%|  | 5228/6500 [15:48:14<3:42:23, 10.49s/it]                                                         80%|  | 5228/6500 [15:48:14<3:42:23, 10.49s/it] 80%|  | 5229/6500 [15:48:27<3:58:51, 11.28s/it]                                                         80%|  | 5229/6500 [15:48:27<3:58:51, 11.28s/it] 80%|  | 5230/6500 [15:48:37<3:53:03, 11.01s/it]                                                         80%|  | 5230/6500 [15:48:37<3:53:03, 11.01s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8678253889083862, 'eval_runtime': 4.9042, 'eval_samples_per_second': 4.69, 'eval_steps_per_second': 1.223, 'epoch': 0.8}
                                                         80%|  | 5230/6500 [15:48:42<3:53:03, 11.01s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5230I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5230

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5230
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5230/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5230/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3025, 'learning_rate': 9.118802032573676e-06, 'epoch': 0.8}
{'loss': 0.3329, 'learning_rate': 9.104888835056352e-06, 'epoch': 0.8}
{'loss': 0.2972, 'learning_rate': 9.090985196522612e-06, 'epoch': 0.81}
{'loss': 0.3048, 'learning_rate': 9.077091120222353e-06, 'epoch': 0.81}
{'loss': 0.3022, 'learning_rate': 9.063206609403224e-06, 'epoch': 0.81}
 80%|  | 5231/6500 [15:48:53<4:23:31, 12.46s/it]                                                         80%|  | 5231/6500 [15:48:53<4:23:31, 12.46s/it] 80%|  | 5232/6500 [15:49:03<4:10:06, 11.83s/it]                                                         80%|  | 5232/6500 [15:49:03<4:10:06, 11.83s/it] 81%|  | 5233/6500 [15:49:14<4:00:42, 11.40s/it]                                                         81%|  | 5233/6500 [15:49:14<4:00:42, 11.40s/it] 81%|  | 5234/6500 [15:49:24<3:54:13, 11.10s/it]                                                         81%|  | 5234/6500 [15:49:24<3:54:13, 11.10s/it] 81%|  | 5235/6500 [15:49:35<3:50:08, 10.92s/it]                                                         81%|  | 5235/6500 [15:49:35<3:50:08, 10.92s/it] 81{'loss': 0.3157, 'learning_rate': 9.049331667310657e-06, 'epoch': 0.81}
{'loss': 0.3094, 'learning_rate': 9.035466297187827e-06, 'epoch': 0.81}
{'loss': 0.3088, 'learning_rate': 9.02161050227568e-06, 'epoch': 0.81}
{'loss': 0.3312, 'learning_rate': 9.007764285812925e-06, 'epoch': 0.81}
{'loss': 0.3192, 'learning_rate': 8.99392765103605e-06, 'epoch': 0.81}
%|  | 5236/6500 [15:49:45<3:48:48, 10.86s/it]                                                         81%|  | 5236/6500 [15:49:45<3:48:48, 10.86s/it] 81%|  | 5237/6500 [15:49:56<3:45:53, 10.73s/it]                                                         81%|  | 5237/6500 [15:49:56<3:45:53, 10.73s/it] 81%|  | 5238/6500 [15:50:06<3:43:30, 10.63s/it]                                                         81%|  | 5238/6500 [15:50:06<3:43:30, 10.63s/it] 81%|  | 5239/6500 [15:50:17<3:41:58, 10.56s/it]                                                         81%|  | 5239/6500 [15:50:17<3:41:58, 10.56s/it] 81%|  | 5240/6500 [15:50:27<3:41:31, 10.55s/it]                                                         81%|  | 5240/6500 [15:50:27<3:41:31, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8725303411483765, 'eval_runtime': 3.9788, 'eval_samples_per_second': 5.781, 'eval_steps_per_second': 1.508, 'epoch': 0.81}
                                                         81%|  | 5240/6500 [15:50:31<3:41:31, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3447, 'learning_rate': 8.980100601179248e-06, 'epoch': 0.81}
{'loss': 0.3302, 'learning_rate': 8.966283139474525e-06, 'epoch': 0.81}
{'loss': 0.3216, 'learning_rate': 8.952475269151628e-06, 'epoch': 0.81}
{'loss': 0.3323, 'learning_rate': 8.938676993438066e-06, 'epoch': 0.81}
{'loss': 0.3174, 'learning_rate': 8.9248883155591e-06, 'epoch': 0.81}
 81%|  | 5241/6500 [15:50:42<4:09:00, 11.87s/it]                                                         81%|  | 5241/6500 [15:50:42<4:09:00, 11.87s/it] 81%|  | 5242/6500 [15:50:53<3:59:27, 11.42s/it]                                                         81%|  | 5242/6500 [15:50:53<3:59:27, 11.42s/it] 81%|  | 5243/6500 [15:51:03<3:52:34, 11.10s/it]                                                         81%|  | 5243/6500 [15:51:03<3:52:34, 11.10s/it] 81%|  | 5244/6500 [15:51:13<3:47:46, 10.88s/it]                                                         81%|  | 5244/6500 [15:51:13<3:47:46, 10.88s/it] 81%|  | 5245/6500 [15:51:24<3:44:23, 10.73s/it]                                                         81%|  | 5245/6500 [15:51:24<3:44:23, 10.73s/it] 81{'loss': 0.3246, 'learning_rate': 8.911109238737747e-06, 'epoch': 0.81}
{'loss': 0.3171, 'learning_rate': 8.897339766194785e-06, 'epoch': 0.81}
{'loss': 0.3369, 'learning_rate': 8.883579901148747e-06, 'epoch': 0.81}
{'loss': 0.299, 'learning_rate': 8.869829646815914e-06, 'epoch': 0.81}
{'loss': 0.3146, 'learning_rate': 8.856089006410328e-06, 'epoch': 0.81}
%|  | 5246/6500 [15:51:34<3:41:54, 10.62s/it]                                                         81%|  | 5246/6500 [15:51:34<3:41:54, 10.62s/it] 81%|  | 5247/6500 [15:51:44<3:40:12, 10.54s/it]                                                         81%|  | 5247/6500 [15:51:44<3:40:12, 10.54s/it] 81%|  | 5248/6500 [15:51:55<3:38:55, 10.49s/it]                                                         81%|  | 5248/6500 [15:51:55<3:38:55, 10.49s/it] 81%|  | 5249/6500 [15:52:05<3:38:02, 10.46s/it]                                                         81%|  | 5249/6500 [15:52:05<3:38:02, 10.46s/it] 81%|  | 5250/6500 [15:52:15<3:37:22, 10.43s/it]                                                         81%|  | 5250/6500 [15:52:15<3:37:22, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8673002123832703, 'eval_runtime': 3.9438, 'eval_samples_per_second': 5.832, 'eval_steps_per_second': 1.521, 'epoch': 0.81}
                                                         81%|  | 5250/6500 [15:52:19<3:37:22, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3145, 'learning_rate': 8.842357983143784e-06, 'epoch': 0.81}
{'loss': 0.3799, 'learning_rate': 8.828636580225813e-06, 'epoch': 0.81}
{'loss': 0.3251, 'learning_rate': 8.81492480086371e-06, 'epoch': 0.81}
{'loss': 0.3072, 'learning_rate': 8.801222648262519e-06, 'epoch': 0.81}
{'loss': 0.3256, 'learning_rate': 8.78753012562505e-06, 'epoch': 0.81}
 81%|  | 5251/6500 [15:52:30<4:04:47, 11.76s/it]                                                         81%|  | 5251/6500 [15:52:30<4:04:47, 11.76s/it] 81%|  | 5252/6500 [15:52:41<3:55:53, 11.34s/it]                                                         81%|  | 5252/6500 [15:52:41<3:55:53, 11.34s/it] 81%|  | 5253/6500 [15:52:52<3:52:48, 11.20s/it]                                                         81%|  | 5253/6500 [15:52:52<3:52:48, 11.20s/it] 81%|  | 5254/6500 [15:53:02<3:47:21, 10.95s/it]                                                         81%|  | 5254/6500 [15:53:02<3:47:21, 10.95s/it] 81%|  | 5255/6500 [15:53:12<3:43:36, 10.78s/it]                                                         81%|  | 5255/6500 [15:53:12<3:43:36, 10.78s/it] 81{'loss': 0.5595, 'learning_rate': 8.773847236151838e-06, 'epoch': 0.81}
{'loss': 0.5801, 'learning_rate': 8.760173983041176e-06, 'epoch': 0.81}
{'loss': 0.329, 'learning_rate': 8.746510369489103e-06, 'epoch': 0.81}
{'loss': 0.3234, 'learning_rate': 8.73285639868942e-06, 'epoch': 0.81}
{'loss': 0.2986, 'learning_rate': 8.719212073833633e-06, 'epoch': 0.81}
%|  | 5256/6500 [15:53:23<3:40:54, 10.65s/it]                                                         81%|  | 5256/6500 [15:53:23<3:40:54, 10.65s/it] 81%|  | 5257/6500 [15:53:33<3:39:02, 10.57s/it]                                                         81%|  | 5257/6500 [15:53:33<3:39:02, 10.57s/it] 81%|  | 5258/6500 [15:53:43<3:37:37, 10.51s/it]                                                         81%|  | 5258/6500 [15:53:43<3:37:37, 10.51s/it] 81%|  | 5259/6500 [15:53:54<3:36:37, 10.47s/it]                                                         81%|  | 5259/6500 [15:53:54<3:36:37, 10.47s/it] 81%|  | 5260/6500 [15:54:04<3:35:48, 10.44s/it]                                                         81%|  | 5260/6500 [15:54:04<3:35:48, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8675512075424194, 'eval_runtime': 3.9673, 'eval_samples_per_second': 5.797, 'eval_steps_per_second': 1.512, 'epoch': 0.81}
                                                         81%|  | 5260/6500 [15:54:08<3:35:48, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5260
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3227, 'learning_rate': 8.705577398111025e-06, 'epoch': 0.81}
{'loss': 0.3219, 'learning_rate': 8.691952374708634e-06, 'epoch': 0.81}
{'loss': 0.286, 'learning_rate': 8.678337006811216e-06, 'epoch': 0.81}
{'loss': 0.3227, 'learning_rate': 8.664731297601286e-06, 'epoch': 0.81}
{'loss': 0.2992, 'learning_rate': 8.651135250259091e-06, 'epoch': 0.81}
 81%|  | 5261/6500 [15:54:19<4:02:56, 11.77s/it]                                                         81%|  | 5261/6500 [15:54:19<4:02:56, 11.77s/it] 81%|  | 5262/6500 [15:54:29<3:54:00, 11.34s/it]                                                         81%|  | 5262/6500 [15:54:29<3:54:00, 11.34s/it] 81%|  | 5263/6500 [15:54:40<3:47:46, 11.05s/it]                                                         81%|  | 5263/6500 [15:54:40<3:47:46, 11.05s/it] 81%|  | 5264/6500 [15:54:50<3:43:24, 10.85s/it]                                                         81%|  | 5264/6500 [15:54:50<3:43:24, 10.85s/it] 81%|  | 5265/6500 [15:55:00<3:40:12, 10.70s/it]                                                         81%|  | 5265/6500 [15:55:00<3:40:12, 10.70s/it] 81{'loss': 0.3217, 'learning_rate': 8.63754886796262e-06, 'epoch': 0.81}
{'loss': 0.3164, 'learning_rate': 8.623972153887622e-06, 'epoch': 0.81}
{'loss': 0.3157, 'learning_rate': 8.610405111207559e-06, 'epoch': 0.81}
{'loss': 0.3079, 'learning_rate': 8.596847743093645e-06, 'epoch': 0.81}
{'loss': 0.3194, 'learning_rate': 8.583300052714838e-06, 'epoch': 0.81}
%|  | 5266/6500 [15:55:11<3:37:57, 10.60s/it]                                                         81%|  | 5266/6500 [15:55:11<3:37:57, 10.60s/it] 81%|  | 5267/6500 [15:55:21<3:36:23, 10.53s/it]                                                         81%|  | 5267/6500 [15:55:21<3:36:23, 10.53s/it] 81%|  | 5268/6500 [15:55:32<3:35:09, 10.48s/it]                                                         81%|  | 5268/6500 [15:55:32<3:35:09, 10.48s/it] 81%|  | 5269/6500 [15:55:42<3:36:34, 10.56s/it]                                                         81%|  | 5269/6500 [15:55:42<3:36:34, 10.56s/it] 81%|  | 5270/6500 [15:55:53<3:35:14, 10.50s/it]                                                         81%|  | 5270/6500 [15:55:53<3:35:14, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8719391226768494, 'eval_runtime': 3.9578, 'eval_samples_per_second': 5.811, 'eval_steps_per_second': 1.516, 'epoch': 0.81}
                                                         81%|  | 5270/6500 [15:55:57<3:35:14, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5270
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3324, 'learning_rate': 8.569762043237839e-06, 'epoch': 0.81}
{'loss': 0.3052, 'learning_rate': 8.55623371782705e-06, 'epoch': 0.81}
{'loss': 0.3278, 'learning_rate': 8.542715079644648e-06, 'epoch': 0.81}
{'loss': 0.315, 'learning_rate': 8.529206131850532e-06, 'epoch': 0.81}
{'loss': 0.3085, 'learning_rate': 8.515706877602336e-06, 'epoch': 0.81}
 81%|  | 5271/6500 [15:56:08<4:01:57, 11.81s/it]                                                         81%|  | 5271/6500 [15:56:08<4:01:57, 11.81s/it] 81%|  | 5272/6500 [15:56:18<3:52:50, 11.38s/it]                                                         81%|  | 5272/6500 [15:56:18<3:52:50, 11.38s/it] 81%|  | 5273/6500 [15:56:28<3:46:26, 11.07s/it]                                                         81%|  | 5273/6500 [15:56:28<3:46:26, 11.07s/it] 81%|  | 5274/6500 [15:56:39<3:42:01, 10.87s/it]                                                         81%|  | 5274/6500 [15:56:39<3:42:01, 10.87s/it] 81%|  | 5275/6500 [15:56:49<3:38:50, 10.72s/it]                                                         81%|  | 5275/6500 [15:56:49<3:38:50, 10.72s/it] 81{'loss': 0.3135, 'learning_rate': 8.502217320055427e-06, 'epoch': 0.81}
{'loss': 0.3318, 'learning_rate': 8.48873746236291e-06, 'epoch': 0.81}
{'loss': 0.3153, 'learning_rate': 8.475267307675616e-06, 'epoch': 0.81}
{'loss': 0.3137, 'learning_rate': 8.461806859142119e-06, 'epoch': 0.81}
{'loss': 0.302, 'learning_rate': 8.448356119908713e-06, 'epoch': 0.81}
%|  | 5276/6500 [15:56:59<3:36:30, 10.61s/it]                                                         81%|  | 5276/6500 [15:56:59<3:36:30, 10.61s/it] 81%|  | 5277/6500 [15:57:10<3:34:54, 10.54s/it]                                                         81%|  | 5277/6500 [15:57:10<3:34:54, 10.54s/it] 81%|  | 5278/6500 [15:57:20<3:33:36, 10.49s/it]                                                         81%|  | 5278/6500 [15:57:20<3:33:36, 10.49s/it] 81%|  | 5279/6500 [15:57:31<3:32:48, 10.46s/it]                                                         81%|  | 5279/6500 [15:57:31<3:32:48, 10.46s/it] 81%|  | 5280/6500 [15:57:41<3:32:11, 10.44s/it]                                                         81%|  | 5280/6500 [15:57:41<3:32:11, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8704752922058105, 'eval_runtime': 3.9659, 'eval_samples_per_second': 5.799, 'eval_steps_per_second': 1.513, 'epoch': 0.81}
                                                         81%|  | 5280/6500 [15:57:45<3:32:11, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5280 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5280

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5280/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5280/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3218, 'learning_rate': 8.434915093119421e-06, 'epoch': 0.81}
{'loss': 0.3814, 'learning_rate': 8.421483781916018e-06, 'epoch': 0.81}
{'loss': 0.3111, 'learning_rate': 8.408062189437971e-06, 'epoch': 0.81}
{'loss': 0.2881, 'learning_rate': 8.3946503188225e-06, 'epoch': 0.81}
{'loss': 0.3302, 'learning_rate': 8.381248173204558e-06, 'epoch': 0.81}
 81%|  | 5281/6500 [15:57:56<3:59:25, 11.78s/it]                                                         81%|  | 5281/6500 [15:57:56<3:59:25, 11.78s/it] 81%| | 5282/6500 [15:58:06<3:50:32, 11.36s/it]                                                         81%| | 5282/6500 [15:58:06<3:50:32, 11.36s/it] 81%| | 5283/6500 [15:58:17<3:44:21, 11.06s/it]                                                         81%| | 5283/6500 [15:58:17<3:44:21, 11.06s/it] 81%| | 5284/6500 [15:58:27<3:40:02, 10.86s/it]                                                         81%| | 5284/6500 [15:58:27<3:40:02, 10.86s/it] 81%| | 5285/6500 [15:58:38<3:39:54, 10.86s/it]                                                         81%| | 5285/6500 [15:58:38<3:39:54{'loss': 0.8234, 'learning_rate': 8.367855755716802e-06, 'epoch': 0.81}
{'loss': 0.3302, 'learning_rate': 8.354473069489643e-06, 'epoch': 0.81}
{'loss': 0.3251, 'learning_rate': 8.341100117651191e-06, 'epoch': 0.81}
{'loss': 0.3227, 'learning_rate': 8.327736903327299e-06, 'epoch': 0.81}
{'loss': 0.2957, 'learning_rate': 8.314383429641531e-06, 'epoch': 0.81}
, 10.86s/it] 81%| | 5286/6500 [15:58:48<3:36:39, 10.71s/it]                                                         81%| | 5286/6500 [15:58:48<3:36:39, 10.71s/it] 81%| | 5287/6500 [15:58:59<3:34:31, 10.61s/it]                                                         81%| | 5287/6500 [15:58:59<3:34:31, 10.61s/it] 81%| | 5288/6500 [15:59:09<3:32:54, 10.54s/it]                                                         81%| | 5288/6500 [15:59:09<3:32:54, 10.54s/it] 81%| | 5289/6500 [15:59:19<3:31:45, 10.49s/it]                                                         81%| | 5289/6500 [15:59:19<3:31:45, 10.49s/it] 81%| | 5290/6500 [15:59:30<3:30:52, 10.46s/it]                                                         81%| | 5290/6500 [15:59:30<3:30:52, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8696795701980591, 'eval_runtime': 3.9732, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.81}
                                                         81%| | 5290/6500 [15:59:34<3:30:52, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5290/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5290/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3377, 'learning_rate': 8.301039699715185e-06, 'epoch': 0.81}
{'loss': 0.3041, 'learning_rate': 8.287705716667276e-06, 'epoch': 0.81}
{'loss': 0.2984, 'learning_rate': 8.274381483614552e-06, 'epoch': 0.81}
{'loss': 0.3173, 'learning_rate': 8.261067003671447e-06, 'epoch': 0.81}
{'loss': 0.3106, 'learning_rate': 8.247762279950156e-06, 'epoch': 0.81}
 81%| | 5291/6500 [15:59:45<3:57:40, 11.80s/it]                                                         81%| | 5291/6500 [15:59:45<3:57:40, 11.80s/it] 81%| | 5292/6500 [15:59:55<3:48:46, 11.36s/it]                                                         81%| | 5292/6500 [15:59:55<3:48:46, 11.36s/it] 81%| | 5293/6500 [16:00:05<3:42:35, 11.06s/it]                                                         81%| | 5293/6500 [16:00:05<3:42:35, 11.06s/it] 81%| | 5294/6500 [16:00:16<3:38:12, 10.86s/it]                                                         81%| | 5294/6500 [16:00:16<3:38:12, 10.86s/it] 81%| | 5295/6500 [16:00:26<3:35:07, 10.71s/it]                                                         81%| | 5295/6500 [16:00:26<3:3{'loss': 0.3151, 'learning_rate': 8.234467315560573e-06, 'epoch': 0.81}
{'loss': 0.3072, 'learning_rate': 8.221182113610314e-06, 'epoch': 0.81}
{'loss': 0.3286, 'learning_rate': 8.207906677204718e-06, 'epoch': 0.82}
{'loss': 0.3088, 'learning_rate': 8.194641009446835e-06, 'epoch': 0.82}
{'loss': 0.3362, 'learning_rate': 8.181385113437439e-06, 'epoch': 0.82}
5:07, 10.71s/it] 81%| | 5296/6500 [16:00:36<3:32:58, 10.61s/it]                                                         81%| | 5296/6500 [16:00:36<3:32:58, 10.61s/it] 81%| | 5297/6500 [16:00:47<3:31:25, 10.54s/it]                                                         81%| | 5297/6500 [16:00:47<3:31:25, 10.54s/it] 82%| | 5298/6500 [16:00:57<3:30:29, 10.51s/it]                                                         82%| | 5298/6500 [16:00:57<3:30:29, 10.51s/it] 82%| | 5299/6500 [16:01:08<3:29:30, 10.47s/it]                                                         82%| | 5299/6500 [16:01:08<3:29:30, 10.47s/it] 82%| | 5300/6500 [16:01:18<3:28:59, 10.45s/it]                                                         82%| | 5300/6500 [16:01:18<3:28:59, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8701429963111877, 'eval_runtime': 3.9466, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.82}
                                                         82%| | 5300/6500 [16:01:22<3:28:59, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5300/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5300/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.325, 'learning_rate': 8.16813899227501e-06, 'epoch': 0.82}
{'loss': 0.3139, 'learning_rate': 8.154902649055746e-06, 'epoch': 0.82}
{'loss': 0.3279, 'learning_rate': 8.141676086873572e-06, 'epoch': 0.82}
{'loss': 0.3189, 'learning_rate': 8.128459308820118e-06, 'epoch': 0.82}
{'loss': 0.3213, 'learning_rate': 8.115252317984707e-06, 'epoch': 0.82}
 82%| | 5301/6500 [16:01:33<3:57:30, 11.89s/it]                                                         82%| | 5301/6500 [16:01:33<3:57:30, 11.89s/it] 82%| | 5302/6500 [16:01:44<3:48:19, 11.44s/it]                                                         82%| | 5302/6500 [16:01:44<3:48:19, 11.44s/it] 82%| | 5303/6500 [16:01:54<3:41:47, 11.12s/it]                                                         82%| | 5303/6500 [16:01:54<3:41:47, 11.12s/it] 82%| | 5304/6500 [16:02:04<3:37:17, 10.90s/it]                                                         82%| | 5304/6500 [16:02:04<3:37:17, 10.90s/it] 82%| | 5305/6500 [16:02:15<3:34:04, 10.75s/it]                                                         82%| | 5305/6500 [16:02:15<3:3{'loss': 0.3182, 'learning_rate': 8.1020551174544e-06, 'epoch': 0.82}
{'loss': 0.3303, 'learning_rate': 8.088867710313969e-06, 'epoch': 0.82}
{'loss': 0.3091, 'learning_rate': 8.075690099645883e-06, 'epoch': 0.82}
{'loss': 0.3144, 'learning_rate': 8.062522288530333e-06, 'epoch': 0.82}
{'loss': 0.2959, 'learning_rate': 8.04936428004522e-06, 'epoch': 0.82}
4:04, 10.75s/it] 82%| | 5306/6500 [16:02:25<3:31:49, 10.64s/it]                                                         82%| | 5306/6500 [16:02:25<3:31:49, 10.64s/it] 82%| | 5307/6500 [16:02:36<3:30:12, 10.57s/it]                                                         82%| | 5307/6500 [16:02:36<3:30:12, 10.57s/it] 82%| | 5308/6500 [16:02:46<3:29:03, 10.52s/it]                                                         82%| | 5308/6500 [16:02:46<3:29:03, 10.52s/it] 82%| | 5309/6500 [16:02:56<3:28:07, 10.48s/it]                                                         82%| | 5309/6500 [16:02:56<3:28:07, 10.48s/it] 82%| | 5310/6500 [16:03:07<3:27:27, 10.46s/it]                                                         82%| | 5310/6500 [16:03:07<3:27:27, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8695188760757446, 'eval_runtime': 3.9674, 'eval_samples_per_second': 5.797, 'eval_steps_per_second': 1.512, 'epoch': 0.82}
                                                         82%| | 5310/6500 [16:03:11<3:27:27, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3417, 'learning_rate': 8.036216077266134e-06, 'epoch': 0.82}
{'loss': 0.3669, 'learning_rate': 8.0230776832664e-06, 'epoch': 0.82}
{'loss': 0.3239, 'learning_rate': 8.009949101117037e-06, 'epoch': 0.82}
{'loss': 0.326, 'learning_rate': 7.996830333886762e-06, 'epoch': 0.82}
{'loss': 0.3124, 'learning_rate': 7.983721384642029e-06, 'epoch': 0.82}
 82%| | 5311/6500 [16:03:22<3:53:32, 11.78s/it]                                                         82%| | 5311/6500 [16:03:22<3:53:32, 11.78s/it] 82%| | 5312/6500 [16:03:32<3:45:06, 11.37s/it]                                                         82%| | 5312/6500 [16:03:32<3:45:06, 11.37s/it] 82%| | 5313/6500 [16:03:42<3:39:11, 11.08s/it]                                                         82%| | 5313/6500 [16:03:42<3:39:11, 11.08s/it] 82%| | 5314/6500 [16:03:53<3:35:12, 10.89s/it]                                                         82%| | 5314/6500 [16:03:53<3:35:12, 10.89s/it] 82%| | 5315/6500 [16:04:03<3:32:06, 10.74s/it]                                                         82%| | 5315/6500 [16:04:03<3:3{'loss': 0.8392, 'learning_rate': 7.970622256446946e-06, 'epoch': 0.82}
{'loss': 0.3305, 'learning_rate': 7.957532952363367e-06, 'epoch': 0.82}
{'loss': 0.3215, 'learning_rate': 7.944453475450842e-06, 'epoch': 0.82}
{'loss': 0.295, 'learning_rate': 7.931383828766609e-06, 'epoch': 0.82}
{'loss': 0.3178, 'learning_rate': 7.918324015365624e-06, 'epoch': 0.82}
2:06, 10.74s/it] 82%| | 5316/6500 [16:04:14<3:29:49, 10.63s/it]                                                         82%| | 5316/6500 [16:04:14<3:29:49, 10.63s/it] 82%| | 5317/6500 [16:04:25<3:30:33, 10.68s/it]                                                         82%| | 5317/6500 [16:04:25<3:30:33, 10.68s/it] 82%| | 5318/6500 [16:04:35<3:28:47, 10.60s/it]                                                         82%| | 5318/6500 [16:04:35<3:28:47, 10.60s/it] 82%| | 5319/6500 [16:04:45<3:27:20, 10.53s/it]                                                         82%| | 5319/6500 [16:04:45<3:27:20, 10.53s/it] 82%| | 5320/6500 [16:04:56<3:26:54, 10.52s/it]                                                         82%| | 5320/6500 [16:04:56<3:26:54, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.867393434047699, 'eval_runtime': 4.1974, 'eval_samples_per_second': 5.48, 'eval_steps_per_second': 1.429, 'epoch': 0.82}
                                                         82%| | 5320/6500 [16:05:00<3:26:54, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3193, 'learning_rate': 7.905274038300547e-06, 'epoch': 0.82}
{'loss': 0.2905, 'learning_rate': 7.89223390062172e-06, 'epoch': 0.82}
{'loss': 0.3093, 'learning_rate': 7.879203605377201e-06, 'epoch': 0.82}
{'loss': 0.3024, 'learning_rate': 7.866183155612734e-06, 'epoch': 0.82}
{'loss': 0.3134, 'learning_rate': 7.85317255437178e-06, 'epoch': 0.82}
 82%| | 5321/6500 [16:05:11<3:53:46, 11.90s/it]                                                         82%| | 5321/6500 [16:05:11<3:53:46, 11.90s/it] 82%| | 5322/6500 [16:05:21<3:44:36, 11.44s/it]                                                         82%| | 5322/6500 [16:05:21<3:44:36, 11.44s/it] 82%| | 5323/6500 [16:05:32<3:38:13, 11.12s/it]                                                         82%| | 5323/6500 [16:05:32<3:38:13, 11.12s/it] 82%| | 5324/6500 [16:05:42<3:33:41, 10.90s/it]                                                         82%| | 5324/6500 [16:05:42<3:33:41, 10.90s/it] 82%| | 5325/6500 [16:05:52<3:30:41, 10.76s/it]                                                         82%| | 5325/6500 [16:05:52<3:3{'loss': 0.3193, 'learning_rate': 7.84017180469549e-06, 'epoch': 0.82}
{'loss': 0.3056, 'learning_rate': 7.827180909622711e-06, 'epoch': 0.82}
{'loss': 0.3157, 'learning_rate': 7.814199872189964e-06, 'epoch': 0.82}
{'loss': 0.3099, 'learning_rate': 7.801228695431501e-06, 'epoch': 0.82}
{'loss': 0.333, 'learning_rate': 7.78826738237926e-06, 'epoch': 0.82}
0:41, 10.76s/it] 82%| | 5326/6500 [16:06:03<3:28:22, 10.65s/it]                                                         82%| | 5326/6500 [16:06:03<3:28:22, 10.65s/it] 82%| | 5327/6500 [16:06:13<3:26:37, 10.57s/it]                                                         82%| | 5327/6500 [16:06:13<3:26:37, 10.57s/it] 82%| | 5328/6500 [16:06:24<3:25:16, 10.51s/it]                                                         82%| | 5328/6500 [16:06:24<3:25:16, 10.51s/it] 82%| | 5329/6500 [16:06:34<3:24:11, 10.46s/it]                                                         82%| | 5329/6500 [16:06:34<3:24:11, 10.46s/it] 82%| | 5330/6500 [16:06:44<3:23:19, 10.43s/it]                                                         82%| | 5330/6500 [16:06:44<3:23:19, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8713435530662537, 'eval_runtime': 3.952, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.82}
                                                         82%| | 5330/6500 [16:06:48<3:23:19, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3035, 'learning_rate': 7.775315936062872e-06, 'epoch': 0.82}
{'loss': 0.3201, 'learning_rate': 7.762374359509656e-06, 'epoch': 0.82}
{'loss': 0.3237, 'learning_rate': 7.749442655744621e-06, 'epoch': 0.82}
{'loss': 0.3027, 'learning_rate': 7.736520827790477e-06, 'epoch': 0.82}
{'loss': 0.322, 'learning_rate': 7.723608878667632e-06, 'epoch': 0.82}
 82%| | 5331/6500 [16:06:59<3:49:01, 11.75s/it]                                                         82%| | 5331/6500 [16:06:59<3:49:01, 11.75s/it] 82%| | 5332/6500 [16:07:10<3:40:42, 11.34s/it]                                                         82%| | 5332/6500 [16:07:10<3:40:42, 11.34s/it] 82%| | 5333/6500 [16:07:20<3:36:08, 11.11s/it]                                                         82%| | 5333/6500 [16:07:20<3:36:08, 11.11s/it] 82%| | 5334/6500 [16:07:30<3:31:37, 10.89s/it]                                                         82%| | 5334/6500 [16:07:30<3:31:37, 10.89s/it] 82%| | 5335/6500 [16:07:41<3:28:26, 10.73s/it]                                                         82%| | 5335/6500 [16:07:41<3:2{'loss': 0.3181, 'learning_rate': 7.710706811394163e-06, 'epoch': 0.82}
{'loss': 0.3252, 'learning_rate': 7.69781462898585e-06, 'epoch': 0.82}
{'loss': 0.3039, 'learning_rate': 7.684932334456162e-06, 'epoch': 0.82}
{'loss': 0.3118, 'learning_rate': 7.672059930816266e-06, 'epoch': 0.82}
{'loss': 0.3015, 'learning_rate': 7.659197421075004e-06, 'epoch': 0.82}
8:26, 10.73s/it] 82%| | 5336/6500 [16:07:51<3:26:17, 10.63s/it]                                                         82%| | 5336/6500 [16:07:51<3:26:17, 10.63s/it] 82%| | 5337/6500 [16:08:02<3:24:41, 10.56s/it]                                                         82%| | 5337/6500 [16:08:02<3:24:41, 10.56s/it] 82%| | 5338/6500 [16:08:12<3:23:36, 10.51s/it]                                                         82%| | 5338/6500 [16:08:12<3:23:36, 10.51s/it] 82%| | 5339/6500 [16:08:22<3:22:42, 10.48s/it]                                                         82%| | 5339/6500 [16:08:22<3:22:42, 10.48s/it] 82%| | 5340/6500 [16:08:33<3:22:02, 10.45s/it]                                                         82%| | 5340/6500 [16:08:33<3:22:02, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8699588775634766, 'eval_runtime': 3.9363, 'eval_samples_per_second': 5.843, 'eval_steps_per_second': 1.524, 'epoch': 0.82}
                                                         82%| | 5340/6500 [16:08:37<3:22:02, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5340I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5340

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5340
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5340/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5340/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.393, 'learning_rate': 7.646344808238903e-06, 'epoch': 0.82}
{'loss': 0.3077, 'learning_rate': 7.633502095312196e-06, 'epoch': 0.82}
{'loss': 0.3034, 'learning_rate': 7.6206692852967775e-06, 'epoch': 0.82}
{'loss': 0.3223, 'learning_rate': 7.607846381192241e-06, 'epoch': 0.82}
{'loss': 0.817, 'learning_rate': 7.595033385995865e-06, 'epoch': 0.82}
 82%| | 5341/6500 [16:08:48<3:47:45, 11.79s/it]                                                         82%| | 5341/6500 [16:08:48<3:47:45, 11.79s/it] 82%| | 5342/6500 [16:08:58<3:39:28, 11.37s/it]                                                         82%| | 5342/6500 [16:08:58<3:39:28, 11.37s/it] 82%| | 5343/6500 [16:09:09<3:33:35, 11.08s/it]                                                         82%| | 5343/6500 [16:09:09<3:33:35, 11.08s/it] 82%| | 5344/6500 [16:09:19<3:29:25, 10.87s/it]                                                         82%| | 5344/6500 [16:09:19<3:29:25, 10.87s/it] 82%| | 5345/6500 [16:09:29<3:26:11, 10.71s/it]                                                         82%| | 5345/6500 [16:09:29<3:2{'loss': 0.3295, 'learning_rate': 7.582230302702626e-06, 'epoch': 0.82}
{'loss': 0.3172, 'learning_rate': 7.569437134305129e-06, 'epoch': 0.82}
{'loss': 0.3211, 'learning_rate': 7.556653883793724e-06, 'epoch': 0.82}
{'loss': 0.3056, 'learning_rate': 7.5438805541564185e-06, 'epoch': 0.82}
{'loss': 0.3289, 'learning_rate': 7.531117148378891e-06, 'epoch': 0.82}
6:11, 10.71s/it] 82%| | 5346/6500 [16:09:40<3:24:05, 10.61s/it]                                                         82%| | 5346/6500 [16:09:40<3:24:05, 10.61s/it] 82%| | 5347/6500 [16:09:50<3:22:32, 10.54s/it]                                                         82%| | 5347/6500 [16:09:50<3:22:32, 10.54s/it] 82%| | 5348/6500 [16:10:00<3:21:23, 10.49s/it]                                                         82%| | 5348/6500 [16:10:00<3:21:23, 10.49s/it] 82%| | 5349/6500 [16:10:11<3:20:26, 10.45s/it]                                                         82%| | 5349/6500 [16:10:11<3:20:26, 10.45s/it] 82%| | 5350/6500 [16:10:21<3:22:04, 10.54s/it]                                                         82%| | 5350/6500 [16:10:21<3:22:04, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8687712550163269, 'eval_runtime': 3.9547, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.82}
                                                         82%| | 5350/6500 [16:10:25<3:22:04, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5350
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3111, 'learning_rate': 7.518363669444517e-06, 'epoch': 0.82}
{'loss': 0.2875, 'learning_rate': 7.505620120334339e-06, 'epoch': 0.82}
{'loss': 0.311, 'learning_rate': 7.4928865040270915e-06, 'epoch': 0.82}
{'loss': 0.2991, 'learning_rate': 7.480162823499176e-06, 'epoch': 0.82}
{'loss': 0.317, 'learning_rate': 7.46744908172467e-06, 'epoch': 0.82}
 82%| | 5351/6500 [16:10:36<3:46:22, 11.82s/it]                                                         82%| | 5351/6500 [16:10:36<3:46:22, 11.82s/it] 82%| | 5352/6500 [16:10:47<3:37:46, 11.38s/it]                                                         82%| | 5352/6500 [16:10:47<3:37:46, 11.38s/it] 82%| | 5353/6500 [16:10:57<3:31:37, 11.07s/it]                                                         82%| | 5353/6500 [16:10:57<3:31:37, 11.07s/it] 82%| | 5354/6500 [16:11:07<3:27:24, 10.86s/it]                                                         82%| | 5354/6500 [16:11:07<3:27:24, 10.86s/it] 82%| | 5355/6500 [16:11:18<3:24:18, 10.71s/it]                                                         82%| | 5355/6500 [16:11:18<3:2{'loss': 0.3064, 'learning_rate': 7.454745281675346e-06, 'epoch': 0.82}
{'loss': 0.3166, 'learning_rate': 7.442051426320628e-06, 'epoch': 0.82}
{'loss': 0.3041, 'learning_rate': 7.42936751862765e-06, 'epoch': 0.82}
{'loss': 0.3238, 'learning_rate': 7.4166935615611664e-06, 'epoch': 0.82}
{'loss': 0.3197, 'learning_rate': 7.404029558083653e-06, 'epoch': 0.82}
4:18, 10.71s/it] 82%| | 5356/6500 [16:11:28<3:22:10, 10.60s/it]                                                         82%| | 5356/6500 [16:11:28<3:22:10, 10.60s/it] 82%| | 5357/6500 [16:11:38<3:20:35, 10.53s/it]                                                         82%| | 5357/6500 [16:11:38<3:20:35, 10.53s/it] 82%| | 5358/6500 [16:11:49<3:19:25, 10.48s/it]                                                         82%| | 5358/6500 [16:11:49<3:19:25, 10.48s/it] 82%| | 5359/6500 [16:11:59<3:18:33, 10.44s/it]                                                         82%| | 5359/6500 [16:11:59<3:18:33, 10.44s/it] 82%| | 5360/6500 [16:12:09<3:17:52, 10.41s/it]                                                         82%| | 5360/6500 [16:12:09<3:17:52, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8726702928543091, 'eval_runtime': 4.1685, 'eval_samples_per_second': 5.518, 'eval_steps_per_second': 1.439, 'epoch': 0.82}
                                                         82%| | 5360/6500 [16:12:14<3:17:52, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5360
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.309, 'learning_rate': 7.391375511155241e-06, 'epoch': 0.82}
{'loss': 0.326, 'learning_rate': 7.378731423733737e-06, 'epoch': 0.82}
{'loss': 0.3186, 'learning_rate': 7.366097298774622e-06, 'epoch': 0.83}
{'loss': 0.3222, 'learning_rate': 7.353473139231049e-06, 'epoch': 0.83}
{'loss': 0.3055, 'learning_rate': 7.340858948053831e-06, 'epoch': 0.83}
 82%| | 5361/6500 [16:12:25<3:44:07, 11.81s/it]                                                         82%| | 5361/6500 [16:12:25<3:44:07, 11.81s/it] 82%| | 5362/6500 [16:12:35<3:35:38, 11.37s/it]                                                         82%| | 5362/6500 [16:12:35<3:35:38, 11.37s/it] 83%| | 5363/6500 [16:12:45<3:29:48, 11.07s/it]                                                         83%| | 5363/6500 [16:12:45<3:29:48, 11.07s/it] 83%| | 5364/6500 [16:12:56<3:25:29, 10.85s/it]                                                         83%| | 5364/6500 [16:12:56<3:25:29, 10.85s/it] 83%| | 5365/6500 [16:13:06<3:22:29, 10.70s/it]                                                         83%| | 5365/6500 [16:13:06<3:2{'loss': 0.3343, 'learning_rate': 7.328254728191464e-06, 'epoch': 0.83}
{'loss': 0.3144, 'learning_rate': 7.315660482590103e-06, 'epoch': 0.83}
{'loss': 0.3029, 'learning_rate': 7.3030762141935825e-06, 'epoch': 0.83}
{'loss': 0.2976, 'learning_rate': 7.290501925943405e-06, 'epoch': 0.83}
{'loss': 0.3208, 'learning_rate': 7.277937620778713e-06, 'epoch': 0.83}
2:29, 10.70s/it] 83%| | 5366/6500 [16:13:17<3:21:41, 10.67s/it]                                                         83%| | 5366/6500 [16:13:17<3:21:41, 10.67s/it] 83%| | 5367/6500 [16:13:27<3:19:43, 10.58s/it]                                                         83%| | 5367/6500 [16:13:27<3:19:43, 10.58s/it] 83%| | 5368/6500 [16:13:37<3:18:20, 10.51s/it]                                                         83%| | 5368/6500 [16:13:37<3:18:20, 10.51s/it] 83%| | 5369/6500 [16:13:48<3:17:19, 10.47s/it]                                                         83%| | 5369/6500 [16:13:48<3:17:19, 10.47s/it] 83%| | 5370/6500 [16:13:58<3:16:33, 10.44s/it]                                                         83%| | 5370/6500 [16:13:58<3:16:33, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8695715069770813, 'eval_runtime': 3.9562, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.83}
                                                         83%| | 5370/6500 [16:14:02<3:16:33, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5370/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3719, 'learning_rate': 7.265383301636347e-06, 'epoch': 0.83}
{'loss': 0.3098, 'learning_rate': 7.252838971450804e-06, 'epoch': 0.83}
{'loss': 0.3005, 'learning_rate': 7.240304633154243e-06, 'epoch': 0.83}
{'loss': 0.32, 'learning_rate': 7.227780289676494e-06, 'epoch': 0.83}
{'loss': 0.8292, 'learning_rate': 7.215265943945038e-06, 'epoch': 0.83}
 83%| | 5371/6500 [16:14:13<3:41:20, 11.76s/it]                                                         83%| | 5371/6500 [16:14:13<3:41:20, 11.76s/it] 83%| | 5372/6500 [16:14:23<3:33:09, 11.34s/it]                                                         83%| | 5372/6500 [16:14:23<3:33:09, 11.34s/it] 83%| | 5373/6500 [16:14:34<3:27:23, 11.04s/it]                                                         83%| | 5373/6500 [16:14:34<3:27:23, 11.04s/it] 83%| | 5374/6500 [16:14:44<3:23:15, 10.83s/it]                                                         83%| | 5374/6500 [16:14:44<3:23:15, 10.83s/it] 83%| | 5375/6500 [16:14:54<3:20:45, 10.71s/it]                                                         83%| | 5375/6500 [16:14:54<3:2{'loss': 0.3238, 'learning_rate': 7.202761598885038e-06, 'epoch': 0.83}
{'loss': 0.3184, 'learning_rate': 7.190267257419297e-06, 'epoch': 0.83}
{'loss': 0.31, 'learning_rate': 7.1777829224683014e-06, 'epoch': 0.83}
{'loss': 0.3039, 'learning_rate': 7.165308596950182e-06, 'epoch': 0.83}
{'loss': 0.3369, 'learning_rate': 7.152844283780752e-06, 'epoch': 0.83}
0:45, 10.71s/it] 83%| | 5376/6500 [16:15:05<3:18:33, 10.60s/it]                                                         83%| | 5376/6500 [16:15:05<3:18:33, 10.60s/it] 83%| | 5377/6500 [16:15:15<3:16:58, 10.52s/it]                                                         83%| | 5377/6500 [16:15:15<3:16:58, 10.52s/it] 83%| | 5378/6500 [16:15:25<3:15:46, 10.47s/it]                                                         83%| | 5378/6500 [16:15:25<3:15:46, 10.47s/it] 83%| | 5379/6500 [16:15:36<3:14:55, 10.43s/it]                                                         83%| | 5379/6500 [16:15:36<3:14:55, 10.43s/it] 83%| | 5380/6500 [16:15:46<3:14:22, 10.41s/it]                                                         83%| | 5380/6500 [16:15:46<3:14:22, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8709390163421631, 'eval_runtime': 4.1029, 'eval_samples_per_second': 5.606, 'eval_steps_per_second': 1.462, 'epoch': 0.83}
                                                         83%| | 5380/6500 [16:15:50<3:14:22, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2991, 'learning_rate': 7.140389985873447e-06, 'epoch': 0.83}
{'loss': 0.3133, 'learning_rate': 7.127945706139388e-06, 'epoch': 0.83}
{'loss': 0.3123, 'learning_rate': 7.115511447487355e-06, 'epoch': 0.83}
{'loss': 0.3252, 'learning_rate': 7.103087212823778e-06, 'epoch': 0.83}
{'loss': 0.3153, 'learning_rate': 7.090673005052751e-06, 'epoch': 0.83}
 83%| | 5381/6500 [16:16:01<3:39:39, 11.78s/it]                                                         83%| | 5381/6500 [16:16:01<3:39:39, 11.78s/it] 83%| | 5382/6500 [16:16:12<3:34:58, 11.54s/it]                                                         83%| | 5382/6500 [16:16:12<3:34:58, 11.54s/it] 83%| | 5383/6500 [16:16:22<3:28:06, 11.18s/it]                                                         83%| | 5383/6500 [16:16:22<3:28:06, 11.18s/it] 83%| | 5384/6500 [16:16:33<3:23:13, 10.93s/it]                                                         83%| | 5384/6500 [16:16:33<3:23:13, 10.93s/it] 83%| | 5385/6500 [16:16:43<3:19:55, 10.76s/it]                                                         83%| | 5385/6500 [16:16:43<3:1{'loss': 0.3126, 'learning_rate': 7.078268827076012e-06, 'epoch': 0.83}
{'loss': 0.3321, 'learning_rate': 7.065874681792966e-06, 'epoch': 0.83}
{'loss': 0.316, 'learning_rate': 7.053490572100669e-06, 'epoch': 0.83}
{'loss': 0.3338, 'learning_rate': 7.041116500893835e-06, 'epoch': 0.83}
{'loss': 0.3204, 'learning_rate': 7.0287524710648256e-06, 'epoch': 0.83}
9:55, 10.76s/it] 83%| | 5386/6500 [16:16:53<3:17:28, 10.64s/it]                                                         83%| | 5386/6500 [16:16:53<3:17:28, 10.64s/it] 83%| | 5387/6500 [16:17:04<3:15:54, 10.56s/it]                                                         83%| | 5387/6500 [16:17:04<3:15:54, 10.56s/it] 83%| | 5388/6500 [16:17:14<3:14:46, 10.51s/it]                                                         83%| | 5388/6500 [16:17:14<3:14:46, 10.51s/it] 83%| | 5389/6500 [16:17:25<3:13:57, 10.47s/it]                                                         83%| | 5389/6500 [16:17:25<3:13:57, 10.47s/it] 83%| | 5390/6500 [16:17:35<3:13:16, 10.45s/it]                                                         83%| | 5390/6500 [16:17:35<3:13:16, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8707754611968994, 'eval_runtime': 3.954, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 1.517, 'epoch': 0.83}
                                                         83%| | 5390/6500 [16:17:39<3:13:16, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5390
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5390/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5390/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3165, 'learning_rate': 7.016398485503662e-06, 'epoch': 0.83}
{'loss': 0.325, 'learning_rate': 7.004054547098004e-06, 'epoch': 0.83}
{'loss': 0.3264, 'learning_rate': 6.991720658733169e-06, 'epoch': 0.83}
{'loss': 0.3193, 'learning_rate': 6.979396823292139e-06, 'epoch': 0.83}
{'loss': 0.3195, 'learning_rate': 6.967083043655531e-06, 'epoch': 0.83}
 83%| | 5391/6500 [16:17:50<3:37:59, 11.79s/it]                                                         83%| | 5391/6500 [16:17:50<3:37:59, 11.79s/it] 83%| | 5392/6500 [16:18:00<3:30:02, 11.37s/it]                                                         83%| | 5392/6500 [16:18:00<3:30:02, 11.37s/it] 83%| | 5393/6500 [16:18:11<3:24:25, 11.08s/it]                                                         83%| | 5393/6500 [16:18:11<3:24:25, 11.08s/it] 83%| | 5394/6500 [16:18:21<3:20:26, 10.87s/it]                                                         83%| | 5394/6500 [16:18:21<3:20:26, 10.87s/it] 83%| | 5395/6500 [16:18:31<3:17:39, 10.73s/it]                                                         83%| | 5395/6500 [16:18:31<3:1{'loss': 0.3363, 'learning_rate': 6.954779322701621e-06, 'epoch': 0.83}
{'loss': 0.2966, 'learning_rate': 6.9424856633063195e-06, 'epoch': 0.83}
{'loss': 0.3121, 'learning_rate': 6.9302020683432055e-06, 'epoch': 0.83}
{'loss': 0.2886, 'learning_rate': 6.917928540683483e-06, 'epoch': 0.83}
{'loss': 0.3414, 'learning_rate': 6.905665083196028e-06, 'epoch': 0.83}
7:39, 10.73s/it] 83%| | 5396/6500 [16:18:42<3:15:31, 10.63s/it]                                                         83%| | 5396/6500 [16:18:42<3:15:31, 10.63s/it] 83%| | 5397/6500 [16:18:52<3:14:11, 10.56s/it]                                                         83%| | 5397/6500 [16:18:52<3:14:11, 10.56s/it] 83%| | 5398/6500 [16:19:03<3:14:30, 10.59s/it]                                                         83%| | 5398/6500 [16:19:03<3:14:30, 10.59s/it] 83%| | 5399/6500 [16:19:13<3:13:14, 10.53s/it]                                                         83%| | 5399/6500 [16:19:13<3:13:14, 10.53s/it] 83%| | 5400/6500 [16:19:24<3:12:13, 10.48s/it]                                                         83%| | 5400/6500 [16:19:24<3:12:13, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8685835003852844, 'eval_runtime': 3.9573, 'eval_samples_per_second': 5.812, 'eval_steps_per_second': 1.516, 'epoch': 0.83}
                                                         83%| | 5400/6500 [16:19:28<3:12:13, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3565, 'learning_rate': 6.893411698747337e-06, 'epoch': 0.83}
{'loss': 0.3079, 'learning_rate': 6.881168390201581e-06, 'epoch': 0.83}
{'loss': 0.323, 'learning_rate': 6.868935160420537e-06, 'epoch': 0.83}
{'loss': 0.3163, 'learning_rate': 6.856712012263655e-06, 'epoch': 0.83}
{'loss': 0.8368, 'learning_rate': 6.844498948588018e-06, 'epoch': 0.83}
 83%| | 5401/6500 [16:19:39<3:36:15, 11.81s/it]                                                         83%| | 5401/6500 [16:19:39<3:36:15, 11.81s/it] 83%| | 5402/6500 [16:19:49<3:28:23, 11.39s/it]                                                         83%| | 5402/6500 [16:19:49<3:28:23, 11.39s/it] 83%| | 5403/6500 [16:19:59<3:22:55, 11.10s/it]                                                         83%| | 5403/6500 [16:19:59<3:22:55, 11.10s/it] 83%| | 5404/6500 [16:20:10<3:18:48, 10.88s/it]                                                         83%| | 5404/6500 [16:20:10<3:18:48, 10.88s/it] 83%| | 5405/6500 [16:20:20<3:15:50, 10.73s/it]                                                         83%| | 5405/6500 [16:20:20<3:1{'loss': 0.317, 'learning_rate': 6.83229597224837e-06, 'epoch': 0.83}
{'loss': 0.3194, 'learning_rate': 6.820103086097074e-06, 'epoch': 0.83}
{'loss': 0.2953, 'learning_rate': 6.807920292984144e-06, 'epoch': 0.83}
{'loss': 0.3164, 'learning_rate': 6.795747595757235e-06, 'epoch': 0.83}
{'loss': 0.3137, 'learning_rate': 6.7835849972616386e-06, 'epoch': 0.83}
5:50, 10.73s/it] 83%| | 5406/6500 [16:20:31<3:13:49, 10.63s/it]                                                         83%| | 5406/6500 [16:20:31<3:13:49, 10.63s/it] 83%| | 5407/6500 [16:20:41<3:12:20, 10.56s/it]                                                         83%| | 5407/6500 [16:20:41<3:12:20, 10.56s/it] 83%| | 5408/6500 [16:20:51<3:11:23, 10.52s/it]                                                         83%| | 5408/6500 [16:20:51<3:11:23, 10.52s/it] 83%| | 5409/6500 [16:21:02<3:10:32, 10.48s/it]                                                         83%| | 5409/6500 [16:21:02<3:10:32, 10.48s/it] 83%| | 5410/6500 [16:21:12<3:09:54, 10.45s/it]                                                         83%| | 5410/6500 [16:21:12<3:09:54, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8705467581748962, 'eval_runtime': 3.9559, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.83}
                                                         83%| | 5410/6500 [16:21:16<3:09:54, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5410
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2821, 'learning_rate': 6.771432500340302e-06, 'epoch': 0.83}
{'loss': 0.3103, 'learning_rate': 6.759290107833771e-06, 'epoch': 0.83}
{'loss': 0.3007, 'learning_rate': 6.747157822580269e-06, 'epoch': 0.83}
{'loss': 0.3147, 'learning_rate': 6.735035647415644e-06, 'epoch': 0.83}
{'loss': 0.3158, 'learning_rate': 6.722923585173385e-06, 'epoch': 0.83}
 83%| | 5411/6500 [16:21:27<3:33:57, 11.79s/it]                                                         83%| | 5411/6500 [16:21:27<3:33:57, 11.79s/it] 83%| | 5412/6500 [16:21:37<3:26:12, 11.37s/it]                                                         83%| | 5412/6500 [16:21:37<3:26:12, 11.37s/it] 83%| | 5413/6500 [16:21:48<3:20:46, 11.08s/it]                                                         83%| | 5413/6500 [16:21:48<3:20:46, 11.08s/it] 83%| | 5414/6500 [16:21:59<3:18:51, 10.99s/it]                                                         83%| | 5414/6500 [16:21:59<3:18:51, 10.99s/it] 83%| | 5415/6500 [16:22:09<3:15:25, 10.81s/it]                                                         83%| | 5415/6500 [16:22:09<3:1{'loss': 0.3213, 'learning_rate': 6.710821638684606e-06, 'epoch': 0.83}
{'loss': 0.3111, 'learning_rate': 6.698729810778065e-06, 'epoch': 0.83}
{'loss': 0.3203, 'learning_rate': 6.6866481042801575e-06, 'epoch': 0.83}
{'loss': 0.3444, 'learning_rate': 6.674576522014908e-06, 'epoch': 0.83}
{'loss': 0.3202, 'learning_rate': 6.66251506680397e-06, 'epoch': 0.83}
5:25, 10.81s/it] 83%| | 5416/6500 [16:22:19<3:12:59, 10.68s/it]                                                         83%| | 5416/6500 [16:22:19<3:12:59, 10.68s/it] 83%| | 5417/6500 [16:22:30<3:11:16, 10.60s/it]                                                         83%| | 5417/6500 [16:22:30<3:11:16, 10.60s/it] 83%| | 5418/6500 [16:22:40<3:10:03, 10.54s/it]                                                         83%| | 5418/6500 [16:22:40<3:10:03, 10.54s/it] 83%| | 5419/6500 [16:22:51<3:09:07, 10.50s/it]                                                         83%| | 5419/6500 [16:22:51<3:09:07, 10.50s/it] 83%| | 5420/6500 [16:23:01<3:08:20, 10.46s/it]                                                         83%| | 5420/6500 [16:23:01<3:08:20, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8723644614219666, 'eval_runtime': 4.2003, 'eval_samples_per_second': 5.476, 'eval_steps_per_second': 1.428, 'epoch': 0.83}
                                                         83%| | 5420/6500 [16:23:05<3:08:20, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.324, 'learning_rate': 6.6504637414666395e-06, 'epoch': 0.83}
{'loss': 0.3391, 'learning_rate': 6.638422548819851e-06, 'epoch': 0.83}
{'loss': 0.3077, 'learning_rate': 6.626391491678136e-06, 'epoch': 0.83}
{'loss': 0.3149, 'learning_rate': 6.614370572853695e-06, 'epoch': 0.83}
{'loss': 0.3384, 'learning_rate': 6.602359795156348e-06, 'epoch': 0.83}
 83%| | 5421/6500 [16:23:16<3:33:20, 11.86s/it]                                                         83%| | 5421/6500 [16:23:16<3:33:20, 11.86s/it] 83%| | 5422/6500 [16:23:27<3:25:12, 11.42s/it]                                                         83%| | 5422/6500 [16:23:27<3:25:12, 11.42s/it] 83%| | 5423/6500 [16:23:37<3:19:28, 11.11s/it]                                                         83%| | 5423/6500 [16:23:37<3:19:28, 11.11s/it] 83%| | 5424/6500 [16:23:47<3:15:24, 10.90s/it]                                                         83%| | 5424/6500 [16:23:47<3:15:24, 10.90s/it] 83%| | 5425/6500 [16:23:58<3:12:30, 10.74s/it]                                                         83%| | 5425/6500 [16:23:58<3:1{'loss': 0.3195, 'learning_rate': 6.590359161393533e-06, 'epoch': 0.83}
{'loss': 0.2997, 'learning_rate': 6.578368674370328e-06, 'epoch': 0.83}
{'loss': 0.3099, 'learning_rate': 6.56638833688944e-06, 'epoch': 0.84}
{'loss': 0.3121, 'learning_rate': 6.554418151751196e-06, 'epoch': 0.84}
{'loss': 0.3991, 'learning_rate': 6.542458121753558e-06, 'epoch': 0.84}
2:30, 10.74s/it] 83%| | 5426/6500 [16:24:08<3:10:22, 10.64s/it]                                                         83%| | 5426/6500 [16:24:08<3:10:22, 10.64s/it] 83%| | 5427/6500 [16:24:18<3:08:53, 10.56s/it]                                                         83%| | 5427/6500 [16:24:18<3:08:53, 10.56s/it] 84%| | 5428/6500 [16:24:29<3:07:46, 10.51s/it]                                                         84%| | 5428/6500 [16:24:29<3:07:46, 10.51s/it] 84%| | 5429/6500 [16:24:39<3:07:00, 10.48s/it]                                                         84%| | 5429/6500 [16:24:39<3:07:00, 10.48s/it] 84%| | 5430/6500 [16:24:50<3:07:54, 10.54s/it]                                                         84%| | 5430/6500 [16:24:50<3:07:54, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8681977987289429, 'eval_runtime': 3.9814, 'eval_samples_per_second': 5.777, 'eval_steps_per_second': 1.507, 'epoch': 0.84}
                                                         84%| | 5430/6500 [16:24:54<3:07:54, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5430
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3048, 'learning_rate': 6.530508249692107e-06, 'epoch': 0.84}
{'loss': 0.3111, 'learning_rate': 6.518568538360054e-06, 'epoch': 0.84}
{'loss': 0.3245, 'learning_rate': 6.506638990548242e-06, 'epoch': 0.84}
{'loss': 0.8172, 'learning_rate': 6.494719609045113e-06, 'epoch': 0.84}
{'loss': 0.3192, 'learning_rate': 6.482810396636757e-06, 'epoch': 0.84}
 84%| | 5431/6500 [16:25:05<3:31:10, 11.85s/it]                                                         84%| | 5431/6500 [16:25:05<3:31:10, 11.85s/it] 84%| | 5432/6500 [16:25:15<3:23:21, 11.42s/it]                                                         84%| | 5432/6500 [16:25:15<3:23:21, 11.42s/it] 84%| | 5433/6500 [16:25:26<3:17:46, 11.12s/it]                                                         84%| | 5433/6500 [16:25:26<3:17:46, 11.12s/it] 84%| | 5434/6500 [16:25:36<3:13:39, 10.90s/it]                                                         84%| | 5434/6500 [16:25:36<3:13:39, 10.90s/it] 84%| | 5435/6500 [16:25:47<3:10:51, 10.75s/it]                                                         84%| | 5435/6500 [16:25:47<3:1{'loss': 0.3187, 'learning_rate': 6.470911356106885e-06, 'epoch': 0.84}
{'loss': 0.3194, 'learning_rate': 6.4590224902368215e-06, 'epoch': 0.84}
{'loss': 0.3047, 'learning_rate': 6.447143801805516e-06, 'epoch': 0.84}
{'loss': 0.3286, 'learning_rate': 6.4352752935895435e-06, 'epoch': 0.84}
{'loss': 0.3041, 'learning_rate': 6.423416968363088e-06, 'epoch': 0.84}
0:51, 10.75s/it] 84%| | 5436/6500 [16:25:57<3:08:56, 10.65s/it]                                                         84%| | 5436/6500 [16:25:57<3:08:56, 10.65s/it] 84%| | 5437/6500 [16:26:07<3:07:28, 10.58s/it]                                                         84%| | 5437/6500 [16:26:07<3:07:28, 10.58s/it] 84%| | 5438/6500 [16:26:18<3:06:23, 10.53s/it]                                                         84%| | 5438/6500 [16:26:18<3:06:23, 10.53s/it] 84%| | 5439/6500 [16:26:28<3:05:35, 10.50s/it]                                                         84%| | 5439/6500 [16:26:28<3:05:35, 10.50s/it] 84%| | 5440/6500 [16:26:39<3:04:44, 10.46s/it]                                                         84%| | 5440/6500 [16:26:39<3:04:44, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8701803088188171, 'eval_runtime': 3.947, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.84}
                                                         84%| | 5440/6500 [16:26:42<3:04:44, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5440
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5440/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5440/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5440/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3003, 'learning_rate': 6.411568828897973e-06, 'epoch': 0.84}
{'loss': 0.3087, 'learning_rate': 6.399730877963617e-06, 'epoch': 0.84}
{'loss': 0.3008, 'learning_rate': 6.387903118327077e-06, 'epoch': 0.84}
{'loss': 0.3254, 'learning_rate': 6.37608555275302e-06, 'epoch': 0.84}
{'loss': 0.3098, 'learning_rate': 6.36427818400373e-06, 'epoch': 0.84}
 84%| | 5441/6500 [16:26:53<3:27:47, 11.77s/it]                                                         84%| | 5441/6500 [16:26:53<3:27:47, 11.77s/it] 84%| | 5442/6500 [16:27:04<3:20:14, 11.36s/it]                                                         84%| | 5442/6500 [16:27:04<3:20:14, 11.36s/it] 84%| | 5443/6500 [16:27:14<3:14:53, 11.06s/it]                                                         84%| | 5443/6500 [16:27:14<3:14:53, 11.06s/it] 84%| | 5444/6500 [16:27:25<3:11:06, 10.86s/it]                                                         84%| | 5444/6500 [16:27:25<3:11:06, 10.86s/it] 84%| | 5445/6500 [16:27:35<3:08:17, 10.71s/it]                                                         84%| | 5445/6500 [16:27:35<3:0{'loss': 0.3251, 'learning_rate': 6.352481014839101e-06, 'epoch': 0.84}
{'loss': 0.3046, 'learning_rate': 6.340694048016649e-06, 'epoch': 0.84}
{'loss': 0.325, 'learning_rate': 6.328917286291514e-06, 'epoch': 0.84}
{'loss': 0.3092, 'learning_rate': 6.317150732416438e-06, 'epoch': 0.84}
{'loss': 0.304, 'learning_rate': 6.305394389141784e-06, 'epoch': 0.84}
8:17, 10.71s/it] 84%| | 5446/6500 [16:27:45<3:06:28, 10.62s/it]                                                         84%| | 5446/6500 [16:27:45<3:06:28, 10.62s/it] 84%| | 5447/6500 [16:27:56<3:07:05, 10.66s/it]                                                         84%| | 5447/6500 [16:27:56<3:07:05, 10.66s/it] 84%| | 5448/6500 [16:28:06<3:05:24, 10.57s/it]                                                         84%| | 5448/6500 [16:28:06<3:05:24, 10.57s/it] 84%| | 5449/6500 [16:28:17<3:04:13, 10.52s/it]                                                         84%| | 5449/6500 [16:28:17<3:04:13, 10.52s/it] 84%| | 5450/6500 [16:28:27<3:03:21, 10.48s/it]                                                         84%| | 5450/6500 [16:28:27<3:03:21, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8726257681846619, 'eval_runtime': 3.9632, 'eval_samples_per_second': 5.803, 'eval_steps_per_second': 1.514, 'epoch': 0.84}
                                                         84%| | 5450/6500 [16:28:31<3:03:21, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3248, 'learning_rate': 6.293648259215517e-06, 'epoch': 0.84}
{'loss': 0.3146, 'learning_rate': 6.281912345383239e-06, 'epoch': 0.84}
{'loss': 0.3147, 'learning_rate': 6.270186650388133e-06, 'epoch': 0.84}
{'loss': 0.3114, 'learning_rate': 6.25847117697102e-06, 'epoch': 0.84}
{'loss': 0.3361, 'learning_rate': 6.246765927870313e-06, 'epoch': 0.84}
 84%| | 5451/6500 [16:28:42<3:27:07, 11.85s/it]                                                         84%| | 5451/6500 [16:28:42<3:27:07, 11.85s/it] 84%| | 5452/6500 [16:28:53<3:19:20, 11.41s/it]                                                         84%| | 5452/6500 [16:28:53<3:19:20, 11.41s/it] 84%| | 5453/6500 [16:29:03<3:13:41, 11.10s/it]                                                         84%| | 5453/6500 [16:29:03<3:13:41, 11.10s/it] 84%| | 5454/6500 [16:29:13<3:09:37, 10.88s/it]                                                         84%| | 5454/6500 [16:29:13<3:09:37, 10.88s/it] 84%| | 5455/6500 [16:29:24<3:06:43, 10.72s/it]                                                         84%| | 5455/6500 [16:29:24<3:0{'loss': 0.3126, 'learning_rate': 6.23507090582206e-06, 'epoch': 0.84}
{'loss': 0.3065, 'learning_rate': 6.2233861135598756e-06, 'epoch': 0.84}
{'loss': 0.2968, 'learning_rate': 6.211711553815025e-06, 'epoch': 0.84}
{'loss': 0.3291, 'learning_rate': 6.200047229316358e-06, 'epoch': 0.84}
{'loss': 0.3731, 'learning_rate': 6.188393142790344e-06, 'epoch': 0.84}
6:43, 10.72s/it] 84%| | 5456/6500 [16:29:34<3:04:41, 10.61s/it]                                                         84%| | 5456/6500 [16:29:34<3:04:41, 10.61s/it] 84%| | 5457/6500 [16:29:44<3:03:14, 10.54s/it]                                                         84%| | 5457/6500 [16:29:44<3:03:14, 10.54s/it] 84%| | 5458/6500 [16:29:55<3:02:15, 10.50s/it]                                                         84%| | 5458/6500 [16:29:55<3:02:15, 10.50s/it] 84%| | 5459/6500 [16:30:05<3:01:31, 10.46s/it]                                                         84%| | 5459/6500 [16:30:05<3:01:31, 10.46s/it] 84%| | 5460/6500 [16:30:16<3:00:53, 10.44s/it]                                                         84%| | 5460/6500 [16:30:16<3:00:53, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8698747754096985, 'eval_runtime': 3.9455, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.84}
                                                         84%| | 5460/6500 [16:30:20<3:00:53, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5460/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3056, 'learning_rate': 6.176749296961054e-06, 'epoch': 0.84}
{'loss': 0.2977, 'learning_rate': 6.165115694550161e-06, 'epoch': 0.84}
{'loss': 0.33, 'learning_rate': 6.1534923382769615e-06, 'epoch': 0.84}
{'loss': 0.8204, 'learning_rate': 6.141879230858333e-06, 'epoch': 0.84}
{'loss': 0.3348, 'learning_rate': 6.130276375008775e-06, 'epoch': 0.84}
 84%| | 5461/6500 [16:30:30<3:23:45, 11.77s/it]                                                         84%| | 5461/6500 [16:30:30<3:23:45, 11.77s/it] 84%| | 5462/6500 [16:30:41<3:16:25, 11.35s/it]                                                         84%| | 5462/6500 [16:30:41<3:16:25, 11.35s/it] 84%| | 5463/6500 [16:30:51<3:12:26, 11.13s/it]                                                         84%| | 5463/6500 [16:30:51<3:12:26, 11.13s/it] 84%| | 5464/6500 [16:31:02<3:08:16, 10.90s/it]                                                         84%| | 5464/6500 [16:31:02<3:08:16, 10.90s/it] 84%| | 5465/6500 [16:31:12<3:05:27, 10.75s/it]                                                         84%| | 5465/6500 [16:31:12<3:0{'loss': 0.3197, 'learning_rate': 6.118683773440376e-06, 'epoch': 0.84}
{'loss': 0.3049, 'learning_rate': 6.107101428862861e-06, 'epoch': 0.84}
{'loss': 0.3072, 'learning_rate': 6.095529343983497e-06, 'epoch': 0.84}
{'loss': 0.3315, 'learning_rate': 6.083967521507206e-06, 'epoch': 0.84}
{'loss': 0.3058, 'learning_rate': 6.072415964136496e-06, 'epoch': 0.84}
5:27, 10.75s/it] 84%| | 5466/6500 [16:31:27<3:25:29, 11.92s/it]                                                         84%| | 5466/6500 [16:31:27<3:25:29, 11.92s/it] 84%| | 5467/6500 [16:31:37<3:17:48, 11.49s/it]                                                         84%| | 5467/6500 [16:31:37<3:17:48, 11.49s/it] 84%| | 5468/6500 [16:31:48<3:12:03, 11.17s/it]                                                         84%| | 5468/6500 [16:31:48<3:12:03, 11.17s/it] 84%| | 5469/6500 [16:31:58<3:07:41, 10.92s/it]                                                         84%| | 5469/6500 [16:31:58<3:07:41, 10.92s/it] 84%| | 5470/6500 [16:32:08<3:04:33, 10.75s/it]                                                         84%| | 5470/6500 [16:32:08<3:04:33, 10.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8707048892974854, 'eval_runtime': 4.0728, 'eval_samples_per_second': 5.647, 'eval_steps_per_second': 1.473, 'epoch': 0.84}
                                                         84%| | 5470/6500 [16:32:13<3:04:33, 10.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5470I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5470/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3102, 'learning_rate': 6.06087467457147e-06, 'epoch': 0.84}
{'loss': 0.3107, 'learning_rate': 6.049343655509831e-06, 'epoch': 0.84}
{'loss': 0.312, 'learning_rate': 6.03782290964689e-06, 'epoch': 0.84}
{'loss': 0.3106, 'learning_rate': 6.026312439675552e-06, 'epoch': 0.84}
{'loss': 0.3111, 'learning_rate': 6.0148122482863115e-06, 'epoch': 0.84}
 84%| | 5471/6500 [16:32:24<3:26:30, 12.04s/it]                                                         84%| | 5471/6500 [16:32:24<3:26:30, 12.04s/it] 84%| | 5472/6500 [16:32:34<3:17:29, 11.53s/it]                                                         84%| | 5472/6500 [16:32:34<3:17:29, 11.53s/it] 84%| | 5473/6500 [16:32:44<3:11:10, 11.17s/it]                                                         84%| | 5473/6500 [16:32:44<3:11:10, 11.17s/it] 84%| | 5474/6500 [16:32:55<3:06:39, 10.92s/it]                                                         84%| | 5474/6500 [16:32:55<3:06:39, 10.92s/it] 84%| | 5475/6500 [16:33:05<3:03:24, 10.74s/it]                                                         84%| | 5475/6500 [16:33:05<3:0{'loss': 0.3191, 'learning_rate': 6.003322338167277e-06, 'epoch': 0.84}
{'loss': 0.3131, 'learning_rate': 5.991842712004142e-06, 'epoch': 0.84}
{'loss': 0.3272, 'learning_rate': 5.980373372480208e-06, 'epoch': 0.84}
{'loss': 0.3211, 'learning_rate': 5.968914322276348e-06, 'epoch': 0.84}
{'loss': 0.3208, 'learning_rate': 5.957465564071035e-06, 'epoch': 0.84}
3:24, 10.74s/it] 84%| | 5476/6500 [16:33:15<3:01:09, 10.61s/it]                                                         84%| | 5476/6500 [16:33:15<3:01:09, 10.61s/it] 84%| | 5477/6500 [16:33:26<2:59:33, 10.53s/it]                                                         84%| | 5477/6500 [16:33:26<2:59:33, 10.53s/it] 84%| | 5478/6500 [16:33:36<2:58:18, 10.47s/it]                                                         84%| | 5478/6500 [16:33:36<2:58:18, 10.47s/it] 84%| | 5479/6500 [16:33:47<2:59:53, 10.57s/it]                                                         84%| | 5479/6500 [16:33:47<2:59:53, 10.57s/it] 84%| | 5480/6500 [16:33:57<2:58:42, 10.51s/it]                                                         84%| | 5480/6500 [16:33:57<2:58:42, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8705588579177856, 'eval_runtime': 4.6688, 'eval_samples_per_second': 4.926, 'eval_steps_per_second': 1.285, 'epoch': 0.84}
                                                         84%| | 5480/6500 [16:34:02<2:58:42, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5480/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3275, 'learning_rate': 5.94602710054038e-06, 'epoch': 0.84}
{'loss': 0.3214, 'learning_rate': 5.934598934358038e-06, 'epoch': 0.84}
{'loss': 0.3202, 'learning_rate': 5.923181068195266e-06, 'epoch': 0.84}
{'loss': 0.3151, 'learning_rate': 5.9117735047209355e-06, 'epoch': 0.84}
{'loss': 0.3236, 'learning_rate': 5.90037624660148e-06, 'epoch': 0.84}
 84%| | 5481/6500 [16:34:13<3:23:57, 12.01s/it]                                                         84%| | 5481/6500 [16:34:13<3:23:57, 12.01s/it] 84%| | 5482/6500 [16:34:23<3:15:13, 11.51s/it]                                                         84%| | 5482/6500 [16:34:23<3:15:13, 11.51s/it] 84%| | 5483/6500 [16:34:33<3:09:03, 11.15s/it]                                                         84%| | 5483/6500 [16:34:33<3:09:03, 11.15s/it] 84%| | 5484/6500 [16:34:44<3:04:44, 10.91s/it]                                                         84%| | 5484/6500 [16:34:44<3:04:44, 10.91s/it] 84%| | 5485/6500 [16:34:54<3:02:15, 10.77s/it]                                                         84%| | 5485/6500 [16:34:54<3:0{'loss': 0.3021, 'learning_rate': 5.888989296500952e-06, 'epoch': 0.84}
{'loss': 0.3055, 'learning_rate': 5.877612657080983e-06, 'epoch': 0.84}
{'loss': 0.2973, 'learning_rate': 5.8662463310007796e-06, 'epoch': 0.84}
{'loss': 0.342, 'learning_rate': 5.8548903209171614e-06, 'epoch': 0.84}
{'loss': 0.3668, 'learning_rate': 5.843544629484521e-06, 'epoch': 0.84}
2:15, 10.77s/it] 84%| | 5486/6500 [16:35:04<2:59:57, 10.65s/it]                                                         84%| | 5486/6500 [16:35:04<2:59:57, 10.65s/it] 84%| | 5487/6500 [16:35:15<2:58:13, 10.56s/it]                                                         84%| | 5487/6500 [16:35:15<2:58:13, 10.56s/it] 84%| | 5488/6500 [16:35:25<2:57:00, 10.49s/it]                                                         84%| | 5488/6500 [16:35:25<2:57:00, 10.49s/it] 84%| | 5489/6500 [16:35:35<2:56:08, 10.45s/it]                                                         84%| | 5489/6500 [16:35:35<2:56:08, 10.45s/it] 84%| | 5490/6500 [16:35:46<2:55:30, 10.43s/it]                                                         84%| | 5490/6500 [16:35:46<2:55:30, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8690077066421509, 'eval_runtime': 3.9506, 'eval_samples_per_second': 5.822, 'eval_steps_per_second': 1.519, 'epoch': 0.84}
                                                         84%| | 5490/6500 [16:35:50<2:55:30, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5490
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5490

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3076, 'learning_rate': 5.832209259354848e-06, 'epoch': 0.84}
{'loss': 0.3293, 'learning_rate': 5.820884213177713e-06, 'epoch': 0.84}
{'loss': 0.4467, 'learning_rate': 5.809569493600281e-06, 'epoch': 0.85}
{'loss': 0.7152, 'learning_rate': 5.798265103267303e-06, 'epoch': 0.85}
{'loss': 0.3225, 'learning_rate': 5.786971044821099e-06, 'epoch': 0.85}
 84%| | 5491/6500 [16:36:01<3:17:16, 11.73s/it]                                                         84%| | 5491/6500 [16:36:01<3:17:16, 11.73s/it] 84%| | 5492/6500 [16:36:11<3:10:04, 11.31s/it]                                                         84%| | 5492/6500 [16:36:11<3:10:04, 11.31s/it] 85%| | 5493/6500 [16:36:21<3:04:59, 11.02s/it]                                                         85%| | 5493/6500 [16:36:21<3:04:59, 11.02s/it] 85%| | 5494/6500 [16:36:32<3:01:23, 10.82s/it]                                                         85%| | 5494/6500 [16:36:32<3:01:23, 10.82s/it] 85%| | 5495/6500 [16:36:42<3:00:50, 10.80s/it]                                                         85%| | 5495/6500 [16:36:42<3:0{'loss': 0.3195, 'learning_rate': 5.775687320901596e-06, 'epoch': 0.85}
{'loss': 0.2936, 'learning_rate': 5.7644139341462955e-06, 'epoch': 0.85}
{'loss': 0.3209, 'learning_rate': 5.75315088719029e-06, 'epoch': 0.85}
{'loss': 0.3133, 'learning_rate': 5.741898182666227e-06, 'epoch': 0.85}
{'loss': 0.2842, 'learning_rate': 5.7306558232043784e-06, 'epoch': 0.85}
0:50, 10.80s/it] 85%| | 5496/6500 [16:36:53<2:58:37, 10.67s/it]                                                         85%| | 5496/6500 [16:36:53<2:58:37, 10.67s/it] 85%| | 5497/6500 [16:37:03<2:56:46, 10.58s/it]                                                         85%| | 5497/6500 [16:37:03<2:56:46, 10.58s/it] 85%| | 5498/6500 [16:37:15<3:05:48, 11.13s/it]                                                         85%| | 5498/6500 [16:37:15<3:05:48, 11.13s/it] 85%| | 5499/6500 [16:37:26<3:02:20, 10.93s/it]                                                         85%| | 5499/6500 [16:37:26<3:02:20, 10.93s/it] 85%| | 5500/6500 [16:37:36<2:59:21, 10.76s/it]                                                         85%| | 5500/6500 [16:37:36<2:59:21, 10.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8708320260047913, 'eval_runtime': 3.9502, 'eval_samples_per_second': 5.822, 'eval_steps_per_second': 1.519, 'epoch': 0.85}
                                                         85%| | 5500/6500 [16:37:40<2:59:21, 10.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5500
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5500/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5500/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5500/pytorch_model.binthe pytorch model path is 
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3209, 'learning_rate': 5.719423811432562e-06, 'epoch': 0.85}
{'loss': 0.295, 'learning_rate': 5.7082021499761994e-06, 'epoch': 0.85}
{'loss': 0.3083, 'learning_rate': 5.696990841458289e-06, 'epoch': 0.85}
{'loss': 0.3164, 'learning_rate': 5.685789888499398e-06, 'epoch': 0.85}
{'loss': 0.3176, 'learning_rate': 5.6745992937176865e-06, 'epoch': 0.85}
 85%| | 5501/6500 [16:37:51<3:20:39, 12.05s/it]                                                         85%| | 5501/6500 [16:37:51<3:20:39, 12.05s/it] 85%| | 5502/6500 [16:38:02<3:12:05, 11.55s/it]                                                         85%| | 5502/6500 [16:38:02<3:12:05, 11.55s/it] 85%| | 5503/6500 [16:38:12<3:06:02, 11.20s/it]                                                         85%| | 5503/6500 [16:38:12<3:06:02, 11.20s/it] 85%| | 5504/6500 [16:38:22<3:01:46, 10.95s/it]                                                         85%| | 5504/6500 [16:38:22<3:01:46, 10.95s/it] 85%| | 5505/6500 [16:38:33<2:58:42, 10.78s/it]                                                         85%| | 5505/6500 [16:38:33<2:5{'loss': 0.3023, 'learning_rate': 5.663419059728892e-06, 'epoch': 0.85}
{'loss': 0.3169, 'learning_rate': 5.652249189146319e-06, 'epoch': 0.85}
{'loss': 0.324, 'learning_rate': 5.641089684580858e-06, 'epoch': 0.85}
{'loss': 0.306, 'learning_rate': 5.629940548640988e-06, 'epoch': 0.85}
{'loss': 0.3217, 'learning_rate': 5.618801783932725e-06, 'epoch': 0.85}
8:42, 10.78s/it] 85%| | 5506/6500 [16:38:43<2:56:34, 10.66s/it]                                                         85%| | 5506/6500 [16:38:43<2:56:34, 10.66s/it] 85%| | 5507/6500 [16:38:54<2:54:57, 10.57s/it]                                                         85%| | 5507/6500 [16:38:54<2:54:57, 10.57s/it] 85%| | 5508/6500 [16:39:04<2:53:50, 10.51s/it]                                                         85%| | 5508/6500 [16:39:04<2:53:50, 10.51s/it] 85%| | 5509/6500 [16:39:14<2:52:59, 10.47s/it]                                                         85%| | 5509/6500 [16:39:14<2:52:59, 10.47s/it] 85%| | 5510/6500 [16:39:25<2:52:19, 10.44s/it]                                                         85%| | 5510/6500 [16:39:25<2:52:19, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8729860186576843, 'eval_runtime': 4.19, 'eval_samples_per_second': 5.489, 'eval_steps_per_second': 1.432, 'epoch': 0.85}
                                                         85%| | 5510/6500 [16:39:29<2:52:19, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5510/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3106, 'learning_rate': 5.607673393059709e-06, 'epoch': 0.85}
{'loss': 0.3073, 'learning_rate': 5.596555378623125e-06, 'epoch': 0.85}
{'loss': 0.3137, 'learning_rate': 5.58544774322175e-06, 'epoch': 0.85}
{'loss': 0.331, 'learning_rate': 5.574350489451913e-06, 'epoch': 0.85}
{'loss': 0.3103, 'learning_rate': 5.563263619907538e-06, 'epoch': 0.85}
 85%| | 5511/6500 [16:39:41<3:20:23, 12.16s/it]                                                         85%| | 5511/6500 [16:39:41<3:20:23, 12.16s/it] 85%| | 5512/6500 [16:39:51<3:11:27, 11.63s/it]                                                         85%| | 5512/6500 [16:39:51<3:11:27, 11.63s/it] 85%| | 5513/6500 [16:40:02<3:05:09, 11.26s/it]                                                         85%| | 5513/6500 [16:40:02<3:05:09, 11.26s/it] 85%| | 5514/6500 [16:40:12<3:00:39, 10.99s/it]                                                         85%| | 5514/6500 [16:40:12<3:00:39, 10.99s/it] 85%| | 5515/6500 [16:40:22<2:57:29, 10.81s/it]                                                         85%| | 5515/6500 [16:40:22<2:5{'loss': 0.3088, 'learning_rate': 5.552187137180115e-06, 'epoch': 0.85}
{'loss': 0.301, 'learning_rate': 5.5411210438586995e-06, 'epoch': 0.85}
{'loss': 0.3225, 'learning_rate': 5.530065342529922e-06, 'epoch': 0.85}
{'loss': 0.3775, 'learning_rate': 5.519020035777994e-06, 'epoch': 0.85}
{'loss': 0.3127, 'learning_rate': 5.507985126184695e-06, 'epoch': 0.85}
7:29, 10.81s/it] 85%| | 5516/6500 [16:40:33<2:55:18, 10.69s/it]                                                         85%| | 5516/6500 [16:40:33<2:55:18, 10.69s/it] 85%| | 5517/6500 [16:40:43<2:53:42, 10.60s/it]                                                         85%| | 5517/6500 [16:40:43<2:53:42, 10.60s/it] 85%| | 5518/6500 [16:40:54<2:52:30, 10.54s/it]                                                         85%| | 5518/6500 [16:40:54<2:52:30, 10.54s/it] 85%| | 5519/6500 [16:41:04<2:51:29, 10.49s/it]                                                         85%| | 5519/6500 [16:41:04<2:51:29, 10.49s/it] 85%| | 5520/6500 [16:41:14<2:50:37, 10.45s/it]                                                         85%| | 5520/6500 [16:41:14<2:50:37, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8702095150947571, 'eval_runtime': 3.9555, 'eval_samples_per_second': 5.815, 'eval_steps_per_second': 1.517, 'epoch': 0.85}
                                                         85%| | 5520/6500 [16:41:18<2:50:37, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5520
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2913, 'learning_rate': 5.4969606163293445e-06, 'epoch': 0.85}
{'loss': 0.3242, 'learning_rate': 5.485946508788864e-06, 'epoch': 0.85}
{'loss': 0.8154, 'learning_rate': 5.47494280613774e-06, 'epoch': 0.85}
{'loss': 0.3241, 'learning_rate': 5.4639495109480185e-06, 'epoch': 0.85}
{'loss': 0.3126, 'learning_rate': 5.452966625789313e-06, 'epoch': 0.85}
 85%| | 5521/6500 [16:41:29<3:12:10, 11.78s/it]                                                         85%| | 5521/6500 [16:41:29<3:12:10, 11.78s/it] 85%| | 5522/6500 [16:41:40<3:05:05, 11.36s/it]                                                         85%| | 5522/6500 [16:41:40<3:05:05, 11.36s/it] 85%| | 5523/6500 [16:41:50<2:59:59, 11.05s/it]                                                         85%| | 5523/6500 [16:41:50<2:59:59, 11.05s/it] 85%| | 5524/6500 [16:42:00<2:56:21, 10.84s/it]                                                         85%| | 5524/6500 [16:42:00<2:56:21, 10.84s/it] 85%| | 5525/6500 [16:42:11<2:53:47, 10.70s/it]                                                         85%| | 5525/6500 [16:42:11<2:5{'loss': 0.3205, 'learning_rate': 5.441994153228813e-06, 'epoch': 0.85}
{'loss': 0.2997, 'learning_rate': 5.431032095831262e-06, 'epoch': 0.85}
{'loss': 0.3335, 'learning_rate': 5.420080456158971e-06, 'epoch': 0.85}
{'loss': 0.2975, 'learning_rate': 5.409139236771827e-06, 'epoch': 0.85}
{'loss': 0.2954, 'learning_rate': 5.398208440227264e-06, 'epoch': 0.85}
3:47, 10.70s/it] 85%| | 5526/6500 [16:42:21<2:51:54, 10.59s/it]                                                         85%| | 5526/6500 [16:42:21<2:51:54, 10.59s/it] 85%| | 5527/6500 [16:42:32<2:52:19, 10.63s/it]                                                         85%| | 5527/6500 [16:42:32<2:52:19, 10.63s/it] 85%| | 5528/6500 [16:42:42<2:50:52, 10.55s/it]                                                         85%| | 5528/6500 [16:42:42<2:50:52, 10.55s/it] 85%| | 5529/6500 [16:42:52<2:49:53, 10.50s/it]                                                         85%| | 5529/6500 [16:42:52<2:49:53, 10.50s/it] 85%| | 5530/6500 [16:43:03<2:49:06, 10.46s/it]                                                         85%| | 5530/6500 [16:43:03<2:49:06, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711923360824585, 'eval_runtime': 3.9557, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.85}
                                                         85%| | 5530/6500 [16:43:07<2:49:06, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5530
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.311, 'learning_rate': 5.387288069080299e-06, 'epoch': 0.85}
{'loss': 0.309, 'learning_rate': 5.376378125883508e-06, 'epoch': 0.85}
{'loss': 0.3167, 'learning_rate': 5.365478613187003e-06, 'epoch': 0.85}
{'loss': 0.3036, 'learning_rate': 5.35458953353849e-06, 'epoch': 0.85}
{'loss': 0.3216, 'learning_rate': 5.343710889483222e-06, 'epoch': 0.85}
 85%| | 5531/6500 [16:43:18<3:10:39, 11.81s/it]                                                         85%| | 5531/6500 [16:43:18<3:10:39, 11.81s/it] 85%| | 5532/6500 [16:43:28<3:03:26, 11.37s/it]                                                         85%| | 5532/6500 [16:43:28<3:03:26, 11.37s/it] 85%| | 5533/6500 [16:43:38<2:58:24, 11.07s/it]                                                         85%| | 5533/6500 [16:43:38<2:58:24, 11.07s/it] 85%| | 5534/6500 [16:43:49<2:54:58, 10.87s/it]                                                         85%| | 5534/6500 [16:43:49<2:54:58, 10.87s/it] 85%| | 5535/6500 [16:43:59<2:52:17, 10.71s/it]                                                         85%| | 5535/6500 [16:43:59<2:5{'loss': 0.3081, 'learning_rate': 5.332842683564021e-06, 'epoch': 0.85}
{'loss': 0.3249, 'learning_rate': 5.321984918321266e-06, 'epoch': 0.85}
{'loss': 0.3162, 'learning_rate': 5.3111375962928865e-06, 'epoch': 0.85}
{'loss': 0.3026, 'learning_rate': 5.300300720014378e-06, 'epoch': 0.85}
{'loss': 0.3244, 'learning_rate': 5.2894742920188036e-06, 'epoch': 0.85}
2:17, 10.71s/it] 85%| | 5536/6500 [16:44:10<2:50:21, 10.60s/it]                                                         85%| | 5536/6500 [16:44:10<2:50:21, 10.60s/it] 85%| | 5537/6500 [16:44:20<2:48:55, 10.53s/it]                                                         85%| | 5537/6500 [16:44:20<2:48:55, 10.53s/it] 85%| | 5538/6500 [16:44:30<2:47:52, 10.47s/it]                                                         85%| | 5538/6500 [16:44:30<2:47:52, 10.47s/it] 85%| | 5539/6500 [16:44:41<2:47:08, 10.44s/it]                                                         85%| | 5539/6500 [16:44:41<2:47:08, 10.44s/it] 85%| | 5540/6500 [16:44:51<2:46:58, 10.44s/it]                                                         85%| | 5540/6500 [16:44:51<2:46:58, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8715197443962097, 'eval_runtime': 3.9501, 'eval_samples_per_second': 5.823, 'eval_steps_per_second': 1.519, 'epoch': 0.85}
                                                         85%| | 5540/6500 [16:44:55<2:46:58, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5540
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5540

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5540/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5540/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3152, 'learning_rate': 5.278658314836765e-06, 'epoch': 0.85}
{'loss': 0.316, 'learning_rate': 5.267852790996436e-06, 'epoch': 0.85}
{'loss': 0.314, 'learning_rate': 5.257057723023551e-06, 'epoch': 0.85}
{'loss': 0.3288, 'learning_rate': 5.246273113441369e-06, 'epoch': 0.85}
{'loss': 0.3007, 'learning_rate': 5.235498964770747e-06, 'epoch': 0.85}
 85%| | 5541/6500 [16:45:06<3:08:23, 11.79s/it]                                                         85%| | 5541/6500 [16:45:06<3:08:23, 11.79s/it] 85%| | 5542/6500 [16:45:16<3:01:31, 11.37s/it]                                                         85%| | 5542/6500 [16:45:16<3:01:31, 11.37s/it] 85%| | 5543/6500 [16:45:27<2:56:42, 11.08s/it]                                                         85%| | 5543/6500 [16:45:27<2:56:42, 11.08s/it] 85%| | 5544/6500 [16:45:37<2:54:34, 10.96s/it]                                                         85%| | 5544/6500 [16:45:37<2:54:34, 10.96s/it] 85%| | 5545/6500 [16:45:48<2:51:46, 10.79s/it]                                                         85%| | 5545/6500 [16:45:48<2:5{'loss': 0.3026, 'learning_rate': 5.224735279530063e-06, 'epoch': 0.85}
{'loss': 0.2906, 'learning_rate': 5.213982060235268e-06, 'epoch': 0.85}
{'loss': 0.3359, 'learning_rate': 5.203239309399865e-06, 'epoch': 0.85}
{'loss': 0.3632, 'learning_rate': 5.19250702953491e-06, 'epoch': 0.85}
{'loss': 0.3033, 'learning_rate': 5.181785223148999e-06, 'epoch': 0.85}
1:46, 10.79s/it] 85%| | 5546/6500 [16:45:58<2:49:47, 10.68s/it]                                                         85%| | 5546/6500 [16:45:58<2:49:47, 10.68s/it] 85%| | 5547/6500 [16:46:09<2:49:28, 10.67s/it]                                                         85%| | 5547/6500 [16:46:09<2:49:28, 10.67s/it] 85%| | 5548/6500 [16:46:19<2:48:04, 10.59s/it]                                                         85%| | 5548/6500 [16:46:19<2:48:04, 10.59s/it] 85%| | 5549/6500 [16:46:31<2:53:48, 10.97s/it]                                                         85%| | 5549/6500 [16:46:31<2:53:48, 10.97s/it] 85%| | 5550/6500 [16:46:42<2:51:18, 10.82s/it]                                                         85%| | 5550/6500 [16:46:42<2:51:18, 10.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8705193996429443, 'eval_runtime': 4.305, 'eval_samples_per_second': 5.343, 'eval_steps_per_second': 1.394, 'epoch': 0.85}
                                                         85%| | 5550/6500 [16:46:46<2:51:18, 10.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5550/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3183, 'learning_rate': 5.17107389274829e-06, 'epoch': 0.85}
{'loss': 0.306, 'learning_rate': 5.160373040836497e-06, 'epoch': 0.85}
{'loss': 0.8299, 'learning_rate': 5.149682669914874e-06, 'epoch': 0.85}
{'loss': 0.3262, 'learning_rate': 5.139002782482244e-06, 'epoch': 0.85}
{'loss': 0.319, 'learning_rate': 5.12833338103495e-06, 'epoch': 0.85}
 85%| | 5551/6500 [16:46:57<3:12:26, 12.17s/it]                                                         85%| | 5551/6500 [16:46:57<3:12:26, 12.17s/it] 85%| | 5552/6500 [16:47:07<3:03:44, 11.63s/it]                                                         85%| | 5552/6500 [16:47:07<3:03:44, 11.63s/it] 85%| | 5553/6500 [16:47:18<2:57:34, 11.25s/it]                                                         85%| | 5553/6500 [16:47:18<2:57:34, 11.25s/it] 85%| | 5554/6500 [16:47:28<2:53:11, 10.98s/it]                                                         85%| | 5554/6500 [16:47:28<2:53:11, 10.98s/it] 85%| | 5555/6500 [16:47:38<2:50:04, 10.80s/it]                                                         85%| | 5555/6500 [16:47:38<2:5{'loss': 0.2997, 'learning_rate': 5.117674468066885e-06, 'epoch': 0.85}
{'loss': 0.3048, 'learning_rate': 5.107026046069541e-06, 'epoch': 0.85}
{'loss': 0.3333, 'learning_rate': 5.096388117531897e-06, 'epoch': 0.86}
{'loss': 0.2884, 'learning_rate': 5.0857606849405214e-06, 'epoch': 0.86}
{'loss': 0.3147, 'learning_rate': 5.075143750779499e-06, 'epoch': 0.86}
0:04, 10.80s/it] 85%| | 5556/6500 [16:47:49<2:48:04, 10.68s/it]                                                         85%| | 5556/6500 [16:47:49<2:48:04, 10.68s/it] 85%| | 5557/6500 [16:47:59<2:46:24, 10.59s/it]                                                         85%| | 5557/6500 [16:47:59<2:46:24, 10.59s/it] 86%| | 5558/6500 [16:48:10<2:45:10, 10.52s/it]                                                         86%| | 5558/6500 [16:48:10<2:45:10, 10.52s/it] 86%| | 5559/6500 [16:48:20<2:44:16, 10.47s/it]                                                         86%| | 5559/6500 [16:48:20<2:44:16, 10.47s/it] 86%| | 5560/6500 [16:48:31<2:46:28, 10.63s/it]                                                         86%| | 5560/6500 [16:48:31<2:46:28, 10.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8717381954193115, 'eval_runtime': 3.946, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 1.521, 'epoch': 0.86}
                                                         86%| | 5560/6500 [16:48:35<2:46:28, 10.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5560/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5560/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3085, 'learning_rate': 5.0645373175304714e-06, 'epoch': 0.86}
{'loss': 0.3163, 'learning_rate': 5.053941387672639e-06, 'epoch': 0.86}
{'loss': 0.3271, 'learning_rate': 5.0433559636827444e-06, 'epoch': 0.86}
{'loss': 0.3113, 'learning_rate': 5.032781048035034e-06, 'epoch': 0.86}
{'loss': 0.3268, 'learning_rate': 5.022216643201355e-06, 'epoch': 0.86}
 86%| | 5561/6500 [16:48:46<3:06:09, 11.89s/it]                                                         86%| | 5561/6500 [16:48:46<3:06:09, 11.89s/it] 86%| | 5562/6500 [16:48:56<2:59:00, 11.45s/it]                                                         86%| | 5562/6500 [16:48:56<2:59:00, 11.45s/it] 86%| | 5563/6500 [16:49:07<2:53:42, 11.12s/it]                                                         86%| | 5563/6500 [16:49:07<2:53:42, 11.12s/it] 86%| | 5564/6500 [16:49:17<2:50:02, 10.90s/it]                                                         86%| | 5564/6500 [16:49:17<2:50:02, 10.90s/it] 86%| | 5565/6500 [16:49:27<2:47:23, 10.74s/it]                                                         86%| | 5565/6500 [16:49:27<2:4{'loss': 0.3125, 'learning_rate': 5.011662751651064e-06, 'epoch': 0.86}
{'loss': 0.3414, 'learning_rate': 5.001119375851071e-06, 'epoch': 0.86}
{'loss': 0.3054, 'learning_rate': 4.9905865182658275e-06, 'epoch': 0.86}
{'loss': 0.3213, 'learning_rate': 4.980064181357319e-06, 'epoch': 0.86}
{'loss': 0.3351, 'learning_rate': 4.96955236758509e-06, 'epoch': 0.86}
7:23, 10.74s/it] 86%| | 5566/6500 [16:49:38<2:45:28, 10.63s/it]                                                         86%| | 5566/6500 [16:49:38<2:45:28, 10.63s/it] 86%| | 5567/6500 [16:49:48<2:44:07, 10.55s/it]                                                         86%| | 5567/6500 [16:49:48<2:44:07, 10.55s/it] 86%| | 5568/6500 [16:49:58<2:43:06, 10.50s/it]                                                         86%| | 5568/6500 [16:49:58<2:43:06, 10.50s/it] 86%| | 5569/6500 [16:50:09<2:42:18, 10.46s/it]                                                         86%| | 5569/6500 [16:50:09<2:42:18, 10.46s/it] 86%| | 5570/6500 [16:50:19<2:41:44, 10.43s/it]                                                         86%| | 5570/6500 [16:50:19<2:41:44, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8707261085510254, 'eval_runtime': 3.9389, 'eval_samples_per_second': 5.839, 'eval_steps_per_second': 1.523, 'epoch': 0.86}
                                                         86%| | 5570/6500 [16:50:23<2:41:44, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3125, 'learning_rate': 4.959051079406202e-06, 'epoch': 0.86}
{'loss': 0.3282, 'learning_rate': 4.94856031927527e-06, 'epoch': 0.86}
{'loss': 0.3154, 'learning_rate': 4.9380800896444424e-06, 'epoch': 0.86}
{'loss': 0.3222, 'learning_rate': 4.927610392963428e-06, 'epoch': 0.86}
{'loss': 0.3034, 'learning_rate': 4.917151231679429e-06, 'epoch': 0.86}
 86%| | 5571/6500 [16:50:34<3:01:55, 11.75s/it]                                                         86%| | 5571/6500 [16:50:34<3:01:55, 11.75s/it] 86%| | 5572/6500 [16:50:44<2:55:22, 11.34s/it]                                                         86%| | 5572/6500 [16:50:44<2:55:22, 11.34s/it] 86%| | 5573/6500 [16:50:55<2:50:45, 11.05s/it]                                                         86%| | 5573/6500 [16:50:55<2:50:45, 11.05s/it] 86%| | 5574/6500 [16:51:05<2:47:27, 10.85s/it]                                                         86%| | 5574/6500 [16:51:05<2:47:27, 10.85s/it] 86%| | 5575/6500 [16:51:16<2:45:10, 10.71s/it]                                                         86%| | 5575/6500 [16:51:16<2:4{'loss': 0.3102, 'learning_rate': 4.9067026082372185e-06, 'epoch': 0.86}
{'loss': 0.2943, 'learning_rate': 4.896264525079109e-06, 'epoch': 0.86}
{'loss': 0.3832, 'learning_rate': 4.885836984644926e-06, 'epoch': 0.86}
{'loss': 0.3133, 'learning_rate': 4.8754199893720486e-06, 'epoch': 0.86}
{'loss': 0.3029, 'learning_rate': 4.865013541695384e-06, 'epoch': 0.86}
5:10, 10.71s/it] 86%| | 5576/6500 [16:51:27<2:47:41, 10.89s/it]                                                         86%| | 5576/6500 [16:51:27<2:47:41, 10.89s/it] 86%| | 5577/6500 [16:51:37<2:45:07, 10.73s/it]                                                         86%| | 5577/6500 [16:51:37<2:45:07, 10.73s/it] 86%| | 5578/6500 [16:51:48<2:43:18, 10.63s/it]                                                         86%| | 5578/6500 [16:51:48<2:43:18, 10.63s/it] 86%| | 5579/6500 [16:51:58<2:42:01, 10.55s/it]                                                         86%| | 5579/6500 [16:51:58<2:42:01, 10.55s/it] 86%| | 5580/6500 [16:52:08<2:41:00, 10.50s/it]                                                         86%| | 5580/6500 [16:52:08<2:41:00, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8686095476150513, 'eval_runtime': 3.9522, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.86}
                                                         86%| | 5580/6500 [16:52:12<2:41:00, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3244, 'learning_rate': 4.854617644047382e-06, 'epoch': 0.86}
{'loss': 0.8222, 'learning_rate': 4.84423229885802e-06, 'epoch': 0.86}
{'loss': 0.324, 'learning_rate': 4.833857508554807e-06, 'epoch': 0.86}
{'loss': 0.3139, 'learning_rate': 4.823493275562785e-06, 'epoch': 0.86}
{'loss': 0.3205, 'learning_rate': 4.81313960230454e-06, 'epoch': 0.86}
 86%| | 5581/6500 [16:52:23<3:00:59, 11.82s/it]                                                         86%| | 5581/6500 [16:52:23<3:00:59, 11.82s/it] 86%| | 5582/6500 [16:52:34<2:54:07, 11.38s/it]                                                         86%| | 5582/6500 [16:52:34<2:54:07, 11.38s/it] 86%| | 5583/6500 [16:52:44<2:49:26, 11.09s/it]                                                         86%| | 5583/6500 [16:52:44<2:49:26, 11.09s/it] 86%| | 5584/6500 [16:52:54<2:46:12, 10.89s/it]                                                         86%| | 5584/6500 [16:52:54<2:46:12, 10.89s/it] 86%| | 5585/6500 [16:53:05<2:43:42, 10.74s/it]                                                         86%| | 5585/6500 [16:53:05<2:4{'loss': 0.297, 'learning_rate': 4.8027964912001624e-06, 'epoch': 0.86}
{'loss': 0.3213, 'learning_rate': 4.792463944667303e-06, 'epoch': 0.86}
{'loss': 0.3132, 'learning_rate': 4.782141965121128e-06, 'epoch': 0.86}
{'loss': 0.2781, 'learning_rate': 4.771830554974344e-06, 'epoch': 0.86}
{'loss': 0.3148, 'learning_rate': 4.761529716637169e-06, 'epoch': 0.86}
3:42, 10.74s/it] 86%| | 5586/6500 [16:53:15<2:42:14, 10.65s/it]                                                         86%| | 5586/6500 [16:53:15<2:42:14, 10.65s/it] 86%| | 5587/6500 [16:53:26<2:41:47, 10.63s/it]                                                         86%| | 5587/6500 [16:53:26<2:41:47, 10.63s/it] 86%| | 5588/6500 [16:53:36<2:40:22, 10.55s/it]                                                         86%| | 5588/6500 [16:53:36<2:40:22, 10.55s/it] 86%| | 5589/6500 [16:53:47<2:39:26, 10.50s/it]                                                         86%| | 5589/6500 [16:53:47<2:39:26, 10.50s/it] 86%| | 5590/6500 [16:53:57<2:38:55, 10.48s/it]                                                         86%| | 5590/6500 [16:53:57<2:38:55, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8710824847221375, 'eval_runtime': 4.7268, 'eval_samples_per_second': 4.866, 'eval_steps_per_second': 1.269, 'epoch': 0.86}
                                                         86%| | 5590/6500 [16:54:02<2:38:55, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5590/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2984, 'learning_rate': 4.751239452517375e-06, 'epoch': 0.86}
{'loss': 0.3189, 'learning_rate': 4.740959765020242e-06, 'epoch': 0.86}
{'loss': 0.2999, 'learning_rate': 4.730690656548581e-06, 'epoch': 0.86}
{'loss': 0.3288, 'learning_rate': 4.720432129502738e-06, 'epoch': 0.86}
{'loss': 0.3044, 'learning_rate': 4.710184186280581e-06, 'epoch': 0.86}
 86%| | 5591/6500 [16:54:13<3:03:04, 12.08s/it]                                                         86%| | 5591/6500 [16:54:13<3:03:04, 12.08s/it] 86%| | 5592/6500 [16:54:24<2:56:47, 11.68s/it]                                                         86%| | 5592/6500 [16:54:24<2:56:47, 11.68s/it] 86%| | 5593/6500 [16:54:34<2:50:38, 11.29s/it]                                                         86%| | 5593/6500 [16:54:34<2:50:38, 11.29s/it] 86%| | 5594/6500 [16:54:44<2:46:22, 11.02s/it]                                                         86%| | 5594/6500 [16:54:44<2:46:22, 11.02s/it] 86%| | 5595/6500 [16:54:55<2:43:26, 10.84s/it]                                                         86%| | 5595/6500 [16:54:55<2:4{'loss': 0.3411, 'learning_rate': 4.699946829277513e-06, 'epoch': 0.86}
{'loss': 0.3242, 'learning_rate': 4.6897200608864374e-06, 'epoch': 0.86}
{'loss': 0.3249, 'learning_rate': 4.679503883497804e-06, 'epoch': 0.86}
{'loss': 0.3304, 'learning_rate': 4.669298299499586e-06, 'epoch': 0.86}
{'loss': 0.3188, 'learning_rate': 4.659103311277274e-06, 'epoch': 0.86}
3:26, 10.84s/it] 86%| | 5596/6500 [16:55:05<2:41:10, 10.70s/it]                                                         86%| | 5596/6500 [16:55:05<2:41:10, 10.70s/it] 86%| | 5597/6500 [16:55:16<2:41:03, 10.70s/it]                                                         86%| | 5597/6500 [16:55:16<2:41:03, 10.70s/it] 86%| | 5598/6500 [16:55:26<2:40:17, 10.66s/it]                                                         86%| | 5598/6500 [16:55:26<2:40:17, 10.66s/it] 86%| | 5599/6500 [16:55:37<2:38:46, 10.57s/it]                                                         86%| | 5599/6500 [16:55:37<2:38:46, 10.57s/it] 86%| | 5600/6500 [16:55:47<2:37:42, 10.51s/it]                                                         86%| | 5600/6500 [16:55:47<2:37:42, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8728221654891968, 'eval_runtime': 4.136, 'eval_samples_per_second': 5.561, 'eval_steps_per_second': 1.451, 'epoch': 0.86}
                                                         86%| | 5600/6500 [16:55:51<2:37:42, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3092, 'learning_rate': 4.648918921213885e-06, 'epoch': 0.86}
{'loss': 0.3164, 'learning_rate': 4.638745131689959e-06, 'epoch': 0.86}
{'loss': 0.3368, 'learning_rate': 4.62858194508356e-06, 'epoch': 0.86}
{'loss': 0.3131, 'learning_rate': 4.618429363770271e-06, 'epoch': 0.86}
{'loss': 0.3078, 'learning_rate': 4.60828739012319e-06, 'epoch': 0.86}
 86%| | 5601/6500 [16:56:02<2:58:07, 11.89s/it]                                                         86%| | 5601/6500 [16:56:02<2:58:07, 11.89s/it] 86%| | 5602/6500 [16:56:13<2:50:59, 11.43s/it]                                                         86%| | 5602/6500 [16:56:13<2:50:59, 11.43s/it] 86%| | 5603/6500 [16:56:23<2:46:00, 11.10s/it]                                                         86%| | 5603/6500 [16:56:23<2:46:00, 11.10s/it] 86%| | 5604/6500 [16:56:33<2:42:26, 10.88s/it]                                                         86%| | 5604/6500 [16:56:33<2:42:26, 10.88s/it] 86%| | 5605/6500 [16:56:44<2:39:54, 10.72s/it]                                                         86%| | 5605/6500 [16:56:44<2:3{'loss': 0.3079, 'learning_rate': 4.598156026512945e-06, 'epoch': 0.86}
{'loss': 0.332, 'learning_rate': 4.588035275307689e-06, 'epoch': 0.86}
{'loss': 0.3722, 'learning_rate': 4.5779251388730735e-06, 'epoch': 0.86}
{'loss': 0.311, 'learning_rate': 4.5678256195722804e-06, 'epoch': 0.86}
{'loss': 0.3099, 'learning_rate': 4.5577367197660205e-06, 'epoch': 0.86}
9:54, 10.72s/it] 86%| | 5606/6500 [16:56:54<2:38:04, 10.61s/it]                                                         86%| | 5606/6500 [16:56:54<2:38:04, 10.61s/it] 86%| | 5607/6500 [16:57:04<2:36:46, 10.53s/it]                                                         86%| | 5607/6500 [16:57:04<2:36:46, 10.53s/it] 86%| | 5608/6500 [16:57:15<2:36:46, 10.55s/it]                                                         86%| | 5608/6500 [16:57:15<2:36:46, 10.55s/it] 86%| | 5609/6500 [16:57:25<2:35:48, 10.49s/it]                                                         86%| | 5609/6500 [16:57:25<2:35:48, 10.49s/it] 86%| | 5610/6500 [16:57:36<2:35:06, 10.46s/it]                                                         86%| | 5610/6500 [16:57:36<2:35:06, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8690903782844543, 'eval_runtime': 3.959, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.86}
                                                         86%| | 5610/6500 [16:57:40<2:35:06, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3178, 'learning_rate': 4.547658441812508e-06, 'epoch': 0.86}
{'loss': 0.8155, 'learning_rate': 4.537590788067481e-06, 'epoch': 0.86}
{'loss': 0.3264, 'learning_rate': 4.52753376088419e-06, 'epoch': 0.86}
{'loss': 0.3208, 'learning_rate': 4.517487362613404e-06, 'epoch': 0.86}
{'loss': 0.3059, 'learning_rate': 4.507451595603412e-06, 'epoch': 0.86}
 86%| | 5611/6500 [16:57:51<2:54:27, 11.77s/it]                                                         86%| | 5611/6500 [16:57:51<2:54:27, 11.77s/it] 86%| | 5612/6500 [16:58:01<2:47:57, 11.35s/it]                                                         86%| | 5612/6500 [16:58:01<2:47:57, 11.35s/it] 86%| | 5613/6500 [16:58:11<2:43:23, 11.05s/it]                                                         86%| | 5613/6500 [16:58:11<2:43:23, 11.05s/it] 86%| | 5614/6500 [16:58:22<2:40:08, 10.84s/it]                                                         86%| | 5614/6500 [16:58:22<2:40:08, 10.84s/it] 86%| | 5615/6500 [16:58:32<2:37:52, 10.70s/it]                                                         86%| | 5615/6500 [16:58:32<2:3{'loss': 0.3068, 'learning_rate': 4.497426462200011e-06, 'epoch': 0.86}
{'loss': 0.334, 'learning_rate': 4.487411964746507e-06, 'epoch': 0.86}
{'loss': 0.3003, 'learning_rate': 4.477408105583741e-06, 'epoch': 0.86}
{'loss': 0.3085, 'learning_rate': 4.467414887050059e-06, 'epoch': 0.86}
{'loss': 0.3042, 'learning_rate': 4.457432311481291e-06, 'epoch': 0.86}
7:52, 10.70s/it] 86%| | 5616/6500 [16:58:42<2:36:13, 10.60s/it]                                                         86%| | 5616/6500 [16:58:42<2:36:13, 10.60s/it] 86%| | 5617/6500 [16:58:53<2:34:57, 10.53s/it]                                                         86%| | 5617/6500 [16:58:53<2:34:57, 10.53s/it] 86%| | 5618/6500 [16:59:03<2:33:58, 10.47s/it]                                                         86%| | 5618/6500 [16:59:03<2:33:58, 10.47s/it] 86%| | 5619/6500 [16:59:13<2:33:17, 10.44s/it]                                                         86%| | 5619/6500 [16:59:13<2:33:17, 10.44s/it] 86%| | 5620/6500 [16:59:24<2:32:43, 10.41s/it]                                                         86%| | 5620/6500 [16:59:24<2:32:43, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8702362775802612, 'eval_runtime': 4.2286, 'eval_samples_per_second': 5.439, 'eval_steps_per_second': 1.419, 'epoch': 0.86}
                                                         86%| | 5620/6500 [16:59:28<2:32:43, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5620/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3112, 'learning_rate': 4.447460381210822e-06, 'epoch': 0.86}
{'loss': 0.3125, 'learning_rate': 4.43749909856952e-06, 'epoch': 0.86}
{'loss': 0.307, 'learning_rate': 4.427548465885783e-06, 'epoch': 0.87}
{'loss': 0.3251, 'learning_rate': 4.417608485485502e-06, 'epoch': 0.87}
{'loss': 0.3127, 'learning_rate': 4.407679159692097e-06, 'epoch': 0.87}
 86%| | 5621/6500 [16:59:39<2:53:17, 11.83s/it]                                                         86%| | 5621/6500 [16:59:39<2:53:17, 11.83s/it] 86%| | 5622/6500 [16:59:49<2:46:40, 11.39s/it]                                                         86%| | 5622/6500 [16:59:49<2:46:40, 11.39s/it] 87%| | 5623/6500 [17:00:00<2:42:12, 11.10s/it]                                                         87%| | 5623/6500 [17:00:00<2:42:12, 11.10s/it] 87%| | 5624/6500 [17:00:10<2:40:08, 10.97s/it]                                                         87%| | 5624/6500 [17:00:10<2:40:08, 10.97s/it] 87%| | 5625/6500 [17:00:21<2:37:13, 10.78s/it]                                                         87%| | 5625/6500 [17:00:21<2:3{'loss': 0.3201, 'learning_rate': 4.397760490826481e-06, 'epoch': 0.87}
{'loss': 0.3068, 'learning_rate': 4.387852481207083e-06, 'epoch': 0.87}
{'loss': 0.3004, 'learning_rate': 4.377955133149841e-06, 'epoch': 0.87}
{'loss': 0.3172, 'learning_rate': 4.368068448968199e-06, 'epoch': 0.87}
{'loss': 0.3125, 'learning_rate': 4.358192430973124e-06, 'epoch': 0.87}
7:13, 10.78s/it] 87%| | 5626/6500 [17:00:31<2:35:05, 10.65s/it]                                                         87%| | 5626/6500 [17:00:31<2:35:05, 10.65s/it] 87%| | 5627/6500 [17:00:41<2:33:34, 10.56s/it]                                                         87%| | 5627/6500 [17:00:41<2:33:34, 10.56s/it] 87%| | 5628/6500 [17:00:52<2:32:31, 10.49s/it]                                                         87%| | 5628/6500 [17:00:52<2:32:31, 10.49s/it] 87%| | 5629/6500 [17:01:02<2:31:44, 10.45s/it]                                                         87%| | 5629/6500 [17:01:02<2:31:44, 10.45s/it] 87%| | 5630/6500 [17:01:12<2:31:08, 10.42s/it]                                                         87%| | 5630/6500 [17:01:12<2:31:08, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8714406490325928, 'eval_runtime': 3.9431, 'eval_samples_per_second': 5.833, 'eval_steps_per_second': 1.522, 'epoch': 0.87}
                                                         87%| | 5630/6500 [17:01:16<2:31:08, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3141, 'learning_rate': 4.348327081473047e-06, 'epoch': 0.87}
{'loss': 0.3126, 'learning_rate': 4.338472402773941e-06, 'epoch': 0.87}
{'loss': 0.3341, 'learning_rate': 4.3286283971792965e-06, 'epoch': 0.87}
{'loss': 0.3, 'learning_rate': 4.318795066990072e-06, 'epoch': 0.87}
{'loss': 0.3148, 'learning_rate': 4.308972414504759e-06, 'epoch': 0.87}
 87%| | 5631/6500 [17:01:27<2:50:06, 11.75s/it]                                                         87%| | 5631/6500 [17:01:27<2:50:06, 11.75s/it] 87%| | 5632/6500 [17:01:38<2:43:51, 11.33s/it]                                                         87%| | 5632/6500 [17:01:38<2:43:51, 11.33s/it] 87%| | 5633/6500 [17:01:48<2:39:31, 11.04s/it]                                                         87%| | 5633/6500 [17:01:48<2:39:31, 11.04s/it] 87%| | 5634/6500 [17:01:58<2:36:22, 10.83s/it]                                                         87%| | 5634/6500 [17:01:58<2:36:22, 10.83s/it] 87%| | 5635/6500 [17:02:09<2:34:06, 10.69s/it]                                                         87%| | 5635/6500 [17:02:09<2:3{'loss': 0.2914, 'learning_rate': 4.29916044201934e-06, 'epoch': 0.87}
{'loss': 0.343, 'learning_rate': 4.289359151827293e-06, 'epoch': 0.87}
{'loss': 0.3603, 'learning_rate': 4.279568546219625e-06, 'epoch': 0.87}
{'loss': 0.3038, 'learning_rate': 4.269788627484833e-06, 'epoch': 0.87}
{'loss': 0.31, 'learning_rate': 4.260019397908898e-06, 'epoch': 0.87}
4:06, 10.69s/it] 87%| | 5636/6500 [17:02:19<2:32:29, 10.59s/it]                                                         87%| | 5636/6500 [17:02:19<2:32:29, 10.59s/it] 87%| | 5637/6500 [17:02:29<2:31:19, 10.52s/it]                                                         87%| | 5637/6500 [17:02:29<2:31:19, 10.52s/it] 87%| | 5638/6500 [17:02:40<2:30:25, 10.47s/it]                                                         87%| | 5638/6500 [17:02:40<2:30:25, 10.47s/it] 87%| | 5639/6500 [17:02:50<2:29:48, 10.44s/it]                                                         87%| | 5639/6500 [17:02:50<2:29:48, 10.44s/it] 87%| | 5640/6500 [17:03:01<2:30:09, 10.48s/it]                                                         87%| | 5640/6500 [17:03:01<2:30:09, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8710086941719055, 'eval_runtime': 3.9506, 'eval_samples_per_second': 5.822, 'eval_steps_per_second': 1.519, 'epoch': 0.87}
                                                         87%| | 5640/6500 [17:03:05<2:30:09, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5640I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5640

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5640
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3214, 'learning_rate': 4.250260859775323e-06, 'epoch': 0.87}
{'loss': 0.821, 'learning_rate': 4.240513015365111e-06, 'epoch': 0.87}
{'loss': 0.3235, 'learning_rate': 4.230775866956754e-06, 'epoch': 0.87}
{'loss': 0.3327, 'learning_rate': 4.221049416826262e-06, 'epoch': 0.87}
{'loss': 0.2942, 'learning_rate': 4.2113336672471245e-06, 'epoch': 0.87}
 87%| | 5641/6500 [17:03:16<2:49:20, 11.83s/it]                                                         87%| | 5641/6500 [17:03:16<2:49:20, 11.83s/it] 87%| | 5642/6500 [17:03:26<2:42:55, 11.39s/it]                                                         87%| | 5642/6500 [17:03:26<2:42:55, 11.39s/it] 87%| | 5643/6500 [17:03:36<2:38:22, 11.09s/it]                                                         87%| | 5643/6500 [17:03:36<2:38:22, 11.09s/it] 87%| | 5644/6500 [17:03:47<2:35:13, 10.88s/it]                                                         87%| | 5644/6500 [17:03:47<2:35:13, 10.88s/it] 87%| | 5645/6500 [17:03:57<2:32:52, 10.73s/it]                                                         87%| | 5645/6500 [17:03:57<2:3{'loss': 0.3276, 'learning_rate': 4.20162862049035e-06, 'epoch': 0.87}
{'loss': 0.314, 'learning_rate': 4.191934278824417e-06, 'epoch': 0.87}
{'loss': 0.2918, 'learning_rate': 4.182250644515334e-06, 'epoch': 0.87}
{'loss': 0.312, 'learning_rate': 4.172577719826587e-06, 'epoch': 0.87}
{'loss': 0.3078, 'learning_rate': 4.162915507019172e-06, 'epoch': 0.87}
2:52, 10.73s/it] 87%| | 5646/6500 [17:04:08<2:31:15, 10.63s/it]                                                         87%| | 5646/6500 [17:04:08<2:31:15, 10.63s/it] 87%| | 5647/6500 [17:04:18<2:30:01, 10.55s/it]                                                         87%| | 5647/6500 [17:04:18<2:30:01, 10.55s/it] 87%| | 5648/6500 [17:04:28<2:29:03, 10.50s/it]                                                         87%| | 5648/6500 [17:04:28<2:29:03, 10.50s/it] 87%| | 5649/6500 [17:04:39<2:28:23, 10.46s/it]                                                         87%| | 5649/6500 [17:04:39<2:28:23, 10.46s/it] 87%| | 5650/6500 [17:04:49<2:27:49, 10.43s/it]                                                         87%| | 5650/6500 [17:04:49<2:27:49, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8712745308876038, 'eval_runtime': 4.3386, 'eval_samples_per_second': 5.301, 'eval_steps_per_second': 1.383, 'epoch': 0.87}
                                                         87%| | 5650/6500 [17:04:53<2:27:49, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5650
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5650/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3151, 'learning_rate': 4.153264008351549e-06, 'epoch': 0.87}
{'loss': 0.3124, 'learning_rate': 4.143623226079718e-06, 'epoch': 0.87}
{'loss': 0.3137, 'learning_rate': 4.1339931624571396e-06, 'epoch': 0.87}
{'loss': 0.3148, 'learning_rate': 4.124373819734795e-06, 'epoch': 0.87}
{'loss': 0.3093, 'learning_rate': 4.114765200161141e-06, 'epoch': 0.87}
 87%| | 5651/6500 [17:05:04<2:48:13, 11.89s/it]                                                         87%| | 5651/6500 [17:05:04<2:48:13, 11.89s/it] 87%| | 5652/6500 [17:05:15<2:41:35, 11.43s/it]                                                         87%| | 5652/6500 [17:05:15<2:41:35, 11.43s/it] 87%| | 5653/6500 [17:05:25<2:36:54, 11.12s/it]                                                         87%| | 5653/6500 [17:05:25<2:36:54, 11.12s/it] 87%| | 5654/6500 [17:05:35<2:33:38, 10.90s/it]                                                         87%| | 5654/6500 [17:05:35<2:33:38, 10.90s/it] 87%| | 5655/6500 [17:05:46<2:31:18, 10.74s/it]                                                         87%| | 5655/6500 [17:05:46<2:3{'loss': 0.3387, 'learning_rate': 4.1051673059821326e-06, 'epoch': 0.87}
{'loss': 0.3103, 'learning_rate': 4.095580139441219e-06, 'epoch': 0.87}
{'loss': 0.3262, 'learning_rate': 4.08600370277935e-06, 'epoch': 0.87}
{'loss': 0.3322, 'learning_rate': 4.07643799823495e-06, 'epoch': 0.87}
{'loss': 0.3057, 'learning_rate': 4.06688302804395e-06, 'epoch': 0.87}
1:18, 10.74s/it] 87%| | 5656/6500 [17:05:56<2:29:34, 10.63s/it]                                                         87%| | 5656/6500 [17:05:56<2:29:34, 10.63s/it] 87%| | 5657/6500 [17:06:07<2:29:54, 10.67s/it]                                                         87%| | 5657/6500 [17:06:07<2:29:54, 10.67s/it] 87%| | 5658/6500 [17:06:17<2:28:25, 10.58s/it]                                                         87%| | 5658/6500 [17:06:17<2:28:25, 10.58s/it] 87%| | 5659/6500 [17:06:28<2:27:27, 10.52s/it]                                                         87%| | 5659/6500 [17:06:28<2:27:27, 10.52s/it] 87%| | 5660/6500 [17:06:38<2:26:40, 10.48s/it]                                                         87%| | 5660/6500 [17:06:38<2:26:40, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8701888918876648, 'eval_runtime': 3.9685, 'eval_samples_per_second': 5.796, 'eval_steps_per_second': 1.512, 'epoch': 0.87}
                                                         87%| | 5660/6500 [17:06:42<2:26:40, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5660I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5660

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3196, 'learning_rate': 4.0573387944397744e-06, 'epoch': 0.87}
{'loss': 0.3216, 'learning_rate': 4.047805299653307e-06, 'epoch': 0.87}
{'loss': 0.3199, 'learning_rate': 4.038282545912958e-06, 'epoch': 0.87}
{'loss': 0.3022, 'learning_rate': 4.028770535444615e-06, 'epoch': 0.87}
{'loss': 0.3026, 'learning_rate': 4.019269270471649e-06, 'epoch': 0.87}
 87%| | 5661/6500 [17:06:53<2:44:54, 11.79s/it]                                                         87%| | 5661/6500 [17:06:53<2:44:54, 11.79s/it] 87%| | 5662/6500 [17:07:04<2:39:35, 11.43s/it]                                                         87%| | 5662/6500 [17:07:04<2:39:35, 11.43s/it] 87%| | 5663/6500 [17:07:14<2:35:00, 11.11s/it]                                                         87%| | 5663/6500 [17:07:14<2:35:00, 11.11s/it] 87%| | 5664/6500 [17:07:24<2:31:41, 10.89s/it]                                                         87%| | 5664/6500 [17:07:24<2:31:41, 10.89s/it] 87%| | 5665/6500 [17:07:35<2:29:19, 10.73s/it]                                                         87%| | 5665/6500 [17:07:35<2:2{'loss': 0.3016, 'learning_rate': 4.0097787532149215e-06, 'epoch': 0.87}
{'loss': 0.397, 'learning_rate': 4.000298985892787e-06, 'epoch': 0.87}
{'loss': 0.316, 'learning_rate': 3.9908299707210775e-06, 'epoch': 0.87}
{'loss': 0.3101, 'learning_rate': 3.981371709913123e-06, 'epoch': 0.87}
{'loss': 0.331, 'learning_rate': 3.971924205679739e-06, 'epoch': 0.87}
9:19, 10.73s/it] 87%| | 5666/6500 [17:07:45<2:27:47, 10.63s/it]                                                         87%| | 5666/6500 [17:07:45<2:27:47, 10.63s/it] 87%| | 5667/6500 [17:07:55<2:26:32, 10.56s/it]                                                         87%| | 5667/6500 [17:07:55<2:26:32, 10.56s/it] 87%| | 5668/6500 [17:08:06<2:25:36, 10.50s/it]                                                         87%| | 5668/6500 [17:08:06<2:25:36, 10.50s/it] 87%| | 5669/6500 [17:08:16<2:24:56, 10.47s/it]                                                         87%| | 5669/6500 [17:08:16<2:24:56, 10.47s/it] 87%| | 5670/6500 [17:08:27<2:24:28, 10.44s/it]                                                         87%| | 5670/6500 [17:08:27<2:24:28, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8690247535705566, 'eval_runtime': 3.9668, 'eval_samples_per_second': 5.798, 'eval_steps_per_second': 1.513, 'epoch': 0.87}
                                                         87%| | 5670/6500 [17:08:31<2:24:28, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8359, 'learning_rate': 3.962487460229214e-06, 'epoch': 0.87}
{'loss': 0.3264, 'learning_rate': 3.953061475767339e-06, 'epoch': 0.87}
{'loss': 0.3157, 'learning_rate': 3.9436462544973685e-06, 'epoch': 0.87}
{'loss': 0.3197, 'learning_rate': 3.934241798620058e-06, 'epoch': 0.87}
{'loss': 0.2941, 'learning_rate': 3.92484811033364e-06, 'epoch': 0.87}
 87%| | 5671/6500 [17:08:42<2:43:01, 11.80s/it]                                                         87%| | 5671/6500 [17:08:42<2:43:01, 11.80s/it] 87%| | 5672/6500 [17:08:52<2:36:58, 11.37s/it]                                                         87%| | 5672/6500 [17:08:52<2:36:58, 11.37s/it] 87%| | 5673/6500 [17:09:03<2:35:08, 11.26s/it]                                                         87%| | 5673/6500 [17:09:03<2:35:08, 11.26s/it] 87%| | 5674/6500 [17:09:13<2:31:17, 10.99s/it]                                                         87%| | 5674/6500 [17:09:13<2:31:17, 10.99s/it] 87%| | 5675/6500 [17:09:24<2:28:33, 10.80s/it]                                                         87%| | 5675/6500 [17:09:24<2:2{'loss': 0.3267, 'learning_rate': 3.915465191833833e-06, 'epoch': 0.87}
{'loss': 0.2972, 'learning_rate': 3.906093045313847e-06, 'epoch': 0.87}
{'loss': 0.2919, 'learning_rate': 3.896731672964349e-06, 'epoch': 0.87}
{'loss': 0.3043, 'learning_rate': 3.887381076973512e-06, 'epoch': 0.87}
{'loss': 0.3058, 'learning_rate': 3.878041259526982e-06, 'epoch': 0.87}
8:33, 10.80s/it] 87%| | 5676/6500 [17:09:34<2:26:37, 10.68s/it]                                                         87%| | 5676/6500 [17:09:34<2:26:37, 10.68s/it] 87%| | 5677/6500 [17:09:44<2:25:12, 10.59s/it]                                                         87%| | 5677/6500 [17:09:44<2:25:12, 10.59s/it] 87%| | 5678/6500 [17:09:55<2:24:13, 10.53s/it]                                                         87%| | 5678/6500 [17:09:55<2:24:13, 10.53s/it] 87%| | 5679/6500 [17:10:05<2:23:24, 10.48s/it]                                                         87%| | 5679/6500 [17:10:05<2:23:24, 10.48s/it] 87%| | 5680/6500 [17:10:16<2:22:49, 10.45s/it]                                                         87%| | 5680/6500 [17:10:16<2:22:49, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8697670102119446, 'eval_runtime': 3.965, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.87}
                                                         87%| | 5680/6500 [17:10:20<2:22:49, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5680/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5680/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3109, 'learning_rate': 3.86871222280788e-06, 'epoch': 0.87}
{'loss': 0.3012, 'learning_rate': 3.85939396899681e-06, 'epoch': 0.87}
{'loss': 0.3245, 'learning_rate': 3.850086500271871e-06, 'epoch': 0.87}
{'loss': 0.306, 'learning_rate': 3.840789818808605e-06, 'epoch': 0.87}
{'loss': 0.3188, 'learning_rate': 3.831503926780072e-06, 'epoch': 0.87}
 87%| | 5681/6500 [17:10:30<2:40:51, 11.78s/it]                                                         87%| | 5681/6500 [17:10:30<2:40:51, 11.78s/it] 87%| | 5682/6500 [17:10:41<2:34:55, 11.36s/it]                                                         87%| | 5682/6500 [17:10:41<2:34:55, 11.36s/it] 87%| | 5683/6500 [17:10:51<2:30:40, 11.07s/it]                                                         87%| | 5683/6500 [17:10:51<2:30:40, 11.07s/it] 87%| | 5684/6500 [17:11:02<2:27:38, 10.86s/it]                                                         87%| | 5684/6500 [17:11:02<2:27:38, 10.86s/it] 87%| | 5685/6500 [17:11:12<2:25:28, 10.71s/it]                                                         87%| | 5685/6500 [17:11:12<2:2{'loss': 0.3117, 'learning_rate': 3.822228826356783e-06, 'epoch': 0.87}
{'loss': 0.3061, 'learning_rate': 3.812964519706741e-06, 'epoch': 0.87}
{'loss': 0.3182, 'learning_rate': 3.80371100899542e-06, 'epoch': 0.88}
{'loss': 0.3103, 'learning_rate': 3.7944682963857727e-06, 'epoch': 0.88}
{'loss': 0.3099, 'learning_rate': 3.785236384038232e-06, 'epoch': 0.88}
5:28, 10.71s/it] 87%| | 5686/6500 [17:11:22<2:23:53, 10.61s/it]                                                         87%| | 5686/6500 [17:11:22<2:23:53, 10.61s/it] 87%| | 5687/6500 [17:11:33<2:22:43, 10.53s/it]                                                         87%| | 5687/6500 [17:11:33<2:22:43, 10.53s/it] 88%| | 5688/6500 [17:11:43<2:21:54, 10.49s/it]                                                         88%| | 5688/6500 [17:11:43<2:21:54, 10.49s/it] 88%| | 5689/6500 [17:11:54<2:22:22, 10.53s/it]                                                         88%| | 5689/6500 [17:11:54<2:22:22, 10.53s/it] 88%| | 5690/6500 [17:12:04<2:21:31, 10.48s/it]                                                         88%| | 5690/6500 [17:12:04<2:21:31, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8714766502380371, 'eval_runtime': 3.9616, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.515, 'epoch': 0.88}
                                                         88%| | 5690/6500 [17:12:08<2:21:31, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3037, 'learning_rate': 3.776015274110689e-06, 'epoch': 0.88}
{'loss': 0.341, 'learning_rate': 3.766804968758536e-06, 'epoch': 0.88}
{'loss': 0.3052, 'learning_rate': 3.757605470134612e-06, 'epoch': 0.88}
{'loss': 0.3064, 'learning_rate': 3.748416780389263e-06, 'epoch': 0.88}
{'loss': 0.2989, 'learning_rate': 3.7392389016702666e-06, 'epoch': 0.88}
 88%| | 5691/6500 [17:12:19<2:39:17, 11.81s/it]                                                         88%| | 5691/6500 [17:12:19<2:39:17, 11.81s/it] 88%| | 5692/6500 [17:12:29<2:33:13, 11.38s/it]                                                         88%| | 5692/6500 [17:12:29<2:33:13, 11.38s/it] 88%| | 5693/6500 [17:12:40<2:28:57, 11.08s/it]                                                         88%| | 5693/6500 [17:12:40<2:28:57, 11.08s/it] 88%| | 5694/6500 [17:12:50<2:25:58, 10.87s/it]                                                         88%| | 5694/6500 [17:12:50<2:25:58, 10.87s/it] 88%| | 5695/6500 [17:13:00<2:23:48, 10.72s/it]                                                         88%| | 5695/6500 [17:13:00<2:2{'loss': 0.3266, 'learning_rate': 3.7300718361229112e-06, 'epoch': 0.88}
{'loss': 0.3752, 'learning_rate': 3.7209155858899393e-06, 'epoch': 0.88}
{'loss': 0.305, 'learning_rate': 3.71177015311156e-06, 'epoch': 0.88}
{'loss': 0.3019, 'learning_rate': 3.702635539925475e-06, 'epoch': 0.88}
{'loss': 0.3171, 'learning_rate': 3.6935117484668436e-06, 'epoch': 0.88}
3:48, 10.72s/it] 88%| | 5696/6500 [17:13:11<2:22:17, 10.62s/it]                                                         88%| | 5696/6500 [17:13:11<2:22:17, 10.62s/it] 88%| | 5697/6500 [17:13:21<2:21:07, 10.55s/it]                                                         88%| | 5697/6500 [17:13:21<2:21:07, 10.55s/it] 88%| | 5698/6500 [17:13:32<2:20:13, 10.49s/it]                                                         88%| | 5698/6500 [17:13:32<2:20:13, 10.49s/it] 88%| | 5699/6500 [17:13:42<2:19:34, 10.46s/it]                                                         88%| | 5699/6500 [17:13:42<2:19:34, 10.46s/it] 88%| | 5700/6500 [17:13:52<2:19:03, 10.43s/it]                                                         88%| | 5700/6500 [17:13:52<2:19:03, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8702406883239746, 'eval_runtime': 3.9501, 'eval_samples_per_second': 5.823, 'eval_steps_per_second': 1.519, 'epoch': 0.88}
                                                         88%| | 5700/6500 [17:13:56<2:19:03, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8235, 'learning_rate': 3.6843987808682868e-06, 'epoch': 0.88}
{'loss': 0.3299, 'learning_rate': 3.675296639259912e-06, 'epoch': 0.88}
{'loss': 0.3113, 'learning_rate': 3.666205325769295e-06, 'epoch': 0.88}
{'loss': 0.3023, 'learning_rate': 3.657124842521464e-06, 'epoch': 0.88}
{'loss': 0.3051, 'learning_rate': 3.6480551916389327e-06, 'epoch': 0.88}
 88%| | 5701/6500 [17:14:07<2:36:18, 11.74s/it]                                                         88%| | 5701/6500 [17:14:07<2:36:18, 11.74s/it] 88%| | 5702/6500 [17:14:17<2:30:43, 11.33s/it]                                                         88%| | 5702/6500 [17:14:18<2:30:43, 11.33s/it] 88%| | 5703/6500 [17:14:28<2:26:43, 11.05s/it]                                                         88%| | 5703/6500 [17:14:28<2:26:43, 11.05s/it] 88%| | 5704/6500 [17:14:38<2:23:56, 10.85s/it]                                                         88%| | 5704/6500 [17:14:38<2:23:56, 10.85s/it] 88%| | 5705/6500 [17:14:49<2:23:14, 10.81s/it]                                                         88%| | 5705/6500 [17:14:49<2:2{'loss': 0.3283, 'learning_rate': 3.638996375241682e-06, 'epoch': 0.88}
{'loss': 0.3018, 'learning_rate': 3.6299483954471356e-06, 'epoch': 0.88}
{'loss': 0.303, 'learning_rate': 3.620911254370224e-06, 'epoch': 0.88}
{'loss': 0.3023, 'learning_rate': 3.6118849541233178e-06, 'epoch': 0.88}
{'loss': 0.3042, 'learning_rate': 3.602869496816258e-06, 'epoch': 0.88}
3:14, 10.81s/it] 88%| | 5706/6500 [17:14:59<2:21:22, 10.68s/it]                                                         88%| | 5706/6500 [17:14:59<2:21:22, 10.68s/it] 88%| | 5707/6500 [17:15:10<2:20:00, 10.59s/it]                                                         88%| | 5707/6500 [17:15:10<2:20:00, 10.59s/it] 88%| | 5708/6500 [17:15:20<2:19:00, 10.53s/it]                                                         88%| | 5708/6500 [17:15:20<2:19:00, 10.53s/it] 88%| | 5709/6500 [17:15:31<2:18:15, 10.49s/it]                                                         88%| | 5709/6500 [17:15:31<2:18:15, 10.49s/it] 88%| | 5710/6500 [17:15:41<2:17:41, 10.46s/it]                                                         88%| | 5710/6500 [17:15:41<2:17:41, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8707190155982971, 'eval_runtime': 4.1797, 'eval_samples_per_second': 5.503, 'eval_steps_per_second': 1.436, 'epoch': 0.88}
                                                         88%| | 5710/6500 [17:15:45<2:17:41, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5710
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.314, 'learning_rate': 3.5938648845563585e-06, 'epoch': 0.88}
{'loss': 0.3026, 'learning_rate': 3.584871119448385e-06, 'epoch': 0.88}
{'loss': 0.3242, 'learning_rate': 3.5758882035945795e-06, 'epoch': 0.88}
{'loss': 0.3115, 'learning_rate': 3.5669161390946503e-06, 'epoch': 0.88}
{'loss': 0.323, 'learning_rate': 3.557954928045748e-06, 'epoch': 0.88}
 88%| | 5711/6500 [17:15:56<2:35:52, 11.85s/it]                                                         88%| | 5711/6500 [17:15:56<2:35:52, 11.85s/it] 88%| | 5712/6500 [17:16:06<2:29:51, 11.41s/it]                                                         88%| | 5712/6500 [17:16:06<2:29:51, 11.41s/it] 88%| | 5713/6500 [17:16:17<2:25:37, 11.10s/it]                                                         88%| | 5713/6500 [17:16:17<2:25:37, 11.10s/it] 88%| | 5714/6500 [17:16:27<2:22:34, 10.88s/it]                                                         88%| | 5714/6500 [17:16:27<2:22:34, 10.88s/it] 88%| | 5715/6500 [17:16:38<2:20:19, 10.73s/it]                                                         88%| | 5715/6500 [17:16:38<2:2{'loss': 0.3131, 'learning_rate': 3.549004572542508e-06, 'epoch': 0.88}
{'loss': 0.3121, 'learning_rate': 3.5400650746770236e-06, 'epoch': 0.88}
{'loss': 0.3238, 'learning_rate': 3.5311364365388455e-06, 'epoch': 0.88}
{'loss': 0.3154, 'learning_rate': 3.5222186602149933e-06, 'epoch': 0.88}
{'loss': 0.316, 'learning_rate': 3.513311747789938e-06, 'epoch': 0.88}
0:19, 10.73s/it] 88%| | 5716/6500 [17:16:48<2:18:46, 10.62s/it]                                                         88%| | 5716/6500 [17:16:48<2:18:46, 10.62s/it] 88%| | 5717/6500 [17:16:58<2:17:35, 10.54s/it]                                                         88%| | 5717/6500 [17:16:58<2:17:35, 10.54s/it] 88%| | 5718/6500 [17:17:09<2:16:44, 10.49s/it]                                                         88%| | 5718/6500 [17:17:09<2:16:44, 10.49s/it] 88%| | 5719/6500 [17:17:19<2:16:06, 10.46s/it]                                                         88%| | 5719/6500 [17:17:19<2:16:06, 10.46s/it] 88%| | 5720/6500 [17:17:29<2:15:35, 10.43s/it]                                                         88%| | 5720/6500 [17:17:29<2:15:35, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8708106279373169, 'eval_runtime': 3.9465, 'eval_samples_per_second': 5.828, 'eval_steps_per_second': 1.52, 'epoch': 0.88}
                                                         88%| | 5720/6500 [17:17:33<2:15:35, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5720/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3095, 'learning_rate': 3.504415701345615e-06, 'epoch': 0.88}
{'loss': 0.3246, 'learning_rate': 3.4955305229614267e-06, 'epoch': 0.88}
{'loss': 0.2984, 'learning_rate': 3.486656214714229e-06, 'epoch': 0.88}
{'loss': 0.2996, 'learning_rate': 3.4777927786783347e-06, 'epoch': 0.88}
{'loss': 0.2898, 'learning_rate': 3.468940216925515e-06, 'epoch': 0.88}
 88%| | 5721/6500 [17:17:44<2:33:41, 11.84s/it]                                                         88%| | 5721/6500 [17:17:44<2:33:41, 11.84s/it] 88%| | 5722/6500 [17:17:55<2:27:47, 11.40s/it]                                                         88%| | 5722/6500 [17:17:55<2:27:47, 11.40s/it] 88%| | 5723/6500 [17:18:05<2:23:30, 11.08s/it]                                                         88%| | 5723/6500 [17:18:05<2:23:30, 11.08s/it] 88%| | 5724/6500 [17:18:16<2:20:25, 10.86s/it]                                                         88%| | 5724/6500 [17:18:16<2:20:25, 10.86s/it] 88%| | 5725/6500 [17:18:26<2:18:14, 10.70s/it]                                                         88%| | 5725/6500 [17:18:26<2:1{'loss': 0.3385, 'learning_rate': 3.460098531525019e-06, 'epoch': 0.88}
{'loss': 0.3528, 'learning_rate': 3.451267724543511e-06, 'epoch': 0.88}
{'loss': 0.2996, 'learning_rate': 3.442447798045151e-06, 'epoch': 0.88}
{'loss': 0.3219, 'learning_rate': 3.4336387540915505e-06, 'epoch': 0.88}
{'loss': 0.3562, 'learning_rate': 3.4248405947417572e-06, 'epoch': 0.88}
8:14, 10.70s/it] 88%| | 5726/6500 [17:18:36<2:16:41, 10.60s/it]                                                         88%| | 5726/6500 [17:18:36<2:16:41, 10.60s/it] 88%| | 5727/6500 [17:18:47<2:15:34, 10.52s/it]                                                         88%| | 5727/6500 [17:18:47<2:15:34, 10.52s/it] 88%| | 5728/6500 [17:18:57<2:14:52, 10.48s/it]                                                         88%| | 5728/6500 [17:18:57<2:14:52, 10.48s/it] 88%| | 5729/6500 [17:19:07<2:14:15, 10.45s/it]                                                         88%| | 5729/6500 [17:19:07<2:14:15, 10.45s/it] 88%| | 5730/6500 [17:19:18<2:13:40, 10.42s/it]                                                         88%| | 5730/6500 [17:19:18<2:13:40, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8706141710281372, 'eval_runtime': 3.9474, 'eval_samples_per_second': 5.827, 'eval_steps_per_second': 1.52, 'epoch': 0.88}
                                                         88%| | 5730/6500 [17:19:22<2:13:40, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5730
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7938, 'learning_rate': 3.416053322052293e-06, 'epoch': 0.88}
{'loss': 0.3115, 'learning_rate': 3.4072769380771363e-06, 'epoch': 0.88}
{'loss': 0.325, 'learning_rate': 3.3985114448677024e-06, 'epoch': 0.88}
{'loss': 0.294, 'learning_rate': 3.3897568444728746e-06, 'epoch': 0.88}
{'loss': 0.3201, 'learning_rate': 3.381013138938993e-06, 'epoch': 0.88}
 88%| | 5731/6500 [17:19:32<2:30:20, 11.73s/it]                                                         88%| | 5731/6500 [17:19:32<2:30:20, 11.73s/it] 88%| | 5732/6500 [17:19:43<2:24:56, 11.32s/it]                                                         88%| | 5732/6500 [17:19:43<2:24:56, 11.32s/it] 88%| | 5733/6500 [17:19:53<2:21:04, 11.04s/it]                                                         88%| | 5733/6500 [17:19:53<2:21:04, 11.04s/it] 88%| | 5734/6500 [17:20:04<2:18:19, 10.84s/it]                                                         88%| | 5734/6500 [17:20:04<2:18:19, 10.84s/it] 88%| | 5735/6500 [17:20:14<2:16:20, 10.69s/it]                                                         88%| | 5735/6500 [17:20:14<2:1{'loss': 0.3162, 'learning_rate': 3.3722803303098403e-06, 'epoch': 0.88}
{'loss': 0.2857, 'learning_rate': 3.363558420626667e-06, 'epoch': 0.88}
{'loss': 0.322, 'learning_rate': 3.3548474119281526e-06, 'epoch': 0.88}
{'loss': 0.3066, 'learning_rate': 3.346147306250447e-06, 'epoch': 0.88}
{'loss': 0.3155, 'learning_rate': 3.3374581056271447e-06, 'epoch': 0.88}
6:20, 10.69s/it] 88%| | 5736/6500 [17:20:24<2:14:53, 10.59s/it]                                                         88%| | 5736/6500 [17:20:24<2:14:53, 10.59s/it] 88%| | 5737/6500 [17:20:35<2:14:56, 10.61s/it]                                                         88%| | 5737/6500 [17:20:35<2:14:56, 10.61s/it] 88%| | 5738/6500 [17:20:45<2:13:52, 10.54s/it]                                                         88%| | 5738/6500 [17:20:45<2:13:52, 10.54s/it] 88%| | 5739/6500 [17:20:56<2:13:00, 10.49s/it]                                                         88%| | 5739/6500 [17:20:56<2:13:00, 10.49s/it] 88%| | 5740/6500 [17:21:06<2:12:21, 10.45s/it]                                                         88%| | 5740/6500 [17:21:06<2:12:21, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8728722333908081, 'eval_runtime': 3.9745, 'eval_samples_per_second': 5.787, 'eval_steps_per_second': 1.51, 'epoch': 0.88}
                                                         88%| | 5740/6500 [17:21:10<2:12:21, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3229, 'learning_rate': 3.3287798120893055e-06, 'epoch': 0.88}
{'loss': 0.3206, 'learning_rate': 3.3201124276654118e-06, 'epoch': 0.88}
{'loss': 0.3111, 'learning_rate': 3.311455954381426e-06, 'epoch': 0.88}
{'loss': 0.3161, 'learning_rate': 3.302810394260736e-06, 'epoch': 0.88}
{'loss': 0.335, 'learning_rate': 3.294175749324191e-06, 'epoch': 0.88}
 88%| | 5741/6500 [17:21:21<2:29:07, 11.79s/it]                                                         88%| | 5741/6500 [17:21:21<2:29:07, 11.79s/it] 88%| | 5742/6500 [17:21:31<2:23:24, 11.35s/it]                                                         88%| | 5742/6500 [17:21:31<2:23:24, 11.35s/it] 88%| | 5743/6500 [17:21:42<2:19:23, 11.05s/it]                                                         88%| | 5743/6500 [17:21:42<2:19:23, 11.05s/it] 88%| | 5744/6500 [17:21:52<2:16:40, 10.85s/it]                                                         88%| | 5744/6500 [17:21:52<2:16:40, 10.85s/it] 88%| | 5745/6500 [17:22:02<2:14:33, 10.69s/it]                                                         88%| | 5745/6500 [17:22:02<2:1{'loss': 0.3152, 'learning_rate': 3.285552021590094e-06, 'epoch': 0.88}
{'loss': 0.3315, 'learning_rate': 3.2769392130741816e-06, 'epoch': 0.88}
{'loss': 0.3254, 'learning_rate': 3.2683373257896497e-06, 'epoch': 0.88}
{'loss': 0.3141, 'learning_rate': 3.2597463617471346e-06, 'epoch': 0.88}
{'loss': 0.3159, 'learning_rate': 3.2511663229547183e-06, 'epoch': 0.88}
4:33, 10.69s/it] 88%| | 5746/6500 [17:22:13<2:13:02, 10.59s/it]                                                         88%| | 5746/6500 [17:22:13<2:13:02, 10.59s/it] 88%| | 5747/6500 [17:22:23<2:11:56, 10.51s/it]                                                         88%| | 5747/6500 [17:22:23<2:11:56, 10.51s/it] 88%| | 5748/6500 [17:22:33<2:11:05, 10.46s/it]                                                         88%| | 5748/6500 [17:22:33<2:11:05, 10.46s/it] 88%| | 5749/6500 [17:22:44<2:10:26, 10.42s/it]                                                         88%| | 5749/6500 [17:22:44<2:10:26, 10.42s/it] 88%| | 5750/6500 [17:22:54<2:10:03, 10.41s/it]                                                         88%| | 5750/6500 [17:22:54<2:10:03, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8719939589500427, 'eval_runtime': 4.1744, 'eval_samples_per_second': 5.51, 'eval_steps_per_second': 1.437, 'epoch': 0.88}
                                                         88%| | 5750/6500 [17:22:58<2:10:03, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5750
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3272, 'learning_rate': 3.242597211417936e-06, 'epoch': 0.88}
{'loss': 0.3188, 'learning_rate': 3.2340390291397684e-06, 'epoch': 0.88}
{'loss': 0.3082, 'learning_rate': 3.225491778120632e-06, 'epoch': 0.89}
{'loss': 0.2997, 'learning_rate': 3.2169554603584064e-06, 'epoch': 0.89}
{'loss': 0.311, 'learning_rate': 3.20843007784839e-06, 'epoch': 0.89}
 88%| | 5751/6500 [17:23:09<2:27:28, 11.81s/it]                                                         88%| | 5751/6500 [17:23:09<2:27:28, 11.81s/it] 88%| | 5752/6500 [17:23:20<2:21:48, 11.38s/it]                                                         88%| | 5752/6500 [17:23:20<2:21:48, 11.38s/it] 89%| | 5753/6500 [17:23:30<2:17:46, 11.07s/it]                                                         89%| | 5753/6500 [17:23:30<2:17:46, 11.07s/it] 89%| | 5754/6500 [17:23:40<2:15:48, 10.92s/it]                                                         89%| | 5754/6500 [17:23:40<2:15:48, 10.92s/it] 89%| | 5755/6500 [17:23:51<2:13:29, 10.75s/it]                                                         89%| | 5755/6500 [17:23:51<2:1{'loss': 0.3805, 'learning_rate': 3.1999156325833444e-06, 'epoch': 0.89}
{'loss': 0.3048, 'learning_rate': 3.1914121265534723e-06, 'epoch': 0.89}
{'loss': 0.2957, 'learning_rate': 3.182919561746417e-06, 'epoch': 0.89}
{'loss': 0.3271, 'learning_rate': 3.1744379401472677e-06, 'epoch': 0.89}
{'loss': 0.8259, 'learning_rate': 3.1659672637385397e-06, 'epoch': 0.89}
3:29, 10.75s/it] 89%| | 5756/6500 [17:24:01<2:11:58, 10.64s/it]                                                         89%| | 5756/6500 [17:24:01<2:11:58, 10.64s/it] 89%| | 5757/6500 [17:24:12<2:10:43, 10.56s/it]                                                         89%| | 5757/6500 [17:24:12<2:10:43, 10.56s/it] 89%| | 5758/6500 [17:24:22<2:09:55, 10.51s/it]                                                         89%| | 5758/6500 [17:24:22<2:09:55, 10.51s/it] 89%| | 5759/6500 [17:24:32<2:09:13, 10.46s/it]                                                         89%| | 5759/6500 [17:24:32<2:09:13, 10.46s/it] 89%| | 5760/6500 [17:24:43<2:08:33, 10.42s/it]                                                         89%| | 5760/6500 [17:24:43<2:08:33, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8705323934555054, 'eval_runtime': 3.9984, 'eval_samples_per_second': 5.752, 'eval_steps_per_second': 1.501, 'epoch': 0.89}
                                                         89%| | 5760/6500 [17:24:47<2:08:33, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3208, 'learning_rate': 3.1575075345002037e-06, 'epoch': 0.89}
{'loss': 0.3127, 'learning_rate': 3.1490587544096782e-06, 'epoch': 0.89}
{'loss': 0.328, 'learning_rate': 3.1406209254418116e-06, 'epoch': 0.89}
{'loss': 0.2847, 'learning_rate': 3.1321940495688874e-06, 'epoch': 0.89}
{'loss': 0.323, 'learning_rate': 3.1237781287606516e-06, 'epoch': 0.89}
 89%| | 5761/6500 [17:24:58<2:25:09, 11.79s/it]                                                         89%| | 5761/6500 [17:24:58<2:25:09, 11.79s/it] 89%| | 5762/6500 [17:25:08<2:19:42, 11.36s/it]                                                         89%| | 5762/6500 [17:25:08<2:19:42, 11.36s/it] 89%| | 5763/6500 [17:25:18<2:15:51, 11.06s/it]                                                         89%| | 5763/6500 [17:25:18<2:15:51, 11.06s/it] 89%| | 5764/6500 [17:25:29<2:13:04, 10.85s/it]                                                         89%| | 5764/6500 [17:25:29<2:13:04, 10.85s/it] 89%| | 5765/6500 [17:25:39<2:11:02, 10.70s/it]                                                         89%| | 5765/6500 [17:25:39<2:1{'loss': 0.2981, 'learning_rate': 3.115373164984259e-06, 'epoch': 0.89}
{'loss': 0.2917, 'learning_rate': 3.1069791602043317e-06, 'epoch': 0.89}
{'loss': 0.3082, 'learning_rate': 3.098596116382907e-06, 'epoch': 0.89}
{'loss': 0.3068, 'learning_rate': 3.0902240354794775e-06, 'epoch': 0.89}
{'loss': 0.3187, 'learning_rate': 3.081862919450973e-06, 'epoch': 0.89}
1:02, 10.70s/it] 89%| | 5766/6500 [17:25:49<2:09:37, 10.60s/it]                                                         89%| | 5766/6500 [17:25:49<2:09:37, 10.60s/it] 89%| | 5767/6500 [17:26:00<2:08:39, 10.53s/it]                                                         89%| | 5767/6500 [17:26:00<2:08:39, 10.53s/it] 89%| | 5768/6500 [17:26:10<2:07:49, 10.48s/it]                                                         89%| | 5768/6500 [17:26:10<2:07:49, 10.48s/it] 89%| | 5769/6500 [17:26:20<2:07:12, 10.44s/it]                                                         89%| | 5769/6500 [17:26:20<2:07:12, 10.44s/it] 89%| | 5770/6500 [17:26:31<2:08:07, 10.53s/it]                                                         89%| | 5770/6500 [17:26:31<2:08:07, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711436986923218, 'eval_runtime': 3.9566, 'eval_samples_per_second': 5.813, 'eval_steps_per_second': 1.516, 'epoch': 0.89}
                                                         89%| | 5770/6500 [17:26:35<2:08:07, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5770/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2993, 'learning_rate': 3.0735127702517296e-06, 'epoch': 0.89}
{'loss': 0.3348, 'learning_rate': 3.0651735898335644e-06, 'epoch': 0.89}
{'loss': 0.3096, 'learning_rate': 3.056845380145701e-06, 'epoch': 0.89}
{'loss': 0.3402, 'learning_rate': 3.0485281431348157e-06, 'epoch': 0.89}
{'loss': 0.3235, 'learning_rate': 3.0402218807450035e-06, 'epoch': 0.89}
 89%| | 5771/6500 [17:26:46<2:23:34, 11.82s/it]                                                         89%| | 5771/6500 [17:26:46<2:23:34, 11.82s/it] 89%| | 5772/6500 [17:26:56<2:18:04, 11.38s/it]                                                         89%| | 5772/6500 [17:26:56<2:18:04, 11.38s/it] 89%| | 5773/6500 [17:27:07<2:14:08, 11.07s/it]                                                         89%| | 5773/6500 [17:27:07<2:14:08, 11.07s/it] 89%| | 5774/6500 [17:27:17<2:11:18, 10.85s/it]                                                         89%| | 5774/6500 [17:27:17<2:11:18, 10.85s/it] 89%| | 5775/6500 [17:27:27<2:09:13, 10.69s/it]                                                         89%| | 5775/6500 [17:27:27<2:0{'loss': 0.3118, 'learning_rate': 3.0319265949178054e-06, 'epoch': 0.89}
{'loss': 0.3248, 'learning_rate': 3.0236422875921988e-06, 'epoch': 0.89}
{'loss': 0.3176, 'learning_rate': 3.0153689607045845e-06, 'epoch': 0.89}
{'loss': 0.3162, 'learning_rate': 3.007106616188804e-06, 'epoch': 0.89}
{'loss': 0.3207, 'learning_rate': 2.9988552559761294e-06, 'epoch': 0.89}
9:13, 10.69s/it] 89%| | 5776/6500 [17:27:38<2:07:42, 10.58s/it]                                                         89%| | 5776/6500 [17:27:38<2:07:42, 10.58s/it] 89%| | 5777/6500 [17:27:48<2:06:39, 10.51s/it]                                                         89%| | 5777/6500 [17:27:48<2:06:39, 10.51s/it] 89%| | 5778/6500 [17:27:58<2:05:51, 10.46s/it]                                                         89%| | 5778/6500 [17:27:58<2:05:51, 10.46s/it] 89%| | 5779/6500 [17:28:09<2:05:12, 10.42s/it]                                                         89%| | 5779/6500 [17:28:09<2:05:12, 10.42s/it] 89%| | 5780/6500 [17:28:19<2:04:42, 10.39s/it]                                                         89%| | 5780/6500 [17:28:19<2:04:42, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8724473118782043, 'eval_runtime': 4.1649, 'eval_samples_per_second': 5.522, 'eval_steps_per_second': 1.441, 'epoch': 0.89}
                                                         89%| | 5780/6500 [17:28:23<2:04:42, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5780
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3316, 'learning_rate': 2.9906148819952797e-06, 'epoch': 0.89}
{'loss': 0.294, 'learning_rate': 2.9823854961723686e-06, 'epoch': 0.89}
{'loss': 0.314, 'learning_rate': 2.9741671004309635e-06, 'epoch': 0.89}
{'loss': 0.3001, 'learning_rate': 2.9659596966920945e-06, 'epoch': 0.89}
{'loss': 0.3393, 'learning_rate': 2.9577632868741654e-06, 'epoch': 0.89}
 89%| | 5781/6500 [17:28:34<2:21:28, 11.81s/it]                                                         89%| | 5781/6500 [17:28:34<2:21:28, 11.81s/it] 89%| | 5782/6500 [17:28:45<2:15:59, 11.36s/it]                                                         89%| | 5782/6500 [17:28:45<2:15:59, 11.36s/it] 89%| | 5783/6500 [17:28:55<2:12:14, 11.07s/it]                                                         89%| | 5783/6500 [17:28:55<2:12:14, 11.07s/it] 89%| | 5784/6500 [17:29:05<2:09:22, 10.84s/it]                                                         89%| | 5784/6500 [17:29:05<2:09:22, 10.84s/it] 89%| | 5785/6500 [17:29:16<2:07:23, 10.69s/it]                                                         89%| | 5785/6500 [17:29:16<2:0{'loss': 0.3667, 'learning_rate': 2.94957787289305e-06, 'epoch': 0.89}
{'loss': 0.3082, 'learning_rate': 2.94140345666204e-06, 'epoch': 0.89}
{'loss': 0.3233, 'learning_rate': 2.9332400400918447e-06, 'epoch': 0.89}
{'loss': 0.2995, 'learning_rate': 2.9250876250906224e-06, 'epoch': 0.89}
{'loss': 0.8201, 'learning_rate': 2.9169462135639535e-06, 'epoch': 0.89}
7:23, 10.69s/it] 89%| | 5786/6500 [17:29:26<2:06:52, 10.66s/it]                                                         89%| | 5786/6500 [17:29:26<2:06:52, 10.66s/it] 89%| | 5787/6500 [17:29:36<2:05:30, 10.56s/it]                                                         89%| | 5787/6500 [17:29:36<2:05:30, 10.56s/it] 89%| | 5788/6500 [17:29:47<2:04:35, 10.50s/it]                                                         89%| | 5788/6500 [17:29:47<2:04:35, 10.50s/it] 89%| | 5789/6500 [17:29:57<2:04:12, 10.48s/it]                                                         89%| | 5789/6500 [17:29:57<2:04:12, 10.48s/it] 89%| | 5790/6500 [17:30:08<2:03:26, 10.43s/it]                                                         89%| | 5790/6500 [17:30:08<2:03:26, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8698105216026306, 'eval_runtime': 3.9556, 'eval_samples_per_second': 5.815, 'eval_steps_per_second': 1.517, 'epoch': 0.89}
                                                         89%| | 5790/6500 [17:30:12<2:03:26, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5790/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5790/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5790/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3242, 'learning_rate': 2.908815807414833e-06, 'epoch': 0.89}
{'loss': 0.3246, 'learning_rate': 2.900696408543696e-06, 'epoch': 0.89}
{'loss': 0.2995, 'learning_rate': 2.8925880188484077e-06, 'epoch': 0.89}
{'loss': 0.31, 'learning_rate': 2.8844906402242465e-06, 'epoch': 0.89}
{'loss': 0.3278, 'learning_rate': 2.8764042745639373e-06, 'epoch': 0.89}
 89%| | 5791/6500 [17:30:22<2:18:58, 11.76s/it]                                                         89%| | 5791/6500 [17:30:22<2:18:58, 11.76s/it] 89%| | 5792/6500 [17:30:33<2:13:46, 11.34s/it]                                                         89%| | 5792/6500 [17:30:33<2:13:46, 11.34s/it] 89%| | 5793/6500 [17:30:43<2:10:08, 11.04s/it]                                                         89%| | 5793/6500 [17:30:43<2:10:08, 11.04s/it] 89%| | 5794/6500 [17:30:54<2:07:34, 10.84s/it]                                                         89%| | 5794/6500 [17:30:54<2:07:34, 10.84s/it] 89%| | 5795/6500 [17:31:04<2:05:40, 10.70s/it]                                                         89%| | 5795/6500 [17:31:04<2:0{'loss': 0.2991, 'learning_rate': 2.868328923757607e-06, 'epoch': 0.89}
{'loss': 0.312, 'learning_rate': 2.8602645896928295e-06, 'epoch': 0.89}
{'loss': 0.3061, 'learning_rate': 2.85221127425459e-06, 'epoch': 0.89}
{'loss': 0.3105, 'learning_rate': 2.8441689793253013e-06, 'epoch': 0.89}
{'loss': 0.3166, 'learning_rate': 2.8361377067848027e-06, 'epoch': 0.89}
5:40, 10.70s/it] 89%| | 5796/6500 [17:31:14<2:04:23, 10.60s/it]                                                         89%| | 5796/6500 [17:31:14<2:04:23, 10.60s/it] 89%| | 5797/6500 [17:31:25<2:03:25, 10.53s/it]                                                         89%| | 5797/6500 [17:31:25<2:03:25, 10.53s/it] 89%| | 5798/6500 [17:31:35<2:02:40, 10.48s/it]                                                         89%| | 5798/6500 [17:31:35<2:02:40, 10.48s/it] 89%| | 5799/6500 [17:31:45<2:02:06, 10.45s/it]                                                         89%| | 5799/6500 [17:31:45<2:02:06, 10.45s/it] 89%| | 5800/6500 [17:31:56<2:01:38, 10.43s/it]                                                         89%| | 5800/6500 [17:31:56<2:01:38, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8709333539009094, 'eval_runtime': 3.9549, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.89}
                                                         89%| | 5800/6500 [17:32:00<2:01:38, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5800I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5800/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3066, 'learning_rate': 2.82811745851036e-06, 'epoch': 0.89}
{'loss': 0.3166, 'learning_rate': 2.820108236376645e-06, 'epoch': 0.89}
{'loss': 0.3135, 'learning_rate': 2.812110042255772e-06, 'epoch': 0.89}
{'loss': 0.3321, 'learning_rate': 2.8041228780172678e-06, 'epoch': 0.89}
{'loss': 0.2951, 'learning_rate': 2.7961467455280834e-06, 'epoch': 0.89}
 89%| | 5801/6500 [17:32:11<2:16:53, 11.75s/it]                                                         89%| | 5801/6500 [17:32:11<2:16:53, 11.75s/it] 89%| | 5802/6500 [17:32:21<2:13:04, 11.44s/it]                                                         89%| | 5802/6500 [17:32:21<2:13:04, 11.44s/it] 89%| | 5803/6500 [17:32:32<2:09:12, 11.12s/it]                                                         89%| | 5803/6500 [17:32:32<2:09:12, 11.12s/it] 89%| | 5804/6500 [17:32:42<2:06:20, 10.89s/it]                                                         89%| | 5804/6500 [17:32:42<2:06:20, 10.89s/it] 89%| | 5805/6500 [17:32:52<2:04:16, 10.73s/it]                                                         89%| | 5805/6500 [17:32:52<2:0{'loss': 0.3108, 'learning_rate': 2.7881816466525935e-06, 'epoch': 0.89}
{'loss': 0.3238, 'learning_rate': 2.7802275832525927e-06, 'epoch': 0.89}
{'loss': 0.3043, 'learning_rate': 2.7722845571872937e-06, 'epoch': 0.89}
{'loss': 0.3215, 'learning_rate': 2.7643525703133334e-06, 'epoch': 0.89}
{'loss': 0.3117, 'learning_rate': 2.7564316244847565e-06, 'epoch': 0.89}
4:16, 10.73s/it] 89%| | 5806/6500 [17:33:03<2:02:48, 10.62s/it]                                                         89%| | 5806/6500 [17:33:03<2:02:48, 10.62s/it] 89%| | 5807/6500 [17:33:13<2:01:42, 10.54s/it]                                                         89%| | 5807/6500 [17:33:13<2:01:42, 10.54s/it] 89%| | 5808/6500 [17:33:23<2:00:53, 10.48s/it]                                                         89%| | 5808/6500 [17:33:23<2:00:53, 10.48s/it] 89%| | 5809/6500 [17:33:34<2:00:17, 10.45s/it]                                                         89%| | 5809/6500 [17:33:34<2:00:17, 10.45s/it] 89%| | 5810/6500 [17:33:44<1:59:48, 10.42s/it]                                                         89%| | 5810/6500 [17:33:44<1:59:48, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8706273436546326, 'eval_runtime': 4.1832, 'eval_samples_per_second': 5.498, 'eval_steps_per_second': 1.434, 'epoch': 0.89}
                                                         89%| | 5810/6500 [17:33:48<1:59:48, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5810
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5810
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5810/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3186, 'learning_rate': 2.7485217215530434e-06, 'epoch': 0.89}
{'loss': 0.3099, 'learning_rate': 2.7406228633670872e-06, 'epoch': 0.89}
{'loss': 0.3052, 'learning_rate': 2.7327350517732e-06, 'epoch': 0.89}
{'loss': 0.2948, 'learning_rate': 2.7248582886151008e-06, 'epoch': 0.89}
{'loss': 0.3833, 'learning_rate': 2.7169925757339344e-06, 'epoch': 0.89}
 89%| | 5811/6500 [17:33:59<2:16:05, 11.85s/it]                                                         89%| | 5811/6500 [17:33:59<2:16:05, 11.85s/it] 89%| | 5812/6500 [17:34:10<2:10:44, 11.40s/it]                                                         89%| | 5812/6500 [17:34:10<2:10:44, 11.40s/it] 89%| | 5813/6500 [17:34:20<2:07:00, 11.09s/it]                                                         89%| | 5813/6500 [17:34:20<2:07:00, 11.09s/it] 89%| | 5814/6500 [17:34:30<2:04:12, 10.86s/it]                                                         89%| | 5814/6500 [17:34:30<2:04:12, 10.86s/it] 89%| | 5815/6500 [17:34:41<2:02:16, 10.71s/it]                                                         89%| | 5815/6500 [17:34:41<2:0{'loss': 0.3152, 'learning_rate': 2.7091379149682685e-06, 'epoch': 0.89}
{'loss': 0.2914, 'learning_rate': 2.701294308154084e-06, 'epoch': 0.89}
{'loss': 0.3178, 'learning_rate': 2.693461757124771e-06, 'epoch': 0.9}
{'loss': 0.8293, 'learning_rate': 2.685640263711148e-06, 'epoch': 0.9}
{'loss': 0.3211, 'learning_rate': 2.677829829741435e-06, 'epoch': 0.9}
2:16, 10.71s/it] 89%| | 5816/6500 [17:34:51<2:00:52, 10.60s/it]                                                         89%| | 5816/6500 [17:34:51<2:00:52, 10.60s/it] 89%| | 5817/6500 [17:35:01<1:59:52, 10.53s/it]                                                         89%| | 5817/6500 [17:35:01<1:59:52, 10.53s/it] 90%| | 5818/6500 [17:35:12<1:59:52, 10.55s/it]                                                         90%| | 5818/6500 [17:35:12<1:59:52, 10.55s/it] 90%| | 5819/6500 [17:35:22<1:59:00, 10.49s/it]                                                         90%| | 5819/6500 [17:35:22<1:59:00, 10.49s/it] 90%| | 5820/6500 [17:35:33<1:58:28, 10.45s/it]                                                         90%| | 5820/6500 [17:35:33<1:58:28, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8706632852554321, 'eval_runtime': 3.9581, 'eval_samples_per_second': 5.811, 'eval_steps_per_second': 1.516, 'epoch': 0.9}
                                                         90%| | 5820/6500 [17:35:37<1:58:28, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5820/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3279, 'learning_rate': 2.6700304570412726e-06, 'epoch': 0.9}
{'loss': 0.327, 'learning_rate': 2.6622421474337243e-06, 'epoch': 0.9}
{'loss': 0.2913, 'learning_rate': 2.6544649027392566e-06, 'epoch': 0.9}
{'loss': 0.3247, 'learning_rate': 2.646698724775759e-06, 'epoch': 0.9}
{'loss': 0.3158, 'learning_rate': 2.6389436153585132e-06, 'epoch': 0.9}
 90%| | 5821/6500 [17:35:48<2:13:11, 11.77s/it]                                                         90%| | 5821/6500 [17:35:48<2:13:11, 11.77s/it] 90%| | 5822/6500 [17:35:58<2:08:13, 11.35s/it]                                                         90%| | 5822/6500 [17:35:58<2:08:13, 11.35s/it] 90%| | 5823/6500 [17:36:08<2:04:42, 11.05s/it]                                                         90%| | 5823/6500 [17:36:08<2:04:42, 11.05s/it] 90%| | 5824/6500 [17:36:19<2:02:11, 10.85s/it]                                                         90%| | 5824/6500 [17:36:19<2:02:11, 10.85s/it] 90%| | 5825/6500 [17:36:29<2:00:18, 10.69s/it]                                                         90%| | 5825/6500 [17:36:29<2:0{'loss': 0.2866, 'learning_rate': 2.631199576300236e-06, 'epoch': 0.9}
{'loss': 0.3186, 'learning_rate': 2.623466609411052e-06, 'epoch': 0.9}
{'loss': 0.3032, 'learning_rate': 2.615744716498492e-06, 'epoch': 0.9}
{'loss': 0.3162, 'learning_rate': 2.608033899367507e-06, 'epoch': 0.9}
{'loss': 0.3053, 'learning_rate': 2.60033415982045e-06, 'epoch': 0.9}
0:18, 10.69s/it] 90%| | 5826/6500 [17:36:39<1:59:01, 10.60s/it]                                                         90%| | 5826/6500 [17:36:39<1:59:01, 10.60s/it] 90%| | 5827/6500 [17:36:50<1:58:06, 10.53s/it]                                                         90%| | 5827/6500 [17:36:50<1:58:06, 10.53s/it] 90%| | 5828/6500 [17:37:00<1:57:21, 10.48s/it]                                                         90%| | 5828/6500 [17:37:00<1:57:21, 10.48s/it] 90%| | 5829/6500 [17:37:11<1:56:48, 10.45s/it]                                                         90%| | 5829/6500 [17:37:11<1:56:48, 10.45s/it] 90%| | 5830/6500 [17:37:21<1:56:21, 10.42s/it]                                                         90%| | 5830/6500 [17:37:21<1:56:21, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8722056746482849, 'eval_runtime': 3.9514, 'eval_samples_per_second': 5.821, 'eval_steps_per_second': 1.518, 'epoch': 0.9}
                                                         90%| | 5830/6500 [17:37:25<1:56:21, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5830
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5830

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3132, 'learning_rate': 2.592645499657087e-06, 'epoch': 0.9}
{'loss': 0.3025, 'learning_rate': 2.584967920674597e-06, 'epoch': 0.9}
{'loss': 0.3328, 'learning_rate': 2.5773014246675665e-06, 'epoch': 0.9}
{'loss': 0.3236, 'learning_rate': 2.5696460134279955e-06, 'epoch': 0.9}
{'loss': 0.3191, 'learning_rate': 2.562001688745291e-06, 'epoch': 0.9}
 90%| | 5831/6500 [17:37:36<2:11:07, 11.76s/it]                                                         90%| | 5831/6500 [17:37:36<2:11:07, 11.76s/it] 90%| | 5832/6500 [17:37:46<2:06:15, 11.34s/it]                                                         90%| | 5832/6500 [17:37:46<2:06:15, 11.34s/it] 90%| | 5833/6500 [17:37:56<2:02:46, 11.04s/it]                                                         90%| | 5833/6500 [17:37:56<2:02:46, 11.04s/it] 90%| | 5834/6500 [17:38:07<2:01:38, 10.96s/it]                                                         90%| | 5834/6500 [17:38:07<2:01:38, 10.96s/it] 90%| | 5835/6500 [17:38:18<1:59:31, 10.78s/it]                                                         90%| | 5835/6500 [17:38:18<1:5{'loss': 0.3254, 'learning_rate': 2.554368452406258e-06, 'epoch': 0.9}
{'loss': 0.3188, 'learning_rate': 2.5467463061951303e-06, 'epoch': 0.9}
{'loss': 0.3179, 'learning_rate': 2.539135251893526e-06, 'epoch': 0.9}
{'loss': 0.3114, 'learning_rate': 2.531535291280496e-06, 'epoch': 0.9}
{'loss': 0.3242, 'learning_rate': 2.523946426132473e-06, 'epoch': 0.9}
9:31, 10.78s/it] 90%| | 5836/6500 [17:38:28<1:58:01, 10.66s/it]                                                         90%| | 5836/6500 [17:38:28<1:58:01, 10.66s/it] 90%| | 5837/6500 [17:38:38<1:56:51, 10.57s/it]                                                         90%| | 5837/6500 [17:38:38<1:56:51, 10.57s/it] 90%| | 5838/6500 [17:38:49<1:55:58, 10.51s/it]                                                         90%| | 5838/6500 [17:38:49<1:55:58, 10.51s/it] 90%| | 5839/6500 [17:38:59<1:55:19, 10.47s/it]                                                         90%| | 5839/6500 [17:38:59<1:55:19, 10.47s/it] 90%| | 5840/6500 [17:39:10<1:54:59, 10.45s/it]                                                         90%| | 5840/6500 [17:39:10<1:54:59, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8727798461914062, 'eval_runtime': 4.6115, 'eval_samples_per_second': 4.988, 'eval_steps_per_second': 1.301, 'epoch': 0.9}
                                                         90%| | 5840/6500 [17:39:14<1:54:59, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5840/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5840/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.316, 'learning_rate': 2.516368658223317e-06, 'epoch': 0.9}
{'loss': 0.3063, 'learning_rate': 2.508801989324283e-06, 'epoch': 0.9}
{'loss': 0.2981, 'learning_rate': 2.5012464212040287e-06, 'epoch': 0.9}
{'loss': 0.315, 'learning_rate': 2.49370195562863e-06, 'epoch': 0.9}
{'loss': 0.371, 'learning_rate': 2.486168594361554e-06, 'epoch': 0.9}
 90%| | 5841/6500 [17:39:25<2:11:30, 11.97s/it]                                                         90%| | 5841/6500 [17:39:25<2:11:30, 11.97s/it] 90%| | 5842/6500 [17:39:35<2:05:58, 11.49s/it]                                                         90%| | 5842/6500 [17:39:35<2:05:58, 11.49s/it] 90%| | 5843/6500 [17:39:46<2:02:03, 11.15s/it]                                                         90%| | 5843/6500 [17:39:46<2:02:03, 11.15s/it] 90%| | 5844/6500 [17:39:56<1:59:22, 10.92s/it]                                                         90%| | 5844/6500 [17:39:56<1:59:22, 10.92s/it] 90%| | 5845/6500 [17:40:06<1:57:21, 10.75s/it]                                                         90%| | 5845/6500 [17:40:06<1:5{'loss': 0.3241, 'learning_rate': 2.4786463391636874e-06, 'epoch': 0.9}
{'loss': 0.3059, 'learning_rate': 2.4711351917933e-06, 'epoch': 0.9}
{'loss': 0.3201, 'learning_rate': 2.4636351540060777e-06, 'epoch': 0.9}
{'loss': 0.8369, 'learning_rate': 2.456146227555117e-06, 'epoch': 0.9}
{'loss': 0.3281, 'learning_rate': 2.4486684141908966e-06, 'epoch': 0.9}
7:21, 10.75s/it] 90%| | 5846/6500 [17:40:17<1:55:56, 10.64s/it]                                                         90%| | 5846/6500 [17:40:17<1:55:56, 10.64s/it] 90%| | 5847/6500 [17:40:27<1:54:52, 10.56s/it]                                                         90%| | 5847/6500 [17:40:27<1:54:52, 10.56s/it] 90%| | 5848/6500 [17:40:38<1:54:05, 10.50s/it]                                                         90%| | 5848/6500 [17:40:38<1:54:05, 10.50s/it] 90%| | 5849/6500 [17:40:48<1:53:28, 10.46s/it]                                                         90%| | 5849/6500 [17:40:48<1:53:28, 10.46s/it] 90%| | 5850/6500 [17:40:58<1:52:58, 10.43s/it]                                                         90%| | 5850/6500 [17:40:58<1:52:58, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8715165853500366, 'eval_runtime': 3.9558, 'eval_samples_per_second': 5.814, 'eval_steps_per_second': 1.517, 'epoch': 0.9}
                                                         90%| | 5850/6500 [17:41:02<1:52:58, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3115, 'learning_rate': 2.441201715661323e-06, 'epoch': 0.9}
{'loss': 0.3049, 'learning_rate': 2.4337461337116894e-06, 'epoch': 0.9}
{'loss': 0.2997, 'learning_rate': 2.4263016700846854e-06, 'epoch': 0.9}
{'loss': 0.3284, 'learning_rate': 2.4188683265204127e-06, 'epoch': 0.9}
{'loss': 0.2957, 'learning_rate': 2.4114461047563706e-06, 'epoch': 0.9}
 90%| | 5851/6500 [17:41:14<2:08:17, 11.86s/it]                                                         90%| | 5851/6500 [17:41:14<2:08:17, 11.86s/it] 90%| | 5852/6500 [17:41:24<2:03:13, 11.41s/it]                                                         90%| | 5852/6500 [17:41:24<2:03:13, 11.41s/it] 90%| | 5853/6500 [17:41:34<1:59:34, 11.09s/it]                                                         90%| | 5853/6500 [17:41:34<1:59:34, 11.09s/it] 90%| | 5854/6500 [17:41:45<1:56:58, 10.86s/it]                                                         90%| | 5854/6500 [17:41:45<1:56:58, 10.86s/it] 90%| | 5855/6500 [17:41:55<1:55:06, 10.71s/it]                                                         90%| | 5855/6500 [17:41:55<1:5{'loss': 0.2995, 'learning_rate': 2.4040350065274554e-06, 'epoch': 0.9}
{'loss': 0.3009, 'learning_rate': 2.39663503356598e-06, 'epoch': 0.9}
{'loss': 0.3124, 'learning_rate': 2.389246187601618e-06, 'epoch': 0.9}
{'loss': 0.3074, 'learning_rate': 2.3818684703614923e-06, 'epoch': 0.9}
{'loss': 0.2978, 'learning_rate': 2.374501883570085e-06, 'epoch': 0.9}
5:06, 10.71s/it] 90%| | 5856/6500 [17:42:05<1:53:46, 10.60s/it]                                                         90%| | 5856/6500 [17:42:05<1:53:46, 10.60s/it] 90%| | 5857/6500 [17:42:16<1:52:46, 10.52s/it]                                                         90%| | 5857/6500 [17:42:16<1:52:46, 10.52s/it] 90%| | 5858/6500 [17:42:26<1:52:01, 10.47s/it]                                                         90%| | 5858/6500 [17:42:26<1:52:01, 10.47s/it] 90%| | 5859/6500 [17:42:36<1:51:27, 10.43s/it]                                                         90%| | 5859/6500 [17:42:36<1:51:27, 10.43s/it] 90%| | 5860/6500 [17:42:47<1:51:01, 10.41s/it]                                                         90%| | 5860/6500 [17:42:47<1:51:01, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8706232309341431, 'eval_runtime': 3.961, 'eval_samples_per_second': 5.807, 'eval_steps_per_second': 1.515, 'epoch': 0.9}
                                                         90%| | 5860/6500 [17:42:51<1:51:01, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3269, 'learning_rate': 2.3671464289492906e-06, 'epoch': 0.9}
{'loss': 0.3091, 'learning_rate': 2.359802108218412e-06, 'epoch': 0.9}
{'loss': 0.3199, 'learning_rate': 2.352468923094131e-06, 'epoch': 0.9}
{'loss': 0.3094, 'learning_rate': 2.345146875290538e-06, 'epoch': 0.9}
{'loss': 0.3038, 'learning_rate': 2.337835966519114e-06, 'epoch': 0.9}
 90%| | 5861/6500 [17:43:02<2:05:15, 11.76s/it]                                                         90%| | 5861/6500 [17:43:02<2:05:15, 11.76s/it] 90%| | 5862/6500 [17:43:12<2:00:34, 11.34s/it]                                                         90%| | 5862/6500 [17:43:12<2:00:34, 11.34s/it] 90%| | 5863/6500 [17:43:22<1:57:16, 11.05s/it]                                                         90%| | 5863/6500 [17:43:22<1:57:16, 11.05s/it] 90%| | 5864/6500 [17:43:33<1:54:54, 10.84s/it]                                                         90%| | 5864/6500 [17:43:33<1:54:54, 10.84s/it] 90%| | 5865/6500 [17:43:43<1:53:17, 10.70s/it]                                                         90%| | 5865/6500 [17:43:43<1:5{'loss': 0.3145, 'learning_rate': 2.330536198488753e-06, 'epoch': 0.9}
{'loss': 0.3112, 'learning_rate': 2.3232475729057122e-06, 'epoch': 0.9}
{'loss': 0.3138, 'learning_rate': 2.315970091473668e-06, 'epoch': 0.9}
{'loss': 0.3118, 'learning_rate': 2.3087037558936987e-06, 'epoch': 0.9}
{'loss': 0.3356, 'learning_rate': 2.3014485678642563e-06, 'epoch': 0.9}
3:17, 10.70s/it] 90%| | 5866/6500 [17:43:53<1:52:05, 10.61s/it]                                                         90%| | 5866/6500 [17:43:53<1:52:05, 10.61s/it] 90%| | 5867/6500 [17:44:04<1:51:58, 10.61s/it]                                                         90%| | 5867/6500 [17:44:04<1:51:58, 10.61s/it] 90%| | 5868/6500 [17:44:14<1:51:01, 10.54s/it]                                                         90%| | 5868/6500 [17:44:14<1:51:01, 10.54s/it] 90%| | 5869/6500 [17:44:25<1:50:20, 10.49s/it]                                                         90%| | 5869/6500 [17:44:25<1:50:20, 10.49s/it] 90%| | 5870/6500 [17:44:35<1:49:47, 10.46s/it]                                                         90%| | 5870/6500 [17:44:35<1:49:47, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.871134877204895, 'eval_runtime': 4.3032, 'eval_samples_per_second': 5.345, 'eval_steps_per_second': 1.394, 'epoch': 0.9}
                                                         90%| | 5870/6500 [17:44:39<1:49:47, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5870
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2967, 'learning_rate': 2.2942045290811955e-06, 'epoch': 0.9}
{'loss': 0.3111, 'learning_rate': 2.286971641237773e-06, 'epoch': 0.9}
{'loss': 0.2905, 'learning_rate': 2.2797499060246253e-06, 'epoch': 0.9}
{'loss': 0.3381, 'learning_rate': 2.2725393251297964e-06, 'epoch': 0.9}
{'loss': 0.3602, 'learning_rate': 2.2653399002387164e-06, 'epoch': 0.9}
 90%| | 5871/6500 [17:44:51<2:05:34, 11.98s/it]                                                         90%| | 5871/6500 [17:44:51<2:05:34, 11.98s/it] 90%| | 5872/6500 [17:45:01<2:00:26, 11.51s/it]                                                         90%| | 5872/6500 [17:45:01<2:00:26, 11.51s/it] 90%| | 5873/6500 [17:45:11<1:56:44, 11.17s/it]                                                         90%| | 5873/6500 [17:45:11<1:56:44, 11.17s/it] 90%| | 5874/6500 [17:45:22<1:54:05, 10.94s/it]                                                         90%| | 5874/6500 [17:45:22<1:54:05, 10.94s/it] 90%| | 5875/6500 [17:45:32<1:52:11, 10.77s/it]                                                         90%| | 5875/6500 [17:45:32<1:5{'loss': 0.3053, 'learning_rate': 2.2581516330342003e-06, 'epoch': 0.9}
{'loss': 0.3172, 'learning_rate': 2.2509745251964697e-06, 'epoch': 0.9}
{'loss': 0.301, 'learning_rate': 2.243808578403117e-06, 'epoch': 0.9}
{'loss': 0.8275, 'learning_rate': 2.236653794329152e-06, 'epoch': 0.9}
{'loss': 0.3109, 'learning_rate': 2.229510174646954e-06, 'epoch': 0.9}
2:11, 10.77s/it] 90%| | 5876/6500 [17:45:43<1:50:46, 10.65s/it]                                                         90%| | 5876/6500 [17:45:43<1:50:46, 10.65s/it] 90%| | 5877/6500 [17:45:53<1:49:44, 10.57s/it]                                                         90%| | 5877/6500 [17:45:53<1:49:44, 10.57s/it] 90%| | 5878/6500 [17:46:03<1:49:00, 10.51s/it]                                                         90%| | 5878/6500 [17:46:03<1:49:00, 10.51s/it] 90%| | 5879/6500 [17:46:14<1:48:21, 10.47s/it]                                                         90%| | 5879/6500 [17:46:14<1:48:21, 10.47s/it] 90%| | 5880/6500 [17:46:24<1:47:55, 10.44s/it]                                                         90%| | 5880/6500 [17:46:24<1:47:55, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8703508973121643, 'eval_runtime': 4.1772, 'eval_samples_per_second': 5.506, 'eval_steps_per_second': 1.436, 'epoch': 0.9}
                                                         90%| | 5880/6500 [17:46:28<1:47:55, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5880I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5880

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3205, 'learning_rate': 2.2223777210262975e-06, 'epoch': 0.9}
{'loss': 0.2971, 'learning_rate': 2.2152564351343664e-06, 'epoch': 0.9}
{'loss': 0.3184, 'learning_rate': 2.208146318635701e-06, 'epoch': 0.91}
{'loss': 0.3141, 'learning_rate': 2.2010473731922553e-06, 'epoch': 0.91}
{'loss': 0.292, 'learning_rate': 2.1939596004633635e-06, 'epoch': 0.91}
 90%| | 5881/6500 [17:46:39<2:02:19, 11.86s/it]                                                         90%| | 5881/6500 [17:46:39<2:02:19, 11.86s/it] 90%| | 5882/6500 [17:46:50<1:57:33, 11.41s/it]                                                         90%| | 5882/6500 [17:46:50<1:57:33, 11.41s/it] 91%| | 5883/6500 [17:47:00<1:55:29, 11.23s/it]                                                         91%| | 5883/6500 [17:47:00<1:55:29, 11.23s/it] 91%| | 5884/6500 [17:47:11<1:52:42, 10.98s/it]                                                         91%| | 5884/6500 [17:47:11<1:52:42, 10.98s/it] 91%| | 5885/6500 [17:47:21<1:50:39, 10.80s/it]                                                         91%| | 5885/6500 [17:47:21<1:5{'loss': 0.3053, 'learning_rate': 2.1868830021057497e-06, 'epoch': 0.91}
{'loss': 0.301, 'learning_rate': 2.1798175797735298e-06, 'epoch': 0.91}
{'loss': 0.3078, 'learning_rate': 2.1727633351182e-06, 'epoch': 0.91}
{'loss': 0.3149, 'learning_rate': 2.16572026978864e-06, 'epoch': 0.91}
{'loss': 0.3109, 'learning_rate': 2.1586883854311346e-06, 'epoch': 0.91}
0:39, 10.80s/it] 91%| | 5886/6500 [17:47:32<1:49:15, 10.68s/it]                                                         91%| | 5886/6500 [17:47:32<1:49:15, 10.68s/it] 91%| | 5887/6500 [17:47:42<1:48:08, 10.58s/it]                                                         91%| | 5887/6500 [17:47:42<1:48:08, 10.58s/it] 91%| | 5888/6500 [17:47:52<1:47:20, 10.52s/it]                                                         91%| | 5888/6500 [17:47:52<1:47:20, 10.52s/it] 91%| | 5889/6500 [17:48:03<1:46:42, 10.48s/it]                                                         91%| | 5889/6500 [17:48:03<1:46:42, 10.48s/it] 91%| | 5890/6500 [17:48:13<1:46:13, 10.45s/it]                                                         91%| | 5890/6500 [17:48:13<1:46:13, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718793392181396, 'eval_runtime': 3.9736, 'eval_samples_per_second': 5.788, 'eval_steps_per_second': 1.51, 'epoch': 0.91}
                                                         91%| | 5890/6500 [17:48:17<1:46:13, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5890
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5890/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3111, 'learning_rate': 2.1516676836893347e-06, 'epoch': 0.91}
{'loss': 0.3025, 'learning_rate': 2.1446581662042942e-06, 'epoch': 0.91}
{'loss': 0.3252, 'learning_rate': 2.1376598346144416e-06, 'epoch': 0.91}
{'loss': 0.2997, 'learning_rate': 2.1306726905555905e-06, 'epoch': 0.91}
{'loss': 0.3215, 'learning_rate': 2.1236967356609515e-06, 'epoch': 0.91}
 91%| | 5891/6500 [17:48:28<1:59:28, 11.77s/it]                                                         91%| | 5891/6500 [17:48:28<1:59:28, 11.77s/it] 91%| | 5892/6500 [17:48:38<1:55:02, 11.35s/it]                                                         91%| | 5892/6500 [17:48:38<1:55:02, 11.35s/it] 91%| | 5893/6500 [17:48:49<1:51:53, 11.06s/it]                                                         91%| | 5893/6500 [17:48:49<1:51:53, 11.06s/it] 91%| | 5894/6500 [17:48:59<1:49:36, 10.85s/it]                                                         91%| | 5894/6500 [17:48:59<1:49:36, 10.85s/it] 91%| | 5895/6500 [17:49:09<1:48:02, 10.72s/it]                                                         91%| | 5895/6500 [17:49:09<1:4{'loss': 0.3329, 'learning_rate': 2.116731971561109e-06, 'epoch': 0.91}
{'loss': 0.3016, 'learning_rate': 2.1097783998840324e-06, 'epoch': 0.91}
{'loss': 0.3137, 'learning_rate': 2.102836022255078e-06, 'epoch': 0.91}
{'loss': 0.3259, 'learning_rate': 2.0959048402969807e-06, 'epoch': 0.91}
{'loss': 0.3127, 'learning_rate': 2.088984855629872e-06, 'epoch': 0.91}
8:02, 10.72s/it] 91%| | 5896/6500 [17:49:20<1:46:50, 10.61s/it]                                                         91%| | 5896/6500 [17:49:20<1:46:50, 10.61s/it] 91%| | 5897/6500 [17:49:30<1:45:56, 10.54s/it]                                                         91%| | 5897/6500 [17:49:30<1:45:56, 10.54s/it] 91%| | 5898/6500 [17:49:41<1:45:17, 10.49s/it]                                                         91%| | 5898/6500 [17:49:41<1:45:17, 10.49s/it] 91%| | 5899/6500 [17:49:51<1:45:32, 10.54s/it]                                                         91%| | 5899/6500 [17:49:51<1:45:32, 10.54s/it] 91%| | 5900/6500 [17:50:02<1:44:56, 10.49s/it]                                                         91%| | 5900/6500 [17:50:02<1:44:56, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8722341060638428, 'eval_runtime': 4.5333, 'eval_samples_per_second': 5.074, 'eval_steps_per_second': 1.324, 'epoch': 0.91}
                                                         91%| | 5900/6500 [17:50:06<1:44:56, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2985, 'learning_rate': 2.0820760698712473e-06, 'epoch': 0.91}
{'loss': 0.2998, 'learning_rate': 2.0751784846359925e-06, 'epoch': 0.91}
{'loss': 0.2897, 'learning_rate': 2.0682921015363787e-06, 'epoch': 0.91}
{'loss': 0.3888, 'learning_rate': 2.061416922182058e-06, 'epoch': 0.91}
{'loss': 0.3079, 'learning_rate': 2.0545529481800608e-06, 'epoch': 0.91}
 91%| | 5901/6500 [17:50:17<1:59:36, 11.98s/it]                                                         91%| | 5901/6500 [17:50:17<1:59:36, 11.98s/it] 91%| | 5902/6500 [17:50:27<1:54:36, 11.50s/it]                                                         91%| | 5902/6500 [17:50:27<1:54:36, 11.50s/it] 91%| | 5903/6500 [17:50:38<1:51:05, 11.17s/it]                                                         91%| | 5903/6500 [17:50:38<1:51:05, 11.17s/it] 91%| | 5904/6500 [17:50:48<1:48:31, 10.93s/it]                                                         91%| | 5904/6500 [17:50:48<1:48:31, 10.93s/it] 91%| | 5905/6500 [17:50:59<1:46:41, 10.76s/it]                                                         91%| | 5905/6500 [17:50:59<1:4{'loss': 0.2974, 'learning_rate': 2.0477001811347985e-06, 'epoch': 0.91}
{'loss': 0.3234, 'learning_rate': 2.0408586226480618e-06, 'epoch': 0.91}
{'loss': 0.8275, 'learning_rate': 2.0340282743190275e-06, 'epoch': 0.91}
{'loss': 0.3181, 'learning_rate': 2.0272091377442458e-06, 'epoch': 0.91}
{'loss': 0.3109, 'learning_rate': 2.020401214517648e-06, 'epoch': 0.91}
6:41, 10.76s/it] 91%| | 5906/6500 [17:51:09<1:45:22, 10.64s/it]                                                         91%| | 5906/6500 [17:51:09<1:45:22, 10.64s/it] 91%| | 5907/6500 [17:51:19<1:44:24, 10.56s/it]                                                         91%| | 5907/6500 [17:51:19<1:44:24, 10.56s/it] 91%| | 5908/6500 [17:51:30<1:43:37, 10.50s/it]                                                         91%| | 5908/6500 [17:51:30<1:43:37, 10.50s/it] 91%| | 5909/6500 [17:51:42<1:49:44, 11.14s/it]                                                         91%| | 5909/6500 [17:51:42<1:49:44, 11.14s/it] 91%| | 5910/6500 [17:51:53<1:47:40, 10.95s/it]                                                         91%| | 5910/6500 [17:51:53<1:47:40, 10.95s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.872092604637146, 'eval_runtime': 4.1048, 'eval_samples_per_second': 5.603, 'eval_steps_per_second': 1.462, 'epoch': 0.91}
                                                         91%| | 5910/6500 [17:51:57<1:47:40, 10.95s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5910
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5910

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5910
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5910/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3224, 'learning_rate': 2.013604506230554e-06, 'epoch': 0.91}
{'loss': 0.297, 'learning_rate': 2.006819014471639e-06, 'epoch': 0.91}
{'loss': 0.3281, 'learning_rate': 2.000044740826973e-06, 'epoch': 0.91}
{'loss': 0.2969, 'learning_rate': 1.9932816868800053e-06, 'epoch': 0.91}
{'loss': 0.2935, 'learning_rate': 1.986529854211555e-06, 'epoch': 0.91}
 91%| | 5911/6500 [17:52:08<1:59:24, 12.16s/it]                                                         91%| | 5911/6500 [17:52:08<1:59:24, 12.16s/it] 91%| | 5912/6500 [17:52:18<1:53:51, 11.62s/it]                                                         91%| | 5912/6500 [17:52:18<1:53:51, 11.62s/it] 91%| | 5913/6500 [17:52:29<1:49:55, 11.24s/it]                                                         91%| | 5913/6500 [17:52:29<1:49:55, 11.24s/it] 91%| | 5914/6500 [17:52:39<1:47:04, 10.96s/it]                                                         91%| | 5914/6500 [17:52:39<1:47:04, 10.96s/it] 91%| | 5915/6500 [17:52:50<1:46:17, 10.90s/it]                                                         91%| | 5915/6500 [17:52:50<1:4{'loss': 0.3103, 'learning_rate': 1.979789244399832e-06, 'epoch': 0.91}
{'loss': 0.3153, 'learning_rate': 1.9730598590203986e-06, 'epoch': 0.91}
{'loss': 0.3234, 'learning_rate': 1.9663416996462183e-06, 'epoch': 0.91}
{'loss': 0.3145, 'learning_rate': 1.959634767847612e-06, 'epoch': 0.91}
{'loss': 0.3291, 'learning_rate': 1.952939065192294e-06, 'epoch': 0.91}
6:17, 10.90s/it] 91%| | 5916/6500 [17:53:00<1:44:28, 10.73s/it]                                                         91%| | 5916/6500 [17:53:00<1:44:28, 10.73s/it] 91%| | 5917/6500 [17:53:10<1:43:14, 10.62s/it]                                                         91%| | 5917/6500 [17:53:10<1:43:14, 10.62s/it] 91%| | 5918/6500 [17:53:21<1:42:16, 10.54s/it]                                                         91%| | 5918/6500 [17:53:21<1:42:16, 10.54s/it] 91%| | 5919/6500 [17:53:31<1:42:08, 10.55s/it]                                                         91%| | 5919/6500 [17:53:31<1:42:08, 10.55s/it] 91%| | 5920/6500 [17:53:42<1:41:28, 10.50s/it]                                                         91%| | 5920/6500 [17:53:42<1:41:28, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8715366125106812, 'eval_runtime': 3.9564, 'eval_samples_per_second': 5.813, 'eval_steps_per_second': 1.517, 'epoch': 0.91}
                                                         91%| | 5920/6500 [17:53:46<1:41:28, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5920
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3037, 'learning_rate': 1.9462545932453336e-06, 'epoch': 0.91}
{'loss': 0.3288, 'learning_rate': 1.9395813535691977e-06, 'epoch': 0.91}
{'loss': 0.3178, 'learning_rate': 1.932919347723705e-06, 'epoch': 0.91}
{'loss': 0.3135, 'learning_rate': 1.9262685772660606e-06, 'epoch': 0.91}
{'loss': 0.3208, 'learning_rate': 1.9196290437508424e-06, 'epoch': 0.91}
 91%| | 5921/6500 [17:53:56<1:53:56, 11.81s/it]                                                         91%| | 5921/6500 [17:53:56<1:53:56, 11.81s/it] 91%| | 5922/6500 [17:54:07<1:49:30, 11.37s/it]                                                         91%| | 5922/6500 [17:54:07<1:49:30, 11.37s/it] 91%| | 5923/6500 [17:54:17<1:46:20, 11.06s/it]                                                         91%| | 5923/6500 [17:54:17<1:46:20, 11.06s/it] 91%| | 5924/6500 [17:54:28<1:44:06, 10.84s/it]                                                         91%| | 5924/6500 [17:54:28<1:44:06, 10.84s/it] 91%| | 5925/6500 [17:54:38<1:42:30, 10.70s/it]                                                         91%| | 5925/6500 [17:54:38<1:4{'loss': 0.3243, 'learning_rate': 1.913000748730004e-06, 'epoch': 0.91}
{'loss': 0.3187, 'learning_rate': 1.9063836937528667e-06, 'epoch': 0.91}
{'loss': 0.3013, 'learning_rate': 1.899777880366127e-06, 'epoch': 0.91}
{'loss': 0.3327, 'learning_rate': 1.8931833101138497e-06, 'epoch': 0.91}
{'loss': 0.3056, 'learning_rate': 1.8865999845374793e-06, 'epoch': 0.91}
2:30, 10.70s/it] 91%| | 5926/6500 [17:54:48<1:41:24, 10.60s/it]                                                         91%| | 5926/6500 [17:54:48<1:41:24, 10.60s/it] 91%| | 5927/6500 [17:54:59<1:40:32, 10.53s/it]                                                         91%| | 5927/6500 [17:54:59<1:40:32, 10.53s/it] 91%| | 5928/6500 [17:55:09<1:39:54, 10.48s/it]                                                         91%| | 5928/6500 [17:55:09<1:39:54, 10.48s/it] 91%| | 5929/6500 [17:55:19<1:39:26, 10.45s/it]                                                         91%| | 5929/6500 [17:55:19<1:39:26, 10.45s/it] 91%| | 5930/6500 [17:55:30<1:39:03, 10.43s/it]                                                         91%| | 5930/6500 [17:55:30<1:39:03, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8706184029579163, 'eval_runtime': 3.9662, 'eval_samples_per_second': 5.799, 'eval_steps_per_second': 1.513, 'epoch': 0.91}
                                                         91%| | 5930/6500 [17:55:34<1:39:03, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5930
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3053, 'learning_rate': 1.8800279051758353e-06, 'epoch': 0.91}
{'loss': 0.2967, 'learning_rate': 1.8734670735650883e-06, 'epoch': 0.91}
{'loss': 0.3216, 'learning_rate': 1.8669174912388066e-06, 'epoch': 0.91}
{'loss': 0.3739, 'learning_rate': 1.860379159727893e-06, 'epoch': 0.91}
{'loss': 0.2976, 'learning_rate': 1.853852080560664e-06, 'epoch': 0.91}
 91%| | 5931/6500 [17:55:46<1:54:11, 12.04s/it]                                                         91%| | 5931/6500 [17:55:46<1:54:11, 12.04s/it] 91%|| 5932/6500 [17:55:56<1:49:11, 11.53s/it]                                                         91%|| 5932/6500 [17:55:56<1:49:11, 11.53s/it] 91%|| 5933/6500 [17:56:06<1:45:36, 11.18s/it]                                                         91%|| 5933/6500 [17:56:06<1:45:36, 11.18s/it] 91%|| 5934/6500 [17:56:17<1:43:01, 10.92s/it]                                                         91%|| 5934/6500 [17:56:17<1:43:01, 10.92s/it] 91%|| 5935/6500 [17:56:27<1:41:15, 10.75s/it]                                                         91%|| 5935/65{'loss': 0.3071, 'learning_rate': 1.8473362552627826e-06, 'epoch': 0.91}
{'loss': 0.3172, 'learning_rate': 1.840831685357275e-06, 'epoch': 0.91}
{'loss': 0.8241, 'learning_rate': 1.834338372364547e-06, 'epoch': 0.91}
{'loss': 0.3312, 'learning_rate': 1.8278563178023733e-06, 'epoch': 0.91}
{'loss': 0.3097, 'learning_rate': 1.8213855231858923e-06, 'epoch': 0.91}
00 [17:56:27<1:41:15, 10.75s/it] 91%|| 5936/6500 [17:56:37<1:39:57, 10.63s/it]                                                         91%|| 5936/6500 [17:56:37<1:39:57, 10.63s/it] 91%|| 5937/6500 [17:56:48<1:38:55, 10.54s/it]                                                         91%|| 5937/6500 [17:56:48<1:38:55, 10.54s/it] 91%|| 5938/6500 [17:56:58<1:38:12, 10.49s/it]                                                         91%|| 5938/6500 [17:56:58<1:38:12, 10.49s/it] 91%|| 5939/6500 [17:57:08<1:37:38, 10.44s/it]                                                         91%|| 5939/6500 [17:57:08<1:37:38, 10.44s/it] 91%|| 5940/6500 [17:57:19<1:37:13, 10.42s/it]                                                         91%|| 5940/6500 [17:57:19<1:37:13, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8712989091873169, 'eval_runtime': 3.9692, 'eval_samples_per_second': 5.795, 'eval_steps_per_second': 1.512, 'epoch': 0.91}
                                                         91%|| 5940/6500 [17:57:23<1:37:13, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5940
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3041, 'learning_rate': 1.8149259900276216e-06, 'epoch': 0.91}
{'loss': 0.2977, 'learning_rate': 1.8084777198374315e-06, 'epoch': 0.91}
{'loss': 0.3259, 'learning_rate': 1.8020407141225548e-06, 'epoch': 0.91}
{'loss': 0.2947, 'learning_rate': 1.7956149743876217e-06, 'epoch': 0.91}
{'loss': 0.3014, 'learning_rate': 1.7892005021345915e-06, 'epoch': 0.91}
 91%|| 5941/6500 [17:57:34<1:49:40, 11.77s/it]                                                         91%|| 5941/6500 [17:57:34<1:49:40, 11.77s/it] 91%|| 5942/6500 [17:57:44<1:45:32, 11.35s/it]                                                         91%|| 5942/6500 [17:57:44<1:45:32, 11.35s/it] 91%|| 5943/6500 [17:57:54<1:42:35, 11.05s/it]                                                         91%|| 5943/6500 [17:57:54<1:42:35, 11.05s/it] 91%|| 5944/6500 [17:58:05<1:40:27, 10.84s/it]                                                         91%|| 5944/6500 [17:58:05<1:40:27, 10.84s/it] 91%|| 5945/6500 [17:58:15<1:38:57, 10.70s/it]                                                         91%|| 594{'loss': 0.3015, 'learning_rate': 1.7827972988628261e-06, 'epoch': 0.91}
{'loss': 0.3066, 'learning_rate': 1.7764053660690228e-06, 'epoch': 0.91}
{'loss': 0.3123, 'learning_rate': 1.7700247052472586e-06, 'epoch': 0.92}
{'loss': 0.3055, 'learning_rate': 1.7636553178889792e-06, 'epoch': 0.92}
{'loss': 0.3314, 'learning_rate': 1.7572972054829884e-06, 'epoch': 0.92}
5/6500 [17:58:15<1:38:57, 10.70s/it] 91%|| 5946/6500 [17:58:25<1:37:51, 10.60s/it]                                                         91%|| 5946/6500 [17:58:25<1:37:51, 10.60s/it] 91%|| 5947/6500 [17:58:36<1:36:59, 10.52s/it]                                                         91%|| 5947/6500 [17:58:36<1:36:59, 10.52s/it] 92%|| 5948/6500 [17:58:46<1:37:26, 10.59s/it]                                                         92%|| 5948/6500 [17:58:46<1:37:26, 10.59s/it] 92%|| 5949/6500 [17:58:57<1:36:41, 10.53s/it]                                                         92%|| 5949/6500 [17:58:57<1:36:41, 10.53s/it] 92%|| 5950/6500 [17:59:07<1:36:03, 10.48s/it]                                                         92%|| 5950/6500 [17:59:07<1:36:03, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8716430068016052, 'eval_runtime': 4.5738, 'eval_samples_per_second': 5.029, 'eval_steps_per_second': 1.312, 'epoch': 0.92}
                                                         92%|| 5950/6500 [17:59:12<1:36:03, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.313, 'learning_rate': 1.7509503695154527e-06, 'epoch': 0.92}
{'loss': 0.336, 'learning_rate': 1.7446148114699134e-06, 'epoch': 0.92}
{'loss': 0.3233, 'learning_rate': 1.7382905328272635e-06, 'epoch': 0.92}
{'loss': 0.3171, 'learning_rate': 1.7319775350657652e-06, 'epoch': 0.92}
{'loss': 0.326, 'learning_rate': 1.7256758196610435e-06, 'epoch': 0.92}
 92%|| 5951/6500 [17:59:23<1:49:45, 12.00s/it]                                                         92%|| 5951/6500 [17:59:23<1:49:45, 12.00s/it] 92%|| 5952/6500 [17:59:33<1:45:01, 11.50s/it]                                                         92%|| 5952/6500 [17:59:33<1:45:01, 11.50s/it] 92%|| 5953/6500 [17:59:43<1:41:38, 11.15s/it]                                                         92%|| 5953/6500 [17:59:43<1:41:38, 11.15s/it] 92%|| 5954/6500 [17:59:54<1:39:27, 10.93s/it]                                                         92%|| 5954/6500 [17:59:54<1:39:27, 10.93s/it] 92%|| 5955/6500 [18:00:04<1:37:37, 10.75s/it]                                                         92%|| 595{'loss': 0.3156, 'learning_rate': 1.7193853880860811e-06, 'epoch': 0.92}
{'loss': 0.3185, 'learning_rate': 1.713106241811241e-06, 'epoch': 0.92}
{'loss': 0.3199, 'learning_rate': 1.7068383823042212e-06, 'epoch': 0.92}
{'loss': 0.3291, 'learning_rate': 1.7005818110301053e-06, 'epoch': 0.92}
{'loss': 0.2934, 'learning_rate': 1.6943365294513236e-06, 'epoch': 0.92}
5/6500 [18:00:04<1:37:37, 10.75s/it] 92%|| 5956/6500 [18:00:14<1:36:18, 10.62s/it]                                                         92%|| 5956/6500 [18:00:14<1:36:18, 10.62s/it] 92%|| 5957/6500 [18:00:25<1:35:20, 10.54s/it]                                                         92%|| 5957/6500 [18:00:25<1:35:20, 10.54s/it] 92%|| 5958/6500 [18:00:35<1:34:40, 10.48s/it]                                                         92%|| 5958/6500 [18:00:35<1:34:40, 10.48s/it] 92%|| 5959/6500 [18:00:46<1:34:07, 10.44s/it]                                                         92%|| 5959/6500 [18:00:46<1:34:07, 10.44s/it] 92%|| 5960/6500 [18:00:56<1:33:51, 10.43s/it]                                                         92%|| 5960/6500 [18:00:56<1:33:51, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711068630218506, 'eval_runtime': 3.9608, 'eval_samples_per_second': 5.807, 'eval_steps_per_second': 1.515, 'epoch': 0.92}
                                                         92%|| 5960/6500 [18:01:00<1:33:51, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3167, 'learning_rate': 1.688102539027675e-06, 'epoch': 0.92}
{'loss': 0.3051, 'learning_rate': 1.6818798412163216e-06, 'epoch': 0.92}
{'loss': 0.3452, 'learning_rate': 1.6756684374717724e-06, 'epoch': 0.92}
{'loss': 0.3606, 'learning_rate': 1.6694683292459157e-06, 'epoch': 0.92}
{'loss': 0.3055, 'learning_rate': 1.6632795179879756e-06, 'epoch': 0.92}
 92%|| 5961/6500 [18:01:11<1:45:41, 11.77s/it]                                                         92%|| 5961/6500 [18:01:11<1:45:41, 11.77s/it] 92%|| 5962/6500 [18:01:21<1:41:40, 11.34s/it]                                                         92%|| 5962/6500 [18:01:21<1:41:40, 11.34s/it] 92%|| 5963/6500 [18:01:31<1:38:46, 11.04s/it]                                                         92%|| 5963/6500 [18:01:31<1:38:46, 11.04s/it] 92%|| 5964/6500 [18:01:42<1:37:20, 10.90s/it]                                                         92%|| 5964/6500 [18:01:42<1:37:20, 10.90s/it] 92%|| 5965/6500 [18:01:52<1:35:41, 10.73s/it]                                                         92%|| 596{'loss': 0.3227, 'learning_rate': 1.6571020051445563e-06, 'epoch': 0.92}
{'loss': 0.3049, 'learning_rate': 1.6509357921596136e-06, 'epoch': 0.92}
{'loss': 0.8255, 'learning_rate': 1.6447808804744668e-06, 'epoch': 0.92}
{'loss': 0.3204, 'learning_rate': 1.638637271527782e-06, 'epoch': 0.92}
{'loss': 0.3209, 'learning_rate': 1.632504966755588e-06, 'epoch': 0.92}
5/6500 [18:01:52<1:35:41, 10.73s/it] 92%|| 5966/6500 [18:02:03<1:34:29, 10.62s/it]                                                         92%|| 5966/6500 [18:02:03<1:34:29, 10.62s/it] 92%|| 5967/6500 [18:02:13<1:33:36, 10.54s/it]                                                         92%|| 5967/6500 [18:02:13<1:33:36, 10.54s/it] 92%|| 5968/6500 [18:02:23<1:32:54, 10.48s/it]                                                         92%|| 5968/6500 [18:02:23<1:32:54, 10.48s/it] 92%|| 5969/6500 [18:02:34<1:32:23, 10.44s/it]                                                         92%|| 5969/6500 [18:02:34<1:32:23, 10.44s/it] 92%|| 5970/6500 [18:02:44<1:31:59, 10.41s/it]                                                         92%|| 5970/6500 [18:02:44<1:31:59, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8709689378738403, 'eval_runtime': 3.9659, 'eval_samples_per_second': 5.799, 'eval_steps_per_second': 1.513, 'epoch': 0.92}
                                                         92%|| 5970/6500 [18:02:48<1:31:59, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5970
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5970

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5970
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2952, 'learning_rate': 1.626383967591283e-06, 'epoch': 0.92}
{'loss': 0.3225, 'learning_rate': 1.6202742754656108e-06, 'epoch': 0.92}
{'loss': 0.3149, 'learning_rate': 1.614175891806674e-06, 'epoch': 0.92}
{'loss': 0.2866, 'learning_rate': 1.6080888180399268e-06, 'epoch': 0.92}
{'loss': 0.3128, 'learning_rate': 1.6020130555881974e-06, 'epoch': 0.92}
 92%|| 5971/6500 [18:03:00<1:45:02, 11.91s/it]                                                         92%|| 5971/6500 [18:03:00<1:45:02, 11.91s/it] 92%|| 5972/6500 [18:03:10<1:40:43, 11.45s/it]                                                         92%|| 5972/6500 [18:03:10<1:40:43, 11.45s/it] 92%|| 5973/6500 [18:03:20<1:37:33, 11.11s/it]                                                         92%|| 5973/6500 [18:03:20<1:37:33, 11.11s/it] 92%|| 5974/6500 [18:03:31<1:35:21, 10.88s/it]                                                         92%|| 5974/6500 [18:03:31<1:35:21, 10.88s/it] 92%|| 5975/6500 [18:03:41<1:33:47, 10.72s/it]                                                         92%|| 597{'loss': 0.3036, 'learning_rate': 1.59594860587165e-06, 'epoch': 0.92}
{'loss': 0.3147, 'learning_rate': 1.5898954703078117e-06, 'epoch': 0.92}
{'loss': 0.3163, 'learning_rate': 1.5838536503115675e-06, 'epoch': 0.92}
{'loss': 0.3131, 'learning_rate': 1.5778231472951598e-06, 'epoch': 0.92}
{'loss': 0.3034, 'learning_rate': 1.571803962668178e-06, 'epoch': 0.92}
5/6500 [18:03:41<1:33:47, 10.72s/it] 92%|| 5976/6500 [18:03:51<1:32:39, 10.61s/it]                                                         92%|| 5976/6500 [18:03:51<1:32:39, 10.61s/it] 92%|| 5977/6500 [18:04:02<1:31:50, 10.54s/it]                                                         92%|| 5977/6500 [18:04:02<1:31:50, 10.54s/it] 92%|| 5978/6500 [18:04:12<1:31:08, 10.48s/it]                                                         92%|| 5978/6500 [18:04:12<1:31:08, 10.48s/it] 92%|| 5979/6500 [18:04:22<1:30:34, 10.43s/it]                                                         92%|| 5979/6500 [18:04:22<1:30:34, 10.43s/it] 92%|| 5980/6500 [18:04:33<1:31:33, 10.56s/it]                                                         92%|| 5980/6500 [18:04:33<1:31:33, 10.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8704949021339417, 'eval_runtime': 3.955, 'eval_samples_per_second': 5.815, 'eval_steps_per_second': 1.517, 'epoch': 0.92}
                                                         92%|| 5980/6500 [18:04:37<1:31:33, 10.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3182, 'learning_rate': 1.5657960978375742e-06, 'epoch': 0.92}
{'loss': 0.3317, 'learning_rate': 1.5597995542076471e-06, 'epoch': 0.92}
{'loss': 0.2971, 'learning_rate': 1.5538143331800536e-06, 'epoch': 0.92}
{'loss': 0.3227, 'learning_rate': 1.5478404361538023e-06, 'epoch': 0.92}
{'loss': 0.3122, 'learning_rate': 1.541877864525254e-06, 'epoch': 0.92}
 92%|| 5981/6500 [18:04:48<1:42:30, 11.85s/it]                                                         92%|| 5981/6500 [18:04:48<1:42:30, 11.85s/it] 92%|| 5982/6500 [18:04:58<1:38:25, 11.40s/it]                                                         92%|| 5982/6500 [18:04:58<1:38:25, 11.40s/it] 92%|| 5983/6500 [18:05:09<1:35:31, 11.09s/it]                                                         92%|| 5983/6500 [18:05:09<1:35:31, 11.09s/it] 92%|| 5984/6500 [18:05:19<1:33:24, 10.86s/it]                                                         92%|| 5984/6500 [18:05:19<1:33:24, 10.86s/it] 92%|| 5985/6500 [18:05:29<1:31:56, 10.71s/it]                                                         92%|| 598{'loss': 0.3039, 'learning_rate': 1.5359266196881216e-06, 'epoch': 0.92}
{'loss': 0.3115, 'learning_rate': 1.5299867030334814e-06, 'epoch': 0.92}
{'loss': 0.3255, 'learning_rate': 1.5240581159497447e-06, 'epoch': 0.92}
{'loss': 0.3146, 'learning_rate': 1.5181408598226865e-06, 'epoch': 0.92}
{'loss': 0.3104, 'learning_rate': 1.5122349360354227e-06, 'epoch': 0.92}
5/6500 [18:05:29<1:31:56, 10.71s/it] 92%|| 5986/6500 [18:05:40<1:30:49, 10.60s/it]                                                         92%|| 5986/6500 [18:05:40<1:30:49, 10.60s/it] 92%|| 5987/6500 [18:05:50<1:29:59, 10.52s/it]                                                         92%|| 5987/6500 [18:05:50<1:29:59, 10.52s/it] 92%|| 5988/6500 [18:06:00<1:29:21, 10.47s/it]                                                         92%|| 5988/6500 [18:06:00<1:29:21, 10.47s/it] 92%|| 5989/6500 [18:06:11<1:28:54, 10.44s/it]                                                         92%|| 5989/6500 [18:06:11<1:28:54, 10.44s/it] 92%|| 5990/6500 [18:06:21<1:28:32, 10.42s/it]                                                         92%|| 5990/6500 [18:06:21<1:28:32, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8720171451568604, 'eval_runtime': 3.9536, 'eval_samples_per_second': 5.818, 'eval_steps_per_second': 1.518, 'epoch': 0.92}
                                                         92%|| 5990/6500 [18:06:25<1:28:32, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-5990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5990
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5990/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-5990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2981, 'learning_rate': 1.5063403459684378e-06, 'epoch': 0.92}
{'loss': 0.3071, 'learning_rate': 1.5004570909995464e-06, 'epoch': 0.92}
{'loss': 0.3805, 'learning_rate': 1.4945851725039262e-06, 'epoch': 0.92}
{'loss': 0.3014, 'learning_rate': 1.4887245918541071e-06, 'epoch': 0.92}
{'loss': 0.2918, 'learning_rate': 1.48287535041996e-06, 'epoch': 0.92}
 92%|| 5991/6500 [18:06:36<1:39:44, 11.76s/it]                                                         92%|| 5991/6500 [18:06:36<1:39:44, 11.76s/it] 92%|| 5992/6500 [18:06:46<1:35:59, 11.34s/it]                                                         92%|| 5992/6500 [18:06:46<1:35:59, 11.34s/it] 92%|| 5993/6500 [18:06:57<1:33:16, 11.04s/it]                                                         92%|| 5993/6500 [18:06:57<1:33:16, 11.04s/it] 92%|| 5994/6500 [18:07:07<1:31:18, 10.83s/it]                                                         92%|| 5994/6500 [18:07:07<1:31:18, 10.83s/it] 92%|| 5995/6500 [18:07:17<1:29:56, 10.69s/it]                                                         92%|| 599{'loss': 0.3267, 'learning_rate': 1.4770374495687134e-06, 'epoch': 0.92}
{'loss': 0.8238, 'learning_rate': 1.4712108906649369e-06, 'epoch': 0.92}
{'loss': 0.3244, 'learning_rate': 1.4653956750705577e-06, 'epoch': 0.92}
{'loss': 0.3204, 'learning_rate': 1.4595918041448442e-06, 'epoch': 0.92}
{'loss': 0.3287, 'learning_rate': 1.4537992792444111e-06, 'epoch': 0.92}
5/6500 [18:07:17<1:29:56, 10.69s/it] 92%|| 5996/6500 [18:07:28<1:29:32, 10.66s/it]                                                         92%|| 5996/6500 [18:07:28<1:29:32, 10.66s/it] 92%|| 5997/6500 [18:07:38<1:28:35, 10.57s/it]                                                         92%|| 5997/6500 [18:07:38<1:28:35, 10.57s/it] 92%|| 5998/6500 [18:07:49<1:27:55, 10.51s/it]                                                         92%|| 5998/6500 [18:07:49<1:27:55, 10.51s/it] 92%|| 5999/6500 [18:07:59<1:27:21, 10.46s/it]                                                         92%|| 5999/6500 [18:07:59<1:27:21, 10.46s/it] 92%|| 6000/6500 [18:08:10<1:26:57, 10.43s/it]                                                         92%|| 6000/6500 [18:08:10<1:26:57, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.872138500213623, 'eval_runtime': 3.9752, 'eval_samples_per_second': 5.786, 'eval_steps_per_second': 1.509, 'epoch': 0.92}
                                                         92%|| 6000/6500 [18:08:14<1:26:57, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6000
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2871, 'learning_rate': 1.4480181017232364e-06, 'epoch': 0.92}
{'loss': 0.3302, 'learning_rate': 1.4422482729326226e-06, 'epoch': 0.92}
{'loss': 0.3036, 'learning_rate': 1.4364897942212462e-06, 'epoch': 0.92}
{'loss': 0.3, 'learning_rate': 1.430742666935103e-06, 'epoch': 0.92}
{'loss': 0.3193, 'learning_rate': 1.4250068924175575e-06, 'epoch': 0.92}
 92%|| 6001/6500 [18:08:25<1:38:42, 11.87s/it]                                                         92%|| 6001/6500 [18:08:25<1:38:42, 11.87s/it] 92%|| 6002/6500 [18:08:35<1:34:45, 11.42s/it]                                                         92%|| 6002/6500 [18:08:35<1:34:45, 11.42s/it] 92%|| 6003/6500 [18:08:45<1:31:55, 11.10s/it]                                                         92%|| 6003/6500 [18:08:45<1:31:55, 11.10s/it] 92%|| 6004/6500 [18:08:56<1:29:51, 10.87s/it]                                                         92%|| 6004/6500 [18:08:56<1:29:51, 10.87s/it] 92%|| 6005/6500 [18:09:06<1:28:20, 10.71s/it]                                                         92%|| 600{'loss': 0.3031, 'learning_rate': 1.419282472009309e-06, 'epoch': 0.92}
{'loss': 0.3124, 'learning_rate': 1.4135694070484096e-06, 'epoch': 0.92}
{'loss': 0.3052, 'learning_rate': 1.407867698870252e-06, 'epoch': 0.92}
{'loss': 0.3198, 'learning_rate': 1.4021773488075706e-06, 'epoch': 0.92}
{'loss': 0.3025, 'learning_rate': 1.3964983581904567e-06, 'epoch': 0.92}
5/6500 [18:09:06<1:28:20, 10.71s/it] 92%|| 6006/6500 [18:09:16<1:27:13, 10.59s/it]                                                         92%|| 6006/6500 [18:09:16<1:27:13, 10.59s/it] 92%|| 6007/6500 [18:09:27<1:26:28, 10.52s/it]                                                         92%|| 6007/6500 [18:09:27<1:26:28, 10.52s/it] 92%|| 6008/6500 [18:09:37<1:25:50, 10.47s/it]                                                         92%|| 6008/6500 [18:09:37<1:25:50, 10.47s/it] 92%|| 6009/6500 [18:09:48<1:25:28, 10.45s/it]                                                         92%|| 6009/6500 [18:09:48<1:25:28, 10.45s/it] 92%|| 6010/6500 [18:09:58<1:25:05, 10.42s/it]                                                         92%|| 6010/6500 [18:09:58<1:25:05, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.872226357460022, 'eval_runtime': 3.9749, 'eval_samples_per_second': 5.786, 'eval_steps_per_second': 1.509, 'epoch': 0.92}
                                                         92%|| 6010/6500 [18:10:02<1:25:05, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6010
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3338, 'learning_rate': 1.3908307283463373e-06, 'epoch': 0.92}
{'loss': 0.3156, 'learning_rate': 1.385174460599986e-06, 'epoch': 0.92}
{'loss': 0.3171, 'learning_rate': 1.3795295562735234e-06, 'epoch': 0.93}
{'loss': 0.3269, 'learning_rate': 1.3738960166864101e-06, 'epoch': 0.93}
{'loss': 0.3219, 'learning_rate': 1.3682738431554487e-06, 'epoch': 0.93}
 92%|| 6011/6500 [18:10:13<1:35:51, 11.76s/it]                                                         92%|| 6011/6500 [18:10:13<1:35:51, 11.76s/it] 92%|| 6012/6500 [18:10:24<1:33:08, 11.45s/it]                                                         92%|| 6012/6500 [18:10:24<1:33:08, 11.45s/it] 93%|| 6013/6500 [18:10:34<1:30:16, 11.12s/it]                                                         93%|| 6013/6500 [18:10:34<1:30:16, 11.12s/it] 93%|| 6014/6500 [18:10:44<1:28:09, 10.88s/it]                                                         93%|| 6014/6500 [18:10:44<1:28:09, 10.88s/it] 93%|| 6015/6500 [18:10:55<1:26:41, 10.72s/it]                                                         93%|| 601{'loss': 0.3167, 'learning_rate': 1.3626630369947935e-06, 'epoch': 0.93}
{'loss': 0.3169, 'learning_rate': 1.3570635995159287e-06, 'epoch': 0.93}
{'loss': 0.3223, 'learning_rate': 1.3514755320277017e-06, 'epoch': 0.93}
{'loss': 0.3031, 'learning_rate': 1.345898835836279e-06, 'epoch': 0.93}
{'loss': 0.3092, 'learning_rate': 1.3403335122451787e-06, 'epoch': 0.93}
5/6500 [18:10:55<1:26:41, 10.72s/it] 93%|| 6016/6500 [18:11:05<1:25:34, 10.61s/it]                                                         93%|| 6016/6500 [18:11:05<1:25:34, 10.61s/it] 93%|| 6017/6500 [18:11:15<1:24:46, 10.53s/it]                                                         93%|| 6017/6500 [18:11:15<1:24:46, 10.53s/it] 93%|| 6018/6500 [18:11:26<1:24:08, 10.47s/it]                                                         93%|| 6018/6500 [18:11:26<1:24:08, 10.47s/it] 93%|| 6019/6500 [18:11:36<1:23:39, 10.44s/it]                                                         93%|| 6019/6500 [18:11:36<1:23:39, 10.44s/it] 93%|| 6020/6500 [18:11:46<1:23:18, 10.41s/it]                                                         93%|| 6020/6500 [18:11:46<1:23:18, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8719571828842163, 'eval_runtime': 3.9661, 'eval_samples_per_second': 5.799, 'eval_steps_per_second': 1.513, 'epoch': 0.93}
                                                         93%|| 6020/6500 [18:11:50<1:23:18, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6020I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2921, 'learning_rate': 1.3347795625552607e-06, 'epoch': 0.93}
{'loss': 0.3381, 'learning_rate': 1.329236988064736e-06, 'epoch': 0.93}
{'loss': 0.3653, 'learning_rate': 1.3237057900691407e-06, 'epoch': 0.93}
{'loss': 0.3207, 'learning_rate': 1.3181859698613575e-06, 'epoch': 0.93}
{'loss': 0.3194, 'learning_rate': 1.3126775287316151e-06, 'epoch': 0.93}
 93%|| 6021/6500 [18:12:01<1:33:53, 11.76s/it]                                                         93%|| 6021/6500 [18:12:01<1:33:53, 11.76s/it] 93%|| 6022/6500 [18:12:12<1:30:19, 11.34s/it]                                                         93%|| 6022/6500 [18:12:12<1:30:19, 11.34s/it] 93%|| 6023/6500 [18:12:22<1:27:44, 11.04s/it]                                                         93%|| 6023/6500 [18:12:22<1:27:44, 11.04s/it] 93%|| 6024/6500 [18:12:32<1:25:56, 10.83s/it]                                                         93%|| 6024/6500 [18:12:32<1:25:56, 10.83s/it] 93%|| 6025/6500 [18:12:43<1:24:37, 10.69s/it]                                                         93%|| 602{'loss': 0.3103, 'learning_rate': 1.3071804679674782e-06, 'epoch': 0.93}
{'loss': 0.8336, 'learning_rate': 1.3016947888538523e-06, 'epoch': 0.93}
{'loss': 0.3273, 'learning_rate': 1.2962204926729725e-06, 'epoch': 0.93}
{'loss': 0.3174, 'learning_rate': 1.2907575807044382e-06, 'epoch': 0.93}
{'loss': 0.2964, 'learning_rate': 1.2853060542251548e-06, 'epoch': 0.93}
5/6500 [18:12:43<1:24:37, 10.69s/it] 93%|| 6026/6500 [18:12:53<1:23:37, 10.59s/it]                                                         93%|| 6026/6500 [18:12:53<1:23:37, 10.59s/it] 93%|| 6027/6500 [18:13:03<1:22:51, 10.51s/it]                                                         93%|| 6027/6500 [18:13:03<1:22:51, 10.51s/it] 93%|| 6028/6500 [18:13:14<1:23:13, 10.58s/it]                                                         93%|| 6028/6500 [18:13:14<1:23:13, 10.58s/it] 93%|| 6029/6500 [18:13:24<1:22:29, 10.51s/it]                                                         93%|| 6029/6500 [18:13:24<1:22:29, 10.51s/it] 93%|| 6030/6500 [18:13:35<1:21:57, 10.46s/it]                                                         93%|| 6030/6500 [18:13:35<1:21:57, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8705072999000549, 'eval_runtime': 4.1922, 'eval_samples_per_second': 5.486, 'eval_steps_per_second': 1.431, 'epoch': 0.93}
                                                         93%|| 6030/6500 [18:13:39<1:21:57, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3006, 'learning_rate': 1.2798659145093927e-06, 'epoch': 0.93}
{'loss': 0.3203, 'learning_rate': 1.274437162828751e-06, 'epoch': 0.93}
{'loss': 0.2919, 'learning_rate': 1.2690198004521647e-06, 'epoch': 0.93}
{'loss': 0.3018, 'learning_rate': 1.2636138286459099e-06, 'epoch': 0.93}
{'loss': 0.3001, 'learning_rate': 1.2582192486735977e-06, 'epoch': 0.93}
 93%|| 6031/6500 [18:13:50<1:32:38, 11.85s/it]                                                         93%|| 6031/6500 [18:13:50<1:32:38, 11.85s/it] 93%|| 6032/6500 [18:14:00<1:28:53, 11.40s/it]                                                         93%|| 6032/6500 [18:14:00<1:28:53, 11.40s/it] 93%|| 6033/6500 [18:14:10<1:26:12, 11.08s/it]                                                         93%|| 6033/6500 [18:14:10<1:26:12, 11.08s/it] 93%|| 6034/6500 [18:14:21<1:24:20, 10.86s/it]                                                         93%|| 6034/6500 [18:14:21<1:24:20, 10.86s/it] 93%|| 6035/6500 [18:14:31<1:23:02, 10.72s/it]                                                         93%|| 603{'loss': 0.3099, 'learning_rate': 1.2528360617961866e-06, 'epoch': 0.93}
{'loss': 0.3159, 'learning_rate': 1.2474642692719586e-06, 'epoch': 0.93}
{'loss': 0.2934, 'learning_rate': 1.242103872356537e-06, 'epoch': 0.93}
{'loss': 0.3215, 'learning_rate': 1.2367548723028754e-06, 'epoch': 0.93}
{'loss': 0.3055, 'learning_rate': 1.2314172703612902e-06, 'epoch': 0.93}
5/6500 [18:14:31<1:23:02, 10.72s/it] 93%|| 6036/6500 [18:14:42<1:22:03, 10.61s/it]                                                         93%|| 6036/6500 [18:14:42<1:22:03, 10.61s/it] 93%|| 6037/6500 [18:14:52<1:21:17, 10.53s/it]                                                         93%|| 6037/6500 [18:14:52<1:21:17, 10.53s/it] 93%|| 6038/6500 [18:15:02<1:20:43, 10.48s/it]                                                         93%|| 6038/6500 [18:15:02<1:20:43, 10.48s/it] 93%|| 6039/6500 [18:15:13<1:20:14, 10.44s/it]                                                         93%|| 6039/6500 [18:15:13<1:20:14, 10.44s/it] 93%|| 6040/6500 [18:15:23<1:19:50, 10.41s/it]                                                         93%|| 6040/6500 [18:15:23<1:19:50, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8720518946647644, 'eval_runtime': 3.9717, 'eval_samples_per_second': 5.791, 'eval_steps_per_second': 1.511, 'epoch': 0.93}
                                                         93%|| 6040/6500 [18:15:27<1:19:50, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3307, 'learning_rate': 1.2260910677793947e-06, 'epoch': 0.93}
{'loss': 0.3078, 'learning_rate': 1.2207762658021593e-06, 'epoch': 0.93}
{'loss': 0.3096, 'learning_rate': 1.215472865671885e-06, 'epoch': 0.93}
{'loss': 0.3205, 'learning_rate': 1.210180868628219e-06, 'epoch': 0.93}
{'loss': 0.3087, 'learning_rate': 1.2049002759081275e-06, 'epoch': 0.93}
 93%|| 6041/6500 [18:15:38<1:29:50, 11.74s/it]                                                         93%|| 6041/6500 [18:15:38<1:29:50, 11.74s/it] 93%|| 6042/6500 [18:15:48<1:26:26, 11.32s/it]                                                         93%|| 6042/6500 [18:15:48<1:26:26, 11.32s/it] 93%|| 6043/6500 [18:15:59<1:24:00, 11.03s/it]                                                         93%|| 6043/6500 [18:15:59<1:24:00, 11.03s/it] 93%|| 6044/6500 [18:16:09<1:22:50, 10.90s/it]                                                         93%|| 6044/6500 [18:16:09<1:22:50, 10.90s/it] 93%|| 6045/6500 [18:16:20<1:21:28, 10.74s/it]                                                         93%|| 604{'loss': 0.3231, 'learning_rate': 1.1996310887459172e-06, 'epoch': 0.93}
{'loss': 0.3093, 'learning_rate': 1.1943733083732312e-06, 'epoch': 0.93}
{'loss': 0.3215, 'learning_rate': 1.189126936019036e-06, 'epoch': 0.93}
{'loss': 0.2972, 'learning_rate': 1.1838919729096453e-06, 'epoch': 0.93}
{'loss': 0.3076, 'learning_rate': 1.1786684202687026e-06, 'epoch': 0.93}
5/6500 [18:16:20<1:21:28, 10.74s/it] 93%|| 6046/6500 [18:16:30<1:20:25, 10.63s/it]                                                         93%|| 6046/6500 [18:16:30<1:20:25, 10.63s/it] 93%|| 6047/6500 [18:16:40<1:19:38, 10.55s/it]                                                         93%|| 6047/6500 [18:16:40<1:19:38, 10.55s/it] 93%|| 6048/6500 [18:16:51<1:18:59, 10.48s/it]                                                         93%|| 6048/6500 [18:16:51<1:18:59, 10.48s/it] 93%|| 6049/6500 [18:17:01<1:18:34, 10.45s/it]                                                         93%|| 6049/6500 [18:17:01<1:18:34, 10.45s/it] 93%|| 6050/6500 [18:17:11<1:18:10, 10.42s/it]                                                         93%|| 6050/6500 [18:17:11<1:18:10, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8717687129974365, 'eval_runtime': 3.9665, 'eval_samples_per_second': 5.799, 'eval_steps_per_second': 1.513, 'epoch': 0.93}
                                                         93%|| 6050/6500 [18:17:15<1:18:10, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2973, 'learning_rate': 1.173456279317181e-06, 'epoch': 0.93}
{'loss': 0.3863, 'learning_rate': 1.1682555512733783e-06, 'epoch': 0.93}
{'loss': 0.3131, 'learning_rate': 1.1630662373529444e-06, 'epoch': 0.93}
{'loss': 0.2983, 'learning_rate': 1.1578883387688366e-06, 'epoch': 0.93}
{'loss': 0.3166, 'learning_rate': 1.1527218567313703e-06, 'epoch': 0.93}
 93%|| 6051/6500 [18:17:26<1:28:03, 11.77s/it]                                                         93%|| 6051/6500 [18:17:26<1:28:03, 11.77s/it] 93%|| 6052/6500 [18:17:37<1:24:42, 11.34s/it]                                                         93%|| 6052/6500 [18:17:37<1:24:42, 11.34s/it] 93%|| 6053/6500 [18:17:47<1:22:19, 11.05s/it]                                                         93%|| 6053/6500 [18:17:47<1:22:19, 11.05s/it] 93%|| 6054/6500 [18:17:57<1:20:37, 10.85s/it]                                                         93%|| 6054/6500 [18:17:57<1:20:37, 10.85s/it] 93%|| 6055/6500 [18:18:08<1:19:22, 10.70s/it]                                                         93%|| 605{'loss': 0.816, 'learning_rate': 1.1475667924481682e-06, 'epoch': 0.93}
{'loss': 0.317, 'learning_rate': 1.1424231471242054e-06, 'epoch': 0.93}
{'loss': 0.3158, 'learning_rate': 1.13729092196177e-06, 'epoch': 0.93}
{'loss': 0.3172, 'learning_rate': 1.1321701181604915e-06, 'epoch': 0.93}
{'loss': 0.3, 'learning_rate': 1.1270607369173291e-06, 'epoch': 0.93}
5/6500 [18:18:08<1:19:22, 10.70s/it] 93%|| 6056/6500 [18:18:18<1:18:23, 10.59s/it]                                                         93%|| 6056/6500 [18:18:18<1:18:23, 10.59s/it] 93%|| 6057/6500 [18:18:28<1:17:45, 10.53s/it]                                                         93%|| 6057/6500 [18:18:28<1:17:45, 10.53s/it] 93%|| 6058/6500 [18:18:39<1:17:12, 10.48s/it]                                                         93%|| 6058/6500 [18:18:39<1:17:12, 10.48s/it] 93%|| 6059/6500 [18:18:49<1:16:47, 10.45s/it]                                                         93%|| 6059/6500 [18:18:49<1:16:47, 10.45s/it] 93%|| 6060/6500 [18:18:59<1:16:24, 10.42s/it]                                                         93%|| 6060/6500 [18:18:59<1:16:24, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718921542167664, 'eval_runtime': 3.9713, 'eval_samples_per_second': 5.792, 'eval_steps_per_second': 1.511, 'epoch': 0.93}
                                                         93%|| 6060/6500 [18:19:03<1:16:24, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3175, 'learning_rate': 1.1219627794265664e-06, 'epoch': 0.93}
{'loss': 0.3153, 'learning_rate': 1.116876246879822e-06, 'epoch': 0.93}
{'loss': 0.2802, 'learning_rate': 1.11180114046604e-06, 'epoch': 0.93}
{'loss': 0.3097, 'learning_rate': 1.1067374613714932e-06, 'epoch': 0.93}
{'loss': 0.2967, 'learning_rate': 1.1016852107797903e-06, 'epoch': 0.93}
 93%|| 6061/6500 [18:19:15<1:27:22, 11.94s/it]                                                         93%|| 6061/6500 [18:19:15<1:27:22, 11.94s/it] 93%|| 6062/6500 [18:19:25<1:23:41, 11.46s/it]                                                         93%|| 6062/6500 [18:19:25<1:23:41, 11.46s/it] 93%|| 6063/6500 [18:19:36<1:21:05, 11.13s/it]                                                         93%|| 6063/6500 [18:19:36<1:21:05, 11.13s/it] 93%|| 6064/6500 [18:19:46<1:19:12, 10.90s/it]                                                         93%|| 6064/6500 [18:19:46<1:19:12, 10.90s/it] 93%|| 6065/6500 [18:19:56<1:17:49, 10.74s/it]                                                         93%|| 606{'loss': 0.3107, 'learning_rate': 1.0966443898718648e-06, 'epoch': 0.93}
{'loss': 0.3071, 'learning_rate': 1.091614999825974e-06, 'epoch': 0.93}
{'loss': 0.3064, 'learning_rate': 1.0865970418177051e-06, 'epoch': 0.93}
{'loss': 0.3106, 'learning_rate': 1.081590517019987e-06, 'epoch': 0.93}
{'loss': 0.3079, 'learning_rate': 1.07659542660305e-06, 'epoch': 0.93}
5/6500 [18:19:56<1:17:49, 10.74s/it] 93%|| 6066/6500 [18:20:07<1:16:51, 10.62s/it]                                                         93%|| 6066/6500 [18:20:07<1:16:51, 10.62s/it] 93%|| 6067/6500 [18:20:17<1:16:08, 10.55s/it]                                                         93%|| 6067/6500 [18:20:17<1:16:08, 10.55s/it] 93%|| 6068/6500 [18:20:27<1:15:33, 10.49s/it]                                                         93%|| 6068/6500 [18:20:27<1:15:33, 10.49s/it] 93%|| 6069/6500 [18:20:38<1:15:05, 10.45s/it]                                                         93%|| 6069/6500 [18:20:38<1:15:05, 10.45s/it] 93%|| 6070/6500 [18:20:48<1:14:42, 10.42s/it]                                                         93%|| 6070/6500 [18:20:48<1:14:42, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8712776303291321, 'eval_runtime': 3.969, 'eval_samples_per_second': 5.795, 'eval_steps_per_second': 1.512, 'epoch': 0.93}
                                                         93%|| 6070/6500 [18:20:52<1:14:42, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3277, 'learning_rate': 1.0716117717344765e-06, 'epoch': 0.93}
{'loss': 0.3056, 'learning_rate': 1.066639553579163e-06, 'epoch': 0.93}
{'loss': 0.3234, 'learning_rate': 1.0616787732993295e-06, 'epoch': 0.93}
{'loss': 0.3218, 'learning_rate': 1.0567294320545428e-06, 'epoch': 0.93}
{'loss': 0.311, 'learning_rate': 1.0517915310016614e-06, 'epoch': 0.93}
 93%|| 6071/6500 [18:21:03<1:24:02, 11.75s/it]                                                         93%|| 6071/6500 [18:21:03<1:24:02, 11.75s/it] 93%|| 6072/6500 [18:21:13<1:20:51, 11.33s/it]                                                         93%|| 6072/6500 [18:21:13<1:20:51, 11.33s/it] 93%|| 6073/6500 [18:21:24<1:18:33, 11.04s/it]                                                         93%|| 6073/6500 [18:21:24<1:18:33, 11.04s/it] 93%|| 6074/6500 [18:21:34<1:16:56, 10.84s/it]                                                         93%|| 6074/6500 [18:21:34<1:16:56, 10.84s/it] 93%|| 6075/6500 [18:21:44<1:15:43, 10.69s/it]                                                         93%|| 607{'loss': 0.3091, 'learning_rate': 1.0468650712949057e-06, 'epoch': 0.93}
{'loss': 0.3238, 'learning_rate': 1.041950054085794e-06, 'epoch': 0.93}
{'loss': 0.3122, 'learning_rate': 1.0370464805231905e-06, 'epoch': 0.94}
{'loss': 0.3011, 'learning_rate': 1.0321543517532727e-06, 'epoch': 0.94}
{'loss': 0.2918, 'learning_rate': 1.0272736689195429e-06, 'epoch': 0.94}
5/6500 [18:21:44<1:15:43, 10.69s/it] 93%|| 6076/6500 [18:21:55<1:14:50, 10.59s/it]                                                         93%|| 6076/6500 [18:21:55<1:14:50, 10.59s/it] 93%|| 6077/6500 [18:22:05<1:14:42, 10.60s/it]                                                         93%|| 6077/6500 [18:22:05<1:14:42, 10.60s/it] 94%|| 6078/6500 [18:22:16<1:14:01, 10.53s/it]                                                         94%|| 6078/6500 [18:22:16<1:14:01, 10.53s/it] 94%|| 6079/6500 [18:22:26<1:13:31, 10.48s/it]                                                         94%|| 6079/6500 [18:22:26<1:13:31, 10.48s/it] 94%|| 6080/6500 [18:22:37<1:13:05, 10.44s/it]                                                         94%|| 6080/6500 [18:22:37<1:13:05, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8726340532302856, 'eval_runtime': 3.9677, 'eval_samples_per_second': 5.797, 'eval_steps_per_second': 1.512, 'epoch': 0.94}
                                                         94%|| 6080/6500 [18:22:40<1:13:05, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6080
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3175, 'learning_rate': 1.0224044331628379e-06, 'epoch': 0.94}
{'loss': 0.369, 'learning_rate': 1.0175466456213034e-06, 'epoch': 0.94}
{'loss': 0.3069, 'learning_rate': 1.0127003074304253e-06, 'epoch': 0.94}
{'loss': 0.2976, 'learning_rate': 1.0078654197230031e-06, 'epoch': 0.94}
{'loss': 0.3203, 'learning_rate': 1.0030419836291605e-06, 'epoch': 0.94}
 94%|| 6081/6500 [18:22:51<1:22:17, 11.78s/it]                                                         94%|| 6081/6500 [18:22:51<1:22:17, 11.78s/it] 94%|| 6082/6500 [18:23:02<1:19:05, 11.35s/it]                                                         94%|| 6082/6500 [18:23:02<1:19:05, 11.35s/it] 94%|| 6083/6500 [18:23:12<1:16:48, 11.05s/it]                                                         94%|| 6083/6500 [18:23:12<1:16:48, 11.05s/it] 94%|| 6084/6500 [18:23:22<1:15:10, 10.84s/it]                                                         94%|| 6084/6500 [18:23:22<1:15:10, 10.84s/it] 94%|| 6085/6500 [18:23:33<1:13:57, 10.69s/it]                                                         94%|| 608{'loss': 0.8239, 'learning_rate': 9.98230000276351e-07, 'epoch': 0.94}
{'loss': 0.3224, 'learning_rate': 9.93429470789342e-07, 'epoch': 0.94}
{'loss': 0.312, 'learning_rate': 9.886403962902246e-07, 'epoch': 0.94}
{'loss': 0.3131, 'learning_rate': 9.838627778984256e-07, 'epoch': 0.94}
{'loss': 0.296, 'learning_rate': 9.790966167306793e-07, 'epoch': 0.94}
5/6500 [18:23:33<1:13:57, 10.69s/it] 94%|| 6086/6500 [18:23:43<1:13:02, 10.59s/it]                                                         94%|| 6086/6500 [18:23:43<1:13:02, 10.59s/it] 94%|| 6087/6500 [18:23:54<1:12:22, 10.51s/it]                                                         94%|| 6087/6500 [18:23:54<1:12:22, 10.51s/it] 94%|| 6088/6500 [18:24:04<1:11:53, 10.47s/it]                                                         94%|| 6088/6500 [18:24:04<1:11:53, 10.47s/it] 94%|| 6089/6500 [18:24:14<1:11:28, 10.43s/it]                                                         94%|| 6089/6500 [18:24:14<1:11:28, 10.43s/it] 94%|| 6090/6500 [18:24:25<1:11:08, 10.41s/it]                                                         94%|| 6090/6500 [18:24:25<1:11:08, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8722180724143982, 'eval_runtime': 3.9605, 'eval_samples_per_second': 5.807, 'eval_steps_per_second': 1.515, 'epoch': 0.94}
                                                         94%|| 6090/6500 [18:24:29<1:11:08, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3318, 'learning_rate': 9.743419139010447e-07, 'epoch': 0.94}
{'loss': 0.2919, 'learning_rate': 9.695986705209048e-07, 'epoch': 0.94}
{'loss': 0.3043, 'learning_rate': 9.64866887698973e-07, 'epoch': 0.94}
{'loss': 0.3079, 'learning_rate': 9.601465665412645e-07, 'epoch': 0.94}
{'loss': 0.3194, 'learning_rate': 9.554377081511301e-07, 'epoch': 0.94}
 94%|| 6091/6500 [18:24:39<1:20:07, 11.76s/it]                                                         94%|| 6091/6500 [18:24:39<1:20:07, 11.76s/it] 94%|| 6092/6500 [18:24:50<1:17:04, 11.33s/it]                                                         94%|| 6092/6500 [18:24:50<1:17:04, 11.33s/it] 94%|| 6093/6500 [18:25:01<1:16:13, 11.24s/it]                                                         94%|| 6093/6500 [18:25:01<1:16:13, 11.24s/it] 94%|| 6094/6500 [18:25:11<1:14:15, 10.97s/it]                                                         94%|| 6094/6500 [18:25:11<1:14:15, 10.97s/it] 94%|| 6095/6500 [18:25:22<1:12:49, 10.79s/it]                                                         94%|| 609{'loss': 0.3104, 'learning_rate': 9.507403136292336e-07, 'epoch': 0.94}
{'loss': 0.3127, 'learning_rate': 9.460543840735636e-07, 'epoch': 0.94}
{'loss': 0.3342, 'learning_rate': 9.413799205794272e-07, 'epoch': 0.94}
{'loss': 0.3156, 'learning_rate': 9.367169242394557e-07, 'epoch': 0.94}
{'loss': 0.3215, 'learning_rate': 9.320653961435943e-07, 'epoch': 0.94}
5/6500 [18:25:22<1:12:49, 10.79s/it] 94%|| 6096/6500 [18:25:32<1:11:46, 10.66s/it]                                                         94%|| 6096/6500 [18:25:32<1:11:46, 10.66s/it] 94%|| 6097/6500 [18:25:42<1:10:57, 10.56s/it]                                                         94%|| 6097/6500 [18:25:42<1:10:57, 10.56s/it] 94%|| 6098/6500 [18:25:53<1:10:20, 10.50s/it]                                                         94%|| 6098/6500 [18:25:53<1:10:20, 10.50s/it] 94%|| 6099/6500 [18:26:03<1:09:53, 10.46s/it]                                                         94%|| 6099/6500 [18:26:03<1:09:53, 10.46s/it] 94%|| 6100/6500 [18:26:13<1:09:30, 10.43s/it]                                                         94%|| 6100/6500 [18:26:13<1:09:30, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8721681833267212, 'eval_runtime': 3.971, 'eval_samples_per_second': 5.792, 'eval_steps_per_second': 1.511, 'epoch': 0.94}
                                                         94%|| 6100/6500 [18:26:17<1:09:30, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6100
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3176, 'learning_rate': 9.274253373791064e-07, 'epoch': 0.94}
{'loss': 0.3108, 'learning_rate': 9.2279674903058e-07, 'epoch': 0.94}
{'loss': 0.3186, 'learning_rate': 9.181796321799163e-07, 'epoch': 0.94}
{'loss': 0.3256, 'learning_rate': 9.135739879063465e-07, 'epoch': 0.94}
{'loss': 0.3117, 'learning_rate': 9.089798172864094e-07, 'epoch': 0.94}
 94%|| 6101/6500 [18:26:28<1:18:19, 11.78s/it]                                                         94%|| 6101/6500 [18:26:28<1:18:19, 11.78s/it] 94%|| 6102/6500 [18:26:39<1:15:16, 11.35s/it]                                                         94%|| 6102/6500 [18:26:39<1:15:16, 11.35s/it] 94%|| 6103/6500 [18:26:49<1:13:06, 11.05s/it]                                                         94%|| 6103/6500 [18:26:49<1:13:06, 11.05s/it] 94%|| 6104/6500 [18:26:59<1:11:33, 10.84s/it]                                                         94%|| 6104/6500 [18:26:59<1:11:33, 10.84s/it] 94%|| 6105/6500 [18:27:10<1:10:24, 10.69s/it]                                                         94%|| 610{'loss': 0.321, 'learning_rate': 9.043971213939573e-07, 'epoch': 0.94}
{'loss': 0.3324, 'learning_rate': 8.998259013001719e-07, 'epoch': 0.94}
{'loss': 0.2942, 'learning_rate': 8.952661580735433e-07, 'epoch': 0.94}
{'loss': 0.3121, 'learning_rate': 8.907178927798965e-07, 'epoch': 0.94}
{'loss': 0.2893, 'learning_rate': 8.861811064823477e-07, 'epoch': 0.94}
5/6500 [18:27:10<1:10:24, 10.69s/it] 94%|| 6106/6500 [18:27:20<1:09:32, 10.59s/it]                                                         94%|| 6106/6500 [18:27:20<1:09:32, 10.59s/it] 94%|| 6107/6500 [18:27:30<1:08:52, 10.52s/it]                                                         94%|| 6107/6500 [18:27:30<1:08:52, 10.52s/it] 94%|| 6108/6500 [18:27:41<1:08:24, 10.47s/it]                                                         94%|| 6108/6500 [18:27:41<1:08:24, 10.47s/it] 94%|| 6109/6500 [18:27:51<1:08:29, 10.51s/it]                                                         94%|| 6109/6500 [18:27:51<1:08:29, 10.51s/it] 94%|| 6110/6500 [18:28:02<1:08:00, 10.46s/it]                                                         94%|| 6110/6500 [18:28:02<1:08:00, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8719051480293274, 'eval_runtime': 3.9732, 'eval_samples_per_second': 5.789, 'eval_steps_per_second': 1.51, 'epoch': 0.94}
                                                         94%|| 6110/6500 [18:28:06<1:08:00, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6110
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3363, 'learning_rate': 8.816558002413488e-07, 'epoch': 0.94}
{'loss': 0.3598, 'learning_rate': 8.771419751146648e-07, 'epoch': 0.94}
{'loss': 0.3026, 'learning_rate': 8.726396321573682e-07, 'epoch': 0.94}
{'loss': 0.3225, 'learning_rate': 8.681487724218618e-07, 'epoch': 0.94}
{'loss': 0.3099, 'learning_rate': 8.636693969578558e-07, 'epoch': 0.94}
 94%|| 6111/6500 [18:28:17<1:16:23, 11.78s/it]                                                         94%|| 6111/6500 [18:28:17<1:16:23, 11.78s/it] 94%|| 6112/6500 [18:28:27<1:13:26, 11.36s/it]                                                         94%|| 6112/6500 [18:28:27<1:13:26, 11.36s/it] 94%|| 6113/6500 [18:28:37<1:11:17, 11.05s/it]                                                         94%|| 6113/6500 [18:28:37<1:11:17, 11.05s/it] 94%|| 6114/6500 [18:28:48<1:09:44, 10.84s/it]                                                         94%|| 6114/6500 [18:28:48<1:09:44, 10.84s/it] 94%|| 6115/6500 [18:28:58<1:08:38, 10.70s/it]                                                         94%|| 611{'loss': 0.8196, 'learning_rate': 8.592015068123738e-07, 'epoch': 0.94}
{'loss': 0.325, 'learning_rate': 8.547451030297526e-07, 'epoch': 0.94}
{'loss': 0.3207, 'learning_rate': 8.503001866516592e-07, 'epoch': 0.94}
{'loss': 0.2911, 'learning_rate': 8.458667587170621e-07, 'epoch': 0.94}
{'loss': 0.3166, 'learning_rate': 8.414448202622494e-07, 'epoch': 0.94}
5/6500 [18:28:58<1:08:38, 10.70s/it] 94%|| 6116/6500 [18:29:08<1:07:44, 10.58s/it]                                                         94%|| 6116/6500 [18:29:08<1:07:44, 10.58s/it] 94%|| 6117/6500 [18:29:19<1:07:06, 10.51s/it]                                                         94%|| 6117/6500 [18:29:19<1:07:06, 10.51s/it] 94%|| 6118/6500 [18:29:29<1:06:37, 10.46s/it]                                                         94%|| 6118/6500 [18:29:29<1:06:37, 10.46s/it] 94%|| 6119/6500 [18:29:39<1:06:14, 10.43s/it]                                                         94%|| 6119/6500 [18:29:39<1:06:14, 10.43s/it] 94%|| 6120/6500 [18:29:50<1:05:56, 10.41s/it]                                                         94%|| 6120/6500 [18:29:50<1:05:56, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711410760879517, 'eval_runtime': 3.9709, 'eval_samples_per_second': 5.792, 'eval_steps_per_second': 1.511, 'epoch': 0.94}
                                                         94%|| 6120/6500 [18:29:54<1:05:56, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6120
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6120/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3085, 'learning_rate': 8.370343723208163e-07, 'epoch': 0.94}
{'loss': 0.2804, 'learning_rate': 8.326354159236882e-07, 'epoch': 0.94}
{'loss': 0.3038, 'learning_rate': 8.282479520990871e-07, 'epoch': 0.94}
{'loss': 0.3012, 'learning_rate': 8.238719818725593e-07, 'epoch': 0.94}
{'loss': 0.3132, 'learning_rate': 8.195075062669588e-07, 'epoch': 0.94}
 94%|| 6121/6500 [18:30:05<1:14:13, 11.75s/it]                                                         94%|| 6121/6500 [18:30:05<1:14:13, 11.75s/it] 94%|| 6122/6500 [18:30:15<1:11:20, 11.32s/it]                                                         94%|| 6122/6500 [18:30:15<1:11:20, 11.32s/it] 94%|| 6123/6500 [18:30:25<1:09:20, 11.04s/it]                                                         94%|| 6123/6500 [18:30:25<1:09:20, 11.04s/it] 94%|| 6124/6500 [18:30:36<1:07:53, 10.83s/it]                                                         94%|| 6124/6500 [18:30:36<1:07:53, 10.83s/it] 94%|| 6125/6500 [18:30:47<1:07:58, 10.88s/it]                                                         94%|| 612{'loss': 0.3155, 'learning_rate': 8.15154526302453e-07, 'epoch': 0.94}
{'loss': 0.3132, 'learning_rate': 8.108130429965388e-07, 'epoch': 0.94}
{'loss': 0.3167, 'learning_rate': 8.064830573639881e-07, 'epoch': 0.94}
{'loss': 0.3165, 'learning_rate': 8.021645704169301e-07, 'epoch': 0.94}
{'loss': 0.3445, 'learning_rate': 7.978575831647683e-07, 'epoch': 0.94}
5/6500 [18:30:47<1:07:58, 10.88s/it] 94%|| 6126/6500 [18:30:57<1:06:50, 10.72s/it]                                                         94%|| 6126/6500 [18:30:57<1:06:50, 10.72s/it] 94%|| 6127/6500 [18:31:07<1:05:59, 10.61s/it]                                                         94%|| 6127/6500 [18:31:07<1:05:59, 10.61s/it] 94%|| 6128/6500 [18:31:18<1:05:21, 10.54s/it]                                                         94%|| 6128/6500 [18:31:18<1:05:21, 10.54s/it] 94%|| 6129/6500 [18:31:28<1:04:50, 10.49s/it]                                                         94%|| 6129/6500 [18:31:28<1:04:50, 10.49s/it] 94%|| 6130/6500 [18:31:38<1:04:26, 10.45s/it]                                                         94%|| 6130/6500 [18:31:38<1:04:26, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8715534806251526, 'eval_runtime': 3.9721, 'eval_samples_per_second': 5.79, 'eval_steps_per_second': 1.511, 'epoch': 0.94}
                                                         94%|| 6130/6500 [18:31:42<1:04:26, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6130I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6130

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6130/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6130/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6130/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3173, 'learning_rate': 7.935620966142476e-07, 'epoch': 0.94}
{'loss': 0.3262, 'learning_rate': 7.892781117694037e-07, 'epoch': 0.94}
{'loss': 0.3337, 'learning_rate': 7.850056296315967e-07, 'epoch': 0.94}
{'loss': 0.3051, 'learning_rate': 7.807446511994942e-07, 'epoch': 0.94}
{'loss': 0.3133, 'learning_rate': 7.764951774690665e-07, 'epoch': 0.94}
 94%|| 6131/6500 [18:31:53<1:12:26, 11.78s/it]                                                         94%|| 6131/6500 [18:31:53<1:12:26, 11.78s/it] 94%|| 6132/6500 [18:32:04<1:09:38, 11.35s/it]                                                         94%|| 6132/6500 [18:32:04<1:09:38, 11.35s/it] 94%|| 6133/6500 [18:32:14<1:07:37, 11.06s/it]                                                         94%|| 6133/6500 [18:32:14<1:07:37, 11.06s/it] 94%|| 6134/6500 [18:32:24<1:06:10, 10.85s/it]                                                         94%|| 6134/6500 [18:32:24<1:06:10, 10.85s/it] 94%|| 6135/6500 [18:32:35<1:05:06, 10.70s/it]                                                         94%|| 613{'loss': 0.3369, 'learning_rate': 7.722572094336133e-07, 'epoch': 0.94}
{'loss': 0.3144, 'learning_rate': 7.680307480837201e-07, 'epoch': 0.94}
{'loss': 0.2947, 'learning_rate': 7.638157944073132e-07, 'epoch': 0.94}
{'loss': 0.3121, 'learning_rate': 7.596123493895991e-07, 'epoch': 0.94}
{'loss': 0.3058, 'learning_rate': 7.554204140131138e-07, 'epoch': 0.94}
5/6500 [18:32:35<1:05:06, 10.70s/it] 94%|| 6136/6500 [18:32:45<1:04:18, 10.60s/it]                                                         94%|| 6136/6500 [18:32:45<1:04:18, 10.60s/it] 94%|| 6137/6500 [18:32:55<1:03:40, 10.53s/it]                                                         94%|| 6137/6500 [18:32:55<1:03:40, 10.53s/it] 94%|| 6138/6500 [18:33:06<1:03:13, 10.48s/it]                                                         94%|| 6138/6500 [18:33:06<1:03:13, 10.48s/it] 94%|| 6139/6500 [18:33:16<1:02:46, 10.43s/it]                                                         94%|| 6139/6500 [18:33:16<1:02:46, 10.43s/it] 94%|| 6140/6500 [18:33:27<1:02:27, 10.41s/it]                                                         94%|| 6140/6500 [18:33:27<1:02:27, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8706585168838501, 'eval_runtime': 3.9584, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 1.516, 'epoch': 0.94}
                                                         94%|| 6140/6500 [18:33:30<1:02:27, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6140/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6140/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6140/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3937, 'learning_rate': 7.512399892576904e-07, 'epoch': 0.94}
{'loss': 0.3046, 'learning_rate': 7.470710761004862e-07, 'epoch': 0.94}
{'loss': 0.3037, 'learning_rate': 7.429136755159493e-07, 'epoch': 0.95}
{'loss': 0.3213, 'learning_rate': 7.387677884758582e-07, 'epoch': 0.95}
{'loss': 0.8195, 'learning_rate': 7.346334159492818e-07, 'epoch': 0.95}
 94%|| 6141/6500 [18:33:42<1:10:40, 11.81s/it]                                                         94%|| 6141/6500 [18:33:42<1:10:40, 11.81s/it] 94%|| 6142/6500 [18:33:52<1:07:56, 11.39s/it]                                                         94%|| 6142/6500 [18:33:52<1:07:56, 11.39s/it] 95%|| 6143/6500 [18:34:02<1:05:59, 11.09s/it]                                                         95%|| 6143/6500 [18:34:02<1:05:59, 11.09s/it] 95%|| 6144/6500 [18:34:13<1:04:33, 10.88s/it]                                                         95%|| 6144/6500 [18:34:13<1:04:33, 10.88s/it] 95%|| 6145/6500 [18:34:23<1:03:28, 10.73s/it]                                                         95%|| 614{'loss': 0.3118, 'learning_rate': 7.305105589026085e-07, 'epoch': 0.95}
{'loss': 0.3163, 'learning_rate': 7.263992182995228e-07, 'epoch': 0.95}
{'loss': 0.3219, 'learning_rate': 7.222993951010338e-07, 'epoch': 0.95}
{'loss': 0.2976, 'learning_rate': 7.182110902654527e-07, 'epoch': 0.95}
{'loss': 0.3301, 'learning_rate': 7.141343047483873e-07, 'epoch': 0.95}
5/6500 [18:34:23<1:03:28, 10.73s/it] 95%|| 6146/6500 [18:34:34<1:02:43, 10.63s/it]                                                         95%|| 6146/6500 [18:34:34<1:02:43, 10.63s/it] 95%|| 6147/6500 [18:34:44<1:02:09, 10.56s/it]                                                         95%|| 6147/6500 [18:34:44<1:02:09, 10.56s/it] 95%|| 6148/6500 [18:34:54<1:01:41, 10.52s/it]                                                         95%|| 6148/6500 [18:34:54<1:01:41, 10.52s/it] 95%|| 6149/6500 [18:35:05<1:01:17, 10.48s/it]                                                         95%|| 6149/6500 [18:35:05<1:01:17, 10.48s/it] 95%|| 6150/6500 [18:35:15<1:00:59, 10.45s/it]                                                         95%|| 6150/6500 [18:35:15<1:00:59, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8707093596458435, 'eval_runtime': 3.9454, 'eval_samples_per_second': 5.83, 'eval_steps_per_second': 1.521, 'epoch': 0.95}
                                                         95%|| 6150/6500 [18:35:19<1:00:59, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6150
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6150/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6150/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3032, 'learning_rate': 7.100690395027699e-07, 'epoch': 0.95}
{'loss': 0.2977, 'learning_rate': 7.06015295478818e-07, 'epoch': 0.95}
{'loss': 0.3077, 'learning_rate': 7.019730736240848e-07, 'epoch': 0.95}
{'loss': 0.3047, 'learning_rate': 6.979423748834035e-07, 'epoch': 0.95}
{'loss': 0.3204, 'learning_rate': 6.93923200198937e-07, 'epoch': 0.95}
 95%|| 6151/6500 [18:35:30<1:08:32, 11.78s/it]                                                         95%|| 6151/6500 [18:35:30<1:08:32, 11.78s/it] 95%|| 6152/6500 [18:35:40<1:05:55, 11.37s/it]                                                         95%|| 6152/6500 [18:35:40<1:05:55, 11.37s/it] 95%|| 6153/6500 [18:35:51<1:04:03, 11.08s/it]                                                         95%|| 6153/6500 [18:35:51<1:04:03, 11.08s/it] 95%|| 6154/6500 [18:36:01<1:02:40, 10.87s/it]                                                         95%|| 6154/6500 [18:36:01<1:02:40, 10.87s/it] 95%|| 6155/6500 [18:36:12<1:01:40, 10.73s/it]                                                         95%|| 615{'loss': 0.3067, 'learning_rate': 6.89915550510134e-07, 'epoch': 0.95}
{'loss': 0.3198, 'learning_rate': 6.859194267537561e-07, 'epoch': 0.95}
{'loss': 0.2925, 'learning_rate': 6.819348298638839e-07, 'epoch': 0.95}
{'loss': 0.3324, 'learning_rate': 6.779617607718835e-07, 'epoch': 0.95}
{'loss': 0.3072, 'learning_rate': 6.7400022040644e-07, 'epoch': 0.95}
5/6500 [18:36:12<1:01:40, 10.73s/it] 95%|| 6156/6500 [18:36:22<1:00:54, 10.62s/it]                                                         95%|| 6156/6500 [18:36:22<1:00:54, 10.62s/it] 95%|| 6157/6500 [18:36:32<1:00:22, 10.56s/it]                                                         95%|| 6157/6500 [18:36:32<1:00:22, 10.56s/it] 95%|| 6158/6500 [18:36:43<1:00:35, 10.63s/it]                                                         95%|| 6158/6500 [18:36:43<1:00:35, 10.63s/it] 95%|| 6159/6500 [18:36:54<1:00:02, 10.56s/it]                                                         95%|| 6159/6500 [18:36:54<1:00:02, 10.56s/it] 95%|| 6160/6500 [18:37:04<59:35, 10.51s/it]                                                         95%|| 6160/6500 [18:37:04<59:35, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711496591567993, 'eval_runtime': 4.2025, 'eval_samples_per_second': 5.473, 'eval_steps_per_second': 1.428, 'epoch': 0.95}
                                                       95%|| 6160/6500 [18:37:08<59:35, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6160
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.307, 'learning_rate': 6.700502096935347e-07, 'epoch': 0.95}
{'loss': 0.3212, 'learning_rate': 6.661117295564623e-07, 'epoch': 0.95}
{'loss': 0.3088, 'learning_rate': 6.621847809158255e-07, 'epoch': 0.95}
{'loss': 0.3178, 'learning_rate': 6.582693646895066e-07, 'epoch': 0.95}
{'loss': 0.2987, 'learning_rate': 6.543654817927292e-07, 'epoch': 0.95}
 95%|| 6161/6500 [18:37:19<1:07:15, 11.91s/it]                                                         95%|| 6161/6500 [18:37:19<1:07:15, 11.91s/it] 95%|| 6162/6500 [18:37:30<1:04:32, 11.46s/it]                                                         95%|| 6162/6500 [18:37:30<1:04:32, 11.46s/it] 95%|| 6163/6500 [18:37:40<1:02:36, 11.15s/it]                                                         95%|| 6163/6500 [18:37:40<1:02:36, 11.15s/it] 95%|| 6164/6500 [18:37:50<1:01:10, 10.92s/it]                                                         95%|| 6164/6500 [18:37:50<1:01:10, 10.92s/it] 95%|| 6165/6500 [18:38:01<1:00:06, 10.77s/it]                                                         95%|| 616{'loss': 0.3323, 'learning_rate': 6.50473133137991e-07, 'epoch': 0.95}
{'loss': 0.3134, 'learning_rate': 6.465923196351087e-07, 'epoch': 0.95}
{'loss': 0.3052, 'learning_rate': 6.427230421911956e-07, 'epoch': 0.95}
{'loss': 0.2993, 'learning_rate': 6.388653017106838e-07, 'epoch': 0.95}
{'loss': 0.32, 'learning_rate': 6.350190990952743e-07, 'epoch': 0.95}
5/6500 [18:38:01<1:00:06, 10.77s/it] 95%|| 6166/6500 [18:38:11<59:19, 10.66s/it]                                                         95%|| 6166/6500 [18:38:11<59:19, 10.66s/it] 95%|| 6167/6500 [18:38:22<58:43, 10.58s/it]                                                       95%|| 6167/6500 [18:38:22<58:43, 10.58s/it] 95%|| 6168/6500 [18:38:32<58:16, 10.53s/it]                                                       95%|| 6168/6500 [18:38:32<58:16, 10.53s/it] 95%|| 6169/6500 [18:38:42<57:53, 10.49s/it]                                                       95%|| 6169/6500 [18:38:42<57:53, 10.49s/it] 95%|| 6170/6500 [18:38:53<57:35, 10.47s/it]                                                       95%|| 6170/6500 [18:38:53<57:35, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8705976009368896, 'eval_runtime': 3.9648, 'eval_samples_per_second': 5.801, 'eval_steps_per_second': 1.513, 'epoch': 0.95}
                                                       95%|| 6170/6500 [18:38:57<57:35, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6170
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3734, 'learning_rate': 6.311844352440144e-07, 'epoch': 0.95}
{'loss': 0.3071, 'learning_rate': 6.273613110532206e-07, 'epoch': 0.95}
{'loss': 0.2958, 'learning_rate': 6.235497274165337e-07, 'epoch': 0.95}
{'loss': 0.3289, 'learning_rate': 6.197496852248796e-07, 'epoch': 0.95}
{'loss': 0.8169, 'learning_rate': 6.159611853665037e-07, 'epoch': 0.95}
 95%|| 6171/6500 [18:39:08<1:04:41, 11.80s/it]                                                         95%|| 6171/6500 [18:39:08<1:04:41, 11.80s/it] 95%|| 6172/6500 [18:39:18<1:02:12, 11.38s/it]                                                         95%|| 6172/6500 [18:39:18<1:02:12, 11.38s/it] 95%|| 6173/6500 [18:39:29<1:00:24, 11.08s/it]                                                         95%|| 6173/6500 [18:39:29<1:00:24, 11.08s/it] 95%|| 6174/6500 [18:39:39<59:28, 10.95s/it]                                                         95%|| 6174/6500 [18:39:39<59:28, 10.95s/it] 95%|| 6175/6500 [18:39:50<58:22, 10.78s/it]                                                       95%|| 6175/6500 [{'loss': 0.3322, 'learning_rate': 6.121842287269419e-07, 'epoch': 0.95}
{'loss': 0.3225, 'learning_rate': 6.084188161890325e-07, 'epoch': 0.95}
{'loss': 0.3047, 'learning_rate': 6.046649486329158e-07, 'epoch': 0.95}
{'loss': 0.3056, 'learning_rate': 6.009226269360402e-07, 'epoch': 0.95}
{'loss': 0.3262, 'learning_rate': 5.971918519731557e-07, 'epoch': 0.95}
18:39:50<58:22, 10.78s/it] 95%|| 6176/6500 [18:40:00<57:34, 10.66s/it]                                                       95%|| 6176/6500 [18:40:00<57:34, 10.66s/it] 95%|| 6177/6500 [18:40:10<56:58, 10.58s/it]                                                       95%|| 6177/6500 [18:40:10<56:58, 10.58s/it] 95%|| 6178/6500 [18:40:21<56:28, 10.52s/it]                                                       95%|| 6178/6500 [18:40:21<56:28, 10.52s/it] 95%|| 6179/6500 [18:40:31<56:05, 10.49s/it]                                                       95%|| 6179/6500 [18:40:31<56:05, 10.49s/it] 95%|| 6180/6500 [18:40:42<55:47, 10.46s/it]                                                       95%|| 6180/6500 [18:40:42<55:47, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718439936637878, 'eval_runtime': 3.9666, 'eval_samples_per_second': 5.798, 'eval_steps_per_second': 1.513, 'epoch': 0.95}
                                                       95%|| 6180/6500 [18:40:45<55:47, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3039, 'learning_rate': 5.934726246162925e-07, 'epoch': 0.95}
{'loss': 0.3048, 'learning_rate': 5.897649457348109e-07, 'epoch': 0.95}
{'loss': 0.3149, 'learning_rate': 5.860688161953564e-07, 'epoch': 0.95}
{'loss': 0.309, 'learning_rate': 5.823842368618715e-07, 'epoch': 0.95}
{'loss': 0.3079, 'learning_rate': 5.787112085956059e-07, 'epoch': 0.95}
 95%|| 6181/6500 [18:40:56<1:02:48, 11.81s/it]                                                         95%|| 6181/6500 [18:40:56<1:02:48, 11.81s/it] 95%|| 6182/6500 [18:41:07<1:00:23, 11.40s/it]                                                         95%|| 6182/6500 [18:41:07<1:00:23, 11.40s/it] 95%|| 6183/6500 [18:41:17<58:35, 11.09s/it]                                                         95%|| 6183/6500 [18:41:17<58:35, 11.09s/it] 95%|| 6184/6500 [18:41:28<57:18, 10.88s/it]                                                       95%|| 6184/6500 [18:41:28<57:18, 10.88s/it] 95%|| 6185/6500 [18:41:38<56:20, 10.73s/it]                                                       95%|| 6185/6500 [18:41:{'loss': 0.3067, 'learning_rate': 5.750497322551118e-07, 'epoch': 0.95}
{'loss': 0.3183, 'learning_rate': 5.713998086962325e-07, 'epoch': 0.95}
{'loss': 0.3115, 'learning_rate': 5.677614387721131e-07, 'epoch': 0.95}
{'loss': 0.3248, 'learning_rate': 5.641346233332123e-07, 'epoch': 0.95}
{'loss': 0.3215, 'learning_rate': 5.605193632272632e-07, 'epoch': 0.95}
38<56:20, 10.73s/it] 95%|| 6186/6500 [18:41:48<55:36, 10.63s/it]                                                       95%|| 6186/6500 [18:41:48<55:36, 10.63s/it] 95%|| 6187/6500 [18:41:59<55:02, 10.55s/it]                                                       95%|| 6187/6500 [18:41:59<55:02, 10.55s/it] 95%|| 6188/6500 [18:42:09<54:37, 10.50s/it]                                                       95%|| 6188/6500 [18:42:09<54:37, 10.50s/it] 95%|| 6189/6500 [18:42:20<54:16, 10.47s/it]                                                       95%|| 6189/6500 [18:42:20<54:16, 10.47s/it] 95%|| 6190/6500 [18:42:31<54:58, 10.64s/it]                                                       95%|| 6190/6500 [18:42:31<54:58, 10.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8714290261268616, 'eval_runtime': 3.9681, 'eval_samples_per_second': 5.796, 'eval_steps_per_second': 1.512, 'epoch': 0.95}
                                                       95%|| 6190/6500 [18:42:35<54:58, 10.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6190/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3089, 'learning_rate': 5.569156592993175e-07, 'epoch': 0.95}
{'loss': 0.326, 'learning_rate': 5.533235123917235e-07, 'epoch': 0.95}
{'loss': 0.3176, 'learning_rate': 5.497429233441098e-07, 'epoch': 0.95}
{'loss': 0.3196, 'learning_rate': 5.46173892993429e-07, 'epoch': 0.95}
{'loss': 0.3114, 'learning_rate': 5.426164221739138e-07, 'epoch': 0.95}
 95%|| 6191/6500 [18:42:46<1:01:30, 11.94s/it]                                                         95%|| 6191/6500 [18:42:46<1:01:30, 11.94s/it] 95%|| 6192/6500 [18:42:56<58:57, 11.49s/it]                                                         95%|| 6192/6500 [18:42:56<58:57, 11.49s/it] 95%|| 6193/6500 [18:43:06<57:06, 11.16s/it]                                                       95%|| 6193/6500 [18:43:06<57:06, 11.16s/it] 95%|| 6194/6500 [18:43:17<55:44, 10.93s/it]                                                       95%|| 6194/6500 [18:43:17<55:44, 10.93s/it] 95%|| 6195/6500 [18:43:27<54:45, 10.77s/it]                                                       95%|| 6195/6500 [18:43:27<54:{'loss': 0.3208, 'learning_rate': 5.390705117171047e-07, 'epoch': 0.95}
{'loss': 0.3015, 'learning_rate': 5.355361624518329e-07, 'epoch': 0.95}
{'loss': 0.3083, 'learning_rate': 5.320133752042378e-07, 'epoch': 0.95}
{'loss': 0.2907, 'learning_rate': 5.285021507977495e-07, 'epoch': 0.95}
{'loss': 0.341, 'learning_rate': 5.250024900530893e-07, 'epoch': 0.95}
45, 10.77s/it] 95%|| 6196/6500 [18:43:38<54:00, 10.66s/it]                                                       95%|| 6196/6500 [18:43:38<54:00, 10.66s/it] 95%|| 6197/6500 [18:43:48<53:26, 10.58s/it]                                                       95%|| 6197/6500 [18:43:48<53:26, 10.58s/it] 95%|| 6198/6500 [18:43:58<53:00, 10.53s/it]                                                       95%|| 6198/6500 [18:43:58<53:00, 10.53s/it] 95%|| 6199/6500 [18:44:10<54:16, 10.82s/it]                                                       95%|| 6199/6500 [18:44:10<54:16, 10.82s/it] 95%|| 6200/6500 [18:44:21<54:06, 10.82s/it]                                                       95%|| 6200/6500 [18:44:21<54:06, 10.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.871863842010498, 'eval_runtime': 4.1987, 'eval_samples_per_second': 5.478, 'eval_steps_per_second': 1.429, 'epoch': 0.95}
                                                       95%|| 6200/6500 [18:44:25<54:06, 10.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6200/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3608, 'learning_rate': 5.215143937882805e-07, 'epoch': 0.95}
{'loss': 0.3082, 'learning_rate': 5.180378628186488e-07, 'epoch': 0.95}
{'loss': 0.3208, 'learning_rate': 5.145728979568165e-07, 'epoch': 0.95}
{'loss': 0.3229, 'learning_rate': 5.111195000126912e-07, 'epoch': 0.95}
{'loss': 0.8392, 'learning_rate': 5.076776697934826e-07, 'epoch': 0.95}
 95%|| 6201/6500 [18:44:36<1:00:22, 12.12s/it]                                                         95%|| 6201/6500 [18:44:36<1:00:22, 12.12s/it] 95%|| 6202/6500 [18:44:46<57:29, 11.58s/it]                                                         95%|| 6202/6500 [18:44:46<57:29, 11.58s/it] 95%|| 6203/6500 [18:44:57<55:26, 11.20s/it]                                                       95%|| 6203/6500 [18:44:57<55:26, 11.20s/it] 95%|| 6204/6500 [18:45:07<53:57, 10.94s/it]                                                       95%|| 6204/6500 [18:45:07<53:57, 10.94s/it] 95%|| 6205/6500 [18:45:17<52:51, 10.75s/it]                                                       95%|| 6205/6500 [18:45:17<52:{'loss': 0.3179, 'learning_rate': 5.04247408103703e-07, 'epoch': 0.95}
{'loss': 0.3195, 'learning_rate': 5.008287157451496e-07, 'epoch': 0.95}
{'loss': 0.2895, 'learning_rate': 4.97421593516928e-07, 'epoch': 0.96}
{'loss': 0.3186, 'learning_rate': 4.94026042215423e-07, 'epoch': 0.96}
{'loss': 0.3113, 'learning_rate': 4.906420626343334e-07, 'epoch': 0.96}
51, 10.75s/it] 95%|| 6206/6500 [18:45:28<52:42, 10.76s/it]                                                       95%|| 6206/6500 [18:45:28<52:42, 10.76s/it] 95%|| 6207/6500 [18:45:38<51:53, 10.63s/it]                                                       95%|| 6207/6500 [18:45:38<51:53, 10.63s/it] 96%|| 6208/6500 [18:45:49<51:15, 10.53s/it]                                                       96%|| 6208/6500 [18:45:49<51:15, 10.53s/it] 96%|| 6209/6500 [18:45:59<50:47, 10.47s/it]                                                       96%|| 6209/6500 [18:45:59<50:47, 10.47s/it] 96%|| 6210/6500 [18:46:09<50:26, 10.44s/it]                                                       96%|| 6210/6500 [18:46:09<50:26, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.871062159538269, 'eval_runtime': 3.9602, 'eval_samples_per_second': 5.808, 'eval_steps_per_second': 1.515, 'epoch': 0.96}
                                                       96%|| 6210/6500 [18:46:13<50:26, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6210
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2793, 'learning_rate': 4.872696555646373e-07, 'epoch': 0.96}
{'loss': 0.3137, 'learning_rate': 4.839088217946208e-07, 'epoch': 0.96}
{'loss': 0.2968, 'learning_rate': 4.805595621098502e-07, 'epoch': 0.96}
{'loss': 0.3124, 'learning_rate': 4.772218772932047e-07, 'epoch': 0.96}
{'loss': 0.3104, 'learning_rate': 4.738957681248379e-07, 'epoch': 0.96}
 96%|| 6211/6500 [18:46:24<56:40, 11.77s/it]                                                       96%|| 6211/6500 [18:46:24<56:40, 11.77s/it] 96%|| 6212/6500 [18:46:34<54:25, 11.34s/it]                                                       96%|| 6212/6500 [18:46:34<54:25, 11.34s/it] 96%|| 6213/6500 [18:46:45<52:46, 11.03s/it]                                                       96%|| 6213/6500 [18:46:45<52:46, 11.03s/it] 96%|| 6214/6500 [18:46:55<51:38, 10.83s/it]                                                       96%|| 6214/6500 [18:46:55<51:38, 10.83s/it] 96%|| 6215/6500 [18:47:05<50:42, 10.68s/it]                                                       96%|| 6215/6500 [18:47:05<50:42, 10.6{'loss': 0.3159, 'learning_rate': 4.7058123538221144e-07, 'epoch': 0.96}
{'loss': 0.3066, 'learning_rate': 4.672782798400777e-07, 'epoch': 0.96}
{'loss': 0.3131, 'learning_rate': 4.639869022704801e-07, 'epoch': 0.96}
{'loss': 0.3281, 'learning_rate': 4.607071034427646e-07, 'epoch': 0.96}
{'loss': 0.3008, 'learning_rate': 4.574388841235566e-07, 'epoch': 0.96}
8s/it] 96%|| 6216/6500 [18:47:16<50:01, 10.57s/it]                                                       96%|| 6216/6500 [18:47:16<50:01, 10.57s/it] 96%|| 6217/6500 [18:47:26<49:29, 10.49s/it]                                                       96%|| 6217/6500 [18:47:26<49:29, 10.49s/it] 96%|| 6218/6500 [18:47:36<49:03, 10.44s/it]                                                       96%|| 6218/6500 [18:47:36<49:03, 10.44s/it] 96%|| 6219/6500 [18:47:47<48:45, 10.41s/it]                                                       96%|| 6219/6500 [18:47:47<48:45, 10.41s/it] 96%|| 6220/6500 [18:47:57<48:29, 10.39s/it]                                                       96%|| 6220/6500 [18:47:57<48:29, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8722875118255615, 'eval_runtime': 3.9443, 'eval_samples_per_second': 5.831, 'eval_steps_per_second': 1.521, 'epoch': 0.96}
                                                       96%|| 6220/6500 [18:48:01<48:29, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6220I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.318, 'learning_rate': 4.541822450767896e-07, 'epoch': 0.96}
{'loss': 0.3116, 'learning_rate': 4.5093718706367136e-07, 'epoch': 0.96}
{'loss': 0.3055, 'learning_rate': 4.4770371084272846e-07, 'epoch': 0.96}
{'loss': 0.3145, 'learning_rate': 4.444818171697618e-07, 'epoch': 0.96}
{'loss': 0.3244, 'learning_rate': 4.41271506797869e-07, 'epoch': 0.96}
 96%|| 6221/6500 [18:48:12<54:38, 11.75s/it]                                                       96%|| 6221/6500 [18:48:12<54:38, 11.75s/it] 96%|| 6222/6500 [18:48:23<53:40, 11.58s/it]                                                       96%|| 6222/6500 [18:48:23<53:40, 11.58s/it] 96%|| 6223/6500 [18:48:34<51:48, 11.22s/it]                                                       96%|| 6223/6500 [18:48:34<51:48, 11.22s/it] 96%|| 6224/6500 [18:48:44<50:28, 10.97s/it]                                                       96%|| 6224/6500 [18:48:44<50:28, 10.97s/it] 96%|| 6225/6500 [18:48:54<49:30, 10.80s/it]                                                       96%|| 6225/6500 [18:48:54<49:30, 10.8{'loss': 0.3115, 'learning_rate': 4.3807278047743317e-07, 'epoch': 0.96}
{'loss': 0.3056, 'learning_rate': 4.348856389561451e-07, 'epoch': 0.96}
{'loss': 0.2995, 'learning_rate': 4.3171008297898107e-07, 'epoch': 0.96}
{'loss': 0.3125, 'learning_rate': 4.2854611328820295e-07, 'epoch': 0.96}
{'loss': 0.3809, 'learning_rate': 4.253937306233691e-07, 'epoch': 0.96}
0s/it] 96%|| 6226/6500 [18:49:05<48:44, 10.67s/it]                                                       96%|| 6226/6500 [18:49:05<48:44, 10.67s/it] 96%|| 6227/6500 [18:49:15<48:10, 10.59s/it]                                                       96%|| 6227/6500 [18:49:15<48:10, 10.59s/it] 96%|| 6228/6500 [18:49:26<47:43, 10.53s/it]                                                       96%|| 6228/6500 [18:49:26<47:43, 10.53s/it] 96%|| 6229/6500 [18:49:36<47:22, 10.49s/it]                                                       96%|| 6229/6500 [18:49:36<47:22, 10.49s/it] 96%|| 6230/6500 [18:49:49<50:26, 11.21s/it]                                                       96%|| 6230/6500 [18:49:49<50:26, 11.21s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8715983629226685, 'eval_runtime': 3.9783, 'eval_samples_per_second': 5.781, 'eval_steps_per_second': 1.508, 'epoch': 0.96}
                                                       96%|| 6230/6500 [18:49:53<50:26, 11.21s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3074, 'learning_rate': 4.222529357213345e-07, 'epoch': 0.96}
{'loss': 0.2968, 'learning_rate': 4.191237293162398e-07, 'epoch': 0.96}
{'loss': 0.3244, 'learning_rate': 4.1600611213951645e-07, 'epoch': 0.96}
{'loss': 0.8119, 'learning_rate': 4.129000849198872e-07, 'epoch': 0.96}
{'loss': 0.319, 'learning_rate': 4.0980564838336566e-07, 'epoch': 0.96}
 96%|| 6231/6500 [18:50:04<55:58, 12.49s/it]                                                       96%|| 6231/6500 [18:50:04<55:58, 12.49s/it] 96%|| 6232/6500 [18:50:15<52:59, 11.86s/it]                                                       96%|| 6232/6500 [18:50:15<52:59, 11.86s/it] 96%|| 6233/6500 [18:50:25<50:50, 11.43s/it]                                                       96%|| 6233/6500 [18:50:25<50:50, 11.43s/it] 96%|| 6234/6500 [18:50:36<49:15, 11.11s/it]                                                       96%|| 6234/6500 [18:50:36<49:15, 11.11s/it] 96%|| 6235/6500 [18:50:46<48:07, 10.90s/it]                                                       96%|| 6235/6500 [18:50:46<48:07, 10.9{'loss': 0.3083, 'learning_rate': 4.067228032532677e-07, 'epoch': 0.96}
{'loss': 0.3215, 'learning_rate': 4.0365155025017807e-07, 'epoch': 0.96}
{'loss': 0.3003, 'learning_rate': 4.0059189009198364e-07, 'epoch': 0.96}
{'loss': 0.3271, 'learning_rate': 3.975438234938733e-07, 'epoch': 0.96}
{'loss': 0.3017, 'learning_rate': 3.9450735116830505e-07, 'epoch': 0.96}
0s/it] 96%|| 6236/6500 [18:50:56<47:19, 10.75s/it]                                                       96%|| 6236/6500 [18:50:56<47:19, 10.75s/it] 96%|| 6237/6500 [18:51:07<46:41, 10.65s/it]                                                       96%|| 6237/6500 [18:51:07<46:41, 10.65s/it] 96%|| 6238/6500 [18:51:18<46:40, 10.69s/it]                                                       96%|| 6238/6500 [18:51:18<46:40, 10.69s/it] 96%|| 6239/6500 [18:51:28<46:10, 10.61s/it]                                                       96%|| 6239/6500 [18:51:28<46:10, 10.61s/it] 96%|| 6240/6500 [18:51:38<45:42, 10.55s/it]                                                       96%|| 6240/6500 [18:51:38<45:42, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8722144365310669, 'eval_runtime': 3.9547, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.96}
                                                       96%|| 6240/6500 [18:51:42<45:42, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.2944, 'learning_rate': 3.914824738250333e-07, 'epoch': 0.96}
{'loss': 0.3068, 'learning_rate': 3.884691921711092e-07, 'epoch': 0.96}
{'loss': 0.2982, 'learning_rate': 3.854675069108693e-07, 'epoch': 0.96}
{'loss': 0.3135, 'learning_rate': 3.8247741874594123e-07, 'epoch': 0.96}
{'loss': 0.3036, 'learning_rate': 3.7949892837523814e-07, 'epoch': 0.96}
 96%|| 6241/6500 [18:51:53<51:21, 11.90s/it]                                                       96%|| 6241/6500 [18:51:53<51:21, 11.90s/it] 96%|| 6242/6500 [18:52:04<49:21, 11.48s/it]                                                       96%|| 6242/6500 [18:52:04<49:21, 11.48s/it] 96%|| 6243/6500 [18:52:14<47:47, 11.16s/it]                                                       96%|| 6243/6500 [18:52:14<47:47, 11.16s/it] 96%|| 6244/6500 [18:52:25<46:38, 10.93s/it]                                                       96%|| 6244/6500 [18:52:25<46:38, 10.93s/it] 96%|| 6245/6500 [18:52:35<45:47, 10.78s/it]                                                       96%|| 6245/6500 [18:52:35<45:47, 10.7{'loss': 0.3236, 'learning_rate': 3.765320364949587e-07, 'epoch': 0.96}
{'loss': 0.3089, 'learning_rate': 3.735767437985982e-07, 'epoch': 0.96}
{'loss': 0.3212, 'learning_rate': 3.706330509769429e-07, 'epoch': 0.96}
{'loss': 0.3111, 'learning_rate': 3.677009587180591e-07, 'epoch': 0.96}
{'loss': 0.3096, 'learning_rate': 3.647804677073097e-07, 'epoch': 0.96}
8s/it] 96%|| 6246/6500 [18:52:46<45:08, 10.66s/it]                                                       96%|| 6246/6500 [18:52:46<45:08, 10.66s/it] 96%|| 6247/6500 [18:52:56<44:38, 10.59s/it]                                                       96%|| 6247/6500 [18:52:56<44:38, 10.59s/it] 96%|| 6248/6500 [18:53:06<44:14, 10.53s/it]                                                       96%|| 6248/6500 [18:53:06<44:14, 10.53s/it] 96%|| 6249/6500 [18:53:17<43:54, 10.50s/it]                                                       96%|| 6249/6500 [18:53:17<43:54, 10.50s/it] 96%|| 6250/6500 [18:53:27<43:38, 10.47s/it]                                                       96%|| 6250/6500 [18:53:27<43:38, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8720190525054932, 'eval_runtime': 3.9516, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.96}
                                                       96%|| 6250/6500 [18:53:31<43:38, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3189, 'learning_rate': 3.6187157862733743e-07, 'epoch': 0.96}
{'loss': 0.3172, 'learning_rate': 3.589742921580763e-07, 'epoch': 0.96}
{'loss': 0.3121, 'learning_rate': 3.5608860897675677e-07, 'epoch': 0.96}
{'loss': 0.3183, 'learning_rate': 3.532145297578837e-07, 'epoch': 0.96}
{'loss': 0.3226, 'learning_rate': 3.5035205517325307e-07, 'epoch': 0.96}
 96%|| 6251/6500 [18:53:42<49:01, 11.81s/it]                                                       96%|| 6251/6500 [18:53:42<49:01, 11.81s/it] 96%|| 6252/6500 [18:53:53<47:06, 11.40s/it]                                                       96%|| 6252/6500 [18:53:53<47:06, 11.40s/it] 96%|| 6253/6500 [18:54:03<45:42, 11.11s/it]                                                       96%|| 6253/6500 [18:54:03<45:42, 11.11s/it] 96%|| 6254/6500 [18:54:13<44:40, 10.90s/it]                                                       96%|| 6254/6500 [18:54:13<44:40, 10.90s/it] 96%|| 6255/6500 [18:54:24<44:12, 10.82s/it]                                                       96%|| 6255/6500 [18:54:24<44:12, 10.8{'loss': 0.3028, 'learning_rate': 3.4750118589196303e-07, 'epoch': 0.96}
{'loss': 0.2992, 'learning_rate': 3.4466192258036935e-07, 'epoch': 0.96}
{'loss': 0.2859, 'learning_rate': 3.4183426590214673e-07, 'epoch': 0.96}
{'loss': 0.328, 'learning_rate': 3.3901821651823875e-07, 'epoch': 0.96}
{'loss': 0.3613, 'learning_rate': 3.3621377508687435e-07, 'epoch': 0.96}
2s/it] 96%|| 6256/6500 [18:54:34<43:31, 10.70s/it]                                                       96%|| 6256/6500 [18:54:34<43:31, 10.70s/it] 96%|| 6257/6500 [18:54:45<43:00, 10.62s/it]                                                       96%|| 6257/6500 [18:54:45<43:00, 10.62s/it] 96%|| 6258/6500 [18:54:55<42:35, 10.56s/it]                                                       96%|| 6258/6500 [18:54:55<42:35, 10.56s/it] 96%|| 6259/6500 [18:55:06<42:15, 10.52s/it]                                                       96%|| 6259/6500 [18:55:06<42:15, 10.52s/it] 96%|| 6260/6500 [18:55:16<42:04, 10.52s/it]                                                       96%|| 6260/6500 [18:55:16<42:04, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8717659115791321, 'eval_runtime': 5.3918, 'eval_samples_per_second': 4.266, 'eval_steps_per_second': 1.113, 'epoch': 0.96}
                                                       96%|| 6260/6500 [18:55:22<42:04, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3037, 'learning_rate': 3.334209422635848e-07, 'epoch': 0.96}
{'loss': 0.2978, 'learning_rate': 3.3063971870117005e-07, 'epoch': 0.96}
{'loss': 0.3224, 'learning_rate': 3.2787010504972125e-07, 'epoch': 0.96}
{'loss': 0.8256, 'learning_rate': 3.251121019566317e-07, 'epoch': 0.96}
{'loss': 0.3235, 'learning_rate': 3.223657100665578e-07, 'epoch': 0.96}
 96%|| 6261/6500 [18:55:33<49:01, 12.31s/it]                                                       96%|| 6261/6500 [18:55:33<49:01, 12.31s/it] 96%|| 6262/6500 [18:55:43<46:34, 11.74s/it]                                                       96%|| 6262/6500 [18:55:43<46:34, 11.74s/it] 96%|| 6263/6500 [18:55:54<44:47, 11.34s/it]                                                       96%|| 6263/6500 [18:55:54<44:47, 11.34s/it] 96%|| 6264/6500 [18:56:04<43:27, 11.05s/it]                                                       96%|| 6264/6500 [18:56:04<43:27, 11.05s/it] 96%|| 6265/6500 [18:56:14<42:32, 10.86s/it]                                                       96%|| 6265/6500 [18:56:14<42:32, 10.8{'loss': 0.3217, 'learning_rate': 3.1963093002145285e-07, 'epoch': 0.96}
{'loss': 0.2906, 'learning_rate': 3.169077624605554e-07, 'epoch': 0.96}
{'loss': 0.3072, 'learning_rate': 3.1419620802038973e-07, 'epoch': 0.96}
{'loss': 0.3329, 'learning_rate': 3.11496267334771e-07, 'epoch': 0.96}
{'loss': 0.2862, 'learning_rate': 3.0880794103478327e-07, 'epoch': 0.96}
6s/it] 96%|| 6266/6500 [18:56:25<41:51, 10.73s/it]                                                       96%|| 6266/6500 [18:56:25<41:51, 10.73s/it] 96%|| 6267/6500 [18:56:35<41:17, 10.63s/it]                                                       96%|| 6267/6500 [18:56:35<41:17, 10.63s/it] 96%|| 6268/6500 [18:56:46<40:52, 10.57s/it]                                                       96%|| 6268/6500 [18:56:46<40:52, 10.57s/it] 96%|| 6269/6500 [18:56:56<40:37, 10.55s/it]                                                       96%|| 6269/6500 [18:56:56<40:37, 10.55s/it] 96%|| 6270/6500 [18:57:07<40:17, 10.51s/it]                                                       96%|| 6270/6500 [18:57:07<40:17, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8723945021629333, 'eval_runtime': 4.0132, 'eval_samples_per_second': 5.731, 'eval_steps_per_second': 1.495, 'epoch': 0.96}
                                                       96%|| 6270/6500 [18:57:11<40:17, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6270/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3167, 'learning_rate': 3.061312297488128e-07, 'epoch': 0.96}
{'loss': 0.3042, 'learning_rate': 3.034661341025258e-07, 'epoch': 0.96}
{'loss': 0.3135, 'learning_rate': 3.0081265471886834e-07, 'epoch': 0.97}
{'loss': 0.3159, 'learning_rate': 2.981707922180776e-07, 'epoch': 0.97}
{'loss': 0.3158, 'learning_rate': 2.955405472176709e-07, 'epoch': 0.97}
 96%|| 6271/6500 [18:57:22<45:45, 11.99s/it]                                                       96%|| 6271/6500 [18:57:22<45:45, 11.99s/it] 96%|| 6272/6500 [18:57:32<43:46, 11.52s/it]                                                       96%|| 6272/6500 [18:57:32<43:46, 11.52s/it] 97%|| 6273/6500 [18:57:43<42:20, 11.19s/it]                                                       97%|| 6273/6500 [18:57:43<42:20, 11.19s/it] 97%|| 6274/6500 [18:57:53<41:17, 10.96s/it]                                                       97%|| 6274/6500 [18:57:53<41:17, 10.96s/it] 97%|| 6275/6500 [18:58:04<40:29, 10.80s/it]                                                       97%|| 6275/6500 [18:58:04<40:29, 10.8{'loss': 0.3236, 'learning_rate': 2.9292192033245623e-07, 'epoch': 0.97}
{'loss': 0.3079, 'learning_rate': 2.9031491217451636e-07, 'epoch': 0.97}
{'loss': 0.3417, 'learning_rate': 2.877195233532248e-07, 'epoch': 0.97}
{'loss': 0.3093, 'learning_rate': 2.8513575447524644e-07, 'epoch': 0.97}
{'loss': 0.3083, 'learning_rate': 2.825636061445092e-07, 'epoch': 0.97}
0s/it] 97%|| 6276/6500 [18:58:14<39:53, 10.68s/it]                                                       97%|| 6276/6500 [18:58:14<39:53, 10.68s/it] 97%|| 6277/6500 [18:58:25<39:25, 10.61s/it]                                                       97%|| 6277/6500 [18:58:25<39:25, 10.61s/it] 97%|| 6278/6500 [18:58:35<39:02, 10.55s/it]                                                       97%|| 6278/6500 [18:58:35<39:02, 10.55s/it] 97%|| 6279/6500 [18:58:45<38:43, 10.51s/it]                                                       97%|| 6279/6500 [18:58:45<38:43, 10.51s/it] 97%|| 6280/6500 [18:58:56<38:26, 10.48s/it]                                                       97%|| 6280/6500 [18:58:56<38:26, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8720155954360962, 'eval_runtime': 3.9514, 'eval_samples_per_second': 5.821, 'eval_steps_per_second': 1.518, 'epoch': 0.97}
                                                       97%|| 6280/6500 [18:59:00<38:26, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6280/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3341, 'learning_rate': 2.8000307896223765e-07, 'epoch': 0.97}
{'loss': 0.3144, 'learning_rate': 2.774541735269476e-07, 'epoch': 0.97}
{'loss': 0.3262, 'learning_rate': 2.7491689043442903e-07, 'epoch': 0.97}
{'loss': 0.3053, 'learning_rate': 2.7239123027775204e-07, 'epoch': 0.97}
{'loss': 0.328, 'learning_rate': 2.6987719364727215e-07, 'epoch': 0.97}
 97%|| 6281/6500 [18:59:11<43:14, 11.85s/it]                                                       97%|| 6281/6500 [18:59:11<43:14, 11.85s/it] 97%|| 6282/6500 [18:59:21<41:29, 11.42s/it]                                                       97%|| 6282/6500 [18:59:21<41:29, 11.42s/it] 97%|| 6283/6500 [18:59:32<40:12, 11.12s/it]                                                       97%|| 6283/6500 [18:59:32<40:12, 11.12s/it] 97%|| 6284/6500 [18:59:42<39:15, 10.90s/it]                                                       97%|| 6284/6500 [18:59:42<39:15, 10.90s/it] 97%|| 6285/6500 [18:59:52<38:33, 10.76s/it]                                                       97%|| 6285/6500 [18:59:52<38:33, 10.7{'loss': 0.2984, 'learning_rate': 2.6737478113063596e-07, 'epoch': 0.97}
{'loss': 0.305, 'learning_rate': 2.6488399331277004e-07, 'epoch': 0.97}
{'loss': 0.2932, 'learning_rate': 2.6240483077586976e-07, 'epoch': 0.97}
{'loss': 0.3795, 'learning_rate': 2.599372940994327e-07, 'epoch': 0.97}
{'loss': 0.3168, 'learning_rate': 2.574813838602308e-07, 'epoch': 0.97}
6s/it] 97%|| 6286/6500 [19:00:03<38:00, 10.65s/it]                                                       97%|| 6286/6500 [19:00:03<38:00, 10.65s/it] 97%|| 6287/6500 [19:00:14<37:50, 10.66s/it]                                                       97%|| 6287/6500 [19:00:14<37:50, 10.66s/it] 97%|| 6288/6500 [19:00:24<37:24, 10.59s/it]                                                       97%|| 6288/6500 [19:00:24<37:24, 10.59s/it] 97%|| 6289/6500 [19:00:34<37:03, 10.54s/it]                                                       97%|| 6289/6500 [19:00:34<37:03, 10.54s/it] 97%|| 6290/6500 [19:00:45<36:45, 10.50s/it]                                                       97%|| 6290/6500 [19:00:45<36:45, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8725243210792542, 'eval_runtime': 4.343, 'eval_samples_per_second': 5.296, 'eval_steps_per_second': 1.382, 'epoch': 0.97}
                                                       97%|| 6290/6500 [19:00:49<36:45, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3032, 'learning_rate': 2.55037100632316e-07, 'epoch': 0.97}
{'loss': 0.3188, 'learning_rate': 2.5260444498702573e-07, 'epoch': 0.97}
{'loss': 0.7407, 'learning_rate': 2.501834174929718e-07, 'epoch': 0.97}
{'loss': 0.4026, 'learning_rate': 2.4777401871606267e-07, 'epoch': 0.97}
{'loss': 0.3192, 'learning_rate': 2.453762492194811e-07, 'epoch': 0.97}
 97%|| 6291/6500 [19:01:00<41:44, 11.98s/it]                                                       97%|| 6291/6500 [19:01:00<41:44, 11.98s/it] 97%|| 6292/6500 [19:01:11<39:53, 11.51s/it]                                                       97%|| 6292/6500 [19:01:11<39:53, 11.51s/it] 97%|| 6293/6500 [19:01:21<38:31, 11.17s/it]                                                       97%|| 6293/6500 [19:01:21<38:31, 11.17s/it] 97%|| 6294/6500 [19:01:31<37:32, 10.94s/it]                                                       97%|| 6294/6500 [19:01:31<37:32, 10.94s/it] 97%|| 6295/6500 [19:01:42<36:45, 10.76s/it]                                                       97%|| 6295/6500 [19:01:42<36:45, 10.7{'loss': 0.3191, 'learning_rate': 2.429901095636844e-07, 'epoch': 0.97}
{'loss': 0.2922, 'learning_rate': 2.4061560030642636e-07, 'epoch': 0.97}
{'loss': 0.3169, 'learning_rate': 2.382527220027242e-07, 'epoch': 0.97}
{'loss': 0.3135, 'learning_rate': 2.3590147520489713e-07, 'epoch': 0.97}
{'loss': 0.2753, 'learning_rate': 2.3356186046252783e-07, 'epoch': 0.97}
6s/it] 97%|| 6296/6500 [19:01:52<36:12, 10.65s/it]                                                       97%|| 6296/6500 [19:01:52<36:12, 10.65s/it] 97%|| 6297/6500 [19:02:03<35:45, 10.57s/it]                                                       97%|| 6297/6500 [19:02:03<35:45, 10.57s/it] 97%|| 6298/6500 [19:02:13<35:22, 10.51s/it]                                                       97%|| 6298/6500 [19:02:13<35:22, 10.51s/it] 97%|| 6299/6500 [19:02:23<35:04, 10.47s/it]                                                       97%|| 6299/6500 [19:02:23<35:04, 10.47s/it] 97%|| 6300/6500 [19:02:34<34:48, 10.44s/it]                                                       97%|| 6300/6500 [19:02:34<34:48, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8723430037498474, 'eval_runtime': 4.4135, 'eval_samples_per_second': 5.211, 'eval_steps_per_second': 1.359, 'epoch': 0.97}
                                                       97%|| 6300/6500 [19:02:38<34:48, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6300I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6300

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3126, 'learning_rate': 2.3123387832248987e-07, 'epoch': 0.97}
{'loss': 0.2907, 'learning_rate': 2.289175293289314e-07, 'epoch': 0.97}
{'loss': 0.3234, 'learning_rate': 2.2661281402328595e-07, 'epoch': 0.97}
{'loss': 0.307, 'learning_rate': 2.24319732944267e-07, 'epoch': 0.97}
{'loss': 0.3207, 'learning_rate': 2.2203828662787363e-07, 'epoch': 0.97}
 97%|| 6301/6500 [19:02:49<39:29, 11.91s/it]                                                       97%|| 6301/6500 [19:02:49<39:29, 11.91s/it] 97%|| 6302/6500 [19:02:59<37:46, 11.45s/it]                                                       97%|| 6302/6500 [19:02:59<37:46, 11.45s/it] 97%|| 6303/6500 [19:03:10<36:57, 11.26s/it]                                                       97%|| 6303/6500 [19:03:10<36:57, 11.26s/it] 97%|| 6304/6500 [19:03:21<35:54, 10.99s/it]                                                       97%|| 6304/6500 [19:03:21<35:54, 10.99s/it] 97%|| 6305/6500 [19:03:31<35:07, 10.81s/it]                                                       97%|| 6305/6500 [19:03:31<35:07, 10.8{'loss': 0.304, 'learning_rate': 2.1976847560737367e-07, 'epoch': 0.97}
{'loss': 0.3218, 'learning_rate': 2.175103004133261e-07, 'epoch': 0.97}
{'loss': 0.3387, 'learning_rate': 2.152637615735642e-07, 'epoch': 0.97}
{'loss': 0.3222, 'learning_rate': 2.1302885961319575e-07, 'epoch': 0.97}
{'loss': 0.3298, 'learning_rate': 2.1080559505462505e-07, 'epoch': 0.97}
1s/it] 97%|| 6306/6500 [19:03:41<34:30, 10.67s/it]                                                       97%|| 6306/6500 [19:03:41<34:30, 10.67s/it] 97%|| 6307/6500 [19:03:52<34:02, 10.58s/it]                                                       97%|| 6307/6500 [19:03:52<34:02, 10.58s/it] 97%|| 6308/6500 [19:04:02<33:40, 10.52s/it]                                                       97%|| 6308/6500 [19:04:02<33:40, 10.52s/it] 97%|| 6309/6500 [19:04:12<33:21, 10.48s/it]                                                       97%|| 6309/6500 [19:04:12<33:21, 10.48s/it] 97%|| 6310/6500 [19:04:23<33:04, 10.45s/it]                                                       97%|| 6310/6500 [19:04:23<33:04, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8724719285964966, 'eval_runtime': 3.9549, 'eval_samples_per_second': 5.816, 'eval_steps_per_second': 1.517, 'epoch': 0.97}
                                                       97%|| 6310/6500 [19:04:27<33:04, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3209, 'learning_rate': 2.0859396841752531e-07, 'epoch': 0.97}
{'loss': 0.3082, 'learning_rate': 2.0639398021884414e-07, 'epoch': 0.97}
{'loss': 0.313, 'learning_rate': 2.0420563097282573e-07, 'epoch': 0.97}
{'loss': 0.334, 'learning_rate': 2.0202892119097204e-07, 'epoch': 0.97}
{'loss': 0.314, 'learning_rate': 1.9986385138208718e-07, 'epoch': 0.97}
 97%|| 6311/6500 [19:04:38<37:09, 11.80s/it]                                                       97%|| 6311/6500 [19:04:38<37:09, 11.80s/it] 97%|| 6312/6500 [19:04:48<35:37, 11.37s/it]                                                       97%|| 6312/6500 [19:04:48<35:37, 11.37s/it] 97%|| 6313/6500 [19:04:58<34:29, 11.07s/it]                                                       97%|| 6313/6500 [19:04:58<34:29, 11.07s/it] 97%|| 6314/6500 [19:05:09<33:40, 10.86s/it]                                                       97%|| 6314/6500 [19:05:09<33:40, 10.86s/it] 97%|| 6315/6500 [19:05:19<33:03, 10.72s/it]                                                       97%|| 6315/6500 [19:05:19<33:03, 10.7{'loss': 0.3051, 'learning_rate': 1.97710422052233e-07, 'epoch': 0.97}
{'loss': 0.3013, 'learning_rate': 1.9556863370476242e-07, 'epoch': 0.97}
{'loss': 0.3337, 'learning_rate': 1.9343848684031384e-07, 'epoch': 0.97}
{'loss': 0.374, 'learning_rate': 1.9131998195678902e-07, 'epoch': 0.97}
{'loss': 0.3077, 'learning_rate': 1.8921311954937516e-07, 'epoch': 0.97}
2s/it] 97%|| 6316/6500 [19:05:30<32:33, 10.62s/it]                                                       97%|| 6316/6500 [19:05:30<32:33, 10.62s/it] 97%|| 6317/6500 [19:05:40<32:10, 10.55s/it]                                                       97%|| 6317/6500 [19:05:40<32:10, 10.55s/it] 97%|| 6318/6500 [19:05:50<31:51, 10.50s/it]                                                       97%|| 6318/6500 [19:05:50<31:51, 10.50s/it] 97%|| 6319/6500 [19:06:01<31:46, 10.53s/it]                                                       97%|| 6319/6500 [19:06:01<31:46, 10.53s/it] 97%|| 6320/6500 [19:06:11<31:27, 10.49s/it]                                                       97%|| 6320/6500 [19:06:11<31:27, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8715034127235413, 'eval_runtime': 4.3151, 'eval_samples_per_second': 5.33, 'eval_steps_per_second': 1.39, 'epoch': 0.97}
                                                       97%|| 6320/6500 [19:06:16<31:27, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6320
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6320/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3026, 'learning_rate': 1.8711790011053942e-07, 'epoch': 0.97}
{'loss': 0.3239, 'learning_rate': 1.8503432413002898e-07, 'epoch': 0.97}
{'loss': 0.8073, 'learning_rate': 1.8296239209486532e-07, 'epoch': 0.97}
{'loss': 0.3227, 'learning_rate': 1.8090210448934443e-07, 'epoch': 0.97}
{'loss': 0.3115, 'learning_rate': 1.788534617950588e-07, 'epoch': 0.97}
 97%|| 6321/6500 [19:06:27<35:34, 11.92s/it]                                                       97%|| 6321/6500 [19:06:27<35:34, 11.92s/it] 97%|| 6322/6500 [19:06:37<34:01, 11.47s/it]                                                       97%|| 6322/6500 [19:06:37<34:01, 11.47s/it] 97%|| 6323/6500 [19:06:47<32:53, 11.15s/it]                                                       97%|| 6323/6500 [19:06:47<32:53, 11.15s/it] 97%|| 6324/6500 [19:06:58<32:05, 10.94s/it]                                                       97%|| 6324/6500 [19:06:58<32:05, 10.94s/it] 97%|| 6325/6500 [19:07:08<31:26, 10.78s/it]                                                       97%|| 6325/6500 [19:07:08<31:26, 10.7{'loss': 0.3139, 'learning_rate': 1.768164644908532e-07, 'epoch': 0.97}
{'loss': 0.3029, 'learning_rate': 1.7479111305287456e-07, 'epoch': 0.97}
{'loss': 0.3322, 'learning_rate': 1.7277740795452747e-07, 'epoch': 0.97}
{'loss': 0.2964, 'learning_rate': 1.7077534966650766e-07, 'epoch': 0.97}
{'loss': 0.3073, 'learning_rate': 1.6878493865678524e-07, 'epoch': 0.97}
8s/it] 97%|| 6326/6500 [19:07:19<30:55, 10.67s/it]                                                       97%|| 6326/6500 [19:07:19<30:55, 10.67s/it] 97%|| 6327/6500 [19:07:29<30:31, 10.59s/it]                                                       97%|| 6327/6500 [19:07:29<30:31, 10.59s/it] 97%|| 6328/6500 [19:07:39<30:10, 10.53s/it]                                                       97%|| 6328/6500 [19:07:39<30:10, 10.53s/it] 97%|| 6329/6500 [19:07:50<30:20, 10.65s/it]                                                       97%|| 6329/6500 [19:07:50<30:20, 10.65s/it] 97%|| 6330/6500 [19:08:01<29:58, 10.58s/it]                                                       97%|| 6330/6500 [19:08:01<29:58, 10.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8717077374458313, 'eval_runtime': 4.8388, 'eval_samples_per_second': 4.753, 'eval_steps_per_second': 1.24, 'epoch': 0.97}
                                                       97%|| 6330/6500 [19:08:06<29:58, 10.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6330
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3032, 'learning_rate': 1.6680617539059918e-07, 'epoch': 0.97}
{'loss': 0.3028, 'learning_rate': 1.64839060330485e-07, 'epoch': 0.97}
{'loss': 0.3141, 'learning_rate': 1.628835939362361e-07, 'epoch': 0.97}
{'loss': 0.306, 'learning_rate': 1.609397766649312e-07, 'epoch': 0.97}
{'loss': 0.326, 'learning_rate': 1.5900760897092358e-07, 'epoch': 0.97}
 97%|| 6331/6500 [19:08:17<34:26, 12.23s/it]                                                       97%|| 6331/6500 [19:08:17<34:26, 12.23s/it] 97%|| 6332/6500 [19:08:27<32:41, 11.68s/it]                                                       97%|| 6332/6500 [19:08:27<32:41, 11.68s/it] 97%|| 6333/6500 [19:08:38<31:25, 11.29s/it]                                                       97%|| 6333/6500 [19:08:38<31:25, 11.29s/it] 97%|| 6334/6500 [19:08:48<30:32, 11.04s/it]                                                       97%|| 6334/6500 [19:08:48<30:32, 11.04s/it] 97%|| 6335/6500 [19:08:59<30:08, 10.96s/it]                                                       97%|| 6335/6500 [19:08:59<30:08, 10.9{'loss': 0.3101, 'learning_rate': 1.5708709130585753e-07, 'epoch': 0.97}
{'loss': 0.3216, 'learning_rate': 1.5517822411862949e-07, 'epoch': 0.97}
{'loss': 0.3052, 'learning_rate': 1.5328100785542697e-07, 'epoch': 0.98}
{'loss': 0.2998, 'learning_rate': 1.5139544295971753e-07, 'epoch': 0.98}
{'loss': 0.3162, 'learning_rate': 1.49521529872243e-07, 'epoch': 0.98}
6s/it] 97%|| 6336/6500 [19:09:09<29:29, 10.79s/it]                                                       97%|| 6336/6500 [19:09:09<29:29, 10.79s/it] 97%|| 6337/6500 [19:09:20<28:58, 10.66s/it]                                                       97%|| 6337/6500 [19:09:20<28:58, 10.66s/it] 98%|| 6338/6500 [19:09:30<28:33, 10.58s/it]                                                       98%|| 6338/6500 [19:09:30<28:33, 10.58s/it] 98%|| 6339/6500 [19:09:40<28:13, 10.52s/it]                                                       98%|| 6339/6500 [19:09:40<28:13, 10.52s/it] 98%|| 6340/6500 [19:09:51<27:58, 10.49s/it]                                                       98%|| 6340/6500 [19:09:51<27:58, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8724209070205688, 'eval_runtime': 3.952, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.98}
                                                       98%|| 6340/6500 [19:09:55<27:58, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6340I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3069, 'learning_rate': 1.4765926903100856e-07, 'epoch': 0.98}
{'loss': 0.3105, 'learning_rate': 1.4580866087131605e-07, 'epoch': 0.98}
{'loss': 0.3152, 'learning_rate': 1.439697058257361e-07, 'epoch': 0.98}
{'loss': 0.3294, 'learning_rate': 1.4214240432410264e-07, 'epoch': 0.98}
{'loss': 0.3032, 'learning_rate': 1.4032675679354067e-07, 'epoch': 0.98}
 98%|| 6341/6500 [19:10:06<31:22, 11.84s/it]                                                       98%|| 6341/6500 [19:10:06<31:22, 11.84s/it] 98%|| 6342/6500 [19:10:16<30:01, 11.40s/it]                                                       98%|| 6342/6500 [19:10:16<30:01, 11.40s/it] 98%|| 6343/6500 [19:10:27<29:03, 11.11s/it]                                                       98%|| 6343/6500 [19:10:27<29:03, 11.11s/it] 98%|| 6344/6500 [19:10:37<28:19, 10.89s/it]                                                       98%|| 6344/6500 [19:10:37<28:19, 10.89s/it] 98%|| 6345/6500 [19:10:47<27:44, 10.74s/it]                                                       98%|| 6345/6500 [19:10:47<27:44, 10.7{'loss': 0.3102, 'learning_rate': 1.3852276365844407e-07, 'epoch': 0.98}
{'loss': 0.2905, 'learning_rate': 1.367304253404922e-07, 'epoch': 0.98}
{'loss': 0.3331, 'learning_rate': 1.3494974225863322e-07, 'epoch': 0.98}
{'loss': 0.3619, 'learning_rate': 1.3318071482908424e-07, 'epoch': 0.98}
{'loss': 0.3061, 'learning_rate': 1.314233434653478e-07, 'epoch': 0.98}
4s/it] 98%|| 6346/6500 [19:10:58<27:18, 10.64s/it]                                                       98%|| 6346/6500 [19:10:58<27:18, 10.64s/it] 98%|| 6347/6500 [19:11:08<26:57, 10.57s/it]                                                       98%|| 6347/6500 [19:11:08<26:57, 10.57s/it] 98%|| 6348/6500 [19:11:19<26:38, 10.51s/it]                                                       98%|| 6348/6500 [19:11:19<26:38, 10.51s/it] 98%|| 6349/6500 [19:11:29<26:21, 10.47s/it]                                                       98%|| 6349/6500 [19:11:29<26:21, 10.47s/it] 98%|| 6350/6500 [19:11:39<26:07, 10.45s/it]                                                       98%|| 6350/6500 [19:11:39<26:07, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8717079162597656, 'eval_runtime': 3.9519, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.98}
                                                       98%|| 6350/6500 [19:11:43<26:07, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6350the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6350

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6350/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6350/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6350/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3121, 'learning_rate': 1.2967762857820087e-07, 'epoch': 0.98}
{'loss': 0.3147, 'learning_rate': 1.2794357057568928e-07, 'epoch': 0.98}
{'loss': 0.8198, 'learning_rate': 1.262211698631388e-07, 'epoch': 0.98}
{'loss': 0.3225, 'learning_rate': 1.2451042684316071e-07, 'epoch': 0.98}
{'loss': 0.332, 'learning_rate': 1.228113419156185e-07, 'epoch': 0.98}
 98%|| 6351/6500 [19:11:55<29:31, 11.89s/it]                                                       98%|| 6351/6500 [19:11:55<29:31, 11.89s/it] 98%|| 6352/6500 [19:12:05<28:10, 11.42s/it]                                                       98%|| 6352/6500 [19:12:05<28:10, 11.42s/it] 98%|| 6353/6500 [19:12:15<27:11, 11.10s/it]                                                       98%|| 6353/6500 [19:12:15<27:11, 11.10s/it] 98%|| 6354/6500 [19:12:26<26:27, 10.87s/it]                                                       98%|| 6354/6500 [19:12:26<26:27, 10.87s/it] 98%|| 6355/6500 [19:12:36<25:54, 10.72s/it]                                                       98%|| 6355/6500 [19:12:36<25:54, 10.7{'loss': 0.2963, 'learning_rate': 1.211239154776611e-07, 'epoch': 0.98}
{'loss': 0.3235, 'learning_rate': 1.1944814792372305e-07, 'epoch': 0.98}
{'loss': 0.3145, 'learning_rate': 1.1778403964550766e-07, 'epoch': 0.98}
{'loss': 0.2865, 'learning_rate': 1.1613159103197602e-07, 'epoch': 0.98}
{'loss': 0.3112, 'learning_rate': 1.1449080246939137e-07, 'epoch': 0.98}
2s/it] 98%|| 6356/6500 [19:12:46<25:28, 10.61s/it]                                                       98%|| 6356/6500 [19:12:46<25:28, 10.61s/it] 98%|| 6357/6500 [19:12:57<25:08, 10.55s/it]                                                       98%|| 6357/6500 [19:12:57<25:08, 10.55s/it] 98%|| 6358/6500 [19:13:07<24:49, 10.49s/it]                                                       98%|| 6358/6500 [19:13:07<24:49, 10.49s/it] 98%|| 6359/6500 [19:13:18<24:33, 10.45s/it]                                                       98%|| 6359/6500 [19:13:18<24:33, 10.45s/it] 98%|| 6360/6500 [19:13:28<24:19, 10.43s/it]                                                       98%|| 6360/6500 [19:13:28<24:19, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8713768124580383, 'eval_runtime': 4.3059, 'eval_samples_per_second': 5.341, 'eval_steps_per_second': 1.393, 'epoch': 0.98}
                                                       98%|| 6360/6500 [19:13:32<24:19, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3119, 'learning_rate': 1.1286167434126915e-07, 'epoch': 0.98}
{'loss': 0.3101, 'learning_rate': 1.1124420702841587e-07, 'epoch': 0.98}
{'loss': 0.3122, 'learning_rate': 1.0963840090889576e-07, 'epoch': 0.98}
{'loss': 0.3065, 'learning_rate': 1.0804425635806415e-07, 'epoch': 0.98}
{'loss': 0.3119, 'learning_rate': 1.064617737485396e-07, 'epoch': 0.98}
 98%|| 6361/6500 [19:13:43<27:37, 11.92s/it]                                                       98%|| 6361/6500 [19:13:43<27:37, 11.92s/it] 98%|| 6362/6500 [19:13:54<26:22, 11.47s/it]                                                       98%|| 6362/6500 [19:13:54<26:22, 11.47s/it] 98%|| 6363/6500 [19:14:04<25:27, 11.15s/it]                                                       98%|| 6363/6500 [19:14:04<25:27, 11.15s/it] 98%|| 6364/6500 [19:14:15<24:45, 10.92s/it]                                                       98%|| 6364/6500 [19:14:15<24:45, 10.92s/it] 98%|| 6365/6500 [19:14:25<24:12, 10.76s/it]                                                       98%|| 6365/6500 [19:14:25<24:12, 10.7{'loss': 0.3129, 'learning_rate': 1.0489095345021516e-07, 'epoch': 0.98}
{'loss': 0.3311, 'learning_rate': 1.033317958302693e-07, 'epoch': 0.98}
{'loss': 0.3076, 'learning_rate': 1.0178430125313277e-07, 'epoch': 0.98}
{'loss': 0.3254, 'learning_rate': 1.0024847008053284e-07, 'epoch': 0.98}
{'loss': 0.3354, 'learning_rate': 9.87243026714546e-08, 'epoch': 0.98}
6s/it] 98%|| 6366/6500 [19:14:35<23:46, 10.65s/it]                                                       98%|| 6366/6500 [19:14:35<23:46, 10.65s/it] 98%|| 6367/6500 [19:14:46<23:25, 10.57s/it]                                                       98%|| 6367/6500 [19:14:46<23:25, 10.57s/it] 98%|| 6368/6500 [19:14:56<23:24, 10.64s/it]                                                       98%|| 6368/6500 [19:14:56<23:24, 10.64s/it] 98%|| 6369/6500 [19:15:07<23:04, 10.57s/it]                                                       98%|| 6369/6500 [19:15:07<23:04, 10.57s/it] 98%|| 6370/6500 [19:15:17<22:47, 10.52s/it]                                                       98%|| 6370/6500 [19:15:17<22:47, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.872043251991272, 'eval_runtime': 3.9717, 'eval_samples_per_second': 5.791, 'eval_steps_per_second': 1.511, 'epoch': 0.98}
                                                       98%|| 6370/6500 [19:15:21<22:47, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3056, 'learning_rate': 9.721179938216862e-08, 'epoch': 0.98}
{'loss': 0.3185, 'learning_rate': 9.571096056620876e-08, 'epoch': 0.98}
{'loss': 0.3175, 'learning_rate': 9.42217865743944e-08, 'epoch': 0.98}
{'loss': 0.3204, 'learning_rate': 9.27442777547971e-08, 'epoch': 0.98}
{'loss': 0.3033, 'learning_rate': 9.127843445279061e-08, 'epoch': 0.98}
 98%|| 6371/6500 [19:15:32<25:31, 11.87s/it]                                                       98%|| 6371/6500 [19:15:32<25:31, 11.87s/it] 98%|| 6372/6500 [19:15:43<24:22, 11.42s/it]                                                       98%|| 6372/6500 [19:15:43<24:22, 11.42s/it] 98%|| 6373/6500 [19:15:53<23:31, 11.11s/it]                                                       98%|| 6373/6500 [19:15:53<23:31, 11.11s/it] 98%|| 6374/6500 [19:16:03<22:51, 10.89s/it]                                                       98%|| 6374/6500 [19:16:03<22:51, 10.89s/it] 98%|| 6375/6500 [19:16:14<22:22, 10.74s/it]                                                       98%|| 6375/6500 [19:16:14<22:22, 10.7{'loss': 0.3024, 'learning_rate': 8.982425701099529e-08, 'epoch': 0.98}
{'loss': 0.2964, 'learning_rate': 8.838174576932256e-08, 'epoch': 0.98}
{'loss': 0.3927, 'learning_rate': 8.695090106495274e-08, 'epoch': 0.98}
{'loss': 0.3175, 'learning_rate': 8.553172323232939e-08, 'epoch': 0.98}
{'loss': 0.3028, 'learning_rate': 8.412421260318159e-08, 'epoch': 0.98}
4s/it] 98%|| 6376/6500 [19:16:24<21:57, 10.63s/it]                                                       98%|| 6376/6500 [19:16:24<21:57, 10.63s/it] 98%|| 6377/6500 [19:16:35<21:37, 10.55s/it]                                                       98%|| 6377/6500 [19:16:35<21:37, 10.55s/it] 98%|| 6378/6500 [19:16:45<21:21, 10.50s/it]                                                       98%|| 6378/6500 [19:16:45<21:21, 10.50s/it] 98%|| 6379/6500 [19:16:55<21:06, 10.47s/it]                                                       98%|| 6379/6500 [19:16:55<21:06, 10.47s/it] 98%|| 6380/6500 [19:17:06<20:54, 10.45s/it]                                                       98%|| 6380/6500 [19:17:06<20:54, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8722830414772034, 'eval_runtime': 3.9692, 'eval_samples_per_second': 5.795, 'eval_steps_per_second': 1.512, 'epoch': 0.98}
                                                       98%|| 6380/6500 [19:17:10<20:54, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6380/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3248, 'learning_rate': 8.27283695065073e-08, 'epoch': 0.98}
{'loss': 0.8245, 'learning_rate': 8.134419426856776e-08, 'epoch': 0.98}
{'loss': 0.3335, 'learning_rate': 7.997168721292082e-08, 'epoch': 0.98}
{'loss': 0.3175, 'learning_rate': 7.861084866037094e-08, 'epoch': 0.98}
{'loss': 0.3161, 'learning_rate': 7.726167892900815e-08, 'epoch': 0.98}
 98%|| 6381/6500 [19:17:21<23:25, 11.81s/it]                                                       98%|| 6381/6500 [19:17:21<23:25, 11.81s/it] 98%|| 6382/6500 [19:17:31<22:22, 11.38s/it]                                                       98%|| 6382/6500 [19:17:31<22:22, 11.38s/it] 98%|| 6383/6500 [19:17:42<21:37, 11.09s/it]                                                       98%|| 6383/6500 [19:17:42<21:37, 11.09s/it] 98%|| 6384/6500 [19:17:53<21:24, 11.07s/it]                                                       98%|| 6384/6500 [19:17:53<21:24, 11.07s/it] 98%|| 6385/6500 [19:18:03<20:49, 10.87s/it]                                                       98%|| 6385/6500 [19:18:03<20:49, 10.8{'loss': 0.293, 'learning_rate': 7.59241783341913e-08, 'epoch': 0.98}
{'loss': 0.3248, 'learning_rate': 7.459834718855363e-08, 'epoch': 0.98}
{'loss': 0.2984, 'learning_rate': 7.32841858020028e-08, 'epoch': 0.98}
{'loss': 0.2894, 'learning_rate': 7.198169448170977e-08, 'epoch': 0.98}
{'loss': 0.3059, 'learning_rate': 7.0690873532131e-08, 'epoch': 0.98}
7s/it] 98%|| 6386/6500 [19:18:13<20:22, 10.73s/it]                                                       98%|| 6386/6500 [19:18:13<20:22, 10.73s/it] 98%|| 6387/6500 [19:18:24<20:00, 10.62s/it]                                                       98%|| 6387/6500 [19:18:24<20:00, 10.62s/it] 98%|| 6388/6500 [19:18:34<19:41, 10.55s/it]                                                       98%|| 6388/6500 [19:18:34<19:41, 10.55s/it] 98%|| 6389/6500 [19:18:44<19:25, 10.50s/it]                                                       98%|| 6389/6500 [19:18:44<19:25, 10.50s/it] 98%|| 6390/6500 [19:18:55<19:11, 10.47s/it]                                                       98%|| 6390/6500 [19:18:55<19:11, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718501925468445, 'eval_runtime': 3.9745, 'eval_samples_per_second': 5.787, 'eval_steps_per_second': 1.51, 'epoch': 0.98}
                                                       98%|| 6390/6500 [19:18:59<19:11, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6390
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3037, 'learning_rate': 6.941172325499179e-08, 'epoch': 0.98}
{'loss': 0.3099, 'learning_rate': 6.814424394926966e-08, 'epoch': 0.98}
{'loss': 0.3009, 'learning_rate': 6.688843591124428e-08, 'epoch': 0.98}
{'loss': 0.3177, 'learning_rate': 6.56442994344475e-08, 'epoch': 0.98}
{'loss': 0.3011, 'learning_rate': 6.441183480969116e-08, 'epoch': 0.98}
 98%|| 6391/6500 [19:19:10<21:28, 11.82s/it]                                                       98%|| 6391/6500 [19:19:10<21:28, 11.82s/it] 98%|| 6392/6500 [19:19:20<20:30, 11.39s/it]                                                       98%|| 6392/6500 [19:19:20<20:30, 11.39s/it] 98%|| 6393/6500 [19:19:31<19:46, 11.09s/it]                                                       98%|| 6393/6500 [19:19:31<19:46, 11.09s/it] 98%|| 6394/6500 [19:19:41<19:13, 10.88s/it]                                                       98%|| 6394/6500 [19:19:41<19:13, 10.88s/it] 98%|| 6395/6500 [19:19:51<18:45, 10.72s/it]                                                       98%|| 6395/6500 [19:19:51<18:45, 10.7{'loss': 0.3255, 'learning_rate': 6.31910423250559e-08, 'epoch': 0.98}
{'loss': 0.3099, 'learning_rate': 6.198192226589682e-08, 'epoch': 0.98}
{'loss': 0.3087, 'learning_rate': 6.078447491482675e-08, 'epoch': 0.98}
{'loss': 0.3178, 'learning_rate': 5.959870055175509e-08, 'epoch': 0.98}
{'loss': 0.3091, 'learning_rate': 5.8424599453843486e-08, 'epoch': 0.98}
2s/it] 98%|| 6396/6500 [19:20:02<18:24, 10.62s/it]                                                       98%|| 6396/6500 [19:20:02<18:24, 10.62s/it] 98%|| 6397/6500 [19:20:12<18:06, 10.55s/it]                                                       98%|| 6397/6500 [19:20:12<18:06, 10.55s/it] 98%|| 6398/6500 [19:20:23<17:51, 10.50s/it]                                                       98%|| 6398/6500 [19:20:23<17:51, 10.50s/it] 98%|| 6399/6500 [19:20:33<17:36, 10.46s/it]                                                       98%|| 6399/6500 [19:20:33<17:36, 10.46s/it] 98%|| 6400/6500 [19:20:44<17:32, 10.53s/it]                                                       98%|| 6400/6500 [19:20:44<17:32, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718389272689819, 'eval_runtime': 3.964, 'eval_samples_per_second': 5.802, 'eval_steps_per_second': 1.514, 'epoch': 0.98}
                                                       98%|| 6400/6500 [19:20:48<17:32, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3181, 'learning_rate': 5.726217189553351e-08, 'epoch': 0.98}
{'loss': 0.2962, 'learning_rate': 5.61114181485356e-08, 'epoch': 0.98}
{'loss': 0.3375, 'learning_rate': 5.497233848182348e-08, 'epoch': 0.99}
{'loss': 0.3019, 'learning_rate': 5.384493316166195e-08, 'epoch': 0.99}
{'loss': 0.3041, 'learning_rate': 5.272920245156798e-08, 'epoch': 0.99}
 98%|| 6401/6500 [19:20:59<19:33, 11.85s/it]                                                       98%|| 6401/6500 [19:20:59<19:33, 11.85s/it] 98%|| 6402/6500 [19:21:09<18:38, 11.42s/it]                                                       98%|| 6402/6500 [19:21:09<18:38, 11.42s/it] 99%|| 6403/6500 [19:21:19<17:57, 11.11s/it]                                                       99%|| 6403/6500 [19:21:19<17:57, 11.11s/it] 99%|| 6404/6500 [19:21:30<17:26, 10.90s/it]                                                       99%|| 6404/6500 [19:21:30<17:26, 10.90s/it] 99%|| 6405/6500 [19:21:40<17:01, 10.75s/it]                                                       99%|| 6405/6500 [19:21:40<17:01, 10.7{'loss': 0.3009, 'learning_rate': 5.162514661233853e-08, 'epoch': 0.99}
{'loss': 0.3253, 'learning_rate': 5.053276590203937e-08, 'epoch': 0.99}
{'loss': 0.378, 'learning_rate': 4.9452060576010705e-08, 'epoch': 0.99}
{'loss': 0.306, 'learning_rate': 4.838303088685603e-08, 'epoch': 0.99}
{'loss': 0.2994, 'learning_rate': 4.732567708445878e-08, 'epoch': 0.99}
5s/it] 99%|| 6406/6500 [19:21:51<16:40, 10.64s/it]                                                       99%|| 6406/6500 [19:21:51<16:40, 10.64s/it] 99%|| 6407/6500 [19:22:01<16:23, 10.57s/it]                                                       99%|| 6407/6500 [19:22:01<16:23, 10.57s/it] 99%|| 6408/6500 [19:22:11<16:08, 10.52s/it]                                                       99%|| 6408/6500 [19:22:11<16:08, 10.52s/it] 99%|| 6409/6500 [19:22:22<15:54, 10.49s/it]                                                       99%|| 6409/6500 [19:22:22<15:54, 10.49s/it] 99%|| 6410/6500 [19:22:32<15:41, 10.47s/it]                                                       99%|| 6410/6500 [19:22:32<15:41, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8713259696960449, 'eval_runtime': 3.952, 'eval_samples_per_second': 5.82, 'eval_steps_per_second': 1.518, 'epoch': 0.99}
                                                       99%|| 6410/6500 [19:22:36<15:41, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6410/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3161, 'learning_rate': 4.627999941596572e-08, 'epoch': 0.99}
{'loss': 0.8134, 'learning_rate': 4.5245998125798e-08, 'epoch': 0.99}
{'loss': 0.3261, 'learning_rate': 4.422367345565115e-08, 'epoch': 0.99}
{'loss': 0.3107, 'learning_rate': 4.321302564448404e-08, 'epoch': 0.99}
{'loss': 0.3018, 'learning_rate': 4.2214054928529925e-08, 'epoch': 0.99}
 99%|| 6411/6500 [19:22:47<17:29, 11.79s/it]                                                       99%|| 6411/6500 [19:22:47<17:29, 11.79s/it] 99%|| 6412/6500 [19:22:57<16:40, 11.37s/it]                                                       99%|| 6412/6500 [19:22:57<16:40, 11.37s/it] 99%|| 6413/6500 [19:23:08<16:03, 11.08s/it]                                                       99%|| 6413/6500 [19:23:08<16:03, 11.08s/it] 99%|| 6414/6500 [19:23:18<15:35, 10.88s/it]                                                       99%|| 6414/6500 [19:23:18<15:35, 10.88s/it] 99%|| 6415/6500 [19:23:29<15:11, 10.73s/it]                                                       99%|| 6415/6500 [19:23:29<15:11, 10.7{'loss': 0.304, 'learning_rate': 4.12267615412909e-08, 'epoch': 0.99}
{'loss': 0.3308, 'learning_rate': 4.025114571354349e-08, 'epoch': 0.99}
{'loss': 0.3034, 'learning_rate': 3.928720767333305e-08, 'epoch': 0.99}
{'loss': 0.3007, 'learning_rate': 3.8334947645968235e-08, 'epoch': 0.99}
{'loss': 0.2987, 'learning_rate': 3.739436585403766e-08, 'epoch': 0.99}
3s/it] 99%|| 6416/6500 [19:23:39<15:01, 10.74s/it]                                                       99%|| 6416/6500 [19:23:39<15:01, 10.74s/it] 99%|| 6417/6500 [19:23:50<14:42, 10.63s/it]                                                       99%|| 6417/6500 [19:23:50<14:42, 10.63s/it] 99%|| 6418/6500 [19:24:00<14:24, 10.55s/it]                                                       99%|| 6418/6500 [19:24:00<14:24, 10.55s/it] 99%|| 6419/6500 [19:24:10<14:10, 10.50s/it]                                                       99%|| 6419/6500 [19:24:10<14:10, 10.50s/it] 99%|| 6420/6500 [19:24:21<13:57, 10.47s/it]                                                       99%|| 6420/6500 [19:24:21<13:57, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718559145927429, 'eval_runtime': 4.1717, 'eval_samples_per_second': 5.513, 'eval_steps_per_second': 1.438, 'epoch': 0.99}
                                                       99%|| 6420/6500 [19:24:25<13:57, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3045, 'learning_rate': 3.64654625173988e-08, 'epoch': 0.99}
{'loss': 0.3088, 'learning_rate': 3.554823785317241e-08, 'epoch': 0.99}
{'loss': 0.3033, 'learning_rate': 3.464269207575366e-08, 'epoch': 0.99}
{'loss': 0.3261, 'learning_rate': 3.374882539681767e-08, 'epoch': 0.99}
{'loss': 0.3093, 'learning_rate': 3.2866638025286226e-08, 'epoch': 0.99}
 99%|| 6421/6500 [19:24:36<15:36, 11.85s/it]                                                       99%|| 6421/6500 [19:24:36<15:36, 11.85s/it] 99%|| 6422/6500 [19:24:46<14:49, 11.40s/it]                                                       99%|| 6422/6500 [19:24:46<14:49, 11.40s/it] 99%|| 6423/6500 [19:24:57<14:14, 11.09s/it]                                                       99%|| 6423/6500 [19:24:57<14:14, 11.09s/it] 99%|| 6424/6500 [19:25:07<13:46, 10.87s/it]                                                       99%|| 6424/6500 [19:25:07<13:46, 10.87s/it] 99%|| 6425/6500 [19:25:17<13:23, 10.72s/it]                                                       99%|| 6425/6500 [19:25:17<13:23, 10.7{'loss': 0.3162, 'learning_rate': 3.199613016737768e-08, 'epoch': 0.99}
{'loss': 0.3094, 'learning_rate': 3.113730202656262e-08, 'epoch': 0.99}
{'loss': 0.3058, 'learning_rate': 3.029015380359157e-08, 'epoch': 0.99}
{'loss': 0.3239, 'learning_rate': 2.9454685696472805e-08, 'epoch': 0.99}
{'loss': 0.3132, 'learning_rate': 2.8630897900494557e-08, 'epoch': 0.99}
2s/it] 99%|| 6426/6500 [19:25:28<13:04, 10.61s/it]                                                       99%|| 6426/6500 [19:25:28<13:04, 10.61s/it] 99%|| 6427/6500 [19:25:38<12:48, 10.53s/it]                                                       99%|| 6427/6500 [19:25:38<12:48, 10.53s/it] 99%|| 6428/6500 [19:25:48<12:34, 10.48s/it]                                                       99%|| 6428/6500 [19:25:48<12:34, 10.48s/it] 99%|| 6429/6500 [19:25:59<12:21, 10.44s/it]                                                       99%|| 6429/6500 [19:25:59<12:21, 10.44s/it] 99%|| 6430/6500 [19:26:09<12:09, 10.42s/it]                                                       99%|| 6430/6500 [19:26:09<12:09, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8719478249549866, 'eval_runtime': 3.9308, 'eval_samples_per_second': 5.851, 'eval_steps_per_second': 1.526, 'epoch': 0.99}
                                                       99%|| 6430/6500 [19:26:13<12:09, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3183, 'learning_rate': 2.781879060821391e-08, 'epoch': 0.99}
{'loss': 0.3082, 'learning_rate': 2.7018364009451234e-08, 'epoch': 0.99}
{'loss': 0.3303, 'learning_rate': 2.622961829131243e-08, 'epoch': 0.99}
{'loss': 0.2964, 'learning_rate': 2.5452553638150023e-08, 'epoch': 0.99}
{'loss': 0.301, 'learning_rate': 2.4687170231602054e-08, 'epoch': 0.99}
 99%|| 6431/6500 [19:26:24<13:30, 11.75s/it]                                                       99%|| 6431/6500 [19:26:24<13:30, 11.75s/it] 99%|| 6432/6500 [19:26:35<12:55, 11.41s/it]                                                       99%|| 6432/6500 [19:26:35<12:55, 11.41s/it] 99%|| 6433/6500 [19:26:45<12:23, 11.10s/it]                                                       99%|| 6433/6500 [19:26:45<12:23, 11.10s/it] 99%|| 6434/6500 [19:26:55<11:58, 10.88s/it]                                                       99%|| 6434/6500 [19:26:55<11:58, 10.88s/it] 99%|| 6435/6500 [19:27:06<11:37, 10.73s/it]                                                       99%|| 6435/6500 [19:27:06<11:37, 10.7{'loss': 0.2846, 'learning_rate': 2.3933468250575408e-08, 'epoch': 0.99}
{'loss': 0.3376, 'learning_rate': 2.319144787124583e-08, 'epoch': 0.99}
{'loss': 0.3542, 'learning_rate': 2.246110926704681e-08, 'epoch': 0.99}
{'loss': 0.299, 'learning_rate': 2.1742452608691788e-08, 'epoch': 0.99}
{'loss': 0.3149, 'learning_rate': 2.1035478064168612e-08, 'epoch': 0.99}
3s/it] 99%|| 6436/6500 [19:27:16<11:20, 10.63s/it]                                                       99%|| 6436/6500 [19:27:16<11:20, 10.63s/it] 99%|| 6437/6500 [19:27:27<11:04, 10.55s/it]                                                       99%|| 6437/6500 [19:27:27<11:04, 10.55s/it] 99%|| 6438/6500 [19:27:37<10:51, 10.50s/it]                                                       99%|| 6438/6500 [19:27:37<10:51, 10.50s/it] 99%|| 6439/6500 [19:27:47<10:38, 10.47s/it]                                                       99%|| 6439/6500 [19:27:47<10:38, 10.47s/it] 99%|| 6440/6500 [19:27:58<10:26, 10.44s/it]                                                       99%|| 6440/6500 [19:27:58<10:26, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8710285425186157, 'eval_runtime': 3.9599, 'eval_samples_per_second': 5.808, 'eval_steps_per_second': 1.515, 'epoch': 0.99}
                                                       99%|| 6440/6500 [19:28:02<10:26, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6440
/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6440
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3102, 'learning_rate': 2.0340185798728427e-08, 'epoch': 0.99}
{'loss': 0.8313, 'learning_rate': 1.9656575974885684e-08, 'epoch': 0.99}
{'loss': 0.3104, 'learning_rate': 1.8984648752429225e-08, 'epoch': 0.99}
{'loss': 0.3228, 'learning_rate': 1.8324404288427853e-08, 'epoch': 0.99}
{'loss': 0.2988, 'learning_rate': 1.7675842737197025e-08, 'epoch': 0.99}
 99%|| 6441/6500 [19:28:13<11:34, 11.77s/it]                                                       99%|| 6441/6500 [19:28:13<11:34, 11.77s/it] 99%|| 6442/6500 [19:28:23<10:58, 11.35s/it]                                                       99%|| 6442/6500 [19:28:23<10:58, 11.35s/it] 99%|| 6443/6500 [19:28:33<10:30, 11.06s/it]                                                       99%|| 6443/6500 [19:28:33<10:30, 11.06s/it] 99%|| 6444/6500 [19:28:44<10:07, 10.85s/it]                                                       99%|| 6444/6500 [19:28:44<10:07, 10.85s/it] 99%|| 6445/6500 [19:28:54<09:48, 10.70s/it]                                                       99%|| 6445/6500 [19:28:54<09:48, 10.7{'loss': 0.3195, 'learning_rate': 1.7038964250343238e-08, 'epoch': 0.99}
{'loss': 0.3135, 'learning_rate': 1.6413768976730747e-08, 'epoch': 0.99}
{'loss': 0.2778, 'learning_rate': 1.5800257062498215e-08, 'epoch': 0.99}
{'loss': 0.3187, 'learning_rate': 1.519842865104204e-08, 'epoch': 0.99}
{'loss': 0.3016, 'learning_rate': 1.4608283883044138e-08, 'epoch': 0.99}
0s/it] 99%|| 6446/6500 [19:29:04<09:31, 10.59s/it]                                                       99%|| 6446/6500 [19:29:04<09:31, 10.59s/it] 99%|| 6447/6500 [19:29:15<09:17, 10.52s/it]                                                       99%|| 6447/6500 [19:29:15<09:17, 10.52s/it] 99%|| 6448/6500 [19:29:25<09:04, 10.47s/it]                                                       99%|| 6448/6500 [19:29:25<09:04, 10.47s/it] 99%|| 6449/6500 [19:29:36<08:58, 10.56s/it]                                                       99%|| 6449/6500 [19:29:36<08:58, 10.56s/it] 99%|| 6450/6500 [19:29:46<08:44, 10.50s/it]                                                       99%|| 6450/6500 [19:29:46<08:44, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8708928227424622, 'eval_runtime': 4.2052, 'eval_samples_per_second': 5.469, 'eval_steps_per_second': 1.427, 'epoch': 0.99}
                                                       99%|| 6450/6500 [19:29:50<08:44, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6450I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6450/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3221, 'learning_rate': 1.4029822896438616e-08, 'epoch': 0.99}
{'loss': 0.3216, 'learning_rate': 1.3463045826445086e-08, 'epoch': 0.99}
{'loss': 0.3241, 'learning_rate': 1.2907952805540913e-08, 'epoch': 0.99}
{'loss': 0.3106, 'learning_rate': 1.2364543963477859e-08, 'epoch': 0.99}
{'loss': 0.3085, 'learning_rate': 1.1832819427270991e-08, 'epoch': 0.99}
 99%|| 6451/6500 [19:30:01<09:43, 11.90s/it]                                                       99%|| 6451/6500 [19:30:01<09:43, 11.90s/it] 99%|| 6452/6500 [19:30:12<09:08, 11.43s/it]                                                       99%|| 6452/6500 [19:30:12<09:08, 11.43s/it] 99%|| 6453/6500 [19:30:22<08:41, 11.11s/it]                                                       99%|| 6453/6500 [19:30:22<08:41, 11.11s/it] 99%|| 6454/6500 [19:30:32<08:20, 10.88s/it]                                                       99%|| 6454/6500 [19:30:32<08:20, 10.88s/it] 99%|| 6455/6500 [19:30:43<08:02, 10.72s/it]                                                       99%|| 6455/6500 [19:30:43<08:02, 10.7{'loss': 0.3386, 'learning_rate': 1.1312779321209777e-08, 'epoch': 0.99}
{'loss': 0.3057, 'learning_rate': 1.0804423766852533e-08, 'epoch': 0.99}
{'loss': 0.3165, 'learning_rate': 1.0307752883020883e-08, 'epoch': 0.99}
{'loss': 0.3355, 'learning_rate': 9.822766785805293e-09, 'epoch': 0.99}
{'loss': 0.3091, 'learning_rate': 9.34946558857619e-09, 'epoch': 0.99}
2s/it] 99%|| 6456/6500 [19:30:53<07:46, 10.61s/it]                                                       99%|| 6456/6500 [19:30:53<07:46, 10.61s/it] 99%|| 6457/6500 [19:31:03<07:32, 10.53s/it]                                                       99%|| 6457/6500 [19:31:03<07:32, 10.53s/it] 99%|| 6458/6500 [19:31:14<07:20, 10.48s/it]                                                       99%|| 6458/6500 [19:31:14<07:20, 10.48s/it] 99%|| 6459/6500 [19:31:24<07:08, 10.44s/it]                                                       99%|| 6459/6500 [19:31:24<07:08, 10.44s/it] 99%|| 6460/6500 [19:31:35<06:56, 10.41s/it]                                                       99%|| 6460/6500 [19:31:35<06:56, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.872139036655426, 'eval_runtime': 3.9695, 'eval_samples_per_second': 5.794, 'eval_steps_per_second': 1.512, 'epoch': 0.99}
                                                       99%|| 6460/6500 [19:31:38<06:56, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6460/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3153, 'learning_rate': 8.887849401961745e-09, 'epoch': 0.99}
{'loss': 0.3274, 'learning_rate': 8.437918333864536e-09, 'epoch': 0.99}
{'loss': 0.306, 'learning_rate': 7.999672489444888e-09, 'epoch': 0.99}
{'loss': 0.306, 'learning_rate': 7.573111971148627e-09, 'epoch': 0.99}
{'loss': 0.3005, 'learning_rate': 7.1582368786737855e-09, 'epoch': 0.99}
 99%|| 6461/6500 [19:31:49<07:38, 11.76s/it]                                                       99%|| 6461/6500 [19:31:49<07:38, 11.76s/it] 99%|| 6462/6500 [19:32:00<07:10, 11.33s/it]                                                       99%|| 6462/6500 [19:32:00<07:10, 11.33s/it] 99%|| 6463/6500 [19:32:10<06:48, 11.03s/it]                                                       99%|| 6463/6500 [19:32:10<06:48, 11.03s/it] 99%|| 6464/6500 [19:32:20<06:29, 10.82s/it]                                                       99%|| 6464/6500 [19:32:20<06:29, 10.82s/it] 99%|| 6465/6500 [19:32:31<06:16, 10.76s/it]                                                       99%|| 6465/6500 [19:32:31<06:16, 10.7{'loss': 0.3158, 'learning_rate': 6.7550473090038925e-09, 'epoch': 0.99}
{'loss': 0.3766, 'learning_rate': 6.36354335638023e-09, 'epoch': 0.99}
{'loss': 0.2997, 'learning_rate': 5.983725112307381e-09, 'epoch': 1.0}
{'loss': 0.2987, 'learning_rate': 5.61559266556988e-09, 'epoch': 1.0}
{'loss': 0.3306, 'learning_rate': 5.259146102221113e-09, 'epoch': 1.0}
6s/it] 99%|| 6466/6500 [19:32:41<06:01, 10.64s/it]                                                       99%|| 6466/6500 [19:32:41<06:01, 10.64s/it] 99%|| 6467/6500 [19:32:52<05:48, 10.55s/it]                                                       99%|| 6467/6500 [19:32:52<05:48, 10.55s/it]100%|| 6468/6500 [19:33:02<05:35, 10.49s/it]                                                      100%|| 6468/6500 [19:33:02<05:35, 10.49s/it]100%|| 6469/6500 [19:33:12<05:24, 10.45s/it]                                                      100%|| 6469/6500 [19:33:12<05:24, 10.45s/it]100%|| 6470/6500 [19:33:23<05:12, 10.43s/it]                                                      100%|| 6470/6500 [19:33:23<05:12, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711671233177185, 'eval_runtime': 3.9367, 'eval_samples_per_second': 5.842, 'eval_steps_per_second': 1.524, 'epoch': 1.0}
                                                      100%|| 6470/6500 [19:33:27<05:12, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8205, 'learning_rate': 4.914385505572217e-09, 'epoch': 1.0}
{'loss': 0.3183, 'learning_rate': 4.581310956208729e-09, 'epoch': 1.0}
{'loss': 0.3165, 'learning_rate': 4.2599225319850385e-09, 'epoch': 1.0}
{'loss': 0.3132, 'learning_rate': 3.950220308029939e-09, 'epoch': 1.0}
{'loss': 0.2904, 'learning_rate': 3.65220435672442e-09, 'epoch': 1.0}
100%|| 6471/6500 [19:33:38<05:42, 11.80s/it]                                                      100%|| 6471/6500 [19:33:38<05:42, 11.80s/it]100%|| 6472/6500 [19:33:48<05:18, 11.36s/it]                                                      100%|| 6472/6500 [19:33:48<05:18, 11.36s/it]100%|| 6473/6500 [19:33:59<04:58, 11.06s/it]                                                      100%|| 6473/6500 [19:33:59<04:58, 11.06s/it]100%|| 6474/6500 [19:34:09<04:41, 10.84s/it]                                                      100%|| 6474/6500 [19:34:09<04:41, 10.84s/it]100%|| 6475/6500 [19:34:19<04:27, 10.69s/it]                                                      100%|| 6475/6500 [19:34:19<04:27, 10.6{'loss': 0.3179, 'learning_rate': 3.365874747740527e-09, 'epoch': 1.0}
{'loss': 0.2997, 'learning_rate': 3.0912315479914024e-09, 'epoch': 1.0}
{'loss': 0.2936, 'learning_rate': 2.828274821686794e-09, 'epoch': 1.0}
{'loss': 0.3081, 'learning_rate': 2.5770046302830974e-09, 'epoch': 1.0}
{'loss': 0.3003, 'learning_rate': 2.3374210325166625e-09, 'epoch': 1.0}
9s/it]100%|| 6476/6500 [19:34:30<04:14, 10.59s/it]                                                      100%|| 6476/6500 [19:34:30<04:14, 10.59s/it]100%|| 6477/6500 [19:34:40<04:01, 10.52s/it]                                                      100%|| 6477/6500 [19:34:40<04:01, 10.52s/it]100%|| 6478/6500 [19:34:50<03:50, 10.47s/it]                                                      100%|| 6478/6500 [19:34:50<03:50, 10.47s/it]100%|| 6479/6500 [19:35:01<03:39, 10.43s/it]                                                      100%|| 6479/6500 [19:35:01<03:39, 10.43s/it]100%|| 6480/6500 [19:35:11<03:28, 10.41s/it]                                                      100%|| 6480/6500 [19:35:11<03:28, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8721948862075806, 'eval_runtime': 3.9612, 'eval_samples_per_second': 5.806, 'eval_steps_per_second': 1.515, 'epoch': 1.0}
                                                      100%|| 6480/6500 [19:35:15<03:28, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6480/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3166, 'learning_rate': 2.109524084381587e-09, 'epoch': 1.0}
{'loss': 0.3011, 'learning_rate': 1.8933138391574732e-09, 'epoch': 1.0}
{'loss': 0.3344, 'learning_rate': 1.6887903473816746e-09, 'epoch': 1.0}
{'loss': 0.3082, 'learning_rate': 1.4959536568492915e-09, 'epoch': 1.0}
{'loss': 0.3392, 'learning_rate': 1.3148038126464813e-09, 'epoch': 1.0}
100%|| 6481/6500 [19:35:26<03:46, 11.90s/it]                                                      100%|| 6481/6500 [19:35:26<03:46, 11.90s/it]100%|| 6482/6500 [19:35:37<03:27, 11.51s/it]                                                      100%|| 6482/6500 [19:35:37<03:27, 11.51s/it]100%|| 6483/6500 [19:35:47<03:09, 11.17s/it]                                                      100%|| 6483/6500 [19:35:47<03:09, 11.17s/it]100%|| 6484/6500 [19:35:58<02:54, 10.93s/it]                                                      100%|| 6484/6500 [19:35:58<02:54, 10.93s/it]100%|| 6485/6500 [19:36:08<02:41, 10.76s/it]                                                      100%|| 6485/6500 [19:36:08<02:41, 10.7{'loss': 0.3116, 'learning_rate': 1.1453408571060476e-09, 'epoch': 1.0}
{'loss': 0.3282, 'learning_rate': 9.875648298518504e-10, 'epoch': 1.0}
{'loss': 0.3251, 'learning_rate': 8.41475767748845e-10, 'epoch': 1.0}
{'loss': 0.315, 'learning_rate': 7.070737049530429e-10, 'epoch': 1.0}
{'loss': 0.3104, 'learning_rate': 5.843586728782046e-10, 'epoch': 1.0}
6s/it]100%|| 6486/6500 [19:36:18<02:28, 10.64s/it]                                                      100%|| 6486/6500 [19:36:18<02:28, 10.64s/it]100%|| 6487/6500 [19:36:29<02:17, 10.55s/it]                                                      100%|| 6487/6500 [19:36:29<02:17, 10.55s/it]100%|| 6488/6500 [19:36:39<02:05, 10.50s/it]                                                      100%|| 6488/6500 [19:36:39<02:05, 10.50s/it]100%|| 6489/6500 [19:36:49<01:55, 10.46s/it]                                                      100%|| 6489/6500 [19:36:49<01:55, 10.46s/it]100%|| 6490/6500 [19:37:00<01:44, 10.43s/it]                                                      100%|| 6490/6500 [19:37:00<01:44, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718813061714172, 'eval_runtime': 3.9538, 'eval_samples_per_second': 5.817, 'eval_steps_per_second': 1.518, 'epoch': 1.0}
                                                      100%|| 6490/6500 [19:37:04<01:44, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6490
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6490/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6490/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3251, 'learning_rate': 4.733307002069421e-10, 'epoch': 1.0}
{'loss': 0.3272, 'learning_rate': 3.739898128962693e-10, 'epoch': 1.0}
{'loss': 0.3009, 'learning_rate': 2.8633603416095e-10, 'epoch': 1.0}
{'loss': 0.311, 'learning_rate': 2.1036938449014998e-10, 'epoch': 1.0}
{'loss': 0.2971, 'learning_rate': 1.4608988164188653e-10, 'epoch': 1.0}
100%|| 6491/6500 [19:37:15<01:46, 11.80s/it]                                                      100%|| 6491/6500 [19:37:15<01:46, 11.80s/it]100%|| 6492/6500 [19:37:25<01:31, 11.38s/it]                                                      100%|| 6492/6500 [19:37:25<01:31, 11.38s/it]100%|| 6493/6500 [19:37:36<01:17, 11.07s/it]                                                      100%|| 6493/6500 [19:37:36<01:17, 11.07s/it]100%|| 6494/6500 [19:37:46<01:05, 10.86s/it]                                                      100%|| 6494/6500 [19:37:46<01:05, 10.86s/it]100%|| 6495/6500 [19:37:56<00:53, 10.72s/it]                                                      100%|| 6495/6500 [19:37:56<00:53, 10.7{'loss': 0.3312, 'learning_rate': 9.349754064302829e-11, 'epoch': 1.0}
{'loss': 0.3758, 'learning_rate': 5.259237378374415e-11, 'epoch': 1.0}
{'loss': 0.3035, 'learning_rate': 2.3374390623054355e-11, 'epoch': 1.0}
{'loss': 0.3067, 'learning_rate': 5.8435979999327265e-12, 'epoch': 1.0}
{'loss': 0.3192, 'learning_rate': 0.0, 'epoch': 1.0}
2s/it]100%|| 6496/6500 [19:38:07<00:42, 10.61s/it]                                                      100%|| 6496/6500 [19:38:07<00:42, 10.61s/it]100%|| 6497/6500 [19:38:17<00:31, 10.61s/it]                                                      100%|| 6497/6500 [19:38:17<00:31, 10.61s/it]100%|| 6498/6500 [19:38:28<00:21, 10.54s/it]                                                      100%|| 6498/6500 [19:38:28<00:21, 10.54s/it]100%|| 6499/6500 [19:38:38<00:10, 10.49s/it]                                                      100%|| 6499/6500 [19:38:38<00:10, 10.49s/it]100%|| 6500/6500 [19:38:48<00:00, 10.46s/it]                                                      100%|| 6500/6500 [19:38:48<00:00, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8714950680732727, 'eval_runtime': 3.9655, 'eval_samples_per_second': 5.8, 'eval_steps_per_second': 1.513, 'epoch': 1.0}
                                                      100%|| 6500/6500 [19:38:52<00:00, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_7_A40/tmp-checkpoint-6500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6500/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_7_A40/checkpoint-6500/pytorch_model.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 70775.5096, 'train_samples_per_second': 5.878, 'train_steps_per_second': 0.092, 'train_loss': 0.48301221432135655, 'epoch': 1.0}
Saving last checkpoint of the model
Saving last checkpoint of the model
Saving last checkpoint of the model
                                                      100%|| 6500/6500 [19:38:53<00:00, 10.46s/it]100%|| 6500/6500 [19:38:53<00:00, 10.88s/it]
Saving last checkpoint of the model
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 1.601 MB uploadedwandb: | 1.612 MB of 1.612 MB uploadedwandb: 
wandb: Run history:
wandb:                      eval/loss 
wandb:                   eval/runtime 
wandb:        eval/samples_per_second 
wandb:          eval/steps_per_second 
wandb:                    train/epoch 
wandb:              train/global_step 
wandb:            train/learning_rate 
wandb:                     train/loss 
wandb:               train/total_flos 
wandb:               train/train_loss 
wandb:            train/train_runtime 
wandb: train/train_samples_per_second 
wandb:   train/train_steps_per_second 
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.8715
wandb:                   eval/runtime 3.9655
wandb:        eval/samples_per_second 5.8
wandb:          eval/steps_per_second 1.513
wandb:                    train/epoch 1.0
wandb:              train/global_step 6500
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.3192
wandb:               train/total_flos 3.701684874248192e+18
wandb:               train/train_loss 0.48301
wandb:            train/train_runtime 70775.5096
wandb: train/train_samples_per_second 5.878
wandb:   train/train_steps_per_second 0.092
wandb: 
wandb:  View run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_7_A40 at: https://wandb.ai/complex_dnn/huggingface/runs/jzqnomsa
wandb:  View job at https://wandb.ai/complex_dnn/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTYzMDY1Mg==/version_details/v9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /projects/bbvz/bzd2/wandb/run-20231213_174945-jzqnomsa/logs
/var/spool/slurmd/job2748384/slurm_script: line 192: --save_freq=50: command not found
